{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one\n",
        "# or more contributor license agreements. See the NOTICE file\n",
        "# distributed with this work for additional information\n",
        "# regarding copyright ownership. The ASF licenses this file\n",
        "# to you under the Apache License, Version 2.0 (the\n",
        "# \"License\"); you may not use this file except in compliance\n",
        "# with the License. You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing,\n",
        "# software distributed under the License is distributed on an\n",
        "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "# KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations\n",
        "# under the License."
      ],
      "metadata": {
        "cellView": "form",
        "id": "fFjof1NgAJwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Beam RunInference with TensorFlow\n",
        "\n",
        "<button>\n",
        "  <a href=\"https://beam.apache.org/documentation/sdks/python-machine-learning/\">\n",
        "    <img src=\"https://beam.apache.org/images/favicon.ico\" alt=\"Open the docs\" height=\"16\"/>\n",
        "    Beam RunInference\n",
        "  </a>\n",
        "</button>\n",
        "\n",
        "The Apache Beam RunInference transform is used for making predictions for\n",
        "a variety of machine learning models. From version 1.10.0 of tfx-bsl you can\n",
        "create a TensorFlow ModelHandler for use with Apache Beam.\n",
        "\n",
        "In this notebook, we walk through the use of the RunInference transform for [TensorFlow](https://www.tensorflow.org/).\n",
        "Beam [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) accepts ModelHandler generated from [tfx-bsl](https://github.com/tensorflow/tfx-bsl) via CreateModelHandler.\n",
        "\n",
        "\n",
        "\n",
        "In this notebook we walk through:\n",
        "- Importing [tfx-bsl](https://github.com/tensorflow/tfx-bsl)\n",
        "- Building a simple TensorFlow model\n",
        "- Setting up example data into TensorFlow protos.\n",
        "- Running those examples and getting a prediction inside an Apache Beam pipeline."
      ],
      "metadata": {
        "id": "HrCtxslBGK8Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBakpNZnAhqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac46d52-4993-4172-9602-75eaece2eb56"
      },
      "source": [
        "!pip install tfx_bsl==1.10.0 --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 21.6 MB 2.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 18.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 17 kB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 270 kB 71.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 69.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 54.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 508 kB 73.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 267 kB 58.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 255 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 148 kB 72.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 68.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 435 kB 59.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 183 kB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 270 kB 70.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 236 kB 68.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 62.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 119 kB 64.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 119 kB 74.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 66.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 78.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 77.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 74.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 62.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 267 kB 74.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 265 kB 75.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 148 kB 71.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 76.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 48.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 65.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 72.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 53.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 65.1 MB/s \n",
            "\u001b[?25h  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use RunInference you will need beam 2.40 or higher. Creation of a ModelHandler is supported in tfx-bsl 1.10 or higher."
      ],
      "metadata": {
        "id": "gVCtGOKTHMm4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hHP9wu98Ld4",
        "outputId": "a96fe28c-403d-4e20-df2b-9b8cb8458be4"
      },
      "source": [
        "!pip freeze | grep beam\n",
        "!pip freeze | grep tfx-bsl\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apache-beam==2.41.0\n",
            "tfx-bsl==1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authenticate With Cloud\n",
        "This notebook relies on saving your model into Google Cloud. First authenticate this notebook to use your Google Cloud account."
      ],
      "metadata": {
        "id": "X80jy3FqHjK4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz9sccyGBqz3"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Your Dependencies and Setup Your Bucket\n",
        "Replace project and bucket with your variables with your project.\n",
        "\n",
        "**Important**: If you get an error, restart your runtime."
      ],
      "metadata": {
        "id": "40qtP6zJuMXm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEle839_Akqx"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_serving.apis import prediction_log_pb2\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.ml.inference.base import RunInference\n",
        "import tfx_bsl\n",
        "from tfx_bsl.public.beam.run_inference import CreateModelHandler\n",
        "from tfx_bsl.public import tfxio\n",
        "from tfx_bsl.public.proto import model_spec_pb2\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "import numpy\n",
        "\n",
        "from typing import Dict, Text, Any, Tuple, List\n",
        "\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "project = \"<Replace With Your Project>\"\n",
        "bucket = \"<Replace With Your Bucket>\" \n",
        "\n",
        "save_model_dir_multiply = f'gs://{bucket}/tfx-inference/model/multiply_five/v1/'\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Test a Simple Model\n",
        "\n",
        "This creates a model that predicts the 5 times table."
      ],
      "metadata": {
        "id": "YzvZWEv-1oiK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH7iq3zeBBJ-",
        "outputId": "7182df9f-82b0-4dfa-e4b1-b9e040065c53"
      },
      "source": [
        "# Create training data which represents the 5 times multiplication table for 0 to 99. \n",
        "# x is the data and y the labels. \n",
        "x = numpy.arange(0, 100)   # Examples\n",
        "y = x * 5                  # Labels\n",
        "\n",
        "# Build a simple linear regression model.\n",
        "# Note the model has a shape of (1) for its input layer, it will expect a single int64 value.\n",
        "input_layer = keras.layers.Input(shape=(1), dtype=tf.float32, name='x')\n",
        "output_layer= keras.layers.Dense(1)(input_layer)\n",
        "\n",
        "model = keras.Model(input_layer, output_layer)\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " x (InputLayer)              [(None, 1)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Model\n"
      ],
      "metadata": {
        "id": "O_a0-4Gb19cy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XkIYXhJBFmS",
        "outputId": "a9e809bd-92c9-495c-ef29-23e4474cfc35"
      },
      "source": [
        "model.fit(x, y, epochs=500, verbose=0)\n",
        "test_examples =[20, 40, 60, 90]\n",
        "value_to_predict = numpy.array(test_examples, dtype=numpy.float32)\n",
        "predictions = model.predict(value_to_predict)\n",
        "\n",
        "print('Test Examples ' + str(test_examples))\n",
        "print('Predictions ' + str(predictions))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "Test Examples [20, 40, 60, 90]\n",
            "Predictions [[ 99.99883]\n",
            " [199.99783]\n",
            " [299.99686]\n",
            " [449.99533]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Populate the Data into a TensorFlow Proto\n",
        "\n",
        "Tensorflow data uses protos. If you are loading from a file there are helpers for this. Since we are using generated data, this code populates a proto."
      ],
      "metadata": {
        "id": "dEmleqiH3t71"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvKc9kQilPjx"
      },
      "source": [
        "# This is an example of a proto that converts the samples and labels into\n",
        "# tensors usable by tensorflow.\n",
        "class ExampleProcessor:\n",
        "    def create_example_with_label(self, feature: numpy.float32,\n",
        "                             label: numpy.float32)-> tf.train.Example:\n",
        "        return tf.train.Example(\n",
        "            features=tf.train.Features(\n",
        "                  feature={'x': self.create_feature(feature),\n",
        "                           'y' : self.create_feature(label)\n",
        "                  }))\n",
        "\n",
        "    def create_example(self, feature: numpy.float32):\n",
        "        return tf.train.Example(\n",
        "            features=tf.train.Features(\n",
        "                  feature={'x' : self.create_feature(feature)})\n",
        "            )\n",
        "\n",
        "    def create_feature(self, element: numpy.float32):\n",
        "        return tf.train.Feature(float_list=tf.train.FloatList(value=[element]))\n",
        "\n",
        "# Create a labeled example file for 5 times table.\n",
        "\n",
        "example_five_times_table = 'example_five_times_table.tfrecord'\n",
        "\n",
        "with tf.io.TFRecordWriter(example_five_times_table) as writer:\n",
        "  for i in zip(x, y):\n",
        "    example = ExampleProcessor().create_example_with_label(\n",
        "        feature=i[0], label=i[1])\n",
        "    writer.write(example.SerializeToString())\n",
        "\n",
        "# Create a file containing the values to predict.\n",
        "\n",
        "predict_values_five_times_table = 'predict_values_five_times_table.tfrecord'\n",
        "\n",
        "with tf.io.TFRecordWriter(predict_values_five_times_table) as writer:\n",
        "  for i in value_to_predict:\n",
        "    example = ExampleProcessor().create_example(feature=i)\n",
        "    writer.write(example.SerializeToString())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit The Model\n",
        "\n",
        "This example builds a model. Since RunInference requires pretrained models, this segment builds a usable model."
      ],
      "metadata": {
        "id": "G-sAu3cf31f3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnbrxXPKeAOQ",
        "outputId": "b1fc6894-75af-4e6f-be77-f07baf33e7f7"
      },
      "source": [
        "RAW_DATA_TRAIN_SPEC = {\n",
        "'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "'y': tf.io.FixedLenFeature([], tf.float32)\n",
        "}\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(example_five_times_table)\n",
        "dataset = dataset.map(lambda e : tf.io.parse_example(e, RAW_DATA_TRAIN_SPEC))\n",
        "dataset = dataset.map(lambda t : (t['x'], t['y']))\n",
        "dataset = dataset.batch(100)\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "model.fit(dataset, epochs=500, steps_per_epoch=1, verbose=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f26c26a3ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Model"
      ],
      "metadata": {
        "id": "r4dpR6dQ4JwX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYvrIYO3qiJx"
      },
      "source": [
        "RAW_DATA_PREDICT_SPEC = {\n",
        "'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "}\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string , name='examples')])\n",
        "def serve_tf_examples_fn(serialized_tf_examples):\n",
        "  \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "  features = tf.io.parse_example(serialized_tf_examples, RAW_DATA_PREDICT_SPEC)\n",
        "  return model(features, training=False)\n",
        "\n",
        "signature = {'serving_default': serve_tf_examples_fn}\n",
        "\n",
        "tf.keras.models.save_model(model, save_model_dir_multiply, signatures=signature)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Pipeline\n",
        "\n",
        "FormatOutput demonstrates how to extract values from the output protos.\n",
        "\n",
        "CreateModelHandler demonstrates the model handler that needs to be passed into beams RunInference API."
      ],
      "metadata": {
        "id": "P2UMmbNW4YQV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzjmXM_KvqHY",
        "outputId": "8448763c-f529-49ef-9a1a-02874a610a87"
      },
      "source": [
        "from tfx_bsl.public.beam.run_inference import CreateModelHandler\n",
        "\n",
        "class FormatOutput(beam.DoFn):\n",
        "    def process(self, element: prediction_log_pb2.PredictionLog):\n",
        "        predict_log = element.predict_log\n",
        "        input_value = tf.train.Example.FromString(predict_log.request.inputs['examples'].string_val[0])\n",
        "        input_float_value = input_value.features.feature['x'].float_list.value\n",
        "        output_value = predict_log.response.outputs\n",
        "        output_float_value = output_value['output_0'].float_val\n",
        "        yield (f\"example is {input_float_value} prediction is {output_float_value}\")\n",
        "\n",
        "tfexample_beam_record = tfx_bsl.public.tfxio.TFExampleRecord(file_pattern=predict_values_five_times_table)\n",
        "saved_model_spec = model_spec_pb2.SavedModelSpec(model_path=save_model_dir_multiply)\n",
        "inferece_spec_type = model_spec_pb2.InferenceSpecType(saved_model_spec=saved_model_spec)\n",
        "model_handler = CreateModelHandler(inferece_spec_type)\n",
        "with beam.Pipeline() as p:\n",
        "    _ = (p | tfexample_beam_record.RawRecordBeamSource() \n",
        "           | RunInference(model_handler)\n",
        "           | beam.ParDo(FormatOutput())\n",
        "           | beam.Map(print)\n",
        "        )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example is [20.0] prediction is [100.00492858886719]\n",
            "example is [40.0] prediction is [200.00970458984375]\n",
            "example is [60.0] prediction is [300.01446533203125]\n",
            "example is [90.0] prediction is [450.0216064453125]\n"
          ]
        }
      ]
    }
  ]
}