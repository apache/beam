{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one\n",
        "# or more contributor license agreements. See the NOTICE file\n",
        "# distributed with this work for additional information\n",
        "# regarding copyright ownership. The ASF licenses this file\n",
        "# to you under the Apache License, Version 2.0 (the\n",
        "# \"License\"); you may not use this file except in compliance\n",
        "# with the License. You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing,\n",
        "# software distributed under the License is distributed on an\n",
        "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "# KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations\n",
        "# under the License"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fFjof1NgAJwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8xNRyZMW1yK"
      },
      "source": [
        "# Apache Beam RunInference with TensorFlow\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_tensorflow.ipynb\"><img src=\"https://raw.githubusercontent.com/google/or-tools/main/tools/colab_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_tensorflow.ipynb\"><img src=\"https://raw.githubusercontent.com/google/or-tools/main/tools/github_32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates the use of the [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) transform for [TensorFlow](https://www.tensorflow.org/).\n",
        "\n",
        "Beam has built in support for 2 TensorFlow Model Handlers: [TFModelHandlerNumpy](https://github.com/apache/beam/blob/ca0787642a6b3804a742326147281c99ae8d08d2/sdks/python/apache_beam/ml/inference/tensorflow_inference.py#L91) and [TFModelHandlerTensor](https://github.com/apache/beam/blob/ca0787642a6b3804a742326147281c99ae8d08d2/sdks/python/apache_beam/ml/inference/tensorflow_inference.py#L184).\n",
        "TFModelHandlerNumpy can be used to run inference on models expecting a `numpy` array as an input while TFModelHandlerTensor can be used to run inference on models expecting a `tf.Tensor` as an input.\n",
        "\n",
        "If your model needs input of type `tf.Example` see the [Apache Beam RunInference with `tfx-bsl` notebook](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_tensorflow_with_tfx.ipynb).\n",
        "\n",
        "The Apache Beam RunInference transform is used to make predictions for\n",
        "a variety of machine learning models. For more information about the RunInference API, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\n",
        "\n",
        "This notebook demonstrates the following steps:\n",
        "- Build a simple TensorFlow model.\n",
        "- Set up example data.\n",
        "- Run those examples with the built-in model handlers and get a prediction inside an Apache Beam pipeline."
      ],
      "metadata": {
        "id": "HrCtxslBGK8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use RunInference with built-in Tensorflow model handler, install Apache Beam version 2.46 or later."
      ],
      "metadata": {
        "id": "gVCtGOKTHMm4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBakpNZnAhqk"
      },
      "source": [
        "!pip install protobuf --quiet\n",
        "!pip install apache_beam==2.46.0 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate with Google Cloud\n",
        "This notebook relies on saving your model to Google Cloud. To use your Google Cloud account, authenticate this notebook."
      ],
      "metadata": {
        "id": "X80jy3FqHjK4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz9sccyGBqz3"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dependencies and set up your bucket\n",
        "Replace `PROJECT_ID` and `BUCKET_NAME` with the ID of your project and the name of your bucket.\n",
        "\n",
        "**Important**: If an error occurs, restart your runtime."
      ],
      "metadata": {
        "id": "40qtP6zJuMXm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEle839_Akqx"
      },
      "source": [
        "import argparse\n",
        "from typing import Dict, Text, Any, Tuple, List\n",
        "import numpy\n",
        "\n",
        "from google.protobuf import text_format\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import apache_beam as beam\n",
        "from apache_beam.ml.inference.base import RunInference\n",
        "from apache_beam.ml.inference.base import KeyedModelHandler\n",
        "from apache_beam.ml.inference.tensorflow_inference import TFModelHandlerNumpy\n",
        "from apache_beam.ml.inference.tensorflow_inference import TFModelHandlerTensor\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "project = \"PROJECT_ID\"\n",
        "bucket = \"BUCKET_NAME\"\n",
        "\n",
        "save_model_dir_multiply = f'gs://{bucket}/tfx-inference/model/multiply_five/v1/'\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and test a simple model\n",
        "\n",
        "This step creates and tests a model that predicts the 5 times table."
      ],
      "metadata": {
        "id": "YzvZWEv-1oiK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIwD_qEpX7Gu"
      },
      "source": [
        "### Create the model\n",
        "Create training data and build a linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH7iq3zeBBJ-",
        "outputId": "e15cab6b-1271-4b0b-bac3-ba76f8991077"
      },
      "source": [
        "# Create training data that represents the 5 times multiplication table for the numbers 0 to 99.\n",
        "# x is the data and y is the labels.\n",
        "x = numpy.arange(0, 100)   # Examples\n",
        "y = x * 5                  # Labels\n",
        "\n",
        "# Build a simple linear regression model.\n",
        "# Note that the model has a shape of (1) for its input layer and expects a single int64 value.\n",
        "input_layer = keras.layers.Input(shape=(1), dtype=tf.float32, name='x')\n",
        "output_layer= keras.layers.Dense(1)(input_layer)\n",
        "\n",
        "model = keras.Model(input_layer, output_layer)\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss='mean_absolute_error')\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " x (InputLayer)              [(None, 1)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model\n",
        "\n",
        "This step tests the model that you created."
      ],
      "metadata": {
        "id": "O_a0-4Gb19cy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XkIYXhJBFmS",
        "outputId": "724cad1b-58f6-4e97-f7ec-9526297a108e"
      },
      "source": [
        "model.fit(x, y, epochs=500, verbose=0)\n",
        "test_examples =[20, 40, 60, 90]\n",
        "value_to_predict = numpy.array(test_examples, dtype=numpy.float32)\n",
        "predictions = model.predict(value_to_predict)\n",
        "\n",
        "print('Test Examples ' + str(test_examples))\n",
        "print('Predictions ' + str(predictions))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n",
            "Test Examples [20, 40, 60, 90]\n",
            "Predictions [[ 51.815357]\n",
            " [101.63492 ]\n",
            " [151.45448 ]\n",
            " [226.18384 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the model\n",
        "\n",
        "This step shows how to save your model."
      ],
      "metadata": {
        "id": "Y3BC2aY8cIMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(save_model_dir_multiply)"
      ],
      "metadata": {
        "id": "2JbE7WkGcAkK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Pipeline\n",
        "Use the following code to run the pipeline."
      ],
      "metadata": {
        "id": "0a1zerXycQ0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FormatOutput(beam.DoFn):\n",
        "  def process(self, element, *args, **kwargs):\n",
        "     yield \"example is {example} prediction is {prediction}\".format(example=element.example, prediction=element.inference)\n",
        "\n",
        "\n",
        "examples = numpy.array([20, 40, 60, 90], dtype=numpy.float32)\n",
        "model_handler = TFModelHandlerNumpy(save_model_dir_multiply)\n",
        "with beam.Pipeline() as p:\n",
        "    _ = (p | beam.Create(examples)\n",
        "           | RunInference(model_handler)\n",
        "           | beam.ParDo(FormatOutput())\n",
        "           | beam.Map(print)\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "St07XoibcQSb",
        "outputId": "028fb751-1f45-4c7b-da3f-5a3e31312798"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example is 20.0 prediction is [51.815357]\n",
            "example is 40.0 prediction is [101.63492]\n",
            "example is 60.0 prediction is [151.45448]\n",
            "example is 90.0 prediction is [226.18384]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KeyedModelHandler with TensorFlow using TFModelHandlerNumpy\n",
        "\n",
        "By default, the `ModelHandler` does not expect a key.\n",
        "\n",
        "* If you know that keys are associated with your examples, wrap the model handler with `beam.KeyedModelHandler`.\n",
        "* If you don't know whether keys are associated with your examples, use `beam.MaybeKeyedModelHandler`."
      ],
      "metadata": {
        "id": "tRLArcjOcYuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FormatOutputKeyed(FormatOutput):\n",
        "  # To simplify, inherit from FormatOutput.\n",
        "  def process(self, tuple_in: Tuple):\n",
        "    key, element = tuple_in\n",
        "    output = super().process(element)\n",
        "    yield \"{} : {}\".format(key, [op for op in output])\n",
        "\n",
        "examples = numpy.array([(1,20), (2,40), (3,60), (4,90)], dtype=numpy.float32)\n",
        "keyed_model_handler = KeyedModelHandler(TFModelHandlerNumpy(save_model_dir_multiply))\n",
        "with beam.Pipeline() as p:\n",
        "    _ = (p | 'CreateExamples' >> beam.Create(examples)\n",
        "           | RunInference(keyed_model_handler)\n",
        "           | beam.ParDo(FormatOutputKeyed())\n",
        "           | beam.Map(print)\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6l9RwL2cAW3",
        "outputId": "03459fea-7d0a-4501-93cb-18bbad915d13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 : ['example is 20.0 prediction is [51.815357]']\n",
            "2.0 : ['example is 40.0 prediction is [101.63492]']\n",
            "3.0 : ['example is 60.0 prediction is [151.45448]']\n",
            "4.0 : ['example is 90.0 prediction is [226.18384]']\n"
          ]
        }
      ]
    }
  ]
}