<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – Blogs</title><link>/blog/</link><description>Recent content in Blogs on Apache Beam</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 17 Oct 2024 15:00:00 -0500</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Apache Beam 2.60.0</title><link>/blog/beam-2.60.0/</link><pubDate>Thu, 17 Oct 2024 15:00:00 -0500</pubDate><guid>/blog/beam-2.60.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.60.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2600-2024-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.60.0, check out the &lt;a href="https://github.com/apache/beam/milestone/24">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for using vLLM in the RunInference transform (Python) (&lt;a href="https://github.com/apache/beam/issues/32528">#32528&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Added support for streaming writes (&lt;a href="https://github.com/apache/beam/pull/32451">#32451&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Added auto-sharding for streaming writes (&lt;a href="https://github.com/apache/beam/pull/32612">#32612&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Added support for writing to dynamic destinations (&lt;a href="https://github.com/apache/beam/pull/32565">#32565&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Dataflow worker can install packages from Google Artifact Registry Python repositories (Python) (&lt;a href="https://github.com/apache/beam/issues/32123">#32123&lt;/a>).&lt;/li>
&lt;li>Added support for Zstd codec in SerializableAvroCodecFactory (Java) (&lt;a href="https://github.com/apache/beam/issues/32349">#32349&lt;/a>)&lt;/li>
&lt;li>Added support for using vLLM in the RunInference transform (Python) (&lt;a href="https://github.com/apache/beam/issues/32528">#32528&lt;/a>)&lt;/li>
&lt;li>Prism release binaries and container bootloaders are now being built with the latest Go 1.23 patch. (&lt;a href="https://github.com/apache/beam/pull/32575">#32575&lt;/a>)&lt;/li>
&lt;li>Prism
&lt;ul>
&lt;li>Prism now supports Bundle Finalization. (&lt;a href="https://github.com/apache/beam/pull/32425">#32425&lt;/a>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Significantly improved performance of Kafka IO reads that enable &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html#commitOffsetsInFinalize--">commitOffsetsInFinalize&lt;/a> by removing the data reshuffle from SDF implementation. (&lt;a href="https://github.com/apache/beam/pull/31682">#31682&lt;/a>).&lt;/li>
&lt;li>Added support for dynamic writing in MqttIO (Java) (&lt;a href="https://github.com/apache/beam/issues/19376">#19376&lt;/a>)&lt;/li>
&lt;li>Optimized Spark Runner parDo transform evaluator (Java) (&lt;a href="https://github.com/apache/beam/issues/32537">#32537&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] More efficient manifest file writes/commits (&lt;a href="https://github.com/apache/beam/issues/32666">#32666&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>In Python, assert_that now throws if it is not in a pipeline context instead of silently succeeding (&lt;a href="https://github.com/apache/beam/pull/30771">#30771&lt;/a>)&lt;/li>
&lt;li>In Python and YAML, ReadFromJson now override the dtype from None to
an explicit False. Most notably, string values like &lt;code>&amp;quot;123&amp;quot;&lt;/code> are preserved
as strings rather than silently coerced (and possibly truncated) to numeric
values. To retain the old behavior, pass &lt;code>dtype=True&lt;/code> (or any other value
accepted by &lt;code>pandas.read_json&lt;/code>).&lt;/li>
&lt;li>Users of KafkaIO Read transform that enable &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html#commitOffsetsInFinalize--">commitOffsetsInFinalize&lt;/a> might encounter pipeline graph compatibility issues when updating the pipeline. To mitigate, set the &lt;code>updateCompatibilityVersion&lt;/code> option to the SDK version used for the original pipeline, example &lt;code>--updateCompatabilityVersion=2.58.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Python 3.8 is reaching EOL and support is being removed in Beam 2.61.0. The 2.60.0 release will warn users
when running on 3.8. (&lt;a href="https://github.com/apache/beam/issues/31192">#31192&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Java) Fixed custom delimiter issues in TextIO (&lt;a href="https://github.com/apache/beam/issues/32249">#32249&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/32251">#32251&lt;/a>).&lt;/li>
&lt;li>(Java, Python, Go) Fixed PeriodicSequence backlog bytes reporting, which was preventing Dataflow Runner autoscaling from functioning properly (&lt;a href="https://github.com/apache/beam/issues/32506">#32506&lt;/a>).&lt;/li>
&lt;li>(Java) Fix improper decoding of rows with schemas containing nullable fields when encoded with a schema with equal encoding positions but modified field order. (&lt;a href="https://github.com/apache/beam/issues/32388">#32388&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.60.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud, Aiden Grossman, Arun Pandian, Bartosz Zablocki, Chamikara Jayalath, Claire McGinty, DKPHUONG, Damon Douglass, Danny McCormick, Dip Patel, Ferran Fernández Garrido, Hai Joey Tran, Hyeonho Kim, Igor Bernstein, Israel Herraiz, Jack McCluskey, Jaehyeon Kim, Jeff Kinard, Jeffrey Kinard, Joey Tran, Kenneth Knowles, Kirill Berezin, Michel Davit, Minbo Bae, Naireen Hussain, Niel Markwick, Nito Buendia, Reeba Qureshi, Reuven Lax, Robert Bradshaw, Robert Burke, Rohit Sinha, Ryan Fu, Sam Whittle, Shunping Huang, Svetak Sundhar, Udaya Chathuranga, Vitaly Terentyev, Vlado Djerek, Yi Hu, Claude van der Merwe, XQ Hu, Martin Trieu, Valentyn Tymofieiev, twosom&lt;/p></description></item><item><title>Blog: Apache Beam Summit 2024: Unlocking the power of ML for data processing</title><link>/blog/beam-summit-2024-overview/</link><pubDate>Wed, 16 Oct 2024 00:00:01 -0800</pubDate><guid>/blog/beam-summit-2024-overview/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>At the recently concluded &lt;a href="https://beamsummit.org/">Beam Summit 2024&lt;/a>, a two-day event held from September 4 to 5, numerous captivating presentations showcased the potential of Beam to address a wide range of challenges, with an emphasis on machine learning (ML). These challenges included feature engineering, data enrichment, and model inference for large-scale distributed data. In all, the summit included &lt;a href="https://beamsummit.org/sessions/2024/">47 talks&lt;/a>, with 16 focused specifically on ML use cases or features and many more touching on these topics.&lt;/p>
&lt;p>The talks displayed the breadth and diversity of the Beam community. Among the speakers and attendees, &lt;a href="https://docs.google.com/presentation/d/1IJ1sExHzrzIFF5QXKWlcAuPdp7lKOepRQKl9BnfHxJw/edit#slide=id.g3058d3e2f5f_0_10">23 countries&lt;/a> were represented. Attendees included Beam users, committers in the Beam project, Beam Google Summer of Code contributors, and data processing/machine learning experts.&lt;/p>
&lt;h2 id="user-friendly-turnkey-transforms-for-ml">User-friendly turnkey transforms for ML&lt;/h2>
&lt;p>With the features recently added to Beam, Beam now offers a set of rich turn-key transforms for ML users that handle a wide range of ML-Ops tasks. These transforms include:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/documentation/ml/overview/#prediction-and-inference">RunInference&lt;/a>: deploy ML models on CPUs and GPUs&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/ml/overview/#data-processing">Enrichment&lt;/a>: enrich data for ML feature enhancements&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/ml/overview/#data-processing">MLTransform&lt;/a>: transform data into ML features&lt;/li>
&lt;/ul>
&lt;p>The Summit talks covering both how to use these features and how people are already using them. Highlights included:&lt;/p>
&lt;ul>
&lt;li>A talk about &lt;a href="https://beamsummit.org/slides/2024/ScalingAutonomousDrivingwithApacheBeam.pdf">scaling autonomous driving at Cruise&lt;/a>&lt;/li>
&lt;li>Multiple talks about deploying LLMs for batch and streaming inference&lt;/li>
&lt;li>Three different talks about streaming processing for &lt;a href="https://cloud.google.com/use-cases/retrieval-augmented-generation">RAG&lt;/a> (including &lt;a href="https://www.youtube.com/watch?v=X_VzKQOcpC4">a talk&lt;/a> from one of Beam&amp;rsquo;s Google Summer of Code contributors!)&lt;/li>
&lt;/ul>
&lt;h2 id="beam-yaml-simplifying-ml-data-processing">Beam YAML: Simplifying ML data processing&lt;/h2>
&lt;p>Beam pipeline creation can be challenging and often requires learning concepts, managing dependencies, debugging, and maintaining code for ML tasks. To simplify the entry point, &lt;a href="https://beam.apache.org/blog/beam-yaml-release/">Beam YAML&lt;/a> introduces a declarative approach that uses YAML configuration files to create data processing pipelines. No coding is required.&lt;/p>
&lt;p>Beam Summit was the first opportunity that the Beam community had to show off some of the use cases of Beam YAML. It featured several talks about how Beam YAML is already a core part of many users&amp;rsquo; workflows at companies like &lt;a href="https://beamsummit.org/slides/2024/ALowCodeStructuredApproachtoDeployingApacheBeamMLWorkloadsonKubernetesusingBeamStack.pdf">MavenCode&lt;/a> and &lt;a href="https://youtu.be/avSXvbScbW0">ChartBoost&lt;/a>. With Beam YAML, these companies are able to build configuration-based data processing systems, significantly lowering the bar for entry at their companies.&lt;/p>
&lt;h2 id="prism-provide-a-unified-ml-pipeline-development-framework-for-local-and-remote-runner-environments">Prism: Provide a unified ML pipeline development framework for local and remote runner environments&lt;/h2>
&lt;p>Beam provides a variety of support for portable runners, but developing a local pipeline has traditionally been painful. Local runners are often incomplete and incompatible with remote runners, such as DataflowRunner and FlinkRunner.&lt;/p>
&lt;p>At Beam Summit, Beam contributors introduced &lt;a href="https://youtu.be/R4iNwLBa3VQ">the Prism local runner&lt;/a> to the community. Prism greatly improves the local developer experience and reduces the gap between local and remote execution. In particular, when handling complicated ML tasks, Prism guarantees consistent runner behavior across these runners, a task that had previously lacked consistent support.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Beam Summit 2024 showcased the tremendous potential of Apache Beam for addressing a wide range of data processing and machine learning challenges. We look forward to seeing even more innovative use cases and contributions in the future.&lt;/p>
&lt;p>To stay updated on the latest Beam developments and events, visit &lt;a href="https://beam.apache.org/get-started/">the Apache Beam website&lt;/a> and follow us on &lt;a href="https://www.linkedin.com/company/apache-beam/">social media&lt;/a>. We encourage you to join &lt;a href="https://beam.apache.org/community/contact-us/">the Beam community&lt;/a> and &lt;a href="https://beam.apache.org/contribute/">contribute to the project&lt;/a>. Together, let&amp;rsquo;s unlock the full potential of Beam and shape the future of data processing and machine learning.&lt;/p></description></item><item><title>Blog: Efficient Streaming Data Processing with Beam YAML and Protobuf</title><link>/blog/beam-yaml-proto/</link><pubDate>Fri, 20 Sep 2024 11:53:38 +0200</pubDate><guid>/blog/beam-yaml-proto/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="efficient-streaming-data-processing-with-beam-yaml-and-protobuf">Efficient Streaming Data Processing with Beam YAML and Protobuf&lt;/h1>
&lt;p>As streaming data processing grows, so do its maintenance, complexity, and costs.
This post explains how to efficiently scale pipelines by using &lt;a href="https://protobuf.dev/">Protobuf&lt;/a>,
which ensures that pipelines are reusable and quick to deploy. The goal is to keep this process simple
for engineers to implement using &lt;a href="https://beam.apache.org/documentation/sdks/yaml/">Beam YAML&lt;/a>.&lt;/p>
&lt;h2 id="simplify-pipelines-with-beam-yaml">Simplify pipelines with Beam YAML&lt;/h2>
&lt;p>Creating a pipeline in Beam can be somewhat difficult, especially for new Apache Beam users.
Setting up the project, managing dependencies, and so on can be challenging.
Beam YAML eliminates most of the boilerplate code,
which allows you to focus on the most important part of the work: data transformation.&lt;/p>
&lt;p>Some of the key benefits of Beam YAML include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Readability:&lt;/strong> By using a declarative language (&lt;a href="https://yaml.org/">YAML&lt;/a>), the pipeline configuration is more human readable.&lt;/li>
&lt;li>&lt;strong>Reusability:&lt;/strong> Reusing the same components across different pipelines is simplified.&lt;/li>
&lt;li>&lt;strong>Maintainability:&lt;/strong> Pipeline maintenance and updates are easier.&lt;/li>
&lt;/ul>
&lt;p>The following template shows an example of reading events from a &lt;a href="https://kafka.apache.org/intro">Kafka&lt;/a> topic and
writing them into &lt;a href="https://cloud.google.com/bigquery?hl=en">BigQuery&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromKafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">topic&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;TOPIC_NAME&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">format&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">RAW/AVRO/JSON/PROTO&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;BOOTSTRAP_SERVERS&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">schema&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;SCHEMA&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToBigQuery&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">table&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;PROJECT_ID.DATASET.MOVIE_EVENTS_TABLE&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">useAtLeastOnceSemantics&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">streaming&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">dataflow_service_options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="l">streaming_mode_at_least_once]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-complete-workflow">The complete workflow&lt;/h2>
&lt;p>This section demonstrates the complete workflow for this pipeline.&lt;/p>
&lt;h3 id="create-a-simple-proto-event">Create a simple proto event&lt;/h3>
&lt;p>The following code creates a simple movie event.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-protobuf" data-lang="protobuf">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// events/v1/movie_event.proto
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="n">syntax&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;proto3&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="kn">package&lt;/span> &lt;span class="nn">event&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v1&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;bq_field.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;bq_table.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;buf/validate/validate.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;google/protobuf/wrappers.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="kd">message&lt;/span> &lt;span class="nc">MovieEvent&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="k">option&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery_opts&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">table_name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;movie_table&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.StringValue&lt;/span> &lt;span class="n">event_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Unique Event ID&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.StringValue&lt;/span> &lt;span class="n">user_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Unique User ID&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.StringValue&lt;/span> &lt;span class="n">movie_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Unique Movie ID&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.Int32Value&lt;/span> &lt;span class="n">rating&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">buf.validate.field&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="kt">int32&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="c1">// validates the average rating is at least 0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">gte&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="c1">// validates the average rating is at most 100
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">lte&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">100&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">},&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Movie rating&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="kt">string&lt;/span> &lt;span class="n">event_dt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">type_override&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;DATETIME&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;UTC Datetime representing when we received this event. Format: YYYY-MM-DDTHH:MM:SS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">buf.validate.field&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="kt">string&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">pattern&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s">&amp;#34;^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}$&amp;#34;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">},&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">ignore_empty&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Because these events are written to BigQuery,
the &lt;a href="https://buf.build/googlecloudplatform/bq-schema-api/file/main:bq_field.proto">&lt;code>bq_field&lt;/code>&lt;/a> proto
and the &lt;a href="https://buf.build/googlecloudplatform/bq-schema-api/file/main:bq_table.proto">&lt;code>bq_table&lt;/code>&lt;/a> proto are imported.
These proto files help generate the BigQuery JSON schema.
This example also demonstrates a shift-left approach, which moves testing, quality,
and performance as early as possible in the development process. For example, to ensure that only valid events are generated from the source, the &lt;code>buf.validate&lt;/code> elements are included.&lt;/p>
&lt;p>After you create the &lt;code>movie_event.proto&lt;/code> proto in the &lt;code>events/v1&lt;/code> folder, you can generate
the necessary &lt;a href="https://buf.build/docs/reference/descriptors">file descriptor&lt;/a>.
A file descriptor is a compiled representation of the schema that allows various tools and systems
to understand and work with protobuf data dynamically. To simplify the process, this example uses Buf,
which requires the following configuration files.&lt;/p>
&lt;p>&lt;b>Buf configuration:&lt;/b>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># buf.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">deps&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">buf.build/googlecloudplatform/bq-schema-api&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">buf.build/bufbuild/protovalidate&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">breaking&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">use&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">FILE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">lint&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">use&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">DEFAULT&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># buf.gen.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">managed&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">enabled&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">plugins&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># Python Plugins&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/protocolbuffers/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/grpc/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># Java Plugins&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/protocolbuffers/java:v25.2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/maven/src/main/java&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/grpc/java&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/maven/src/main/java&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># BQ Schemas&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/googlecloudplatform/bq-schema:v1.1.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">protoc-gen/bq_schema&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Run the following two commands to generate the necessary Java, Python, BigQuery schema, and Descriptor file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">// Generate the buf.lock file
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">buf deps update
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">// It generates the descriptor in descriptor.binp.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">buf build . -o descriptor.binp --exclude-imports
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">// It generates the Java, Python and BigQuery schema as described in buf.gen.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">buf generate --include-imports
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="make-the-beam-yaml-read-proto">Make the Beam YAML read proto&lt;/h3>
&lt;p>Make the following modifications to the to the YAML file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># movie_events_pipeline.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromKafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">topic&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;movie_proto&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">format&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">PROTO&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;BOOTSTRAP_SERVERS&amp;gt;&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">file_descriptor_path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;gs://my_proto_bucket/movie/v1.0.0/descriptor.binp&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">message_name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;event.v1.MovieEvent&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToBigQuery&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">table&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;PROJECT_ID&amp;gt;.raw.movie_table&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">useAtLeastOnceSemantics&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">streaming&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">dataflow_service_options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="l">streaming_mode_at_least_once]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This step changes the format to &lt;code>PROTO&lt;/code> and adds the &lt;code>file_descriptor_path&lt;/code> and the &lt;code>message_name&lt;/code>.&lt;/p>
&lt;h3 id="deploy-the-pipeline-with-terraform">Deploy the pipeline with Terraform&lt;/h3>
&lt;p>You can use &lt;a href="https://www.terraform.io/">Terraform&lt;/a> to deploy the Beam YAML pipeline
with &lt;a href="https://cloud.google.com/products/dataflow?hl=en">Dataflow&lt;/a> as the runner.
The following Terraform code example demonstrates how to achieve this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-hcl" data-lang="hcl">&lt;span class="line">&lt;span class="cl">&lt;span class="err">//&lt;/span> &lt;span class="k">Enable&lt;/span> &lt;span class="k">Dataflow&lt;/span> &lt;span class="k">API&lt;/span>&lt;span class="p">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">resource&lt;/span> &lt;span class="s2">&amp;#34;google_project_service&amp;#34; &amp;#34;enable_dataflow_api&amp;#34;&lt;/span> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_project_id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;dataflow.googleapis.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">//&lt;/span> &lt;span class="k">DF&lt;/span> &lt;span class="k">Beam&lt;/span> &lt;span class="k">YAML&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">resource&lt;/span> &lt;span class="s2">&amp;#34;google_dataflow_flex_template_job&amp;#34; &amp;#34;data_movie_job&amp;#34;&lt;/span> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> provider&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">google&lt;/span>&lt;span class="err">-&lt;/span>&lt;span class="k">beta&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_project_id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;movie-proto-events&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> container_spec_gcs_path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;gs://dataflow-templates-${var.gcp_region}/latest/flex/Yaml_Template&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> region&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_region&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> on_delete&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;drain&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> machine_type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;n2d-standard-4&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> enable_streaming_engine&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kt">true&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> subnetwork&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">subnetwork&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> skip_wait_on_job_termination&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kt">true&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> parameters&lt;/span> &lt;span class="o">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> yaml_pipeline_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;gs://${var.bucket_name}/yamls/${var.package_version}/movie_events_pipeline.yml&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> max_num_workers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">40&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> worker_zone&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_zone&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> depends_on&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="k">google_project_service&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">enable_dataflow_api&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Assuming the BigQuery table exists, which you can do by using Terraform and Proto,
this code creates a Dataflow job by using the Beam YAML code that reads Proto events from
Kafka and writes them into BigQuery.&lt;/p>
&lt;h2 id="improvements-and-conclusions">Improvements and conclusions&lt;/h2>
&lt;p>The following community contributions could improve the Beam YAML code in this example:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Support schema registries:&lt;/strong> Integrate with schema registries such as Buf Registry or Apicurio for
better schema management. The current workflow generates the descriptors by using Buf and store them in Google Cloud Storage.
The descriptors could be stored in a schema registry instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Enhanced Monitoring:&lt;/strong> Implement advanced monitoring and alerting mechanisms to quickly identify and address
issues in the data pipeline.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Leveraging Beam YAML and Protobuf lets us streamline the creation and maintenance of
data processing pipelines, significantly reducing complexity. This approach ensures that engineers can more
efficiently implement and scale robust, reusable pipelines without needs to manually write Beam code.&lt;/p>
&lt;h2 id="contribute">Contribute&lt;/h2>
&lt;p>Developers who want to help build out and add functionalities are welcome to start contributing to the effort in the
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml">Beam YAML module&lt;/a>.&lt;/p>
&lt;p>There is also a list of open &lt;a href="https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Ayaml">bugs&lt;/a> found
on the GitHub repo - now marked with the &lt;code>yaml&lt;/code> tag.&lt;/p>
&lt;p>Although Beam YAML is marked stable as of Beam 2.52, it is still under heavy development, with new features being
added with each release. Those who want to be part of the design decisions and give insights to how the framework is
being used are highly encouraged to join the &lt;a href="https://beam.apache.org/community/contact-us/">dev mailing list&lt;/a>, where those discussions are occurring.&lt;/p></description></item><item><title>Blog: Unit Testing in Beam: An opinionated guide</title><link>/blog/unit-testing-in-beam/</link><pubDate>Fri, 13 Sep 2024 00:00:01 -0800</pubDate><guid>/blog/unit-testing-in-beam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Testing remains one of the most fundamental components of software engineering. In this blog post, we shed light on some of the constructs that Apache Beam provides for testing.
We cover an opinionated set of best practices to write unit tests for your data pipeline. This post doesn&amp;rsquo;t include integration tests, and you need to author those separately.
All snippets in this post are included in &lt;a href="https://github.com/apache/beam/blob/master/examples/notebooks/blog/unittests_in_beam.ipynb">this notebook&lt;/a>. Additionally, to see tests that exhibit best practices, look at the &lt;a href="https://beam.apache.org/blog/beam-starter-projects/">Beam starter projects&lt;/a>, which contain tests that exhibit best practices.&lt;/p>
&lt;h2 id="best-practices">Best practices&lt;/h2>
&lt;p>When testing Beam pipelines, we recommend the following best practices:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Don&amp;rsquo;t write unit tests for the already supported connectors in the Beam Library, such as &lt;code>ReadFromBigQuery&lt;/code> and &lt;code>WriteToText&lt;/code>. These connectors are already tested in Beam’s test suite to ensure correct functionality. They add unnecessary cost and dependencies to a unit test.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ensure that your function is well tested when using it with &lt;code>Map&lt;/code>, &lt;code>FlatMap&lt;/code>, or &lt;code>Filter&lt;/code>. You can assume your function will work as intended when using &lt;code>Map(your_function)&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For more complex transforms such as &lt;code>ParDo&lt;/code>’s, side inputs, timestamp inspection, etc., treat the entire transform as a unit, and test it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If needed, use mocking to mock any API calls that might be present in your DoFn. The purpose of mocking is to test your functionality extensively, even if this testing requires a specific response from an API call.&lt;/p>
&lt;ol>
&lt;li>Be sure to modularize your API calls in separate functions, rather than making the API call directly in the &lt;code>DoFn&lt;/code>. This step provides a cleaner experience when mocking the external API calls.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="example-1">Example 1&lt;/h2>
&lt;p>Use the following pipeline as an example. You don&amp;rsquo;t have to write a separate unit test to test this function in the context of this pipeline, assuming the function &lt;code>median_house_value_per_bedroom&lt;/code> is unit tested elsewhere in the code. You can trust that the &lt;code>Map&lt;/code> primitive works as expected (this illustrates point #2 noted previously).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The following code computes the median house value per bedroom.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/california_housing_test.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">skip_header_lines&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">median_house_value_per_bedroom&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example2&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example-2">Example 2&lt;/h2>
&lt;p>Use the following function as the example. The functions &lt;code>median_house_value_per_bedroom&lt;/code> and &lt;code>multiply_by_factor&lt;/code> are tested elsewhere, but the pipeline as a whole, which consists of composite transforms, is not.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/california_housing_test.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">skip_header_lines&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">median_house_value_per_bedroom&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply_by_factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombinePerKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example3&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The best practice for the previous code is to create a transform with all functions between &lt;code>ReadFromText&lt;/code> and &lt;code>WriteToText&lt;/code>. This step separates the transformation logic from the I/Os, allowing you to unit test the transformation logic. The following example is a refactoring of the previous code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">transform_data_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">median_house_value_per_bedroom&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply_by_factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombinePerKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Define a new class that inherits from beam.PTransform.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MapAndCombineTransform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pcoll&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">transform_data_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/california_housing_test.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">skip_header_lines&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">MapAndCombineTransform&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example3&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This code shows the corresponding unit test for the previous example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">unittest&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">beam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.testing.test_pipeline&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">TestPipeline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.testing.util&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">assert_that&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">equal_to&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">TestBeam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unittest&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TestCase&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># This test corresponds to example 3, and is written to confirm the pipeline works as intended.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">test_transform_data_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expected&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">10570.185786231425&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">13.375337533753376&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">13.315649867374006&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;121.05,99.99,23.30,39.5,55.55,41.01,10,34,74.30,91.91&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;122.05,100.99,24.30,40.5,56.55,42.01,11,35,75.30,92.91&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;-120.05,39.37,29.00,4085.00,681.00,1557.00,626.00,6.8085,364700.00&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_elements&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MapAndCombineTransform&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">assert_that&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">equal_to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">expected&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example-3">Example 3&lt;/h2>
&lt;p>Suppose we write a pipeline that reads data from a JSON file, passes it through a custom function that makes external API calls for parsing, and then writes it to a custom destination (for example, if we need to do some custom data formatting to have data prepared for a downstream application).&lt;/p>
&lt;p>The pipeline has the following structure:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The following packages are used to run the example pipelines.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">beam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.io&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">WriteToText&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">PipelineOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MyDoFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">returned_record&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">MyApiCall&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;http://my-api-call.com&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">returned_record&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">!=&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">raise&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Length of record does not match expected length&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">yield&lt;/span> &lt;span class="n">returned_record&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p3&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/anscombe.json&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MyDoFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example1&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This test checks whether the API response is a record of the wrong length and throws the expected error if the test fails.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="err">!&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">mock&lt;/span> &lt;span class="c1"># Install the &amp;#39;mock&amp;#39; module.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Import the mock package for mocking functionality.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">unittest.mock&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Mock&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">patch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># from MyApiCall import get_data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">mock&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># MyApiCall is a function that calls get_data to fetch some data by using an API call.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@patch&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;MyApiCall.get_data&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">test_error_message_wrong_length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mock_get_data&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">response&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;field1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;field2&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mock_get_data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Mock&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mock_get_data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_value&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_value&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">response&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1">#input length 9&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">assertRaisesRegex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="ne">ValueError&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Length of record does not match expected length&amp;#39;&amp;#34;&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p3&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_elements&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MyDoFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="other-testing-best-practices">Other testing best practices:&lt;/h2>
&lt;ol>
&lt;li>Test all error messages that you raise.&lt;/li>
&lt;li>Cover any edge cases that might exist in your data.&lt;/li>
&lt;li>Example 1 could have written the &lt;code>beam.Map&lt;/code> step with lambda functions instead of with &lt;code>beam.Map(median_house_value_per_bedroom)&lt;/code>:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>beam.Map(lambda x: x.strip().split(&amp;#39;,&amp;#39;)) | beam.Map(lambda x: float(x[8])/float(x[4])
&lt;/code>&lt;/pre>&lt;p>Separating lambdas into a helper function by using &lt;code>beam.Map(median_house_value_per_bedroom)&lt;/code> is the recommended approach for more testable code, because changes to the function would be modularized.&lt;/p>
&lt;ol start="4">
&lt;li>Use the &lt;code>assert_that&lt;/code> statement to ensure that &lt;code>PCollection&lt;/code> values match correctly, as in the previous example.&lt;/li>
&lt;/ol>
&lt;p>For more guidance about testing on Beam and Dataflow, see the &lt;a href="https://cloud.google.com/dataflow/docs/guides/develop-and-test-pipelines">Google Cloud documentation&lt;/a>. For more examples of unit testing in Beam, see the &lt;code>base_test.py&lt;/code> &lt;a href="https://github.com/apache/beam/blob/736cf50430b375d32093e793e1556567557614e9/sdks/python/apache_beam/ml/inference/base_test.py#L262">code&lt;/a>.&lt;/p>
&lt;p>Special thanks to Robert Bradshaw, Danny McCormick, XQ Hu, Surjit Singh, and Rebecca Spzer, who helped refine the ideas in this post.&lt;/p></description></item><item><title>Blog: Apache Beam 2.59.0</title><link>/blog/beam-2.59.0/</link><pubDate>Wed, 11 Sep 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.59.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.59.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2590-2024-09-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.59.0, check out the &lt;a href="https://github.com/apache/beam/milestone/23">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for setting a configureable timeout when loading a model and performing inference in the &lt;a href="https://beam.apache.org/documentation/ml/inference-overview/">RunInference&lt;/a> transform using &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference.with_exception_handling">with_exception_handling&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/32137">#32137&lt;/a>)&lt;/li>
&lt;li>Initial experimental support for using &lt;a href="/documentation/runners/prism/">Prism&lt;/a> with the Java and Python SDKs
&lt;ul>
&lt;li>Prism is presently targeting local testing usage, or other small scale execution.&lt;/li>
&lt;li>For Java, use &amp;lsquo;PrismRunner&amp;rsquo;, or &amp;lsquo;TestPrismRunner&amp;rsquo; as an argument to the &lt;code>--runner&lt;/code> flag.&lt;/li>
&lt;li>For Python, use &amp;lsquo;PrismRunner&amp;rsquo; as an argument to the &lt;code>--runner&lt;/code> flag.&lt;/li>
&lt;li>Go already uses Prism as the default local runner.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Improvements to the performance of BigqueryIO when using withPropagateSuccessfulStorageApiWrites(true) method (Java) (&lt;a href="https://github.com/apache/beam/pull/31840">#31840&lt;/a>).&lt;/li>
&lt;li>[Managed Iceberg] Added support for writing to partitioned tables (&lt;a href="https://github.com/apache/beam/pull/32102">#32102&lt;/a>)&lt;/li>
&lt;li>Update ClickHouseIO to use the latest version of the ClickHouse JDBC driver (&lt;a href="https://github.com/apache/beam/issues/32228">#32228&lt;/a>).&lt;/li>
&lt;li>Add ClickHouseIO dedicated User-Agent (&lt;a href="https://github.com/apache/beam/issues/32252">#32252&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>BigQuery endpoint can be overridden via PipelineOptions, this enables BigQuery emulators (Java) (&lt;a href="https://github.com/apache/beam/issues/28149">#28149&lt;/a>).&lt;/li>
&lt;li>Go SDK Minimum Go Version updated to 1.21 (&lt;a href="https://github.com/apache/beam/pull/32092">#32092&lt;/a>).&lt;/li>
&lt;li>[BigQueryIO] Added support for withFormatRecordOnFailureFunction() for STORAGE_WRITE_API and STORAGE_API_AT_LEAST_ONCE methods (Java) (&lt;a href="https://github.com/apache/beam/issues/31354">#31354&lt;/a>).&lt;/li>
&lt;li>Updated Go protobuf package to new version (Go) (&lt;a href="https://github.com/apache/beam/issues/21515">#21515&lt;/a>).&lt;/li>
&lt;li>Added support for setting a configureable timeout when loading a model and performing inference in the &lt;a href="https://beam.apache.org/documentation/ml/inference-overview/">RunInference&lt;/a> transform using &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference.with_exception_handling">with_exception_handling&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/32137">#32137&lt;/a>)&lt;/li>
&lt;li>Adds OrderedListState support for Java SDK via FnApi.&lt;/li>
&lt;li>Initial support for using Prism from the Python and Java SDKs.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed incorrect service account impersonation flow for Python pipelines using BigQuery IOs (&lt;a href="https://github.com/apache/beam/issues/32030">#32030&lt;/a>).&lt;/li>
&lt;li>Auto-disable broken and meaningless &lt;code>upload_graph&lt;/code> feature when using Dataflow Runner V2 (&lt;a href="https://github.com/apache/beam/issues/32159">#32159&lt;/a>).&lt;/li>
&lt;li>(Python) Upgraded google-cloud-storage to version 2.18.2 to fix a data corruption issue (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>).&lt;/li>
&lt;li>(Go) Fix corruption on State API writes. (&lt;a href="https://github.com/apache/beam/issues/32245">#32245&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Prism is under active development and does not yet support all pipelines. See &lt;a href="https://github.com/apache/beam/issues/29650">#29650&lt;/a> for progress.
&lt;ul>
&lt;li>In the 2.59.0 release, Prism passes most runner validations tests with the exceptions of pipelines using the following features:
OrderedListState, OnWindowExpiry (eg. GroupIntoBatches), CustomWindows, MergingWindowFns, Trigger and WindowingStrategy associated features, Bundle Finalization, Looping Timers, and some Coder related issues such as with Python combiner packing, and Java Schema transforms, and heterogenous flatten coders. Processing Time timers do not yet have real time support.&lt;/li>
&lt;li>If your pipeline is having difficulty with the Python or Java direct runners, but runs well on Prism, please let us know.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Java file-based IOs read or write lots (100k+) files could experience slowness and/or broken metrics visualization on Dataflow UI &lt;a href="https://github.com/apache/beam/issues/32649">#32649&lt;/a>.&lt;/li>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.59.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,Ahmet Altay,Andrew Crites,atask-g,Axel Magnuson,Ayush Pandey,Bartosz Zablocki,Chamikara Jayalath,cutiepie-10,Damon,Danny McCormick,dependabot[bot],Eddie Phillips,Francis O&amp;rsquo;Hara,Hyeonho Kim,Israel Herraiz,Jack McCluskey,Jaehyeon Kim,Jan Lukavský,Jeff Kinard,Jeffrey Kinard,jonathan-lemos,jrmccluskey,Kirill Berezin,Kiruphasankaran Nataraj,lahariguduru,liferoad,lostluck,Maciej Szwaja,Manit Gupta,Mark Zitnik,martin trieu,Naireen Hussain,Prerit Chandok,Radosław Stankiewicz,Rebecca Szper,Robert Bradshaw,Robert Burke,ron-gal,Sam Whittle,Sergei Lilichenko,Shunping Huang,Svetak Sundhar,Thiago Nunes,Timothy Itodo,tvalentyn,twosom,Vatsal,Vitaly Terentyev,Vlado Djerek,Yifan Ye,Yi Hu&lt;/p></description></item><item><title>Blog: Apache Beam 2.58.1</title><link>/blog/beam-2.58.1/</link><pubDate>Thu, 15 Aug 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.58.1/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.58.1 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2580-2024-08-06">download page&lt;/a> for this release.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Fixed issue where KafkaIO Records read with &lt;code>ReadFromKafkaViaSDF&lt;/code> are redistributed and may contain duplicates regardless of the configuration. This affects Java pipelines with Dataflow v2 runner and xlang pipelines reading from Kafka, (&lt;a href="https://github.com/apache/beam/issues/32196">#32196&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Large Dataflow graphs using runner v2, or pipelines explicitly enabling the &lt;code>upload_graph&lt;/code> experiment, will fail at construction time (&lt;a href="https://github.com/apache/beam/issues/32159">#32159&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.58.1 release. Thank you to all contributors!&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Sam Whittle&lt;/p></description></item><item><title>Blog: Apache Beam 2.58.0</title><link>/blog/beam-2.58.0/</link><pubDate>Tue, 06 Aug 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.58.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.58.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2580-2024-08-06">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information about changes in 2.58.0, check out the &lt;a href="https://github.com/apache/beam/milestone/22">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for &lt;a href="https://solace.com/">Solace&lt;/a> source (&lt;code>SolaceIO.Read&lt;/code>) added (Java) (&lt;a href="https://github.com/apache/beam/issues/31440">#31440&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Multiple RunInference instances can now share the same model instance by setting the model_identifier parameter (Python) (&lt;a href="https://github.com/apache/beam/issues/31665">#31665&lt;/a>).&lt;/li>
&lt;li>Added options to control the number of Storage API multiplexing connections (&lt;a href="https://github.com/apache/beam/pull/31721">#31721&lt;/a>)&lt;/li>
&lt;li>[BigQueryIO] Better handling for batch Storage Write API when it hits AppendRows throughput quota (&lt;a href="https://github.com/apache/beam/pull/31837">#31837&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] All specified catalog properties are passed through to the connector (&lt;a href="https://github.com/apache/beam/pull/31726">#31726&lt;/a>)&lt;/li>
&lt;li>Removed a third-party LGPL dependency from the Go SDK (&lt;a href="https://github.com/apache/beam/issues/31765">#31765&lt;/a>).&lt;/li>
&lt;li>Support for &lt;code>MapState&lt;/code> and &lt;code>SetState&lt;/code> when using Dataflow Runner v1 with Streaming Engine (Java) ([&lt;a href="https://github.com/apache/beam/issues/18200">#18200&lt;/a>])&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>[IcebergIO] &lt;code>IcebergCatalogConfig&lt;/code> was changed to support specifying catalog properties in a key-store fashion (&lt;a href="https://github.com/apache/beam/pull/31726">#31726&lt;/a>)&lt;/li>
&lt;li>[SpannerIO] Added validation that query and table cannot be specified at the same time for &lt;code>SpannerIO.read()&lt;/code>. Previously &lt;code>withQuery&lt;/code> overrides &lt;code>withTable&lt;/code>, if set (&lt;a href="https://github.com/apache/beam/issues/24956">#24956&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bug-fixes">Bug fixes&lt;/h2>
&lt;ul>
&lt;li>[BigQueryIO] Fixed a bug in batch Storage Write API that frequently exhausted concurrent connections quota (&lt;a href="https://github.com/apache/beam/pull/31710">#31710&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;li>[KafkaIO] Records read with &lt;code>ReadFromKafkaViaSDF&lt;/code> are redistributed and may contain duplicates regardless of the configuration. This affects Java pipelines with Dataflow v2 runner and xlang pipelines reading from Kafka, (&lt;a href="https://github.com/apache/beam/issues/32196">#32196&lt;/a>)&lt;/li>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.58.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexandre Moueddene&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Andrew Crites&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon Douglass&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Dilnaz Amanzholova&lt;/p>
&lt;p>Florian Bernard&lt;/p>
&lt;p>Francis O&amp;rsquo;Hara&lt;/p>
&lt;p>George Ma&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jaehyeon Kim&lt;/p>
&lt;p>James Roseman&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Maciej Szwaja&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Minh Son Nguyen&lt;/p>
&lt;p>Naireen&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Oliver Cardoza&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rohit Sinha&lt;/p>
&lt;p>S. Veyrié&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>TongruiLi&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Yi Hu&lt;/p></description></item><item><title>Blog: Apache Beam 2.57.0</title><link>/blog/beam-2.57.0/</link><pubDate>Wed, 26 Jun 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.57.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.57.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2570-2024-06-26">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.57.0, check out the &lt;a href="https://github.com/apache/beam/milestone/21">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam adds Python 3.12 support (&lt;a href="https://github.com/apache/beam/issues/29149">#29149&lt;/a>).&lt;/li>
&lt;li>Added FlinkRunner for Flink 1.18 (&lt;a href="https://github.com/apache/beam/issues/30789">#30789&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Ensure that BigtableIO closes the reader streams (&lt;a href="https://github.com/apache/beam/issues/31477">#31477&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added Feast feature store handler for enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/issues/30964">#30957&lt;/a>).&lt;/li>
&lt;li>BigQuery per-worker metrics are reported by default for Streaming Dataflow Jobs (Java) (&lt;a href="https://github.com/apache/beam/pull/31015">#31015&lt;/a>)&lt;/li>
&lt;li>Adds &lt;code>inMemory()&lt;/code> variant of Java List and Map side inputs for more efficient lookups when the entire side input fits into memory.&lt;/li>
&lt;li>Beam YAML now supports the jinja templating syntax.
Template variables can be passed with the (json-formatted) &lt;code>--jinja_variables&lt;/code> flag.&lt;/li>
&lt;li>DataFrame API now supports pandas 2.1.x and adds 12 more string functions for Series.(&lt;a href="https://github.com/apache/beam/pull/31185">#31185&lt;/a>).&lt;/li>
&lt;li>Added BigQuery handler for enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/pull/31295">#31295&lt;/a>)&lt;/li>
&lt;li>Disable soft delete policy when creating the default bucket for a project (Java) (&lt;a href="https://github.com/apache/beam/pull/31324">#31324&lt;/a>).&lt;/li>
&lt;li>Added &lt;code>DoFn.SetupContextParam&lt;/code> and &lt;code>DoFn.BundleContextParam&lt;/code> which can be used
as a python &lt;code>DoFn.process&lt;/code>, &lt;code>Map&lt;/code>, or &lt;code>FlatMap&lt;/code> parameter to invoke a context
manager per DoFn setup or bundle (analogous to using &lt;code>setup&lt;/code>/&lt;code>teardown&lt;/code>
or &lt;code>start_bundle&lt;/code>/&lt;code>finish_bundle&lt;/code> respectively.)&lt;/li>
&lt;li>Go SDK Prism Runner
&lt;ul>
&lt;li>Pre-built Prism binaries are now part of the release and are available via the Github release page. (&lt;a href="https://github.com/apache/beam/issues/29697">#29697&lt;/a>).&lt;/li>
&lt;li>ProcessingTime is now handled synthetically with TestStream pipelines and Non-TestStream pipelines, for fast test pipeline execution by default. (&lt;a href="https://github.com/apache/beam/issues/30083">#30083&lt;/a>).
&lt;ul>
&lt;li>Prism does NOT yet support &amp;ldquo;real time&amp;rdquo; execution for this release.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Improve processing for large elements to reduce the chances for exceeding 2GB protobuf limits (Python)([https://github.com/apache/beam/issues/31607]).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Java&amp;rsquo;s View.asList() side inputs are now optimized for iterating rather than
indexing when in the global window.
This new implementation still supports all (immutable) List methods as before,
but some of the random access methods like get() and size() will be slower.
To use the old implementation one can use View.asList().withRandomAccess().&lt;/li>
&lt;li>SchemaTransforms implemented with TypedSchemaTransformProvider now produce a
configuration Schema with snake_case naming convention
(&lt;a href="https://github.com/apache/beam/pull/31374">#31374&lt;/a>). This will make the following
cases problematic:
&lt;ul>
&lt;li>Running a pre-2.57.0 remote SDK pipeline containing a 2.57.0+ Java SchemaTransform,
and vice versa:&lt;/li>
&lt;li>Running a 2.57.0+ remote SDK pipeline containing a pre-2.57.0 Java SchemaTransform&lt;/li>
&lt;li>All direct uses of Python&amp;rsquo;s &lt;a href="https://github.com/apache/beam/blob/a998107a1f5c3050821eef6a5ad5843d8adb8aec/sdks/python/apache_beam/transforms/external.py#L381">SchemaAwareExternalTransform&lt;/a>
should be updated to use new snake_case parameter names.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Upgraded Jackson Databind to 2.15.4 (Java) (&lt;a href="https://github.com/apache/beam/issues/26743">#26743&lt;/a>).
jackson-2.15 has known breaking changes. An important one is it imposed a buffer limit for parser.
If your custom PTransform/DoFn are affected, refer to &lt;a href="https://github.com/apache/beam/pull/31580">#31580&lt;/a> for mitigation.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.57.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anody Zhang&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Ben Konz&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Colm O hEigeartaigh&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Justin Uang&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kevin Zhou&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Maarten Vercruysse&lt;/p>
&lt;p>Maciej Szwaja&lt;/p>
&lt;p>Maja Kontrec Rönn&lt;/p>
&lt;p>Marc hurabielle&lt;/p>
&lt;p>Martin Trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Min Zhu&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nick Anikin&lt;/p>
&lt;p>Pablo Rodriguez Defino&lt;/p>
&lt;p>Paul King&lt;/p>
&lt;p>Priyans Desai&lt;/p>
&lt;p>Radosław Stankiewicz&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rodrigo Bozzolo&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shahar Epstein&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tomo Suzuki&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vincent Stollenwerk&lt;/p>
&lt;p>Vineet Kumar&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>kberezin&lt;/p></description></item><item><title>Blog: Deploy Python pipelines on Kubernetes using the Flink runner</title><link>/blog/deploy-python-pipeline-on-flink-runner/</link><pubDate>Thu, 20 Jun 2024 13:56:15 +1000</pubDate><guid>/blog/deploy-python-pipeline-on-flink-runner/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="deploy-python-pipelines-on-kubernetes-using-the-flink-runner">Deploy Python pipelines on Kubernetes using the Flink runner&lt;/h1>
&lt;p>The &lt;a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/concepts/overview/">Apache Flink Kubernetes Operator&lt;/a> acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. With the operator, we can simplify the deployment and management of Apache Beam pipelines.&lt;/p>
&lt;p>In this post, we develop an &lt;a href="https://beam.apache.org/">Apache Beam&lt;/a> pipeline using the &lt;a href="https://beam.apache.org/documentation/sdks/python/">Python SDK&lt;/a> and deploy it on an &lt;a href="https://flink.apache.org/">Apache Flink&lt;/a> cluster by using the &lt;a href="https://beam.apache.org/documentation/runners/flink/">Apache Flink runner&lt;/a>. We first deploy an &lt;a href="https://kafka.apache.org/">Apache Kafka&lt;/a> cluster on a &lt;a href="https://minikube.sigs.k8s.io/docs/">minikube&lt;/a> cluster, because the pipeline uses Kafka topics for its data source and sink. Then, we develop the pipeline as a Python package and add the package to a custom Docker image so that Python user code can be executed externally. For deployment, we create a Flink session cluster using the Flink Kubernetes Operator, and deploy the pipeline using a Kubernetes job. Finally, we check the output of the application by sending messages to the input Kafka topic using a Python producer application.&lt;/p>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#resources-to-run-a-python-beam-pipeline-on-flink">Resources to run a Python Beam pipeline on Flink&lt;/a>&lt;/li>
&lt;li>&lt;a href="#set-up-the-kafka-cluster">Set up the Kafka cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#deploy-the-strimzi-operator">Deploy the Strimzi operator&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deploy-the-kafka-cluster">Deploy the Kafka cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deploy-the-kafka-ui">Deploy the Kafka UI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#develop-a-stream-processing-app">Develop a stream processing app&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#beam-pipeline-code">Beam pipeline code&lt;/a>&lt;/li>
&lt;li>&lt;a href="#build-docker-images">Build Docker images&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#deploy-the-stream-processing-app">Deploy the stream processing app&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#deploy-the-flink-kubernetes-operator">Deploy the Flink Kubernetes Operator&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deploy-the-beam-pipeline">Deploy the Beam pipeline&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kafka-producer">Kafka producer&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h2 id="resources-to-run-a-python-beam-pipeline-on-flink">Resources to run a Python Beam pipeline on Flink&lt;/h2>
&lt;p>We develop an Apache Beam pipeline using the Python SDK and deploy it on an Apache Flink cluster using the Apache Flink runner. Although the Flink cluster is created by the Flink Kubernetes Operator, we need two components to run the pipeline on the &lt;em>Flink runner&lt;/em>: the &lt;strong>job service&lt;/strong> and the &lt;a href="https://beam.apache.org/documentation/runtime/sdk-harness-config/">&lt;strong>SDK harness&lt;/strong>&lt;/a>. Roughly speaking, the job service converts details about a Python pipeline into a format that the Flink runner can understand. The SDK harness executes the Python user code. The Python SDK provides convenience wrappers to manage those components, and you can use it by specifying &lt;em>FlinkRunner&lt;/em> in the pipeline option, for example, &lt;code>--runner=FlinkRunner&lt;/code>. The &lt;em>job service&lt;/em> is managed automatically. We rely on our own &lt;em>SDK harness&lt;/em> as a sidecar container for simplicity. Also, we need the &lt;strong>Java IO Expansion Service&lt;/strong>, because the pipeline uses Apache Kafka topics for its data source and sink, and the Kafka Connector I/O is developed in Java. Simply put, the expansion service is used to serialize data for the Java SDK.&lt;/p>
&lt;h2 id="set-up-the-kafka-cluster">Set up the Kafka cluster&lt;/h2>
&lt;p>An Apache Kafka cluster is deployed using the &lt;a href="https://strimzi.io/">Strimzi Operator&lt;/a> on a minikube cluster. We install Strimzi version 0.39.0 and Kubernetes version 1.25.3. After the &lt;a href="https://minikube.sigs.k8s.io/docs/start/">minikube CLI&lt;/a> and &lt;a href="https://www.docker.com/">Docker&lt;/a> are installed, you can create a minikube cluster by specifying the Kubernetes version. You can find the source code for this blog post in the &lt;a href="https://github.com/jaehyeon-kim/beam-demos/tree/master/beam-deploy">GitHub repository&lt;/a>.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">minikube start --cpus&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;max&amp;#39;&lt;/span> --memory&lt;span class="o">=&lt;/span>&lt;span class="m">20480&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --addons&lt;span class="o">=&lt;/span>metrics-server --kubernetes-version&lt;span class="o">=&lt;/span>v1.25.3&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-strimzi-operator">Deploy the Strimzi operator&lt;/h3>
&lt;p>The GitHub repository keeps manifest files that you can use to deploy the Strimzi operator, Kafka cluster, and Kafka management application. To download a different version of the operator, download the relevant manifest file by specifying the version. By default, the manifest file assumes that the resources are deployed in the &lt;em>myproject&lt;/em> namespace. However, because we deploy them in the &lt;em>default&lt;/em> namespace, we need to change the resource namespace. We change the resource namespace using &lt;a href="https://www.gnu.org/software/sed/manual/sed.html">sed&lt;/a>.&lt;/p>
&lt;p>To deploy the operator, use the &lt;code>kubectl create&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Download and deploy the Strimzi operator.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">STRIMZI_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.39.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Optional: If downloading a different version, include this step.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">DOWNLOAD_URL&lt;/span>&lt;span class="o">=&lt;/span>https://github.com/strimzi/strimzi-kafka-operator/releases/download/&lt;span class="nv">$STRIMZI_VERSION&lt;/span>/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">curl -L -o kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> &lt;span class="si">${&lt;/span>&lt;span class="nv">DOWNLOAD_URL&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Update the namespace from myproject to default.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sed -i &lt;span class="s1">&amp;#39;s/namespace: .*/namespace: default/&amp;#39;&lt;/span> kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Deploy the Strimzi cluster operator.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create -f kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Verify that the Strimzi Operator runs as a Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">deployment&lt;/a>.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get deploy,rs,po
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY UP-TO-DATE AVAILABLE AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># deployment.apps/strimzi-cluster-operator 1/1 1 1 2m50s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME DESIRED CURRENT READY AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># replicaset.apps/strimzi-cluster-operator-8d6d4795c 1 1 1 2m50s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/strimzi-cluster-operator-8d6d4795c-94t8c 1/1 Running 0 2m49s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-kafka-cluster">Deploy the Kafka cluster&lt;/h3>
&lt;p>We deploy a Kafka cluster with a single broker and Zookeeper node. It has both internal and external listeners on ports 9092 and 29092, respectively. The external listener is used to access the Kafka cluster outside the minikube cluster. Also, the cluster is configured to allow automatic creation of topics (&lt;code>auto.create.topics.enable: &amp;quot;true&amp;quot;&lt;/code>), and the default number of partitions is set to 3 (&lt;code>num.partitions: 3&lt;/code>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># kafka/manifests/kafka-cluster.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka.strimzi.io/v1beta2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Kafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">kafka&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3.5.2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">256Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">250m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">512Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">500m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">listeners&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">plain&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">9092&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">internal&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">tls&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">external&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">29092&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nodeport&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">tls&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">storage&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">jbod&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">id&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">persistent-claim&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">size&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">20Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">deleteClaim&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">offsets.topic.replication.factor&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transaction.state.log.replication.factor&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transaction.state.log.min.isr&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">default.replication.factor&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">min.insync.replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">inter.broker.protocol.version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;3.5&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">auto.create.topics.enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;true&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">num.partitions&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">zookeeper&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">256Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">250m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">512Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">500m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">storage&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">persistent-claim&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">size&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">10Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">deleteClaim&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy he Kafka cluster using the &lt;code>kubectl create&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f kafka/manifests/kafka-cluster.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>The Kafka and Zookeeper nodes are managed by the &lt;a href="https://strimzi.io/docs/operators/latest/configuring.html#type-StrimziPodSet-reference">&lt;em>StrimziPodSet&lt;/em>&lt;/a> custom resource. It also creates multiple Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">services&lt;/a>. In this series, we use the following services:&lt;/p>
&lt;ul>
&lt;li>communication within the Kubernetes cluster
&lt;ul>
&lt;li>&lt;code>demo-cluster-kafka-bootstrap&lt;/code> - to access Kafka brokers from the client and management apps&lt;/li>
&lt;li>&lt;code>demo-cluster-zookeeper-client&lt;/code> - to access Zookeeper node from the management app&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>communication from the host
&lt;ul>
&lt;li>&lt;code>demo-cluster-kafka-external-bootstrap&lt;/code> - to access Kafka brokers from the producer app&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get po,strimzipodsets.core.strimzi.io,svc -l app.kubernetes.io/instance&lt;span class="o">=&lt;/span>demo-cluster
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/demo-cluster-kafka-0 1/1 Running 0 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/demo-cluster-zookeeper-0 1/1 Running 0 2m20s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME PODS READY PODS CURRENT PODS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># strimzipodset.core.strimzi.io/demo-cluster-kafka 1 1 1 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># strimzipodset.core.strimzi.io/demo-cluster-zookeeper 1 1 1 2m20s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-bootstrap ClusterIP 10.101.175.64 &amp;lt;none&amp;gt; 9091/TCP,9092/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-brokers ClusterIP None &amp;lt;none&amp;gt; 9090/TCP,9091/TCP,8443/TCP,9092/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-external-0 NodePort 10.106.155.20 &amp;lt;none&amp;gt; 29092:32475/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-external-bootstrap NodePort 10.111.244.128 &amp;lt;none&amp;gt; 29092:32674/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-zookeeper-client ClusterIP 10.100.215.29 &amp;lt;none&amp;gt; 2181/TCP 2m20s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-zookeeper-nodes ClusterIP None &amp;lt;none&amp;gt; 2181/TCP,2888/TCP,3888/TCP 2m20s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-kafka-ui">Deploy the Kafka UI&lt;/h3>
&lt;p>&lt;a href="https://docs.kafka-ui.provectus.io/overview/readme">UI for Apache Kafka (&lt;code>kafka-ui&lt;/code>)&lt;/a> is a free and open-source Kafka management application. It&amp;rsquo;s deployed as a Kubernetes Deployment. The Deployment is configured to have a single instance, and the Kafka cluster access details are specified as environment variables.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># kafka/manifests/kafka-ui.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Service&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ClusterIP&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">8080&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">targetPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">8080&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nn">---&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">provectuslabs/kafka-ui:v0.7.1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">containerPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">8080&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">env&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">KAFKA_CLUSTERS_0_NAME&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster-kafka-bootstrap:9092&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">KAFKA_CLUSTERS_0_ZOOKEEPER&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster-zookeeper-client:2181&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">256Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">250m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">512Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">500m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy the Kafka management app (&lt;code>kafka-ui&lt;/code>) using the &lt;code>kubectl create&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f kafka/manifests/kafka-ui.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl get all -l &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>kafka-ui
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/kafka-ui-65dbbc98dc-zl5gv 1/1 Running 0 35s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/kafka-ui ClusterIP 10.109.14.33 &amp;lt;none&amp;gt; 8080/TCP 36s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY UP-TO-DATE AVAILABLE AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># deployment.apps/kafka-ui 1/1 1 1 35s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME DESIRED CURRENT READY AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># replicaset.apps/kafka-ui-65dbbc98dc 1 1 1 35s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>We use &lt;code>kubectl port-forward&lt;/code> to connect to the &lt;code>kafka-ui&lt;/code> server running in the minikube cluster on port 8080.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl port-forward svc/kafka-ui &lt;span class="m">8080&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/kafka-ui.png"
alt="Kafka UI">&lt;/p>
&lt;h2 id="develop-a-stream-processing-app">Develop a stream processing app&lt;/h2>
&lt;p>We develop an Apache Beam pipeline as a Python package and add it to a custom Docker image, which is used to execute Python user code (&lt;em>SDK harness&lt;/em>). We also build another custom Docker image, which adds the Java SDK of Apache Beam to the official Flink base image. This image is used to deploy a Flink cluster and to execute Java user code of the &lt;em>Kafka Connector I/O&lt;/em>.&lt;/p>
&lt;h3 id="beam-pipeline-code">Beam pipeline code&lt;/h3>
&lt;p>The application first reads text messages from an input Kafka topic. Next, it extracts words by splitting the messages (&lt;code>ReadWordsFromKafka&lt;/code>). Then, the elements (words) are added to a fixed time window of 5 seconds, and their average length is calculated (&lt;code>CalculateAvgWordLen&lt;/code>). Finally, we include the window start and end timestamps, and send the updated element to an output Kafka topic (&lt;code>WriteWordLenToKafka&lt;/code>).&lt;/p>
&lt;p>We create a custom &lt;em>Java IO Expansion Service&lt;/em> (&lt;code>get_expansion_service&lt;/code>) and add it to the &lt;code>ReadFromKafka&lt;/code> and &lt;code>WriteToKafka&lt;/code> transforms of the Kafka Connector I/O. Although the Kafka I/O provides a function to create that service, it did not work for me (or I do not understand how to make use of it yet). Instead, I created a custom service, as illustrated in &lt;a href="https://www.packtpub.com/product/building-big-data-pipelines-with-apache-beam/9781800564930">Building Big Data Pipelines with Apache Beam by Jan Lukavský&lt;/a>. The expansion service Jar file (&lt;code>beam-sdks-java-io-expansion-service.jar&lt;/code>) must exist in the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">&lt;em>job&lt;/em>&lt;/a> that executes the pipeline, while the Java SDK (&lt;code>/opt/apache/beam/boot&lt;/code>) must exist in the runner worker.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># beam/word_len/word_len.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">argparse&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">re&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">logging&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">typing&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">beam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">pvalue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.io&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">kafka&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.transforms.window&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">FixedWindows&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">PipelineOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">SetupOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.transforms.external&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">JavaJarExpansionService&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_expansion_service&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">jar&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/opt/apache/beam/jars/beam-sdks-java-io-expansion-service.jar&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">args&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--defaultEnvironmentType=PROCESS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;--defaultEnvironmentConfig={&amp;#34;command&amp;#34;: &amp;#34;/opt/apache/beam/boot&amp;#34;}&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--experiments=use_deprecated_read&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">JavaJarExpansionService&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">jar&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;{{PORT}}&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">length&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">count&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">WordAccum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RowCoder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">decode_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kafka_kv&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kafka_kv&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">kafka_kv&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">tokenize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">re&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">findall&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">r&lt;/span>&lt;span class="s2">&amp;#34;[A-Za-z\&amp;#39;]+&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">create_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">]):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">msg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dumps&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">dict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">zip&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="s2">&amp;#34;window_start&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;window_end&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;avg_len&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">msg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">msg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">AverageFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombineFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">create_accumulator&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">add_input&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mutable_accumulator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mutable_accumulator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">length&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">merge_accumulators&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">accumulators&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">WordAccum&lt;/span>&lt;span class="p">]):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lengths&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">counts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">zip&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">accumulators&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lengths&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">counts&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">extract_output&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">accumulator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">accumulator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">length&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;NaN&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">get_accumulator_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">WordAccum&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">AddWindowTS&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">avg_len&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">win_param&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowParam&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">yield&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">win_param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to_rfc3339&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">win_param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to_rfc3339&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">avg_len&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ReadWordsFromKafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topics&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">group_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Any&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">label&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">label&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topics&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">topics&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">group_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">group_id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">verbose&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">verbose&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expansion_service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PBegin&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;ReadFromKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">kafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">consumer_config&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;bootstrap.servers&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;auto.offset.reset&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;latest&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># &amp;#34;enable.auto.commit&amp;#34;: &amp;#34;true&amp;#34;,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;group.id&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">group_id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topics&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topics&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">timestamp_policy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">kafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromKafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create_time_policy&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">commit_offset_in_finalize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;DecodeMessage&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">decode_message&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;Tokenize&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">FlatMap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokenize&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">CalculateAvgWordLen&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PCollection&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;Windowing&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowInto&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FixedWindows&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;GetAvgWordLength&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombineGlobally&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">AverageFn&lt;/span>&lt;span class="p">())&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">without_defaults&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">WriteWordLenToKafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topic&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Any&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">label&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">label&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">topic&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expansion_service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PCollection&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;AddWindowTS&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">AddWindowTS&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;CreateMessages&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">create_message&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">bytes&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">bytes&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;WriteToKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">kafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">producer_config&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;bootstrap.servers&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topic&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">argv&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">save_main_session&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">argparse&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ArgumentParser&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">description&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Beam pipeline arguments&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--deploy&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;deploy&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;store_true&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Flag to indicate whether to deploy to a cluster&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--bootstrap_servers&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;bootstrap&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;host.docker.internal:29092&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka bootstrap server addresses&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--input_topic&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;input&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;input-topic&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka input topic name&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--output_topic&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;output&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;output-topic-beam&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka output topic name&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--group_id&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;group&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;beam-word-len&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka output group ID&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">known_args&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pipeline_args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parse_known_args&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">argv&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pipeline_args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># We use the save_main_session option because one or more DoFn elements in this&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># workflow rely on global context. That is, a module imported at the module level.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pipeline_options&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">PipelineOptions&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pipeline_args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pipeline_options&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">SetupOptions&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save_main_session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">save_main_session&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deploy&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_expansion_service&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">options&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">pipeline_options&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;ReadWordsFromKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">ReadWordsFromKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topics&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">group_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">group&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;CalculateAvgWordLen&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">CalculateAvgWordLen&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;WriteWordLenToKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">WriteWordLenToKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topic&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logging&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getLogger&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">setLevel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logging&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logging&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">info&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Building pipeline ...&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="vm">__name__&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;__main__&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The pipeline script is added to a Python package under a folder named &lt;code>word_len&lt;/code>. A simple module named &lt;code>run&lt;/code> is created, because it is executed as a module, for example, &lt;code>python -m ...&lt;/code>. When I ran the pipeline as a script, I encountered an error. This packaging method is for demonstration only. For a recommended way of packaging a pipeline, see &lt;a href="https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/">Managing Python Pipeline Dependencies&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># beam/word_len/run.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">.&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="o">*&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Overall, the pipeline package uses the following structure.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">tree beam/word_len
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">beam/word_len
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">├── __init__.py
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">├── run.py
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">└── word_len.py&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="build-docker-images">Build Docker images&lt;/h3>
&lt;p>As discussed previously, we build a custom Docker image (&lt;em>beam-python-example:1.16&lt;/em>) and use it to deploy a Flink cluster and to run the Java user code of the Kafka Connector I/O.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/Dockerfile&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> flink:1.16&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> --from&lt;span class="o">=&lt;/span>apache/beam_java11_sdk:2.56.0 /opt/apache/beam/ /opt/apache/beam/&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We also build a custom Docker image (&lt;em>beam-python-harness:2.56.0&lt;/em>) to run Python user code (&lt;em>SDK harness&lt;/em>). From the Python SDK Docker image, it first installs the Java Development Kit (JDK) and downloads the &lt;em>Java IO Expansion Service&lt;/em> Jar file. Then, the Beam pipeline packages are copied to the &lt;code>/app&lt;/code> folder. The app folder is added to the &lt;code>PYTHONPATH&lt;/code> environment variable, which makes the packages searchable.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/Dockerfile-python-harness&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> apache/beam_python3.10_sdk:2.56.0&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ARG&lt;/span> BEAM_VERSION&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="k">:-&lt;/span>&lt;span class="nv">2&lt;/span>&lt;span class="p">.56.0&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">REPO_BASE_URL&lt;/span>&lt;span class="o">=&lt;/span>https://repo1.maven.org/maven2/org/apache/beam&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> apt-get update &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> apt-get install -y default-jdk&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> mkdir -p /opt/apache/beam/jars &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> wget &lt;span class="si">${&lt;/span>&lt;span class="nv">REPO_BASE_URL&lt;/span>&lt;span class="si">}&lt;/span>/beam-sdks-java-io-expansion-service/&lt;span class="si">${&lt;/span>&lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="si">}&lt;/span>/beam-sdks-java-io-expansion-service-&lt;span class="si">${&lt;/span>&lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="si">}&lt;/span>.jar &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --progress&lt;span class="o">=&lt;/span>bar:force:noscroll -O /opt/apache/beam/jars/beam-sdks-java-io-expansion-service.jar&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> word_len /app/word_len&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> word_count /app/word_count&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">PYTHONPATH&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="nv">$PYTHONPATH&lt;/span>&lt;span class="s2">:/app&amp;#34;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Because the custom images need to be accessible in the minikube cluster, we point the terminal&amp;rsquo;s &lt;code>docker-cli&lt;/code> to the minikube&amp;rsquo;s Docker engine. Then, we can build the images using the &lt;code>docker build&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">eval&lt;/span> &lt;span class="k">$(&lt;/span>minikube docker-env&lt;span class="k">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker build -t beam-python-example:1.16 beam/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker build -t beam-python-harness:2.56.0 -f beam/Dockerfile-python-harness beam/&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="deploy-the-stream-processing-app">Deploy the stream processing app&lt;/h2>
&lt;p>The Beam pipeline is executed on a &lt;a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/overview/#session-cluster-deployments">Flink session cluster&lt;/a>, which is deployed by the Flink Kubernetes Operator. The &lt;a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/overview/#application-deployments">application deployment mode&lt;/a> where the Beam pipeline is deployed as a Flink job doesn&amp;rsquo;t seem to work (or I don&amp;rsquo;t understand how to do so yet) due to either a job submission timeout error or a failure to upload the job artifact. After the pipeline is deployed, we check the output of the application by sending text messages to the input Kafka topic.&lt;/p>
&lt;h3 id="deploy-the-flink-kubernetes-operator">Deploy the Flink Kubernetes Operator&lt;/h3>
&lt;p>First, to make it possible to add the webhook component, install the &lt;a href="https://github.com/cert-manager/cert-manager">certificate manager&lt;/a> on the minikube cluster. Then, use a Helm chart to install the operator. Version 1.8.0 is installed in the post.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.8.0/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm install flink-kubernetes-operator flink-operator-repo/flink-kubernetes-operator
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME: flink-kubernetes-operator&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># LAST DEPLOYED: Mon Jun 03 21:37:45 2024&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAMESPACE: default&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># STATUS: deployed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># REVISION: 1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># TEST SUITE: None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm list
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># flink-kubernetes-operator default 1 2024-06-03 21:37:45.579302452 +1000 AEST deployed flink-kubernetes-operator-1.8.0 1.8.0&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-beam-pipeline">Deploy the Beam pipeline&lt;/h3>
&lt;p>First, create a Flink session cluster. In the manifest file, configure common properties, such as the Docker image, Flink version, cluster configuration, and pod template. These properties are applied to the Flink job manager and task manager. In addition, specify the replica and resource. We add a sidecar container to the task manager, and this &lt;em>SDK harness&lt;/em> container is configured to execute Python user code - see the following job configuration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/word_len_cluster.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink.apache.org/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FlinkDeployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">word-len-cluster&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-python-example:1.16&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">imagePullPolicy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Never&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1_16&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskmanager.numberOfTaskSlots&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;10&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">serviceAccount&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-main-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeMounts&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/opt/flink/log&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-logs&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-logs&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">emptyDir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>{}&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jobManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048Mi&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048Mi&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">python-harness&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-python-harness:2.56.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">args&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;-worker_pool&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">containerPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">50000&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">harness-port&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The pipeline is deployed using a Kubernetes job, and the custom &lt;em>SDK harness&lt;/em> image is used to execute the pipeline as a module. The first two arguments are application-specific. The rest of the arguments are for pipeline options. For more information about the pipeline arguments, see the &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/options/pipeline_options.py">pipeline options source&lt;/a> and &lt;a href="https://beam.apache.org/documentation/runners/flink/">Flink Runner document&lt;/a>. To execute Python user code in the sidecar container, we set the environment type to &lt;code>EXTERNAL&lt;/code> and the environment config to &lt;code>localhost:50000&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/word_len_job.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">batch/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">word-len-job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">word-len-job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-word-len-job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-python-harness:2.56.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">command&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;python&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">args&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;-m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;word_len.run&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--deploy&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--bootstrap_servers=demo-cluster-kafka-bootstrap:9092&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--runner=FlinkRunner&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--flink_master=word-len-cluster-rest:8081&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--job_name=beam-word-len&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--streaming&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--parallelism=3&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--flink_submit_uber_jar&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--environment_type=EXTERNAL&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--environment_config=localhost:50000&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--checkpointing_interval=10000&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">restartPolicy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Never&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy the session cluster and job using the &lt;code>kubectl create&lt;/code> command. The session cluster is created by the &lt;em>FlinkDeployment&lt;/em> custom resource, and it manages the job manager deployment, task manager pod, and associated services. When we check the log of the job&amp;rsquo;s pod, we see that it does the following tasks:&lt;/p>
&lt;ul>
&lt;li>starts the &lt;em>Job Service&lt;/em> after downloading the Jar file&lt;/li>
&lt;li>uploads the pipeline artifact&lt;/li>
&lt;li>submits the pipeline as a Flink job&lt;/li>
&lt;li>continuously monitors the job status&lt;/li>
&lt;/ul>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f beam/word_len_cluster.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># flinkdeployment.flink.apache.org/word-len-cluster created&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create -f beam/word_len_job.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># job.batch/word-len-job created&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl logs word-len-job-p5rph -f
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:root:Building pipeline ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.flink_runner:Adding HTTP protocol scheme to flink_master parameter: http://word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:apache_beam.runners.portability.abstract_job_service:Got Prepare request.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/config HTTP/1.1&amp;#34; 200 240&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.utils.subprocess_server:Downloading job server jar from https://repo.maven.apache.org/maven2/org/apache/beam/beam-runners-flink-1.16-job-server/2.56.0/beam-runners-flink-1.16-job-server-2.56.0.jar&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.abstract_job_service:Artifact server started on port 43287&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:apache_beam.runners.portability.abstract_job_service:Prepared job &amp;#39;job&amp;#39; as &amp;#39;job-edc1c2f1-80ef-48b7-af14-7e6fc86f338a&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.abstract_job_service:Running job &amp;#39;job-edc1c2f1-80ef-48b7-af14-7e6fc86f338a&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;POST /v1/jars/upload HTTP/1.1&amp;#34; 200 148&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;POST /v1/jars/e1984c45-d8bc-4aa1-9b66-369a23826921_beam.jar/run HTTP/1.1&amp;#34; 200 44&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.flink_uber_jar_job_server:Started Flink job as a403cb2f92fecee65b8fd7cc8ac6e68a&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ...&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>After the deployment completes, we can see the following Flink session cluster and job related resources.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get all -l &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>word-len-cluster
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/word-len-cluster-7c98f6f868-d4hbx 1/1 Running 0 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/word-len-cluster-taskmanager-1-1 2/2 Running 0 4m3s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/word-len-cluster ClusterIP None &amp;lt;none&amp;gt; 6123/TCP,6124/TCP 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/word-len-cluster-rest ClusterIP 10.104.23.28 &amp;lt;none&amp;gt; 8081/TCP 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY UP-TO-DATE AVAILABLE AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># deployment.apps/word-len-cluster 1/1 1 1 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME DESIRED CURRENT READY AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># replicaset.apps/word-len-cluster-7c98f6f868 1 1 1 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl get all -l &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>word-len-job
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/word-len-job-24r6q 1/1 Running 0 5m24s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME COMPLETIONS DURATION AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># job.batch/word-len-job 0/1 5m24s 5m24s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>You can access the Flink web UI using the &lt;code>kubectl port-forward&lt;/code> command on port 8081. The job graph shows two tasks. The first task adds word elements into a fixed time window. The second task sends the average word length records to the output topic.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl port-forward svc/flink-word-len-rest &lt;span class="m">8081&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/flink-ui.png"
alt="Flink UI">&lt;/p>
&lt;p>The Kafka I/O automatically creates a topic if it doesn&amp;rsquo;t exist, and we can see the input topic is created on &lt;code>kafka-ui&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/kafka-topics-1.png"
alt="Kafka Input Topic">&lt;/p>
&lt;h3 id="kafka-producer">Kafka producer&lt;/h3>
&lt;p>A simple Python Kafka producer is created to check the output of the application. By default, the producer app sends random text from the &lt;a href="https://faker.readthedocs.io/en/master/">Faker&lt;/a> package to the input Kafka topic every one second.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># kafka/client/producer.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">os&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">faker&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Faker&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">kafka&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">KafkaProducer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">TextProducer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">topic_name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap_servers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic_name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">topic_name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kafka_producer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create_producer&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">create_producer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Returns a KafkaProducer instance
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">KafkaProducer&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_serializer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">send_to_kafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">timestamp_ms&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Sends text to a Kafka topic.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;topic&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic_name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;value&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">timestamp_ms&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;timestamp_ms&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">timestamp_ms&lt;/span>&lt;span class="p">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kafka_producer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kafka_producer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flush&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">except&lt;/span> &lt;span class="ne">Exception&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">raise&lt;/span> &lt;span class="ne">RuntimeError&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;fails to send a message&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">e&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="vm">__name__&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;__main__&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">producer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TextProducer&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">os&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getenv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;BOOTSTRAP_SERVERS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;localhost:29092&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">os&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getenv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;TOPIC_NAME&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;input-topic&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fake&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Faker&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_events&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fake&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">producer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send_to_kafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">num_events&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">num_events&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> text sent... current&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">os&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getenv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;DELAY_SECONDS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;1&amp;#34;&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Expose the Kafka bootstrap server on port 29092 using the &lt;code>kubectl port-forward&lt;/code> command. Execute the Python script to start the producer app.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl port-forward svc/demo-cluster-kafka-external-bootstrap &lt;span class="m">29092&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python kafka/client/producer.py&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>We can see the output topic (&lt;code>output-topic-beam&lt;/code>) is created on &lt;code>kafka-ui&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/kafka-topics-2.png"
alt="Kafka Output Topic">&lt;/p>
&lt;p>Also, we can check that the output messages are created as expected in the &lt;strong>Topics&lt;/strong> tab.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/output-topic-messages.png"
alt="Kafka Output Topic Messages">&lt;/p>
&lt;h1 id="delete-resources">Delete resources&lt;/h1>
&lt;p>Delete the Kubernetes resources and the minikube cluster using the following steps.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Delete the Flink Operator and related resources.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f beam/word_len_cluster.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f beam/word_len_job.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm uninstall flink-kubernetes-operator
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo remove flink-operator-repo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Delete the Kafka cluster and related resources.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">STRIMZI_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.39.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f kafka/manifests/kafka-cluster.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f kafka/manifests/kafka-ui.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Delete the minikube.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">minikube delete&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Blog: Apache Beam 2.56.0</title><link>/blog/beam-2.56.0/</link><pubDate>Wed, 01 May 2024 10:00:00 -0400</pubDate><guid>/blog/beam-2.56.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.56.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2550-2023-03-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.56.0, check out the &lt;a href="https://github.com/apache/beam/milestone/20">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added FlinkRunner for Flink 1.17, removed support for Flink 1.12 and 1.13. Previous version of Pipeline running on Flink 1.16 and below can be upgraded to 1.17, if the Pipeline is first updated to Beam 2.56.0 with the same Flink version. After Pipeline runs with Beam 2.56.0, it should be possible to upgrade to FlinkRunner with Flink 1.17. (&lt;a href="https://github.com/apache/beam/issues/29939">#29939&lt;/a>)&lt;/li>
&lt;li>New Managed I/O Java API (&lt;a href="https://github.com/apache/beam/pull/30830">#30830&lt;/a>).&lt;/li>
&lt;li>New Ordered Processing PTransform added for processing order-sensitive stateful data (&lt;a href="https://github.com/apache/beam/pull/30735">#30735&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Upgraded Avro version to 1.11.3, kafka-avro-serializer and kafka-schema-registry-client versions to 7.6.0 (Java) (&lt;a href="https://github.com/apache/beam/pull/30638">#30638&lt;/a>).
The newer Avro package is known to have breaking changes. If you are affected, you can keep pinned to older Avro versions which are also tested with Beam.&lt;/li>
&lt;li>Iceberg read/write support is available through the new Managed I/O Java API (&lt;a href="https://github.com/apache/beam/pull/30830">#30830&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Profiling of Cythonized code has been disabled by default. This might improve performance for some Python pipelines (&lt;a href="https://github.com/apache/beam/pull/30938">#30938&lt;/a>).&lt;/li>
&lt;li>Bigtable enrichment handler now accepts a custom function to build a composite row key. (Python) (&lt;a href="https://github.com/apache/beam/issues/30975">#30974&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Default consumer polling timeout for KafkaIO.Read was increased from 1 second to 2 seconds. Use KafkaIO.read().withConsumerPollingTimeout(Duration duration) to configure this timeout value when necessary (&lt;a href="https://github.com/apache/beam/issues/30870">#30870&lt;/a>).&lt;/li>
&lt;li>Python Dataflow users no longer need to manually specify &amp;ndash;streaming for pipelines using unbounded sources such as ReadFromPubSub.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed locking issue when shutting down inactive bundle processors. Symptoms of this issue include slowness or stuckness in long-running jobs (Python) (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Fixed logging issue that caused silecing the pip output when installing of dependencies provided in &lt;code>--requirements_file&lt;/code> (Python).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.56.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Aravind Pedapudi&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Brachi Packter&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clement DAL PALU&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Daria Bezkorovaina&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Evan Burrell&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Luís Bianchin&lt;/p>
&lt;p>Maciej Szwaja&lt;/p>
&lt;p>Melody Shen&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shahar Epstein&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timothy Itodo&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>dmitryor&lt;/p>
&lt;p>github-actions[bot]&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xianhualiu&lt;/p></description></item><item><title>Blog: Introducing Beam YAML: Apache Beam's First No-code SDK</title><link>/blog/beam-yaml-release/</link><pubDate>Thu, 11 Apr 2024 10:00:00 -0400</pubDate><guid>/blog/beam-yaml-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Writing a Beam pipeline can be a daunting task. Learning the Beam model, downloading dependencies for the SDK language
of choice, debugging the pipeline, and maintaining the pipeline code is a lot of overhead for users who want to write a
simple to intermediate data processing pipeline. There have been strides in making the SDK&amp;rsquo;s entry points easier, but
for many, it is still a long way from being a painless process.&lt;/p>
&lt;p>To address some of these issues and simplify the entry point to Beam, we have introduced a new way to specify Beam
pipelines by using configuration files rather than code. This new SDK, known as
&lt;a href="https://beam.apache.org/documentation/sdks/yaml/">Beam YAML&lt;/a>, employs a declarative approach to creating
data processing pipelines using &lt;a href="https://yaml.org/">YAML&lt;/a>, a widely used data serialization language.&lt;/p>
&lt;h1 id="benefits-of-using-beam-yaml">Benefits of using Beam YAML&lt;/h1>
&lt;p>The primary goal of Beam YAML is to make the entry point to Beam as welcoming as possible. However, this should not
come at the expense of sacrificing the rich features that Beam offers.&lt;/p>
&lt;p>Here are some of the benefits of using Beam YAML:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>No-code development:&lt;/strong> Allows users to develop pipelines without writing any code. This makes it easier to get
started with Beam and to develop pipelines quickly and easily.&lt;/li>
&lt;li>&lt;strong>Maintainability&lt;/strong>: Configuration-based pipelines are easier to maintain than code-based pipelines. YAML format
enables clear separation of concerns, simplifying changes and updates without affecting other code sections.&lt;/li>
&lt;li>&lt;strong>Declarative language:&lt;/strong> Provides a declarative language, which means that it is based on the description of the
desired outcome rather than expressing the intent through code. This makes it easy to understand the structure and
flow of a pipeline. The YAML syntax is also widely used with a rich community of resources for learning and
leveraging the YAML syntax.&lt;/li>
&lt;li>&lt;strong>Powerful features:&lt;/strong> Supports a wide range of features, including a variety of data sources and sinks, turn-key
transforms, and execution parameters. This makes it possible to develop complex data processing pipelines with Beam
YAML.&lt;/li>
&lt;li>&lt;strong>Reusability&lt;/strong>: Beam YAML promotes code reuse by providing a way to define and share common pipeline patterns. You
can create reusable YAML snippets or blocks that can be easily shared and reused in different pipelines. This reduces
the need to write repetitive tasks and helps maintain consistency across pipelines.&lt;/li>
&lt;li>&lt;strong>Extensibility&lt;/strong>: Beam YAML offers a structure for integrating custom transformations into a pipeline, enabling
organizations to contribute or leverage a pre-existing catalog of transformations that can be seamlessly accessed
using the Beam YAML syntax across multiple pipelines. It is also possible to build third-party extensions, including
custom parsers and other tools, that do not need to depend on Beam directly.&lt;/li>
&lt;li>&lt;strong>Backwards Compatibility&lt;/strong>: Beam YAML is still being actively worked on, bringing exciting new features and
capabilities, but as these features are added, backwards compatibility will be preserved. This way, once a pipeline
is written, it will continue to work despite future released versions of the SDK.&lt;/li>
&lt;/ul>
&lt;p>Overall, using Beam YAML provides a number of advantages. It makes pipeline development and management more efficient
and effective, enabling users to focus on the business logic and data processing tasks, rather than spending time on
low-level coding details.&lt;/p>
&lt;h1 id="case-study-a-simple-business-analytics-use-case">Case Study: A simple business analytics use-case&lt;/h1>
&lt;p>Let&amp;rsquo;s take the following sample transaction data for a department store:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">transaction_id&lt;/th>
&lt;th style="text-align:left">product_name&lt;/th>
&lt;th style="text-align:left">category&lt;/th>
&lt;th style="text-align:left">price&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">T0012&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T5034&lt;/td>
&lt;td style="text-align:left">Leather Jacket&lt;/td>
&lt;td style="text-align:left">Apparel&lt;/td>
&lt;td style="text-align:left">109.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0024&lt;/td>
&lt;td style="text-align:left">Aluminum Mug&lt;/td>
&lt;td style="text-align:left">Kitchen&lt;/td>
&lt;td style="text-align:left">29.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0104&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0302&lt;/td>
&lt;td style="text-align:left">Monitor&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">249.99&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now, let&amp;rsquo;s say that the business wants to get a record of transactions for all purchases made in the Electronics
department for audit purposes. Assuming the records are stored as a CSV file, a Beam YAML pipeline may look something
like this:&lt;/p>
&lt;p>Source code for this example can be found
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/simple_filter.yaml">here&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/input.csv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Filter&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">language&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">keep&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">category == &amp;#34;Electronics&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteOutputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/output&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This would leave us with the following data:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">transaction_id&lt;/th>
&lt;th style="text-align:left">product_name&lt;/th>
&lt;th style="text-align:left">category&lt;/th>
&lt;th style="text-align:left">price&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">T0012&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0104&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0302&lt;/td>
&lt;td style="text-align:left">Monitor&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">249.99&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now, let&amp;rsquo;s say the business wants to determine how much of each Electronics item is being sold to ensure that the
correct number is being ordered from the supplier. Let&amp;rsquo;s also assume that they want to determine the total revenue for
each item. This simple aggregation can follow the Filter from the previous example as such:&lt;/p>
&lt;p>Source code for this example can be found
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/simple_filter_and_combine.yaml">here&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/input.csv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Filter&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">language&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">keep&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">category == &amp;#34;Electronics&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Combine&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">CountNumberSold&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">group_by&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">product_name&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">combine&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">num_sold&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">product_name&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">fn&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">count&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">total_revenue&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">price&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">fn&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">sum&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteOutputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">CountNumberSold&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/output&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This would leave us with the following data:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">product_name&lt;/th>
&lt;th style="text-align:left">num_sold&lt;/th>
&lt;th style="text-align:left">total_revenue&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">2&lt;/td>
&lt;td style="text-align:left">119.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Monitor&lt;/td>
&lt;td style="text-align:left">1&lt;/td>
&lt;td style="text-align:left">249.99&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>While this was a relatively simple use-case, it shows the power of Beam YAML and how easy it is to go from business
use-case to a prototype data pipeline in just a few lines of YAML.&lt;/p>
&lt;h1 id="getting-started-with-beam-yaml">Getting started with Beam YAML&lt;/h1>
&lt;p>There are several resources that have been compiled to help users get familiar with Beam YAML.&lt;/p>
&lt;h2 id="day-zero-notebook">Day Zero Notebook&lt;/h2>
&lt;a target="_blank" href="https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-yaml.ipynb">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>
&lt;p>To help get started with Apache Beam, there is a Day Zero Notebook available on
&lt;a href="https://colab.sandbox.google.com/">Google Colab&lt;/a>, an online Python notebook environment with a free attachable
runtime, containing some basic YAML pipeline examples.&lt;/p>
&lt;h2 id="documentation">Documentation&lt;/h2>
&lt;p>The Apache Beam website provides a set of &lt;a href="https://beam.apache.org/documentation/sdks/yaml/">docs&lt;/a> that demonstrate the
current capabilities of the Beam YAML SDK. There is also a catalog of currently-supported turnkey transforms found
&lt;a href="https://beam.apache.org/releases/yamldoc/current/">here&lt;/a>.&lt;/p>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>A catalog of examples can be found
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples">here&lt;/a>. These examples showcase
all the turnkey transforms that can be utilized in Beam YAML. There are also a number of Dataflow Cookbook examples
that can be found &lt;a href="https://github.com/GoogleCloudPlatform/dataflow-cookbook/tree/main/Python/yaml">here&lt;/a>.&lt;/p>
&lt;h2 id="contributing">Contributing&lt;/h2>
&lt;p>Developers who wish to help build out and add functionalities are welcome to start contributing to the effort in the
Beam YAML module found &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml">here&lt;/a>.&lt;/p>
&lt;p>There is also a list of open &lt;a href="https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Ayaml">bugs&lt;/a> found
on the GitHub repo - now marked with the &amp;lsquo;yaml&amp;rsquo; tag.&lt;/p>
&lt;p>While Beam YAML has been marked stable as of Beam 2.52, it is still under heavy development, with new features being
added with each release. Those who wish to be part of the design decisions and give insights to how the framework is
being used are highly encouraged to join the dev mailing list as those discussions will be directed there. A link to
the dev list can be found &lt;a href="https://beam.apache.org/community/contact-us/">here&lt;/a>.&lt;/p></description></item><item><title>Blog: Apache Beam 2.55.0</title><link>/blog/beam-2.55.0/</link><pubDate>Mon, 25 Mar 2024 10:00:00 -0400</pubDate><guid>/blog/beam-2.55.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.55.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2550-2023-03-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.55.0, check out the &lt;a href="https://github.com/apache/beam/milestone/19">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>The Python SDK will now include automatically generated wrappers for external Java transforms! (&lt;a href="https://github.com/apache/beam/pull/29834">#29834&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added support for handling bad records to BigQueryIO (&lt;a href="https://github.com/apache/beam/pull/30081">#30081&lt;/a>).
&lt;ul>
&lt;li>Full Support for Storage Read and Write APIs&lt;/li>
&lt;li>Partial Support for File Loads (Failures writing to files supported, failures loading files to BQ unsupported)&lt;/li>
&lt;li>No Support for Extract or Streaming Inserts&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for handling bad records to PubSubIO (&lt;a href="https://github.com/apache/beam/pull/30372">#30372&lt;/a>).
&lt;ul>
&lt;li>Support is not available for handling schema mismatches, and enabling error handling for writing to Pub/Sub topics with schemas is not recommended&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--enableBundling&lt;/code> pipeline option for BigQueryIO DIRECT_READ is replaced by &lt;code>--enableStorageReadApiV2&lt;/code>. Both were considered experimental and subject to change (Java) (&lt;a href="https://github.com/apache/beam/issues/26354">#26354&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow writing clustered and not time-partitioned BigQuery tables (Java) (&lt;a href="https://github.com/apache/beam/pull/30094">#30094&lt;/a>).&lt;/li>
&lt;li>Redis cache support added to RequestResponseIO and Enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/pull/30307">#30307&lt;/a>)&lt;/li>
&lt;li>Merged &lt;code>sdks/java/fn-execution&lt;/code> and &lt;code>runners/core-construction-java&lt;/code> into the main SDK. These artifacts were never meant for users, but noting
that they no longer exist. These are steps to bring portability into the core SDK alongside all other core functionality.&lt;/li>
&lt;li>Added Vertex AI Feature Store handler for Enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/pull/30388">#30388&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Arrow version was bumped to 15.0.0 from 5.0.0 (&lt;a href="https://github.com/apache/beam/pull/30181">#30181&lt;/a>).&lt;/li>
&lt;li>Go SDK users who build custom worker containers may run into issues with the move to distroless containers as a base (see Security Fixes).
&lt;ul>
&lt;li>The issue stems from distroless containers lacking additional tools, which current custom container processes may rely on.&lt;/li>
&lt;li>See &lt;a href="https://beam.apache.org/documentation/runtime/environments/#from-scratch-go">https://beam.apache.org/documentation/runtime/environments/#from-scratch-go&lt;/a> for instructions on building and using a custom container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Python SDK has changed the default value for the &lt;code>--max_cache_memory_usage_mb&lt;/code> pipeline option from 100 to 0. This option was first introduced in the 2.52.0 SDK version. This change restores the behavior of the 2.51.0 SDK, which does not use the state cache. If your pipeline uses iterable side inputs views, consider increasing the cache size by setting the option manually. (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="bug-fixes">Bug fixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &lt;code>SpannerIO.readChangeStream&lt;/code> to support propagating credentials from pipeline options
to the &lt;code>getDialect&lt;/code> calls for authenticating with Spanner (Java) (&lt;a href="https://github.com/apache/beam/pull/30361">#30361&lt;/a>).&lt;/li>
&lt;li>Reduced the number of HTTP requests in GCSIO function calls (Python) (&lt;a href="https://github.com/apache/beam/pull/30205">#30205&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Go SDK base container image moved to distroless/base-nossl-debian12, reducing vulnerable container surface to kernel and glibc (&lt;a href="https://github.com/apache/beam/pull/30011">#30011&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.55.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Crites&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Chris Gray&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Ellis&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Daria Bezkorovaina&lt;/p>
&lt;p>Dima I&lt;/p>
&lt;p>Edward Cui&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>GStravinsky&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jason Mitchell&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Ritesh Tarway&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Scott Strong&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>Ukjae Jeong (Jay)&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>dengwe1&lt;/p>
&lt;p>dhruvdua&lt;/p>
&lt;p>hardshah&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p></description></item><item><title>Blog: Apache Beam 2.54.0</title><link>/blog/beam-2.54.0/</link><pubDate>Wed, 14 Feb 2024 09:00:00 -0400</pubDate><guid>/blog/beam-2.54.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.54.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.54.0, check out the &lt;a href="https://github.com/apache/beam/milestone/18">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://s.apache.org/enrichment-transform">Enrichment Transform&lt;/a> along with GCP BigTable handler added to Python SDK (&lt;a href="https://github.com/apache/beam/pull/30001">#30001&lt;/a>).&lt;/li>
&lt;li>Beam Java Batch pipelines run on Google Cloud Dataflow will default to the Portable Runner (v2) starting with this version. (All other languages are already on Runner V2.) See &lt;a href="https://cloud.google.com/dataflow/docs/runner-v2">Runner V2 documentation&lt;/a> for how to enable or disable it intentionally.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added support for writing to BigQuery dynamic destinations with Python&amp;rsquo;s Storage Write API (&lt;a href="https://github.com/apache/beam/pull/30045">#30045&lt;/a>)&lt;/li>
&lt;li>Adding support for Tuples DataType in ClickHouse (Java) (&lt;a href="https://github.com/apache/beam/pull/29715">#29715&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to FileIO, TextIO, AvroIO (&lt;a href="https://github.com/apache/beam/pull/29670">#29670&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to BigtableIO (&lt;a href="https://github.com/apache/beam/pull/29885">#29885&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://s.apache.org/enrichment-transform">Enrichment Transform&lt;/a> along with GCP BigTable handler added to Python SDK (&lt;a href="https://github.com/apache/beam/pull/30001">#30001&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed a memory leak affecting some Go SDK since 2.46.0. (&lt;a href="https://github.com/apache/beam/pull/28142">#28142&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the &lt;code>--max_cache_memory_usage_mb=0&lt;/code> pipeline option. (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.54.0 SDKs and perform file operations on GCS might be affected by excess HTTP requests. This could lead to a performance regression or a permission issue. (&lt;a href="https://github.com/apache/beam/issues/28398">#28398&lt;/a>)&lt;/li>
&lt;li>In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.54.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Crites&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Changyu Li&lt;/p>
&lt;p>Cheskel Twersky&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>Edward Cheng&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>hugo-syn&lt;/p>
&lt;p>Issac&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Jerry Wang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>Joey Tran&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Knut Olav Løite&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>Marc&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Neeraj Bansal&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>S. Veyrié&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jian&lt;/p></description></item><item><title>Blog: Behind the Scenes: Crafting an Autoscaler for Apache Beam in a High-Volume Streaming Environment</title><link>/blog/apache-beam-flink-and-kubernetes-part3/</link><pubDate>Mon, 05 Feb 2024 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes-part3/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h3 id="introduction-to-the-design-of-our-autoscaler-for-apache-beam-jobs">Introduction to the Design of Our Autoscaler for Apache Beam Jobs&lt;/h3>
&lt;p>Welcome to the third and final part of our blog series on building a scalable, self-managed streaming infrastructure with Beam and Flink. &lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">In our previous post&lt;/a>, we delved into the scale of our streaming platforms, highlighting our capacity to manage over 40,000 streaming jobs and process upwards of 10 million events per second. This impressive scale sets the stage for the challenge we address today: the intricate task of resource allocation in a dynamic streaming environment.&lt;/p>
&lt;p>In this blog post &lt;a href="https://www.linkedin.com/in/talatuyarer/">Talat Uyarer (Architect / Senior Principal Engineer)&lt;/a> and &lt;a href="https://www.linkedin.com/in/rishabhkedia/">Rishabh Kedia (Principal Engineer)&lt;/a> describe more details about our Autoscaler. Imagine a scenario where your streaming system is inundated with fluctuating workloads. Our case presents a unique challenge, as our customers, equipped with firewalls distributed globally, generate logs at various times of the day. This results in workloads that not only vary by time but also escalate over time due to changes in settings or the addition of new cybersecurity solutions from PANW. Furthermore, updates to our codebase necessitate rolling out changes across all streaming jobs, leading to a temporary surge in demand as the system processes unprocessed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/resource-allocation.png"
alt="Resource Allocation">&lt;/p>
&lt;p>Traditionally, managing this ebb and flow of demand involves a manual, often inefficient approach. One might over-provision resources to handle peak loads, inevitably leading to resource wastage during off-peak hours. Conversely, a more cost-conscious strategy might involve accepting delays during peak times, with the expectation of catching up later. However, both methods demand constant monitoring and manual adjustment - a far from ideal situation.&lt;/p>
&lt;p>In this modern era, where automated scaling of web front-ends is a given, we aspire to bring the same level of efficiency and automation to streaming infrastructure. Our goal is to develop a system that can dynamically track and adjust to the workload demands of our streaming operations. In this blog post, we will introduce you to our innovative solution - an autoscaler designed specifically for Apache Beam jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/auto-tuned-worker.png"
alt="Auto Tuned Resource Allocation">&lt;/p>
&lt;p>For clarity, when we refer to &amp;ldquo;resources&amp;rdquo; in this context, we mean the number of Flink Task Managers, or Kubernetes Pods, that process your streaming pipeline. These Task Managers aren&amp;rsquo;t just about CPU; they also involve RAM, Network, Disk IO, and other computational resources.&lt;/p>
&lt;p>However, our solution is predicated on certain assumptions. Primarily, it&amp;rsquo;s geared towards operations processing substantial data volumes. If your workload only requires a couple of Task Managers, this system might not be the best fit. In Our case we have 10K+ workload and each each of them has different workload. Manual tuning was not an option for us. We also assume that the data is evenly distributed, allowing for increased throughput with the addition of more Task Managers. This assumption is crucial for effective horizontal scaling. While there are real-world complexities that might challenge these assumptions, for the scope of this discussion, we will focus on scenarios where these conditions hold true.&lt;/p>
&lt;p>Join us as we delve into the design and functionality of our autoscaler, a solution tailored to bring efficiency, adaptability, and a touch of intelligence to the world of streaming infrastructure.&lt;/p>
&lt;h2 id="identifying-the-right-signals-for-autoscaling">Identifying the Right Signals for Autoscaling&lt;/h2>
&lt;p>When we&amp;rsquo;re overseeing a system like Apache Beam jobs on Flink, it&amp;rsquo;s crucial to identify key signals that help us understand the relationship between our workload and resources. These signals are our guiding lights, showing us when we&amp;rsquo;re lagging behind or wasting resources. By accurately identifying these signals, we can formulate effective scaling policies and implement changes in real-time. Imagine needing to expand from 100 to 200 TaskManagers — how do we smoothly make that transition? That&amp;rsquo;s where these signals come into play.&lt;/p>
&lt;p>Remember, we&amp;rsquo;re aiming for a universal solution applicable to any workload and pipeline. While specific problems might benefit from unique signals, our focus here is on creating a one-size-fits-all approach.&lt;/p>
&lt;p>In Flink, tasks form the basic execution unit and consist of one or more operators, such as map, filter, or reduce. Flink optimizes performance by chaining these operators into single tasks when possible, minimizing overheads like thread context switching and network I/O. Your pipeline, when optimized, turns into a directed acyclic graph of stages, each processing elements based on your code. Don&amp;rsquo;t confuse stages with physical machines — they&amp;rsquo;re separate concepts. In our job we measure backlog information by using Apache Beam&amp;rsquo;s &lt;a href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/SourceMetrics.java#L32">&lt;code>backlog_bytes&lt;/code> and &lt;code>backlog_elements&lt;/code>&lt;/a> metrics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/flink-operator-chaining.png"
alt="Apache Beam Pipeline Optimization by Apache Flink">&lt;/p>
&lt;h5 id="upscaling-signals">&lt;strong>Upscaling Signals&lt;/strong>&lt;/h5>
&lt;h5 id="backlog-growth">&lt;em>Backlog Growth&lt;/em>&lt;/h5>
&lt;p>Let’s take a practical example. Consider a pipeline reading from Kafka, where different operators handle data parsing, formatting, and accumulation. The key metric here is throughput — how much data each operstor processes over time. But throughput alone isn&amp;rsquo;t enough. We need to examine the queue size or backlog at each operator. A growing backlog indicates we&amp;rsquo;re falling behind. We measure this as backlog growth — the first derivative of backlog size over time, highlighting our processing deficit.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/backlog_growth.png"
alt="Backlog Growth Calculation">&lt;/p>
&lt;h5 id="backlog-time">&lt;em>Backlog Time&lt;/em>&lt;/h5>
&lt;p>This leads us to backlog time, a derived metric that compares backlog size with throughput. It’s a measure of how long it would take to clear the current backlog, assuming no new data arrives. This helps us identify if a backlog of a certain size is acceptable or problematic, based on our specific processing needs and thresholds.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/backlog_time.png"
alt="Backlog Time Calculation">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/operator-backlog.png"
alt="Close looks at Operator Backlog">&lt;/p>
&lt;h4 id="downscaling-when-less-is-more">&lt;strong>Downscaling: When Less is More&lt;/strong>&lt;/h4>
&lt;h5 id="cpu-utilization">&lt;em>CPU Utilization&lt;/em>&lt;/h5>
&lt;p>A key signal for downscaling is CPU utilization. Low CPU utilization suggests we&amp;rsquo;re using more resources than necessary. By monitoring this, we can scale down efficiently without compromising performance.&lt;/p>
&lt;h4 id="signals-summary">&lt;strong>Signals Summary&lt;/strong>&lt;/h4>
&lt;p>In summary, the signals we&amp;rsquo;ve identified for effective autoscaling are:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Throughput:&lt;/strong> The baseline of our performance.&lt;/li>
&lt;li>&lt;strong>Backlog Growth:&lt;/strong> Indicates if we’re keeping pace with incoming data.&lt;/li>
&lt;li>&lt;strong>Backlog Time:&lt;/strong> Helps understand the severity of backlog.&lt;/li>
&lt;li>&lt;strong>CPU Utilization:&lt;/strong> Guides us in resource optimization.&lt;/li>
&lt;/ol>
&lt;p>These signals might seem straightforward, but their simplicity is key to a scalable, workload-agnostic autoscaling solution.&lt;/p>
&lt;h2 id="simplifying-autoscaling-policies-for-apache-beam-jobs-on-flink">Simplifying Autoscaling Policies for Apache Beam Jobs on Flink&lt;/h2>
&lt;p>In the world of Apache Beam jobs running on Flink, deciding when to scale up or down is a bit like being a chef in a busy kitchen. You need to keep an eye on several ingredients — your workload, virtual machines (VMs), and how they interact. It&amp;rsquo;s about maintaining a perfect balance. Our main goals? Avoid falling behind in processing (no backlog growth), ensure that any existing backlog is manageable (short backlog time), and use our resources (like CPU) efficiently.&lt;/p>
&lt;h4 id="up-scaling-keeping-up-and-catching-up">&lt;strong>Up-scaling: Keeping Up and Catching Up&lt;/strong>&lt;/h4>
&lt;p>Imagine your system is like a team of chefs working together. Here&amp;rsquo;s how we decide when to bring more chefs into the kitchen (a.k.a. upscaling):&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Keeping Up:&lt;/strong> First, we look at our current team size (number of VMs) and how much they&amp;rsquo;re processing (throughput). We then adjust our team size based on the amount of incoming orders (input rate). It&amp;rsquo;s about ensuring that our team is big enough to handle the current demand.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Catching Up:&lt;/strong> Sometimes, we might have a backlog of orders. In that case, we decide how many extra chefs we need to clear this backlog within a desired time (like 60 seconds). This part of the policy helps us get back on track swiftly.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="scaling-example-a-practical-look">&lt;strong>Scaling Example: A Practical Look&lt;/strong>&lt;/h4>
&lt;p>Let&amp;rsquo;s paint a picture with an example. Initially, we have a steady flow of orders (input rate) matching our processing capacity (throughput), so there&amp;rsquo;s no backlog. But suddenly, orders increase, and our team starts falling behind, creating a backlog. We respond by increasing our team size to match the new rate of orders. Though the backlog doesn&amp;rsquo;t grow further, it still exists. Finally, we add a few more chefs to the team, which allows us to clear the backlog quickly and return to a new, balanced state.&lt;/p>
&lt;h4 id="downscaling-when-to-reduce-resources">&lt;strong>Downscaling: When to Reduce Resources&lt;/strong>&lt;/h4>
&lt;p>Downscaling is like knowing when some chefs can take a break after a rush hour. We consider this when:&lt;/p>
&lt;ul>
&lt;li>Our backlog is low — we&amp;rsquo;ve caught up with the orders.&lt;/li>
&lt;li>The backlog isn&amp;rsquo;t growing — we&amp;rsquo;re keeping up with incoming orders.&lt;/li>
&lt;li>Our kitchen (CPU) isn&amp;rsquo;t working too hard — we&amp;rsquo;re using our resources efficiently.&lt;/li>
&lt;/ul>
&lt;p>Downscaling is all about reducing resources without affecting the quality of service. It&amp;rsquo;s about ensuring that we&amp;rsquo;re not overstaffed when the rush hour is over.&lt;/p>
&lt;h4 id="summary-a-recipe-for-effective-scaling">&lt;strong>Summary: A Recipe for Effective Scaling&lt;/strong>&lt;/h4>
&lt;p>In summary, our scaling policy is for scale up, we first ensure that the time to drain the backlog is beyond the threshold (120s) or the cpu is above the threshold (90%)&lt;/p>
&lt;p>Increasing Backlog aka Backlog Growth &amp;gt; 0 :&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/worker_require.png"
alt="Required Worker Calculation">&lt;/p>
&lt;p>Consistent Backlog aka Backlog Growth = 0:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/worker_extra.png"
alt="Extra Worker Calculation">&lt;/p>
&lt;p>To Sum up:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/worker_scaleup.png"
alt="Scale up Worker Calculation">&lt;/p>
&lt;p>To scale down, we need to ensure the machine utilization is low (&amp;lt; 70%) and there is no backlog growth and current time to drain backlog is less than the limit (10s)&lt;/p>
&lt;p>So the only driving factor to calculate the required resources after a scale down is CPU&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/cpurate_desired.png"
alt="Desired Cpu Rate Calculation">&lt;/p>
&lt;h2 id="executing-autoscaling-decision">Executing Autoscaling Decision&lt;/h2>
&lt;p>In our setup we use Reactive Mode which uses Adaptive Scheduler and Declarative Resources manager. We wanted to align resources with slots. As advised in most of the Flink documentation we set one per vCPU slot. Most of our jobs use 1 vCPU 4GB Memory combination for TaskManager.&lt;/p>
&lt;p>Reactive Mode, a unique feature of the Adaptive Scheduler, operates under the principle of one job per cluster, a rule enforced in Application Mode. In this mode, a job is configured to utilize all available resources within the cluster. Adding a TaskManager will increase the job&amp;rsquo;s scale, while removing resources will decrease it. In this setup, Flink autonomously manages the job&amp;rsquo;s parallelism, always maximizing it.&lt;/p>
&lt;p>During a rescaling event, Reactive Mode restarts the job using the most recent checkpoint. This eliminates the need for creating a savepoint, typically required for manual job rescaling. The volume of data reprocessed after rescaling is influenced by the checkpointing interval(10 seconds for us), and the time it takes to restore depends on the size of the state.&lt;/p>
&lt;p>The scheduler determines the parallelism of each operator within a job. This setting is not user-configurable and any attempts to set it, whether for individual operators or the entire job, will be overlooked.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/adaptive_scheduler_rescale.png"
alt="How Reactive Mode Works">&lt;/p>
&lt;p>Parallelism can only be influenced by setting a maximum for pipelines, which the scheduler will honor. Our maxParallelism is limited by the total count of partitions that the pipeline will process, as well as by the job itself. We cap the maximum number of TaskManagers with maxWorker count and control the job&amp;rsquo;s key count in shuffle by setting maxParallelism. Additionally, we set maxParallelism per pipeline to manage pipeline parallelism. The job cannot exceed the job&amp;rsquo;s maxParallelism in terms of workers.&lt;/p>
&lt;p>After autoscaler analysis, we will tag if the job needs to be scaled up, no action or scaled down. To interact with the job, we use a library we have built over Flink Kubernetes Operator. This library allows us to interact with our flink jobs via a simple java method call. Library will convert our method call to a kubernetes command.&lt;/p>
&lt;p>In the kubernetes world, the call will look like this for a scale up:&lt;/p>
&lt;p>&lt;code>kubectl scale flinkdeployment job-name --replicas=100&lt;/code>&lt;/p>
&lt;p>Apache Flink will handle the rest of the work needed to scale up.&lt;/p>
&lt;h2 id="maintaining-state-for-stateful-streaming-application-with-autoscaling">Maintaining State for Stateful Streaming Application with Autoscaling&lt;/h2>
&lt;p>Adapting Apache Flink&amp;rsquo;s state recovery mechanisms for autoscaling involves leveraging its robust features like max parallelism, checkpointing, and the Adaptive Scheduler to ensure efficient and resilient stream processing, even as the system dynamically adjusts to varying loads. Here&amp;rsquo;s how these components work together in an autoscaling context:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Max Parallelism&lt;/strong> sets an upper limit on how much a job can scale out, ensuring that state can be redistributed across a larger or smaller number of nodes without exceeding predefined boundaries. This is crucial for autoscaling because it allows Flink to manage state effectively, even as the number of task slots changes to accommodate varying workloads.&lt;/li>
&lt;li>&lt;strong>Checkpointing&lt;/strong> is at the heart of Flink&amp;rsquo;s fault tolerance mechanism, periodically saving the state of each job to a durable storage (in our case it is GCS bucket). In an autoscaling scenario, checkpointing enables Flink to recover to a consistent state after scaling operations. When the system scales out (adds more resources) or scales in (removes resources), Flink can restore the state from these checkpoints, ensuring data integrity and processing continuity without losing critical information. In scale down or up situations there could be a moment to reprocess data from last checkpoint. To reduce that amount we reduce the checkpointing interval to 10 seconds.&lt;/li>
&lt;li>&lt;strong>Reactive Mode&lt;/strong> is a special mode for Adaptive Scheduler, that assumes a single job per-cluster (enforced by the Application Mode). Reactive Mode configures a job so that it always uses all resources available in the cluster. Adding a TaskManager will scale up your job, removing resources will scale it down. Flink will manage the parallelism of the job, always setting it to the highest possible values. When a job undergoes resizing, Reactive Mode triggers a restart using the most recent successful checkpoint.&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this blog series, we&amp;rsquo;ve taken a deep dive into the creation of an autoscaler for Apache Beam in a high-volume streaming environment, highlighting the journey from conceptualization to implementation. This endeavor not only tackled the complexities of dynamic resource allocation but also set a new standard for efficiency and adaptability in streaming infrastructure. By marrying intelligent scaling policies with the robust capabilities of Apache Beam and Flink, we&amp;rsquo;ve showcased a scalable solution that optimizes resource use and maintains performance under varying loads. This project stands as a testament to the power of teamwork, innovation, and a forward-thinking approach to streaming data processing. As we wrap up this series, we express our gratitude to all contributors and look forward to the continuous evolution of this technology, inviting the community to join us in further discussions and developments.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;p>[1] Streaming Auto-scaling in Google Cloud Dataflow &lt;a href="https://www.infoq.com/presentations/google-cloud-dataflow/">https://www.infoq.com/presentations/google-cloud-dataflow/&lt;/a>&lt;/p>
&lt;p>[2] Pipeline lifecycle &lt;a href="https://cloud.google.com/dataflow/docs/pipeline-lifecycle">https://cloud.google.com/dataflow/docs/pipeline-lifecycle&lt;/a>&lt;/p>
&lt;p>[3] Flink Elastic Scaling &lt;a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/&lt;/a>&lt;/p>
&lt;h1 id="acknowledgements">Acknowledgements&lt;/h1>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed Flink based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitik Dani, and David He.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Explore More:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">Part 1: Introduction to Building and Managing Apache Beam Flink Services on Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes-part2/">Part 2: Build a scalable, self-managed streaming infrastructure with Flink: Tackling Autoscaling Challenges - Part 2&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Join the conversation and share your experiences on our &lt;a href="https://beam.apache.org/community/">Community&lt;/a> or contribute to our ongoing projects on &lt;a href="https://github.com/apache/beam">GitHub&lt;/a>. Your feedback is invaluable. If you have any comments or questions about this series, please feel free to reach out to us via &lt;a href="https://beam.apache.org/community/contact-us/">User Mailist&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>Stay connected with us for more updates and insights into Apache Beam, Flink, and Kubernetes.&lt;/em>&lt;/p></description></item><item><title>Blog: Apache Beam 2.53.0</title><link>/blog/beam-2.53.0/</link><pubDate>Thu, 04 Jan 2024 09:00:00 -0400</pubDate><guid>/blog/beam-2.53.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.53.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.53.0, check out the &lt;a href="https://github.com/apache/beam/milestone/17">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python streaming users that use 2.47.0 and newer versions of Beam should update to version 2.53.0, which fixes a known issue: (&lt;a href="https://github.com/apache/beam/issues/27330">#27330&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>TextIO now supports skipping multiple header lines (Java) (&lt;a href="https://github.com/apache/beam/issues/17990">#17990&lt;/a>).&lt;/li>
&lt;li>Python GCSIO is now implemented with GCP GCS Client instead of apitools (&lt;a href="https://github.com/apache/beam/issues/25676">#25676&lt;/a>)&lt;/li>
&lt;li>Adding support for LowCardinality DataType in ClickHouse (Java) (&lt;a href="https://github.com/apache/beam/pull/29533">#29533&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to KafkaIO (Java) (&lt;a href="https://github.com/apache/beam/pull/29546">#29546&lt;/a>)&lt;/li>
&lt;li>Add support for generating text embeddings in MLTransform for Vertex AI and Hugging Face Hub models.(&lt;a href="https://github.com/apache/beam/pull/29564">#29564&lt;/a>)&lt;/li>
&lt;li>NATS IO connector added (Go) (&lt;a href="https://github.com/apache/beam/issues/29000">#29000&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now type checks &lt;code>collections.abc.Collections&lt;/code> types properly. Some type hints that were erroneously allowed by the SDK may now fail. (&lt;a href="https://github.com/apache/beam/pull/29272">#29272&lt;/a>)&lt;/li>
&lt;li>Running multi-language pipelines locally no longer requires Docker.
Instead, the same (generally auto-started) subprocess used to perform the
expansion can also be used as the cross-language worker.&lt;/li>
&lt;li>Framework for adding Error Handlers to composite transforms added in Java (&lt;a href="https://github.com/apache/beam/pull/29164">#29164&lt;/a>).&lt;/li>
&lt;li>Python 3.11 images now include google-cloud-profiler (&lt;a href="https://github.com/apache/beam/pull/29651">#29561&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Euphoria DSL is deprecated and will be removed in a future release (not before 2.56.0) (&lt;a href="https://github.com/apache/beam/issues/29451">#29451&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Python) Fixed sporadic crashes in streaming pipelines that affected some users of 2.47.0 and newer SDKs (&lt;a href="https://github.com/apache/beam/issues/27330">#27330&lt;/a>).&lt;/li>
&lt;li>(Python) Fixed a bug that caused MLTransform to drop identical elements in the output PCollection (&lt;a href="https://github.com/apache/beam/issues/29600">#29600&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Upgraded to go 1.21.5 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-45285">CVE-2023-45285&lt;/a> and &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39326">CVE-2023-39326&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Potential race condition causing NPE in DataflowExecutionStateSampler in Dataflow Java Streaming pipelines (&lt;a href="https://github.com/apache/beam/issues/29987">#29987&lt;/a>).&lt;/li>
&lt;li>Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the &lt;code>--max_cache_memory_usage_mb=0&lt;/code> pipeline option. (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.54.0 SDKs and perform file operations on GCS might be affected by excess HTTP requests. This could lead to a performance regression or a permission issue. (&lt;a href="https://github.com/apache/beam/issues/28398">#28398&lt;/a>)&lt;/li>
&lt;li>In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.53.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Calvin Swenson Jr&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Georgii Zemlianyi&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jacob Tomlinson&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julian Braha&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Lawrence Qiu&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Mike Williamson&lt;/p>
&lt;p>Naireen&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Radosław Stankiewicz&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>Tom Stepp&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lrakla&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p></description></item><item><title>Blog: Scaling a streaming workload on Apache Beam, 1 million events per second and beyond</title><link>/blog/scaling-streaming-workload/</link><pubDate>Wed, 03 Jan 2024 00:00:01 -0800</pubDate><guid>/blog/scaling-streaming-workload/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/0-intro.png"
alt="Streaming Processing">&lt;/p>
&lt;p>Scaling a streaming workload is critical for ensuring that a pipeline can process large amounts of data while also minimizing latency and executing efficiently. Without proper scaling, a pipeline may experience performance issues or even fail entirely, delaying the time to insights for the business.&lt;/p>
&lt;p>Given the Apache Beam support for the sources and sinks needed by the workload, developing a streaming pipeline can be easy. You can focus on the processing (transformations, enrichments, or aggregations) and on setting the right configurations for each case.&lt;/p>
&lt;p>However, you need to identify the key performance bottlenecks and make sure that the pipeline has the resources it needs to handle the load efficiently. This can involve right-sizing the number of workers, understanding the settings needed for the source and sinks of the pipeline, optimizing the processing logic, and even determining the transport formats.&lt;/p>
&lt;p>This article illustrates how to manage the problem of scaling and optimizing a streaming workload developed in Apache Beam and run on Google Cloud using Dataflow. The goal is to reach one million events per second, while also minimizing latency and resource use during execution. The workload uses Pub/Sub as the streaming source and BigQuery as the sink. We describe the reasoning behind the configuration settings and code changes we used to help the workload achieve the desired scale and beyond.&lt;/p>
&lt;p>The progression described in this article maps to the evolution of a real-life workload, with simplifications. After the initial business requirements for the pipeline were achieved, the focus shifted to optimizing the performance and reducing the resources needed for the pipeline execution.&lt;/p>
&lt;h2 id="execution-setup">Execution setup&lt;/h2>
&lt;p>For this article, we created a test suite that creates the necessary components for the pipelines to execute. You can find the code in &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests">this Github repository&lt;/a>. You can find the subsequent configuration changes that are introduced on every run in this &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/scaling-streaming-workload-blog">folder&lt;/a> as scripts that you can run to achieve similar results.&lt;/p>
&lt;p>All of the execution scripts can also execute a Terraform-based automation to create a Pub/Sub topic and subscription as well as a BigQuery dataset and table to run the workload. Also, it launches two pipelines: one data generation pipeline that pushes events to the Pub/Sub topic, and an ingestion pipeline that demonstrates the potential improvement points.&lt;/p>
&lt;p>In all cases, the pipelines start with an empty Pub/Sub topic and subscription and an empty BigQuery table. The plan is to generate one million events per second and, after a few minutes, review how the ingestion pipeline scales with time. The data being autogenerated is based on provided schemas or IDL (or Interface Description Language) given the configuration, and the goal is to have messages ranging between 800 bytes and 2 KB, adding up to approximately 1 GB/s volume throughput. Also, the ingestion pipelines are using the same worker type configuration on all runs (&lt;code>n2d-standard-4&lt;/code> GCE machines) and are capping the maximum workers number to avoid very large fleets.&lt;/p>
&lt;p>All of the executions run on Google Cloud using Dataflow, but you can apply all of the configurations and format changes to the suite while executing on other supported Apache Beam runners. Changes and recommendations are not runner specific.&lt;/p>
&lt;h3 id="local-environment-requirements">Local environment requirements&lt;/h3>
&lt;p>Before launching the startup scripts, install the following items in your local environment:&lt;/p>
&lt;ul>
&lt;li>&lt;code>gcloud&lt;/code>, along with the correct permissions&lt;/li>
&lt;li>Terraform&lt;/li>
&lt;li>JDK 17 or later&lt;/li>
&lt;li>Maven 3.6 or later&lt;/li>
&lt;/ul>
&lt;p>For more information, see the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests#requisites">requirements&lt;/a> section in the GitHub repository.&lt;/p>
&lt;p>Also, review the service quotas and resources available in your Google Cloud project. Specifically: Pub/Sub regional capacity, BigQuery ingestion quota, and Compute Engine instances available in the selected region for the tests.&lt;/p>
&lt;h3 id="workload-description">Workload description&lt;/h3>
&lt;p>Focusing on the ingestion pipeline, our &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/canonical-streaming-pipelines/src/main/java/com/google/cloud/pso/beam/pipelines/StreamingSourceToBigQuery.java#L55">workload&lt;/a> is straightforward. It completes the following steps:&lt;/p>
&lt;ol>
&lt;li>reads data in a specific format from Pub/Sub (Apache Thrift in this case)&lt;/li>
&lt;li>deals with potential compression and batching settings (not enabled by default)&lt;/li>
&lt;li>executes a UDF (identity function by default)&lt;/li>
&lt;li>transforms the input format to one of the formats supported by the &lt;code>BigQueryIO&lt;/code> transform&lt;/li>
&lt;li>writes the data to the configured table&lt;/li>
&lt;/ol>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/0-pipeline.png"
alt="Example Workload">&lt;/p>
&lt;p>The pipeline we used for the tests is highly configurable. For more details about how to tweak the ingestion, see the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/canonical-streaming-pipelines/src/main/java/com/google/cloud/pso/beam/pipelines/StreamingSourceToBigQuery.java#L39">options&lt;/a> in the file. No code changes are needed on any of our steps. The execution scripts take care of the configurations needed.&lt;/p>
&lt;p>Although these tests are focused on reading data from Pub/Sub, the ingestion pipeline is capable of reading data from a generic streaming source. The repository contains other &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/example-suite-scripts">examples&lt;/a> that show how to launch this same test suite reading data from Pub/Sub Lite and Kafka. In all cases, the pipeline automation sets up the streaming infrastructure.&lt;/p>
&lt;p>Finally, you can see in the &lt;a href="https://github.com/prodriguezdefino/apache-beam-ptransforms/blob/a0dd229081625c7b593512543614daf995a9f870/common/src/main/java/com/google/cloud/pso/beam/common/formats/options/TransportFormatOptions.java">configuration options&lt;/a> that the pipeline supports many transport format options for the input, such as Thrift, Avro, and JSON. This suite focuses on Thrift, because it is a common open source format, and because it generates a format transformation need. The intent is to put some strain in the workload processing. You can run similar tests for Avro and JSON input data. The streaming data generator pipeline can generate random data for the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/streaming-data-generator/src/main/java/com/google/cloud/pso/beam/generator/formats">three supported formats&lt;/a> by walking directly on the schema (Avro and JSON) or IDL (Thrift) provided for execution.&lt;/p>
&lt;h2 id="first-run-default-settings">First run: default settings&lt;/h2>
&lt;p>The default values for the execution writes the data to BigQuery using &lt;code>STREAMING_INSERTS&lt;/code> mode for &lt;code>BigQueryIO&lt;/code>. This mode correlates with the &lt;a href="https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll">&lt;code>tableData insertAll&lt;/code> API&lt;/a> for BigQuery. This API supports data in JSON format. From the Apache Beam perspective, using the &lt;code>BigQueryIO.writeTableRows&lt;/code> method lets us resolve the writes into BigQuery.&lt;/p>
&lt;p>For our ingestion pipeline, the Thrift format needs to be transformed into &lt;code>TableRow&lt;/code>. To do that, we need to translate the Thrift IDL into a BigQuery table schema. That can be achieved by translating the Thrift IDL into an Avro schema, and then using Beam utilities to translate the table schema for BigQuery. We can do this at bootstrap. The schema transformation is cached at the &lt;code>DoFn&lt;/code> level.&lt;/p>
&lt;p>After setting up the data generation and ingestion pipelines, and after letting the pipelines run for some minutes, we see that the pipeline is unable to sustain the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/1-default-ps.png"
alt="PubSub metrics">&lt;/p>
&lt;p>The previous image shows that the number of messages that are not being processed by the ingestion pipeline start to show as unacknowledged messages in Pub/Sub metrics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/1-default-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Reviewing the per stage performance metrics, we see that the pipeline shows a saw-like shape, which is often associated with the throttling mechanisms the Dataflow runner uses when some of the stages are acting as bottlenecks for the throughput. Also, we see that the &lt;code>Reshuffle&lt;/code> step on the &lt;code>BigQueryIO&lt;/code> write transform does not scale as expected.&lt;/p>
&lt;p>This behavior happens because by default the &lt;a href="https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java#L57">&lt;code>BigQueryOptions&lt;/code>&lt;/a> uses 50 different keys to shuffle data to workers before the writes happen on BigQuery. To solve this problem, we can add a configuration to our launch script that enables the write operations to scale to a larger number of workers, which improves performance.&lt;/p>
&lt;h2 id="second-run-improve-the-write-bottleneck">Second run: improve the write bottleneck&lt;/h2>
&lt;p>After increasing the number of streaming keys to a higher number, 512 keys in our case, we restarted the test suite. The Pub/Sub metrics started to improve. After an initial ramp on the size of the backlog, the curve started to ease out.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/2-skeys-ps.png"
alt="PubSub metrics">&lt;/p>
&lt;p>This is good, but we should take a look at the throughput per stage numbers to understand if we are achieving the goal we set up for this exercise.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/2-skeys-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Although the performance has clearly improved, and the Pub/Sub backlog no longer increases monotonically, we are still far from the goal of processing one million events per second (1 GB/s) for our ingestion pipeline. In fact, the throughput metrics jump all over, indicating that bottlenecks are preventing the processing from scaling further.&lt;/p>
&lt;h2 id="third-run-unleash-autoscale">Third run: unleash autoscale&lt;/h2>
&lt;p>Luckily for us, when writing into BigQuery, we can autoscale the writes. This step simplifies the configuration so that we don&amp;rsquo;t have to guess the right number of shards. We switched the pipeline’s configuration and enabled this setting for the next &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/scaling-streaming-workload-blog/3-ps2bq-si-tr-streamingautoshard.sh">launch script&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-parallelism.png"
alt="Key Parallelism">&lt;/p>
&lt;p>Immediately, we see that the autosharding mechanism tweaks the number of keys very aggressively and in a dynamic way. This change is good, because different moments in time might have different scale needs, such as early backlog recoveries and spikes in the execution.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-throughput-tr.png"
alt="Throughput">&lt;/p>
&lt;p>Inspecting the throughput performance per stage, we see that as the number of keys increases, the performance of the writes also increases. In fact, it reaches very large numbers!&lt;/p>
&lt;p>After the initial backlog was consumed and the pipeline stabilized, we saw that the desired performance numbers were reached. The pipeline can sustain processing many more than a million events per second from Pub/Sub and several GB/s of BigQuery ingestion. Yay!&lt;/p>
&lt;p>Still, we want to see if we can do better. We can introduce several improvements to the pipeline to make the execution more efficient. In most cases, the improvements are configuration changes. We just need to know where to focus next.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-autoscale.png"
alt="Resources">&lt;/p>
&lt;p>The previous image shows that the number of workers needed to sustain this throughput is still quite high. The workload itself is not CPU intensive. Most of the cost is spent on transforming formats and on I/O interactions, such as shuffles and the actual writes. To understand what to improve, we first investigate the transport formats.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-input.png"
alt="Thrift Input Size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-output.png"
alt="TableRow Output Size">&lt;/p>
&lt;p>Looking at the input size, right before the identity UDF execution, the data format is binary Thrift, which is a decently compact format even when no compression is used. However, while comparing the &lt;code>PCollection&lt;/code> approximated size with the &lt;code>TableRow&lt;/code> format needed for BigQuery ingestion, a clear size increase is visible. This is something we can improve by changing the BigQuery write API in use.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-overhead.png"
alt="Translation Overhead">&lt;/p>
&lt;p>When we inspect the &lt;code>StoreInBigQuery&lt;/code> transform, we see that the majority of the wall time is spent on the actual writes. Also, the wall time spent converting data to the destination format (&lt;code>TableRows&lt;/code>) compared with how much is spent in the actual writes is quite large: 13 times bigger for the writes. To improve this behavior, we can switch the pipeline write mode.&lt;/p>
&lt;h2 id="fourth-run-in-with-the-new">Fourth run: in with the new&lt;/h2>
&lt;p>In this run, we use the &lt;code>StorageWrite&lt;/code> API. Enabling the &lt;code>StorageWrite&lt;/code> API for this pipeline is straightforward. We set the write mode as &lt;code>STORAGE_WRITE_API&lt;/code> and define a write triggering frequency. For this test, we write data at most every ten seconds. The write triggering frequency controls how long the per-stream data accumulate. A higher number defines a larger output to be written after the stream assignment but also imposes a larger end-to-end latency for every element read from Pub/Sub. Similar to the &lt;code>STREAMING_WRITES&lt;/code> configuration, &lt;code>BigQueryIO&lt;/code> can handle autosharding for the writes, which we already demonstrated to be the best setting for performance.&lt;/p>
&lt;p>After both pipelines become stable, the performance benefits seen when using the &lt;code>StorageWrite&lt;/code> API in &lt;code>BigQueryIO&lt;/code> are apparent. After enabling the new implementation, the wall time rate between the format transformation and write operation decreases. The wall time spent on writes is only about 34 percent larger than the format transformation.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-format-transformation.png"
alt="Translation Overhead">&lt;/p>
&lt;p>After stabilization, the pipeline throughput is also quite smooth. The runner can quickly and steadily downscale the pipeline resources needed to sustain the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Looking at the resource scale needed to process the data, another dramatic improvement is visible. Whereas the streaming inserts-based pipeline needed more than 80 workers to sustain the throughput, the storage writes pipeline only needs 49, a 40 percent improvement.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>We can use the data generation pipeline as reference. This pipeline only needs to randomly generate data and write the events to Pub/Sub. It runs steadily with an average of 40 workers. The improvements on the ingestion pipeline using the right configuration for the workload makes it closer to those resources needed for the generation.&lt;/p>
&lt;p>Similar to the streaming inserts-based pipeline, writing the data into BigQuery requires running a format translation, from Thrift to &lt;code>TableRow&lt;/code> in the former and from Thrift to Protocol Buffers (protobuf) in the latter. Because we are using the &lt;code>BigQueryIO.writeTableRows&lt;/code> method, we add another step in the format translation. Because the &lt;code>TableRow&lt;/code> format also increases the size of the &lt;code>PCollection&lt;/code> being processed, we want to see if we can improve this step.&lt;/p>
&lt;h2 id="fifth-run-a-better-write-format">Fifth run: a better write format&lt;/h2>
&lt;p>When using &lt;code>STORAGE_WRITE_API&lt;/code>, the &lt;code>BigQueryIO&lt;/code> transform exposes a method that we can use to write the Beam row type directly into BigQuery. This step is useful because of the flexibility that the row type provides for interoperability and schema management. Also, it&amp;rsquo;s both efficient for shuffling and denser than &lt;code>TableRow&lt;/code>, so our pipeline will have smaller &lt;code>PCollection&lt;/code> sizes.&lt;/p>
&lt;p>For the next run, because our data volume is not small, we decrease the triggering frequency when writing to BigQuery. Because we use a different format, slightly different code runs. For this change, the test pipeline script is configured with the flag &lt;code>--formatToStore=BEAM_ROW&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-input-size.png"
alt="Thrift input size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-output-size.png"
alt="Row output size">&lt;/p>
&lt;p>The &lt;code>PCollection&lt;/code> size written into BigQuery is considerably smaller than on previous executions. In fact, for this particular execution, the Beam row format is a smaller size than the Thrift format. A larger &lt;code>PCollection&lt;/code> conformed by bigger per-element sizes can put nontrivial memory pressure in smaller worker configurations, reducing the overall throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-format-trasformation.png"
alt="Translation overhead">&lt;/p>
&lt;p>The wall clock rate for the format transformation and the actual BigQuery writes also maintain a very similar rate. Handling the Beam row format does not impose a performance penalty in the format translation and subsequent writes. This is confirmed by the number of workers in use by the pipeline when throughput becomes stable, slightly smaller than the previous run but clearly in the same range.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>Although we are in a much better position than when we started, given our test pipeline input format, there&amp;rsquo;s still room for improvement.&lt;/p>
&lt;h2 id="sixth-run-further-reduce-the-format-translation-effort">Sixth run: further reduce the format translation effort&lt;/h2>
&lt;p>Another supported format for the input &lt;code>PCollection&lt;/code> in the &lt;code>BigQueryIO&lt;/code> transform might be advantageous for our input format. The method &lt;code>writeGenericRecords&lt;/code> enables the transform to transform Avro &lt;code>GenericRecords&lt;/code> directly into protobuf before the write operation. Apache Thrift can be transformed into Avro &lt;code>GenericRecords&lt;/code> very efficiently. We can make another test run configuring our test ingestion pipeline by setting the option &lt;code>--formatToStore=AVRO_GENERIC_RECORD&lt;/code> on our execution script.&lt;/p>
&lt;p>This time, the difference between format translation and writes increases significantly, improving performance. The translation to Avro &lt;code>GenericRecords&lt;/code> is only 20 percent of the write effort spent on writing those records into BigQuery. Given that the test pipelines had similar runtimes and that the wall clock seen in the &lt;code>WriteIntoBigQuery&lt;/code> stage is also aligned with other &lt;code>StorageWrite&lt;/code> related runs, using this format is appropriate for this workload.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-format-transformation.png"
alt="Translation overhead">&lt;/p>
&lt;p>We see further gains when we look at resource utilization. We need less CPU time to execute the format translations for our workload while achieving the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>This pipeline improves upon the previous run, running steadily on 42 workers when throughput is stable. Given the worker configuration used (&lt;code>nd2-standard-4&lt;/code>), and the volume throughput of the workload process (about 1 GB/s), we are achieving about 6 MB/s throughput per CPU core, which is quite impressive for a streaming pipeline with exactly-once semantics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-latencies.png"
alt="Latencies">&lt;/p>
&lt;p>When we add up all of the stages executed in the main path of the pipeline, the latency seen at this scale achieves sub-second end-to-end latencies during sustained periods of time.&lt;/p>
&lt;p>Given the workload requirements and the implemented pipeline code, this performance is the best that we can extract without further tuning the runner’s specific settings.&lt;/p>
&lt;h2 id="seventh-run--lets-just-relax-at-least-some-constraints">Seventh run : lets just relax (at least some constraints)&lt;/h2>
&lt;p>When using the &lt;code>STORAGE\_WRITE\_API&lt;/code> setting for &lt;code>BigQueryIO&lt;/code>, we enforce exactly-once semantics on the writes. This configuration is great for use cases that need strong consistency on the data that gets processed, but it imposes a performance and cost penalty.&lt;/p>
&lt;p>From a high-level perspective, writes into BigQuery are made in batches, which are released based on the current sharding and the triggering frequency. If a write fails during the execution of a particular bundle, it is retried. A bundle of data is committed into BigQuery only when all the data in that particular bundle is correctly appended to a stream. This implementation needs to shuffle the full volume of data to create the batches that are written, and also the information of the finished batches for later commit (although this last piece is very small compared with the first).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-previous-data-input.png"
alt="Read data size">&lt;/p>
&lt;p>Looking at the previous pipeline execution, the total data being processed for the pipeline by Streaming Engine is larger than the data being read from Pub/Sub. For example, 7 TB of data is read from Pub/Sub, whereas the processing of data for the whole execution of the pipeline moves 25 TB of data to and from Streaming Engine.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-previous-shuffle-total.png"
alt="Streamed data size">&lt;/p>
&lt;p>When data consistency is not a hard requirement for ingestion, you can use at-least-once semantics with &lt;code>BigQueryIO&lt;/code> write mode. This implementation avoids shuffling and grouping data for the writes. However, this change might cause a small number of repeated rows to be written into the destination table. This can happen with append errors, infrequent worker restarts, and other even less frequent errors.&lt;/p>
&lt;p>Therefore, we add the configuration to use &lt;code>STORAGE_API_AT_LEAST_ONCE&lt;/code> write mode. To instruct the &lt;code>StorageWrite&lt;/code> client to reuse connections while writing data, we also add the configuration flag &lt;code>–useStorageApiConnectionPool&lt;/code>. This configuration option only works with &lt;code>STORAGE_API_AT_LEAST_ONCE&lt;/code> mode, and it reduces the occurrences of warnings similars to &lt;code>Storage Api write delay more than 8 seconds&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-resources.png"
alt="Resources">&lt;/p>
&lt;p>When pipeline throughput stabilizes, we see a similar pattern for resource utilization for the workload. The number of workers in use reaches 40, a small improvement compared with the last run. However, the amount of data being moved from Streaming Engine is much closer to the amount of data read from Pub/Sub.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-current-input.png"
alt="Read data size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-current-shuffle-total.png"
alt="Streamed data size">&lt;/p>
&lt;p>Considering all of these factors, this change further optimizes the workload, achieving a throughput of 6.4 MB/s per CPU core. This improvement is small compared to the same workload when using consistent writes into BigQuery, but it uses less streaming data resources. This configuration represents the most optimal setup for our workload, with the highest throughput per resource and the lowest streaming data across workers.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-latency.png"
alt="Streamed data size">&lt;/p>
&lt;p>This configuration also has impressively low latency for the end-to-end processing. Given that the main path of our pipeline has been fused in a single execution stage from reads to writes, we see that even at p99, the latency tends to be below 300 milliseconds at a quite large volume throughput (as previously mentioned around 1 GB/s).&lt;/p>
&lt;h2 id="recap">Recap&lt;/h2>
&lt;p>Optimizing Apache Beam streaming workloads for low latency and efficient execution requires careful analysis and decision-making, and the right configurations.&lt;/p>
&lt;p>Considering the scenario discussed in this article, it is essential to consider factors like overall CPU utilization, throughput and latency per stage, &lt;code>PCollection&lt;/code> sizes, wall time per stage, write mode, and transport formats, in addition to writing the right pipeline for the workload.&lt;/p>
&lt;p>Our experiments revealed that using the &lt;code>StorageWrite&lt;/code> API, autosharding for writes, and Avro &lt;code>GenericRecords&lt;/code> as the transport format yielded the most efficient results. Relaxing the consistency for writes can further improve performance.&lt;/p>
&lt;p>The accompanying &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests">Github repository&lt;/a> contains a test suite that you can use to replicate the analysis on your Google Cloud project or with a different runner setup. Feel free to take it for a spin. Comments and PRs are always welcome.&lt;/p></description></item><item><title>Blog: Build a scalable, self-managed streaming infrastructure with Beam and Flink: Tackling Autoscaling Challenges - Part 2</title><link>/blog/apache-beam-flink-and-kubernetes-part2/</link><pubDate>Mon, 18 Dec 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes-part2/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink-tackling-autoscaling-challenges---part-2">Build a scalable, self-managed streaming infrastructure with Flink: Tackling Autoscaling Challenges - Part 2&lt;/h1>
&lt;p>Welcome to Part 2 of our in-depth series about building and managing a service for Apache Beam Flink on Kubernetes. In this segment, we&amp;rsquo;re taking a closer look at the hurdles we encountered while implementing autoscaling. These challenges weren&amp;rsquo;t just roadblocks. They were opportunities for us to innovate and enhance our system. Let’s break down these issues, understand their context, and explore the solutions we developed.&lt;/p>
&lt;h2 id="understand-apache-beam-backlog-metrics-in-the-flink-runner-environment">Understand Apache Beam backlog metrics in the Flink runner environment&lt;/h2>
&lt;p>&lt;strong>The Challenge:&lt;/strong> In our current setup, we are using Apache Flink for processing data streams. However, we&amp;rsquo;ve encountered a puzzling issue: our Flink job isn&amp;rsquo;t showing the backlog metrics from Apache Beam. These metrics are critical for understanding the state and performance of our data pipelines.&lt;/p>
&lt;p>&lt;strong>What We Found:&lt;/strong> Interestingly, we noticed that the metrics are actually being generated in &lt;code>KafkaIO&lt;/code>, which is a part of our data pipeline that handles Kafka streams. But when we try to monitor these metrics through the Apache Flink Metric system, we can&amp;rsquo;t find them. We suspected that there might be an issue with the integration (or &amp;lsquo;wiring&amp;rsquo;) between Apache Beam and Apache Flink.&lt;/p>
&lt;p>&lt;strong>Digging Deeper:&lt;/strong> On closer inspection, we found that the metrics should be emitted during the &amp;lsquo;Checkpointing&amp;rsquo; phase of the data stream processing. During this crucial step, the system takes a snapshot of the stream&amp;rsquo;s state, and the metrics are typically metrics that are generated for unbounded sources. Unbounded sources are sources that continuously stream data, like Kafka.&lt;/p>
&lt;p>&lt;strong>A Potential Solution:&lt;/strong> We believe the root of the problem lies in how the metric context is set during the checkpointing phase. A disconnect appears to prevent the Beam metrics from being properly captured in the Flink Metric system. We proposed a fix for this issue, which you can review and contribute to on our GitHub pull request: &lt;a href="https://github.com/apache/beam/pull/29793">Apache Beam PR #29793&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/flink-backlog-metrics.png"
alt="Apache Flink Beam Backlog Metrics">&lt;/p>
&lt;h2 id="overcoming-challenges-in-checkpoint-size-reduction-for-autoscaling-beam-jobs">Overcoming challenges in checkpoint size reduction for autoscaling Beam jobs&lt;/h2>
&lt;p>In this section we will discuss strategies for reducing the size of checkpoints in autoscaling Apache Beam jobs, focusing on efficient checkpointing in Apache Flink and optimizing bundle sizes and PipelineOptions to manage frequent checkpoint timeouts and large-scale job requirements.&lt;/p>
&lt;h3 id="understand-the-basics-of-checkpointing-in-apache-flink">Understand the basics of checkpointing in Apache Flink&lt;/h3>
&lt;p>In stream processing, maintaining state consistency and fault tolerance is crucial. Apache Flink achieves this through a process called &lt;em>checkpointing&lt;/em>. Checkpointing periodically captures the state of a job&amp;rsquo;s operators and stores it in a stable storage location, like Google Cloud Storage or AWS S3. Specifically, Flink checkpoints a job every ten seconds and allows up to one minute for this process to complete. This process is vital for ensuring that, in case of failures, the job can resume from the last checkpoint, providing exactly-once semantics and fault tolerance.&lt;/p>
&lt;h3 id="the-role-of-bundles-in-apache-beam">The role of bundles in Apache Beam&lt;/h3>
&lt;p>Apache Beam introduces the concept of a &lt;em>bundle&lt;/em>. A bundle is essentially a group of elements that are processed together. This step enhances processing efficiency and throughput by reducing the overhead of handling each element separately. For more information, see &lt;a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence">Bundling and persistence&lt;/a>. In the Flink runner &lt;a href="https://beam.apache.org/releases/javadoc/2.52.0/org/apache/beam/runners/flink/FlinkPipelineOptions.html#getMaxBundleSize--">default configuration&lt;/a>, a bundle&amp;rsquo;s default size is 1000 elements with a one-second timeout. However, based on our performance tests, we adjusted the bundle size to &lt;em>10,000 elements with a 10-second timeout&lt;/em>.&lt;/p>
&lt;h3 id="challenge-frequent-checkpoint-timeouts">Challenge: frequent checkpoint timeouts&lt;/h3>
&lt;p>When we configured checkpointing every 10 seconds, we faced frequent checkpoint timeouts, often exceeding 1 minute. This was due to the large size of the checkpoints.&lt;/p>
&lt;h3 id="solution-manage-checkpoint-size">Solution: Manage checkpoint size&lt;/h3>
&lt;p>In Apache Beam Flink jobs, the &lt;code>finishBundleBeforeCheckpointing&lt;/code> option plays a pivotal role. When enabled, it ensures that all bundles are completely processed before initiating a checkpoint. This results in checkpoints that only contain the state post-bundle completion, significantly reducing checkpoint size. Initially, our checkpoints were around 2 MB per pipeline. With this change, they consistently dropped to 150 KB.&lt;/p>
&lt;h3 id="address-the-checkpoint-size-in-large-scale-jobs">Address the checkpoint size in large-scale jobs&lt;/h3>
&lt;p>Despite reducing checkpoint sizes, a 150 KB checkpoint every ten seconds can still be substantial, especially in jobs that run multiple pipelines. For instance, with 100 pipelines in a single job, this size balloons to 15 MB per 10-second interval.&lt;/p>
&lt;h3 id="further-optimization-reduce-checkpoint-size-with-pipelineoptions">Further optimization: reduce checkpoint size with PipelineOptions&lt;/h3>
&lt;p>We discovered that due to a specific issue (BEAM-8577), our Flink runner was including our large &lt;code>PipelineOptions&lt;/code> objects in every checkpoint. We solved this problem by removing unnecessary application-related options from &lt;code>PipelineOptions&lt;/code>, further reducing the checkpoint size to a more manageable 10 KB per pipeline.&lt;/p>
&lt;h2 id="kafka-reader-wait-time-solving-autoscaling-challenges-in-beam-jobs">Kafka Reader wait time: solving autoscaling challenges in Beam jobs&lt;/h2>
&lt;h3 id="understand-unaligned-checkpointing">Understand unaligned checkpointing&lt;/h3>
&lt;p>In our system, we use unaligned checkpointing to speed up the process of checkpointing, which is essential for ensuring data consistency in distributed systems. However, when we activated the &lt;code>finishBundleBeforeCheckpointing&lt;/code> feature, we began facing checkpoint timeout issues and delays in checkpointing steps. Apache Beam leverages Apache Flink&amp;rsquo;s legacy source implementation for processing unbounded sources. In Flink, tasks are categorized into two types: source tasks and non-source tasks.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Source tasks&lt;/strong>: fetch data from external systems into a Flink job&lt;/li>
&lt;li>&lt;strong>Non-source tasks&lt;/strong>: process the incoming data&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/kafkaio-wait-reader.png"
alt="Apache Flink Task Types">&lt;/p>
&lt;p>In the standard configuration, non-source tasks check for an available buffer before pulling data. If source tasks don&amp;rsquo;t perform this check, they might experience checkpointing delays in writing data to the output buffer. This delay affects the efficiency of unaligned checkpoints, which are only recognized by legacy source tasks when an output buffer is available.&lt;/p>
&lt;h3 id="address-the-challenge-with-unboundedsourcewrapper-in-beam">Address the challenge with UnboundedSourceWrapper in Beam&lt;/h3>
&lt;p>To solve this problem, Apache Flink introduced a new source implementation that operates in a pull mode. In this mode, a task checks for a free buffer before fetching data, aligning with the approach of non-source tasks.&lt;/p>
&lt;p>However, the legacy source, still used by Apache Beam&amp;rsquo;s Flink Runner, operates in a push mode. It sends data to downstream tasks immediately. This setup might create bottlenecks when buffers are full, causing delays in detecting unaligned checkpoint barriers.&lt;/p>
&lt;h3 id="our-solution">Our solution&lt;/h3>
&lt;p>Despite its deprecation, Apache Beam&amp;rsquo;s Flink Runner still uses the legacy source implementation. To address its issues, we implemented our modifications and the quick workarounds suggested in &lt;a href="https://issues.apache.org/jira/browse/FLINK-26759">FLINK-26759&lt;/a>. These enhancements are detailed in our &lt;a href="#">Pull Request&lt;/a>. You can also find more information about unaligned checkpoint issues in the &lt;a href="https://blog.51cto.com/u_14286418/7000028">Flink Unaligned Checkpoint&lt;/a> blog post.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/checkpoint_monitoring-history-subtasks.png"
alt="Apache Flink UI Checkpoint History">&lt;/p>
&lt;h2 id="address-slow-reads-in-high-traffic-scenarios">Address slow reads in high-traffic scenarios&lt;/h2>
&lt;p>In our journey with Apache Beam and the Flink Runner, we encountered a significant challenge similar to one documented in the post &lt;a href="https://antonio-si.medium.com/how-intuit-debug-consumer-lags-in-apache-beam-22ca3b39602e">How Intuit Debug Consumer Lags in Apache Beam&lt;/a> by &lt;a href="https://antonio-si.medium.com/">Antonio Si&lt;/a> in his experience at Intuit. Their real-time data processing pipelines had increasing Kafka consumer lag, particularly with topics experiencing high message traffic. This issue was traced to Apache Beam&amp;rsquo;s handling of Kafka partitions through &lt;code>UnboundedSourceWrapper&lt;/code> and &lt;code>KafkaUnboundedReader&lt;/code>. Specifically, for topics with lower traffic, the processing thread paused unnecessarily, delaying the processing of high-traffic topics. We faced a parallel situation in our system, where the imbalance in processing speeds between high- and low-traffic topics led to inefficiencies.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/adaptive-timeout-kafka.png"
alt="UnboundedSourceWrapper Design">&lt;/p>
&lt;p>To resolve this issue, we developed an innovative solution: an adaptive timeout strategy in &lt;code>KafkaIO&lt;/code>. This strategy dynamically adjusts the timeout duration based on the traffic of each topic. For low-traffic topics, it shortens the timeout, preventing unnecessary delays. For high-traffic topics, it extends the timeout, providing more processing opportunities. This approach is detailed in our recent pull request.&lt;/p>
&lt;h2 id="unbalanced-partition-distribution-in-beam-job-autoscaling">Unbalanced partition distribution in Beam job autoscaling&lt;/h2>
&lt;p>At the heart of this system is the adaptive scheduler, a component designed for rapid resource allocation. It intelligently adjusts the number of parallel tasks (parallelism) a job performs based on the availability of computing slots. These slots are like individual workstations, each capable of handling certain parts of the job.&lt;/p>
&lt;p>However, we encountered a problem. Our jobs consist of multiple independent pipelines, each needing its own set of resources. Initially, the system tended to overburden the first few workers by assigning them more tasks, while others remained underutilized. This issue was due to the way Flink allocated tasks, favoring the first workers for each pipeline.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/flink-partition-assignment.png"
alt="Flink split assignment on slots">&lt;/p>
&lt;p>To address this issue, we developed a custom patch for Flink&amp;rsquo;s &lt;em>SlotSharingSlotAllocator&lt;/em>, a component responsible for task distribution. This patch ensures a more balanced workload distribution across all available workers, improving efficiency and preventing bottlenecks.
With this improvement, each worker gets a fair share of tasks, leading to better resource utilization and smoother operation of our Beam Jobs.&lt;/p>
&lt;h2 id="drain-support-in-kubernetes-operator-with-flink">Drain support in Kubernetes Operator with Flink&lt;/h2>
&lt;h3 id="the-challenge">The challenge&lt;/h3>
&lt;p>In the world of data processing with Apache Flink, a common task is to manage and update data-processing jobs. These jobs could be either stateful, where they remember past data, or stateless, where they don&amp;rsquo;t.&lt;/p>
&lt;p>In the past, when we needed to update or delete a Flink job managed by the Kubernetes Operator, the system saved the current state of the job using a savepoint or checkpoint. However, a crucial step was missing: the system didn&amp;rsquo;t stop the job from processing new data (this is what we mean by draining the job). This oversight could lead to two major issues:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>For stateful jobs:&lt;/strong> potential data inconsistencies, because the job might process new data that wasn&amp;rsquo;t accounted for in the savepoint&lt;/li>
&lt;li>&lt;strong>For stateless jobs:&lt;/strong> data duplication, because the job might reprocess data it already processed&lt;/li>
&lt;/ol>
&lt;h3 id="the-solution-drain-function">The solution: drain function&lt;/h3>
&lt;p>This is where the update referenced as &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> is needed. This update introduced a drain function. Think of it as telling the job, &amp;ldquo;Finish what you&amp;rsquo;re currently processing, but don&amp;rsquo;t take on anything new.&amp;rdquo; Here&amp;rsquo;s how it works:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Stop new data:&lt;/strong> The job stops reading new input.&lt;/li>
&lt;li>&lt;strong>Mark the source:&lt;/strong> The job marks the source with an infinite watermark. Think of this watermark as a marker that tells the system that there&amp;rsquo;s no more new data to process.&lt;/li>
&lt;li>&lt;strong>Propagate through the pipeline:&lt;/strong> This marker is then passed through the job&amp;rsquo;s processing pipeline, ensuring that every part of the job knows not to expect any new data.&lt;/li>
&lt;/ol>
&lt;p>This seemingly small change has a big impact. It ensures that when a job is updated or deleted, the data it processes remains consistent and accurate. This is crucial for any data-processing task, because it maintains the integrity and reliability of the data. Furthermore, in cases where the drainage fails, you can cancel the job without needing a savepoint, which adds a layer of flexibility and safety to the whole process.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>As we conclude Part 2 of our series on building and managing Apache Beam Flink services on Kubernetes, it&amp;rsquo;s evident that the journey of implementing autoscaling has been both challenging and enlightening. The obstacles we faced, from understanding Apache Beam backlog metrics in the Flink Runner environment to addressing slow reads in high-traffic scenarios, pushed us to develop innovative solutions and deepen our understanding of streaming infrastructure.&lt;/p>
&lt;p>Our exploration into the intricacies of checkpointing, Kafka Reader wait times, and unbalanced partition distribution revealed the complexities of autoscaling Beam jobs. These challenges prompted us to devise strategies like the adaptive timeout in &lt;code>KafkaIO&lt;/code> and the balanced workload distribution in Flink&amp;rsquo;s &lt;code>SlotSharingSlotAllocator&lt;/code>. Additionally, the introduction of the drain support in Kubernetes Operator with Flink marks a significant advancement in managing stateful and stateless jobs effectively.&lt;/p>
&lt;p>This journey has not only enhanced the robustness and efficiency of our system but has also contributed valuable insights to the broader community working with Apache Beam and Flink. We hope that our experiences and solutions will aid others facing similar challenges in their projects.&lt;/p>
&lt;p>Stay tuned for our next blog post, where we&amp;rsquo;ll delve into the specifics of autoscaling in Apache Beam. We&amp;rsquo;ll break down the concepts, strategies, and best practices to effectively scale your Beam jobs. Thank you for following our series, and we look forward to sharing more of our journey and learnings with you.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed, Flink-based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitk Dani, and David He.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Explore More:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">Part 1: Introduction to Building and Managing Apache Beam Flink Services on Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Join the conversation and share your experiences on our &lt;a href="https://beam.apache.org/community/">Community&lt;/a> or contribute to our ongoing projects on &lt;a href="https://github.com/apache/beam">GitHub&lt;/a>. Your feedback is invaluable. If you have any comments or questions about this series, please feel free to reach out to us via &lt;a href="https://beam.apache.org/community/contact-us/">User Mailist&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>Stay connected with us for more updates and insights into Apache Beam, Flink, and Kubernetes.&lt;/em>&lt;/p></description></item><item><title>Blog: Apache Beam 2.52.0</title><link>/blog/beam-2.52.0/</link><pubDate>Fri, 17 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.52.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.52.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2520-2023-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.52.0, check out the &lt;a href="https://github.com/apache/beam/milestone/16">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Previously deprecated Avro-dependent code (Beam Release 2.46.0) has been finally removed from Java SDK &amp;ldquo;core&amp;rdquo; package.
Please, use &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead. This will allow to easily update Avro version in user code without
potential breaking changes in Beam &amp;ldquo;core&amp;rdquo; since the Beam Avro extension already supports the latest Avro versions and
should handle this. (&lt;a href="https://github.com/apache/beam/issues/25252">#25252&lt;/a>).&lt;/li>
&lt;li>Publishing Java 21 SDK container images now supported as part of Apache Beam release process. (&lt;a href="https://github.com/apache/beam/issues/28120">#28120&lt;/a>)
&lt;ul>
&lt;li>Direct Runner and Dataflow Runner support running pipelines on Java21 (experimental until tests fully setup). For other runners (Flink, Spark, Samza, etc) support status depend on runner projects.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add &lt;code>UseDataStreamForBatch&lt;/code> pipeline option to the Flink runner. When it is set to true, Flink runner will run batch
jobs using the DataStream API. By default the option is set to false, so the batch jobs are still executed
using the DataSet API.&lt;/li>
&lt;li>&lt;code>upload_graph&lt;/code> as one of the Experiments options for DataflowRunner is no longer required when the graph is larger than 10MB for Java SDK (&lt;a href="https://github.com/apache/beam/pull/28621">PR#28621&lt;/a>).&lt;/li>
&lt;li>state amd side input cache has been enabled to a default of 100 MB. Use &lt;code>--max_cache_memory_usage_mb=X&lt;/code> to provide cache size for the user state API and side inputs. (Python) (&lt;a href="https://github.com/apache/beam/issues/28770">#28770&lt;/a>).&lt;/li>
&lt;li>Beam YAML stable release. Beam pipelines can now be written using YAML and leverage the Beam YAML framework which includes a preliminary set of IO&amp;rsquo;s and turnkey transforms. More information can be found in the YAML root folder and in the &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/README.md">README&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>org.apache.beam.sdk.io.CountingSource.CounterMark&lt;/code> uses custom &lt;code>CounterMarkCoder&lt;/code> as a default coder since all Avro-dependent
classes finally moved to &lt;code>extensions/avro&lt;/code>. In case if it&amp;rsquo;s still required to use &lt;code>AvroCoder&lt;/code> for &lt;code>CounterMark&lt;/code>, then,
as a workaround, a copy of &amp;ldquo;old&amp;rdquo; &lt;code>CountingSource&lt;/code> class should be placed into a project code and used directly
(&lt;a href="https://github.com/apache/beam/issues/25252">#25252&lt;/a>).&lt;/li>
&lt;li>Renamed &lt;code>host&lt;/code> to &lt;code>firestoreHost&lt;/code> in &lt;code>FirestoreOptions&lt;/code> to avoid potential conflict of command line arguments (Java) (&lt;a href="https://github.com/apache/beam/pull/29201">#29201&lt;/a>).&lt;/li>
&lt;li>Transforms which use &lt;code>SnappyCoder&lt;/code> are update incompatible with previous versions of the same transform (Java) on some runners. This includes PubSubIO&amp;rsquo;s read (&lt;a href="https://github.com/apache/beam/pull/28655#issuecomment-2407839769">#28655&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &amp;ldquo;Desired bundle size 0 bytes must be greater than 0&amp;rdquo; in Java SDK&amp;rsquo;s BigtableIO.BigtableSource when you have more cores than bytes to read (Java) &lt;a href="https://github.com/apache/beam/issues/28793">#28793&lt;/a>.&lt;/li>
&lt;li>&lt;code>watch_file_pattern&lt;/code> arg of the &lt;a href="https://github.com/apache/beam/blob/104c10b3ee536a9a3ea52b4dbf62d86b669da5d9/sdks/python/apache_beam/ml/inference/base.py#L997">RunInference&lt;/a> arg had no effect prior to 2.52.0. To use the behavior of arg &lt;code>watch_file_pattern&lt;/code> prior to 2.52.0, follow the documentation at &lt;a href="https://beam.apache.org/documentation/ml/side-input-updates/">https://beam.apache.org/documentation/ml/side-input-updates/&lt;/a> and use &lt;code>WatchFilePattern&lt;/code> PTransform as a SideInput. (&lt;a href="https://github.com/apache/beam/pulls/28948">#28948&lt;/a>)&lt;/li>
&lt;li>&lt;code>MLTransform&lt;/code> doesn&amp;rsquo;t output artifacts such as min, max and quantiles. Instead, &lt;code>MLTransform&lt;/code> will add a feature to output these artifacts as human readable format - &lt;a href="https://github.com/apache/beam/issues/29017">#29017&lt;/a>. For now, to use the artifacts such as min and max that were produced by the eariler &lt;code>MLTransform&lt;/code>, use &lt;code>read_artifact_location&lt;/code> of &lt;code>MLTransform&lt;/code>, which reads artifacts that were produced earlier in a different &lt;code>MLTransform&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/29016/">#29016&lt;/a>)&lt;/li>
&lt;li>Fixed a memory leak, which affected some long-running Python pipelines: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &lt;a href="https://www.cve.org/CVERecord?id=CVE-2023-39325">CVE-2023-39325&lt;/a> (Java/Python/Go) (&lt;a href="https://github.com/apache/beam/issues/29118">#29118&lt;/a>).&lt;/li>
&lt;li>Mitigated &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47248">CVE-2023-47248&lt;/a> (Python) &lt;a href="https://github.com/apache/beam/issues/29392">#29392&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known issues&lt;/h2>
&lt;ul>
&lt;li>MLTransform drops the identical elements in the output PCollection. For any duplicate elements, a single element will be emitted downstream. (&lt;a href="https://github.com/apache/beam/issues/29600">#29600&lt;/a>).&lt;/li>
&lt;li>Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the &lt;code>--max_cache_memory_usage_mb=0&lt;/code> pipeline option. (Python) (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;li>Users who lauch Python pipelines in an environment without internet access and use the &lt;code>--setup_file&lt;/code> pipeline option might experience an increase in pipeline submission time. This has been fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/31070">#31070&lt;/a>).&lt;/li>
&lt;li>Transforms which use &lt;code>SnappyCoder&lt;/code> are update incompatible with previous versions of the same transform (Java) on some runners. This includes PubSubIO&amp;rsquo;s read (&lt;a href="https://github.com/apache/beam/pull/28655#issuecomment-2407839769">#28655&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.52.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Devansh Modi&lt;/p>
&lt;p>Dominik Dębowczyk&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Jiangjie Qin&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>Joar Wandborg&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Luís Bianchin&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vivek Sumanth&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>aku019&lt;/p>
&lt;p>brucearctor&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>ddebowczyk92&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>dpcollins-google&lt;/p>
&lt;p>edman124&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>illoise&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p></description></item><item><title>Blog: Contributor Spotlight: Johanna Öjeling</title><link>/blog/contributor-spotlight-johanna-ojeling/</link><pubDate>Sat, 11 Nov 2023 15:00:00 -0800</pubDate><guid>/blog/contributor-spotlight-johanna-ojeling/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Johanna Öjeling is a Senior Software Engineer at &lt;a href="https://normative.io/">Normative&lt;/a>. She started using Apache Beam in 2020 at her previous company &lt;a href="http://datatonic.com">Datatonic&lt;/a> and began contributing in 2022 at a personal capacity. We interviewed Johanna to learn more about her interests and we hope that this will inspire new, future, diverse set of contributors to participate in OSS projects.&lt;/p>
&lt;p>&lt;strong>What areas of interest are you passionate about in your career?&lt;/strong>&lt;/p>
&lt;p>My core interest lies in distributed and data-intensive systems, and I enjoy working on challenges related to performance, scalability and maintainability. I also feel strongly about developer experience, and like to build tools and frameworks that make developers happier and more productive. Aside from that, I take pleasure in mentoring and coaching other software engineers to grow their skills and pursue a fulfilling career.&lt;/p>
&lt;p>&lt;strong>What motivated you to make your first contribution?&lt;/strong>&lt;/p>
&lt;p>I was already a user of the Apache Beam Java and Python SDKs and Google Cloud Dataflow in my previous job, and had started to play around with the Go SDK to learn Go. When I noticed that a feature I wanted was missing, it seemed like a great opportunity to implement it. I had been curious about developing open source software for some time, but did not have a good idea until then of what to contribute with.&lt;/p>
&lt;p>&lt;strong>In which way have you contributed to Apache Beam?&lt;/strong>&lt;/p>
&lt;p>I have primarily worked on the Go SDK with implementation of new features, bug fixes, tests, documentation and code reviews. Some examples include a MongoDB I/O connector with dynamically scalable reads and writes, a file I/O connector supporting continuous file discovery, and an Amazon S3 file system implementation.&lt;/p>
&lt;p>&lt;strong>How has your open source engagement impacted your personal or professional growth?&lt;/strong>&lt;/p>
&lt;p>Contributing to open source is one of the best decisions I have taken professionally. The Beam community has been incredibly welcoming and appreciative, and it has been rewarding to collaborate with talented people around the world to create software that is free for anyone to benefit from. Open source has opened up new opportunities to challenge myself, dive deeper into technologies I like, and learn from highly skilled professionals. To me, it has served as an outlet for creativity, problem solving and purposeful work.&lt;/p>
&lt;p>&lt;strong>How have you noticed contributing to open source is different from contributing to closed source/proprietary software?&lt;/strong>&lt;/p>
&lt;p>My observation has been that there are higher requirements for software quality in open source, and it is more important to get things right the first time. My closed source software experience is from startups/scale-ups where speed is prioritized. When not working on public facing APIs or libraries, one can also more easily change things, whereas we need to be mindful about breaking changes in Beam. I care for software quality and value the high standards the Beam committers hold.&lt;/p>
&lt;p>&lt;strong>What do you like to do with your spare time when you&amp;rsquo;re not contributing to Beam?&lt;/strong>&lt;/p>
&lt;p>Coding is a passion of mine so I tend to spend a lot of my free time on hobby projects, reading books and articles, listening to talks and attending events. When I was younger I loved learning foreign languages and studied English, French, German and Spanish. Later I discovered an interest in computer science and switched focus to programming languages. I decided to change careers to software engineering and have tried to learn as much as possible ever since. I love that it never ends.&lt;/p>
&lt;p>&lt;strong>What future features/improvements are you most excited about, or would you like to see on Beam?&lt;/strong>&lt;/p>
&lt;p>The multi-language pipeline support is an impressive feature of Beam, and I like that new SDKs such as TypeScript and Swift are emerging, which enables developers to write pipelines in their preferred language. Naturally, I am also excited to see where the Go SDK is headed and how we can make use of newer features of the Go language.&lt;/p>
&lt;p>&lt;strong>What types of contributions or support do you think the Beam community needs more of?&lt;/strong>&lt;/p>
&lt;p>Many data and machine learning engineers feel more comfortable with Python than Java and wish the Python SDK were as feature rich as the Java SDK. This presents great opportunities for Python developers to start contributing to Beam. As an SDK author, one can take advantage of Beam&amp;rsquo;s multiple SDKs. When I have developed in Go I have often studied the Java and Python implementations to get ideas for how to solve specific problems and make sure the Go SDK follows a similar pattern.&lt;/p>
&lt;p>&lt;strong>What advice would you give to someone who wants to contribute but does not know where to begin?&lt;/strong>&lt;/p>
&lt;p>Start with asking yourself what prior knowledge you have and what you would like to learn, then look for opportunities that match that. The contribution guidelines will tell you where to find open issues and what the process looks like. There are tasks labeled as &amp;ldquo;good first issue&amp;rdquo; which can be a good starting point. I was quite nervous about making my first contribution and had my mentor pre-review my PR. There was no need to worry though, as people will be grateful for your effort to improve the project. The pride I felt when a committer approved my PR and welcomed me to Beam is something I still remember.&lt;/p>
&lt;p>&lt;strong>What advice would you give to the Beam community? What could we improve?&lt;/strong>&lt;/p>
&lt;p>We can make it easier for new community members to get involved by providing more examples of tasks that we need help with, both in the form of code and non-code contributions. I will take it as an action point myself to label more issues accordingly and tailor the descriptions for newcomers. However, this is contingent on community members visiting the GitHub project. To address this, we could also proactively promote opportunities through social channels and the user mailing list.&lt;/p>
&lt;p>&lt;em>We thank Johanna for the interview and for her contributions! If you would like to learn more about contributing to Beam you can learn more about it here: &lt;a href="https://beam.apache.org/contribute/">https://beam.apache.org/contribute/&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Blog: Build a scalable, self-managed streaming infrastructure with Beam and Flink</title><link>/blog/apache-beam-flink-and-kubernetes/</link><pubDate>Fri, 03 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog series, &lt;a href="https://www.linkedin.com/in/talatuyarer/">Talat Uyarer (Architect / Senior Principal Engineer)&lt;/a>, &lt;a href="https://www.linkedin.com/in/rishabhkedia/">Rishabh Kedia (Principal Engineer)&lt;/a>, and &lt;a href="https://www.linkedin.com/in/davidqhe/">David He (Engineering Director)&lt;/a> describe how we built a self-managed streaming platform by using Apache Beam and Flink. In this part of the series, we describe why and how we built a large-scale, self-managed streaming infrastructure and services based on Flink by migrating from a cloud managed streaming service. We also outline the learnings for operational scalability and observability, performance, and cost effectiveness. We summarize techniques that we found useful in our journey.&lt;/p>
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink---part-1">Build a scalable, self-managed streaming infrastructure with Flink - part 1&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Palo Alto Networks (PANW) is a leader in cybersecurity, providing products, services and solutions to our customers. Data is the center of our products and services. We stream and store exabytes of data in our data lake, with near real-time ingestion, data transformation, data insertion to data store, and forwarding data to our internal ML-based systems and external SIEM’s. We support multi-tenancy in each component so that we can isolate tenants and provide optimal performance and SLA. Streaming processing plays a critical role in the pipelines.&lt;/p>
&lt;p>In the second part of the series, we provide a more thorough description of the core building blocks of our streaming infrastructure, such as autoscaler. We also give more details about our customizations, which enabled us to build a high-performance, large-scale streaming system. Finally, we explain how we solved challenging problems.&lt;/p>
&lt;h2 id="the-importance-of-self-managed-streaming-infrastructure">The importance of self-managed streaming Infrastructure&lt;/h2>
&lt;p>We built a large-scale data platform on Google Cloud. We used Dataflow as a managed streaming service. With Dataflow, we used the streaming engine running our application using Apache Beam and observability tools such as Cloud Logging and Cloud Monitoring. For more details, see [1]. The system can handle 15 million of events per second and one trillion events daily, at four petabytes of data volume daily. We run about 30,000 Dataflow jobs. Each job can have one or hundreds of workers, depending on the customer’s event throughputs.&lt;/p>
&lt;p>We support various applications using different endpoints: BigQuery data store, HTTPS-based external SIEMs or internal endpoints, Syslog based SIEMs, and Google Cloud Storage endpoints. Our customers and products rely on this data platform to handle cybersecurity postures and reactions. Our streaming infrastructure is highly flexible to add, update, and delete use cases through a streaming job subscription. For example, a customer wants to ingest log events from a firewall device into the data lake buffered in Kafka topics. A streaming job is subscribed to extract and filter the data, transform the data format, and do a streaming insert to our BigQuery data warehouse endpoint in real-time. The customer can use our visualization and dashboard products to view traffic or threads captured by this firewall. The following diagram illustrates the event producer, the use case subscription workflow, and the key components of the streaming platform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/image1.png"
alt="Streaming service design">&lt;/p>
&lt;p>This managed, Dataflow-based streaming infrastructure runs fine, but with some caveats:&lt;/p>
&lt;ol>
&lt;li>Cost is high, because it is a managed service. For the same resources used in a Dataflow application, such as vCPU and memory, the cost is much more expensive than using an open source streaming engine such as Flink running the same Beam application code.&lt;/li>
&lt;li>It&amp;rsquo;s not easy to achieve our latency and SLA goals, because it&amp;rsquo;s difficult to extend features, such as autoscaling based on different applications, endpoints, or different parameters within one application.&lt;/li>
&lt;li>The pipeline only runs on Google Cloud.&lt;/li>
&lt;/ol>
&lt;p>The uniqueness of PANW’s streaming use cases is another reason that we use a self-managed service. We support multi-tenancy. A tenant (a customer) can ingest data at a very high rate (&amp;gt;100k requests per second), or at a very low rate (&amp;lt; 100 requests per second). A Dataflow job runs on VMs instead of Kubernetes, requiring a minimal one vCPU core. With a small tenant, this wastes resources. Our streaming infrastructure supports thousands of jobs, and the CPU utilization is more efficient if we do not have to use one core for a job. It is natural for us to use a streaming engine running on Kubernetes, so that we can allocate minimal resources for a small tenant, for example, using a Google Kubernetes Engine (GKE) pod with ½ or less vCPU core.&lt;/p>
&lt;h2 id="the-choice-of-apache-flink-and-kubernetes">The choice of Apache Flink and Kubernetes&lt;/h2>
&lt;p>In an effort to handle the problems already stated and to find the most efficient solution, we evaluated various streaming frameworks, including Apache Samza, Apache Flink, and Apache Spark, against Dataflow.&lt;/p>
&lt;h3 id="performance">Performance&lt;/h3>
&lt;ul>
&lt;li>One notable factor was Apache Flink’s native Kubernetes support. Unlike Samza, which lacked native Kubernetes support and required Apache Zookeeper for coordination, Flink seamlessly integrated with Kubernetes. This integration eliminated unnecessary complexities. In terms of performance, both Samza and Flink were close competitors.&lt;/li>
&lt;li>Apache Spark, while popular, proved to be significantly slower in our tests. A presentation at the Beam Summit revealed that Apache Beam’s Spark Runner was approximately ten times slower than Native Apache Spark [3]. We could not afford such a drastic performance hit. Rewriting our entire Beam codebase with native Spark was not a viable option, especially given the extensive codebase we had built over the past four years with Apache Beam.&lt;/li>
&lt;/ul>
&lt;h3 id="community">Community&lt;/h3>
&lt;p>The robustness of community support played a pivotal role in our decision making. Dataflow provided excellent support, but we needed assurance in our choice of an open-source framework. Apache Flink’s vibrant community and active contributions from multiple companies offered a level of confidence that was unmatched. This collaborative environment meant that bug identification and fixes were ongoing processes. In fact, in our journey, we have patched our system using many Flink fixes from the community:&lt;/p>
&lt;ul>
&lt;li>We fixed the Google Cloud Storage file reading exceptions by merging Flink 1.15 open source fix &lt;a href="https://issues.apache.org/jira/browse/FLINK-26063?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&amp;amp;focusedCommentId=17504555#comment-17504555">FLINK-26063&lt;/a> (we are using 1.13).&lt;/li>
&lt;li>We fixed an issue with workers restarting for stateful jobs from &lt;a href="https://issues.apache.org/jira/browse/FLINK-31963">FLINK-31963&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>We also contributed to the community during our journey by founding and fixing bugs in the open source code. For details, see &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> for Flink Kubernetes Operator. We also created a new GKE Auth support for Kubernetes clients and merged it to GitHub at [4].&lt;/p>
&lt;h3 id="integration">Integration&lt;/h3>
&lt;p>The seamless integration of Apache Flink with Kubernetes provided us with a flexible and scalable platform for orchestration. The synergy between Apache Flink and Kubernetes not only optimized our data processing workflows but also future-proofed our system.&lt;/p>
&lt;h2 id="architecture-and-deployment-workflow">Architecture and deployment workflow&lt;/h2>
&lt;p>In the realm of real-time data processing and analytics, Apache Flink distinguishes itself as a powerful and versatile framework. When combined with Kubernetes, the industry-standard container orchestration system, Flink applications can scale horizontally and have robust management capabilities. We explore a cutting-edge design where Apache Flink and Kubernetes synergize seamlessly, thanks to the Apache Flink Kubernetes Operator.&lt;/p>
&lt;p>At its core, the Flink Kubernetes Operator serves as a control plane, mirroring the knowledge and actions of a human operator managing Flink deployments. Unlike traditional methods, the Operator automates critical activities, from starting and stopping applications to handling upgrades and errors. Its versatile feature set includes fully-automated job lifecycle management, support for different Flink versions, and multiple deployment modes, such as application clusters and session jobs. Moreover, the Operator&amp;rsquo;s operational prowess extends to metrics, logging, and even dynamic scaling by using the Job Autoscaler.&lt;/p>
&lt;h3 id="build-a-seamless-deployment-workflow">Build a seamless deployment workflow&lt;/h3>
&lt;p>Imagine a robust system where Flink jobs are deployed effortlessly, monitored diligently, and managed proactively. Our team created this workflow by integrating Apache Flink, Apache Flink Kubernetes Operator, and Kubernetes. Central to this setup is our custom-built Apache Flink Kubernetes Operator Client Library. This library acts as a bridge, enabling atomic operations such as starting, stopping, updating, and canceling Flink jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/stream-service-changes.png"
alt="Streaming service changes">&lt;/p>
&lt;h3 id="the-deployment-process">The deployment process&lt;/h3>
&lt;p>In our code, the client provides Apache Beam pipeline options, which include essential information such as the Kubernetes cluster&amp;rsquo;s API endpoint, authentication details, the Google Cloud/S3 temporary location for uploading the JAR file, and worker type specifications. The Kubernetes Operator Library uses this information to orchestrate a seamless deployment process. The following sections explain the steps taken. Most of the core steps are automated in our code base.&lt;/p>
&lt;p>&lt;strong>Step 1:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>The client wants to start a job for a customer and a specific application.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 2:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Generate a unique job ID:&lt;/strong> The library generates a unique job ID, which is set as a Kubernetes label. This identifier helps track and manage the deployed Flink job.&lt;/li>
&lt;li>&lt;strong>Configuration and code upload:&lt;/strong> The library uploads all necessary configurations and user code to a designated location on Google Cloud Storage or Amazon S3. This step ensures that the Flink application&amp;rsquo;s resources are available for deployment.&lt;/li>
&lt;li>&lt;strong>YAML payload generation:&lt;/strong> After the upload process completes, the library constructs a YAML payload. This payload contains crucial deployment information, including resource settings based on the specified worker type.&lt;/li>
&lt;/ul>
&lt;p>We used a convention for naming our worker VM instance types. Our convention is similar to the naming convention that Google Cloud uses. The name &lt;code>n1-standard-1&lt;/code> refers to a specific, predefined VM machine type. Let’s break down what each component of the name means:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>n1&lt;/strong> indicates the CPU type of the instance. In this case, it refers to the Intel based on instances in the N1 series. Google Cloud has multiple generations of instances with varying hardware and performance characteristics.&lt;/li>
&lt;li>&lt;strong>standard&lt;/strong> signifies the machine type family. Standard machine types offer a balanced ratio of 1 virtual CPU (vCPUs) and 4 GB of memory for Task Manager, and 0.5 vCPU and 2 GB memory for Job Manager.&lt;/li>
&lt;li>&lt;strong>1&lt;/strong> represents the number of vCPUs available in the instance. In the case of n1-standard-1, it means the instance has 1 vCPU.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 3:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Calling the Kubernetes API with Fabric8&lt;/strong>: To initiate the deployment, the library interacts with the Kubernetes API using Fabric8. Fabric8 initially lacked support for authentication in Google Kubernetes Engine or Amazon Elastic Kubernetes Service (EKS). To address this limitation, our team implemented the necessary authentication support, which can be found in our merge request on GitHub PR [4].&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 4:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flink Operator deployment&lt;/strong>: When it receives the YAML payload, the Flink Operator takes charge of deploying the various components of the Flink job. Tasks include provisioning resources and managing the deployment of the Flink Job Manager, Task Manager, and Job Service.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 5:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Job submission and execution&lt;/strong>: When the Flink Job Manager is running, it fetches the JAR file and configurations from the designated Google Cloud Storage or S3 location. With all necessary resources in place, it submits the Flink job to the standalone Flink cluster for execution.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 6&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Continuous monitoring&lt;/strong>: Post-deployment, our operator continuously monitors the status of the running Flink job. This real-time feedback loop enables us to promptly address any issues that arise, ensuring the overall health and optimal performance of our Flink applications.&lt;/li>
&lt;/ul>
&lt;p>In summary, our deployment process leverages Apache Beam pipeline options, integrates seamlessly with Kubernetes and the Flink Operator, and employs custom logic to handle configuration uploads and authentication. This end-to-end workflow ensures a reliable and efficient deployment of Flink applications in Kubernetes clusters while maintaining vigilant monitoring for smooth operation. The following sequence diagram shows the steps.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-start-activity-diagram.png"
alt="Job Start Activity Diagram">&lt;/p>
&lt;h2 id="develope-an-autoscaler">Develope an autoscaler&lt;/h2>
&lt;p>Having an autoscaler is critical to having a self-managed streaming service. There are not enough resources available on the internet for us to learn to build our own autoscaler, which makes this part of the workflow difficult.&lt;/p>
&lt;p>The autoscaler scales up the number of task managers to drain the lag and to keep up with the throughput. It also scales down the minimum number of resources required to process the incoming traffic to reduce costs. We need to do this frequently while keeping the processing disruption to minimum.&lt;/p>
&lt;p>We extensively tuned the autoscaler to meet the SLA for latency. This tuning involved a cost trade off. We also made the autoscaler application-specific to meet specific needs for certain applications. Every decision has a hidden cost. The second part of this blog provides more details about the autoscaler.&lt;/p>
&lt;h2 id="create-a-client-library-for-steaming-job-development">Create a client library for steaming job development&lt;/h2>
&lt;p>To deploy the job using the Flink Kubernetes Operator, you need to know about how Kubernetes works. The following steps explain how to create a single Flink job.&lt;/p>
&lt;ol>
&lt;li>Define a YAML file with proper specifications. The following image provides an example.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink.apache.org/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FlinkDeployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">basic-reactive-example&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink:1.13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1_13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">scheduler-mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">REACTIVE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskmanager.numberOfTaskSlots&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.savepoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/savepoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.checkpoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/checkpoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability.storageDir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/ha&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">serviceAccount&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jobManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-main-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeMounts&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/flink-data&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hostPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># directory location on host&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/tmp/flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># this field is optional&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Directory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">job&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jarURI&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">local:///opt/flink/examples/streaming/StateMachineExample.jar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">parallelism&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">upgradeMode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">savepoint&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">running&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">savepointTriggerNonce&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">standalone&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>SSH into your Flink cluster and run the command following command:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl create -f job1.yaml
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Use the following command to check the status of the job:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl get flinkdeployment job1
&lt;/code>&lt;/pre>&lt;p>This process impacts our scalability. Because we frequently update our jobs, we can&amp;rsquo;t manually follow these steps for every running job. To do so would be highly error prone and time consuming. One wrong space in the YAML can fail the deployment. This approach also acts as a barrier to innovation, because you need to know Kubernetes to interact with Flink jobs.&lt;/p>
&lt;p>We built a library to provide an interface for any teams and applications that want to to start, delete, update, or get the status of their jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/fko-library.png"
alt="Flink Kubernetes Operator Library">&lt;/p>
&lt;p>This library extends the Fabric8 client and FlinkDeployment CRD. FlinkDeployment CRD is exposed by the Flink Kubernetes Operator. CRD lets you store and retrieve structured data. By extending the CRD, we get access to POJO, making it easier to manipulate the YAML file.&lt;/p>
&lt;p>The library supports the following tasks:&lt;/p>
&lt;ol>
&lt;li>Authentication to ensure that you are allowed to perform actions on the Flink cluster.&lt;/li>
&lt;li>Validation (fetches the template from AWS/Google Cloud Storage for validation) takes user variable input and validates it against the policy, rules, YAML format.&lt;/li>
&lt;li>Action execution converts the Java call to invoke the Kubernetes operation.&lt;/li>
&lt;/ol>
&lt;p>During this process, we learned the following lessons:&lt;/p>
&lt;ol>
&lt;li>App specific operator service: At our large scale, the operator was unable to handle such a large number of jobs. Kubernetes calls started to time out and fail. To solve this problem, we created multiple operators (about 4) in high-traffic regions to handle each application.&lt;/li>
&lt;li>Kube call caching: To prevent overloading, we cached the results of Kubernetes calls for thirty to sixty seconds.&lt;/li>
&lt;li>Label support: Providing label support to search jobs using client-specific variables reduced the load on Kube and improved the job search speed by 5x.&lt;/li>
&lt;/ol>
&lt;p>The following are some of the biggest wins we achieved by exposing the library:&lt;/p>
&lt;ol>
&lt;li>Standardized job management: Users can start, delete, and get status updates for their Flink jobs in a Kubernetes environment using a single library.&lt;/li>
&lt;li>Abstracted Kubernetes complexity: Teams no longer need to worry about the inner workings of Kubernetes or the formatting job deployment YAML files. The library handles these details internally.&lt;/li>
&lt;li>Simplified upgrades: With the underlying Kubernetes infrastructure, the library brings robustness and fault tolerance to Flink job management, ensuring minimal downtime and efficient recovery.&lt;/li>
&lt;/ol>
&lt;h2 id="observability-and-alerting">Observability and alerting&lt;/h2>
&lt;p>Observability is important when runing a production system at a large scale. We have about 30,000 streaming jobs in PANW. Each job serves a customer for a specific application. Each job also reads data from multiple topics in Kafka, performs transformations, and then writes the data to various sinks and endpoints.&lt;/p>
&lt;p>Constraints can occur anywhere in the pipeline or its endpoints, such as the customer API, BigQuery, and so on. We want to make sure the latency of streaming meets the SLA. Therefore, understanding if a job is healthy, meeting SLA, and alerting and intervening when needed is very challenging.&lt;/p>
&lt;p>To achieve our operational goals, we built a sophisticated observability and alerting capability. We provide three kinds of observability and debugging tools, described in the following sections.&lt;/p>
&lt;h3 id="flink-job-list-and-job-insights-from-prometheus-and-grafana">Flink job list and job insights from Prometheus and Grafana&lt;/h3>
&lt;p>Each Flink job sends various metrics to our Prometheus with cardinality details, such as application name, customer Id, and regions, so that we can look at each job. Critical metrics include the input traffic rate, output throughput, backlogs in Kafka, timestamp-based latency, task CPU usage, task numbers, OOM counts, and so on.&lt;/p>
&lt;p>The following charts provide a few examples. The charts provide details about the ingestion traffic rate to Kafka for a specific customer, the streaming job’s overall throughput, each vCPU’s throughput, backlogs in Kafka, and worker autoscaling based on the observed backlog.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-metrics.png"
alt="Flink Job Metrics">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/autoscaling-metrics.png"
alt="Flink Job Autoscaling Metrics">&lt;/p>
&lt;p>The following chart shows streaming latency based on the timestamp watermark. In addition to the numbers of events in Kafka as backlogs, it is important to know the time latency for end-to-end streaming so that we can define and monitor the SLA. The latency is defined as the time taken for the streaming processing, starting from ingestion timestamp, to the timestamp sending to the streaming endpoint. A watermark is the last processed event’s time. With the watermark, we are tracking P100 latency. We track each event’s stream latency, so that we can understand each Kafka topic and partition or Flink job pipeline issue. The following example shows each event stream and its latency:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/watermark-metrics.png"
alt="Apache Beam Watermark Metrics">&lt;/p>
&lt;h3 id="flink-open-source-ui">Flink open source UI&lt;/h3>
&lt;p>We use and extend the Apache Flink dashboard UI to monitor jobs and tasks, such as the checkpoint duration, size, and failure. One important extension we used is a job history page that lets us see a job&amp;rsquo;s start and update timeline and details, which helps us to debug issues.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/flink-checkpoint-ui.png"
alt="Flink Checkpoint UI">&lt;/p>
&lt;h3 id="dashboards-and-alerting-for-backlog-and-latency">Dashboards and alerting for backlog and latency&lt;/h3>
&lt;p>We have about 30,000 jobs, and we want to closely monitor the jobs and receive alerts for jobs in abnormal states so that we can intervene. We created dashboards for each application so that we can show the list of jobs with the highest latency and create thresholds for alerts. The following example shows the timestamp-based latency dashboard for one application. We can set the alerting if the latency is larger than a threshold, such as 10 minutes, for a certain time continuously:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/latency-graph.png"
alt="Latency Graph">&lt;/p>
&lt;p>The following example shows more backlog-based dashboards:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/backlog-graph.png"
alt="Backlog Graph">&lt;/p>
&lt;p>The alerts are based on thresholds, and we frequently check metrics. If a threshold is met and continues for a certain amount of times, we alert our internal Slack channels or PagerDuty for immediate attention. We tune the alerting so that the accuracy is high.&lt;/p>
&lt;h2 id="cost-optimization-strategies-and-tuning">Cost optimization strategies and tuning&lt;/h2>
&lt;p>We also moved to a self-managed streaming service to improve cost efficiency. Several minor tunings have allowed us to reduce costs by half, and we have more opportunities for improvement.&lt;/p>
&lt;p>The following list includes a few tips that have helped us:&lt;/p>
&lt;ul>
&lt;li>Use Google Cloud Storage as checkpointing storage.&lt;/li>
&lt;li>Reduce the write frequency to Google Cloud Storage.&lt;/li>
&lt;li>Use appropriate machine types. For example, in Google Cloud, N2D machines are 15% less expensive than N2 machines.&lt;/li>
&lt;li>Autoscale tasks to use optimal resources while maintaining the latency SLA.&lt;/li>
&lt;/ul>
&lt;p>The following sections provide more details about the first two tips.&lt;/p>
&lt;h3 id="google-cloud-storage-and-checkpointing">Google Cloud Storage and checkpointing&lt;/h3>
&lt;p>We use Google Cloud Storage as our checkpoint store because it is cost-effective, scalable, and durable. When working with Google Cloud Storage, the following design considerations and best practices can help you optimize scaling and performance:&lt;/p>
&lt;ul>
&lt;li>Use data partitioning methods like range partitioning, which divides data based on specific attributes, and hash partitioning, which distributes data evenly using hash functions.&lt;/li>
&lt;li>Avoid sequential key names, especially timestamps, to avoid hotspots and uneven data distribution. Instead, introduce random prefixes for object distribution.&lt;/li>
&lt;li>Use a hierarchical folder structure to improve data management and reduce the number of objects in a single directory.&lt;/li>
&lt;li>Combine small files into larger ones to improve read throughput. Minimizing the number of small files reduces inefficient storage use and metadata operations.&lt;/li>
&lt;/ul>
&lt;h3 id="tune-the-frequency-of-writing-to-google-cloud-storage">Tune the frequency of writing to Google Cloud Storage&lt;/h3>
&lt;p>Scaling jobs efficiently was one of our primary challenges. Stateless jobs, which are relatively simpler, still present hurdles, especially in scenarios where Flink needed to process an overwhelming number of workers. To overcome this challenge, We increased the &lt;code>state.storage.fs.memory-threshold&lt;/code> settings to 1 MB from 20KB (??). This configuration allowed us to combine small checkpoint files into larger ones at the Job Manager level and to reduce metadata calls.&lt;/p>
&lt;p>Optimizing the performance of Google Cloud operations was another challenge. Although Google Cloud Storage is excellent for streaming large amounts of data, it has limitations when it comes to handling high-frequency I/O requests. To mitigate this issue, we introduced random prefixes in key names, avoided sequential key names, and optimized our Google Cloud Storage sharding techniques. These methods significantly enhanced our Google Cloud Storage performance, enabling the smooth operation of our stateless jobs.&lt;/p>
&lt;p>The following chart shows the Google Cloud Storage writes reduction after changing the memory-threshold:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/gcs-write-graph.png"
alt="GCS write Graph">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Palo Alto Networks® Cortex Data Lake is fully migrated from Dataflow streaming engine to Flink self managed streaming engine infrastructure. We have achieved our goals to run the system more cost efficiently (more than half cost cut), and run the infrastructure on multiple clouds such as GCP and AWS. We have learned how to build a large scale reliable production system based on open sources. We see large potentials to customize the system based on our specific needs as we have a lot of freedom to customize the open source code and configuration. In the next Part 2 post we will give more details on autoscaling and performance tuning parts. We hope our experience will be helpful for readers who will explore similar solutions for their own organizations.&lt;/p>
&lt;h1 id="additional-resources">Additional Resources&lt;/h1>
&lt;p>We provide links here for related presentations as further reading for readers interested in implementing similar solutions. By adding this section, we hope you can find more details to build a fully managed streaming infrastructure, making it easier for readers to follow our stories and learnings.&lt;/p>
&lt;p>[1] Streaming framework at PANW published at Apache Beam: &lt;a href="https://beam.apache.org/case-studies/paloalto/">https://beam.apache.org/case-studies/paloalto/&lt;/a>&lt;/p>
&lt;p>[2] PANW presentation at Beam Summit 2023: &lt;a href="https://youtu.be/IsGW8IU3NfA?feature=shared">https://youtu.be/IsGW8IU3NfA?feature=shared&lt;/a>&lt;/p>
&lt;p>[3] Benchmark presented at Beam Summit 2021: &lt;a href="https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/">https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/&lt;/a>&lt;/p>
&lt;p>[4] PANW open source contribution to Flink for GKE Auth support: &lt;a href="https://github.com/fabric8io/kubernetes-client/pull/4185">https://github.com/fabric8io/kubernetes-client/pull/4185&lt;/a>&lt;/p>
&lt;h1 id="acknowledgements">Acknowledgements&lt;/h1>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed Flink based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitk Dani, and David He.&lt;/p></description></item><item><title>Blog: Apache Beam 2.51.0</title><link>/blog/beam-2.51.0/</link><pubDate>Wed, 11 Oct 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.51.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.51.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2510-2023-10-03">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.51.0, check out the &lt;a href="https://github.com/apache/beam/milestone/15">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>In Python, &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#why-use-the-runinference-api">RunInference&lt;/a> now supports loading many models in the same transform using a &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#use-a-keyed-modelhandler">KeyedModelHandler&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/27628">#27628&lt;/a>).&lt;/li>
&lt;li>In Python, the &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.vertex_ai_inference.html#apache_beam.ml.inference.vertex_ai_inference.VertexAIModelHandlerJSON">VertexAIModelHandlerJSON&lt;/a> now supports passing in inference_args. These will be passed through to the Vertex endpoint as parameters.&lt;/li>
&lt;li>Added support to run &lt;code>mypy&lt;/code> on user pipelines (&lt;a href="https://github.com/apache/beam/issues/27906">#27906&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Removed fastjson library dependency for Beam SQL. Table property is changed to be based on jackson ObjectNode (Java) (&lt;a href="https://github.com/apache/beam/issues/24154">#24154&lt;/a>).&lt;/li>
&lt;li>Removed TensorFlow from Beam Python container images &lt;a href="https://github.com/apache/beam/pull/28424">PR&lt;/a>. If you have been negatively affected by this change, please comment on &lt;a href="https://github.com/apache/beam/issues/20605">#20605&lt;/a>.&lt;/li>
&lt;li>Removed the parameter &lt;code>t reflect.Type&lt;/code> from &lt;code>parquetio.Write&lt;/code>. The element type is derived from the input PCollection (Go) (&lt;a href="https://github.com/apache/beam/issues/28490">#28490&lt;/a>)&lt;/li>
&lt;li>Refactor BeamSqlSeekableTable.setUp adding a parameter joinSubsetType. &lt;a href="https://github.com/apache/beam/issues/28283">#28283&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed exception chaining issue in GCS connector (Python) (&lt;a href="https://github.com/apache/beam/issues/26769#issuecomment-1700422615">#26769&lt;/a>).&lt;/li>
&lt;li>Fixed streaming inserts exception handling, GoogleAPICallErrors are now retried according to retry strategy and routed to failed rows where appropriate rather than causing a pipeline error (Python) (&lt;a href="https://github.com/apache/beam/issues/21080">#21080&lt;/a>).&lt;/li>
&lt;li>Fixed a bug in Python SDK&amp;rsquo;s cross-language Bigtable sink that mishandled records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Python containers updated, fixing &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>&lt;/li>
&lt;li>Used go 1.21.1 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39320">CVE-2023-39320&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines using BigQuery Storage Read API must pin &lt;code>fastavro&lt;/code> dependency to 1.8.3
or earlier: &lt;a href="https://github.com/apache/beam/issues/28811">#28811&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Whitmore&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hao Xu&lt;/p>
&lt;p>Haruka Abe&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Joey Tran&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Melissa Pashniak&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruwann&lt;/p>
&lt;p>Ryan Tam&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sereana Seim&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tim Grein&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zbynek Konecny&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>gDuperran&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kberezin-nshl&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lostluck&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>olalamichelle&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p>
&lt;p>Łukasz Spyra&lt;/p></description></item><item><title>Blog: DIY GenAI Content Discovery Platform with Apache Beam</title><link>/blog/dyi-content-discovery-platform-genai-beam/</link><pubDate>Mon, 02 Oct 2023 00:00:01 -0800</pubDate><guid>/blog/dyi-content-discovery-platform-genai-beam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="diy-genai-content-discovery-platform-with-apache-beam">DIY GenAI Content Discovery Platform with Apache Beam&lt;/h1>
&lt;p>Your digital assets, such as documents, PDFs, spreadsheets, and presentations, contain a wealth of valuable information, but sometimes it&amp;rsquo;s hard to find what you&amp;rsquo;re looking for. This blog post explains how to build a DIY starter architecture, based on near real-time ingestion processing and large language models (LLMs), to extract meaningful information from your assets. The model makes the information available and discoverable through a simple natural language query.&lt;/p>
&lt;p>Building a near real-time processing pipeline for content ingestion might seem like a complex task, and it can be. To make pipeline building easier, the Apache Beam framework exposes a set of powerful constructs. These constructs remove the following complexities: interacting with multiple types of content sources and destinations, error handling, and modularity. They also maintain resiliency and scalability with minimal effort. You can use an Apache Beam streaming pipeline to complete the following tasks:&lt;/p>
&lt;ul>
&lt;li>Connect to the many components of a solution.&lt;/li>
&lt;li>Quickly process content ingestion requests of documents.&lt;/li>
&lt;li>Make the information in the documents available a few seconds after ingestion.&lt;/li>
&lt;/ul>
&lt;p>LLMs are often used to extract content and summarize information stored in many different places. Organizations can use LLMs to quickly find relevant information disseminated in multiple documents written across the years. The information might be in different formats, or the documents might be too long and complex to read and understand quickly. Use LLMs to process this content to make it easier for people to find the information that they need.&lt;/p>
&lt;p>Follow the steps in this guide to create a custom scalable solution for data extraction, content ingestion, and storage. Learn how to kickstart the development of a LLM-based solution using Google Cloud products and generative AI offerings. Google Cloud is designed to be simple to use, scalable, and flexible, so you can use it as a starting point for further expansion or experimentation.&lt;/p>
&lt;h3 id="high-level-flow">High-level Flow&lt;/h3>
&lt;p>In this workflow, content uptake and query interactions are completely separated. An external content owner can send documents stored in Google Docs or in a binary text format and receive a tracking ID for the ingestion request. The ingestion process gets the content of the document and creates chunks that are configurable in size. Each document chunk is used to generate embeddings. These embeddings represent the content semantics, in the form of a vector of 768 dimensions. Given the document identifier and the chunk identifier, you can store the embeddings in a Vector database for semantic matching. This process is central to contextualizing user inquiries.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-highlevel.png"
alt="Content Discovery Platform Overview">&lt;/p>
&lt;p>The query resolution process doesn&amp;rsquo;t depend directly on information ingestion. The user receives relevant answers based on the content ingested until the moment of the query request. Even if the platform doesn&amp;rsquo;t have any relevant content stored, the platform returns an answer stating that it doesn&amp;rsquo;t have relevant content. Therefore, the query resolution process first generates embeddings from the query content and from the previously existing context, like previous exchanges with the platform, then matches these embeddings with the existing embedding vectors stored from the content. When the platform has positive matches, it retrieves the plain-text content represented by the content embeddings. Finally, by using the textual representation of the query and the textual representation of the matched content, the platform formulates a request to the LLM to provide a final answer to the original user inquiry.&lt;/p>
&lt;h2 id="components-of-the-solution">Components of the solution&lt;/h2>
&lt;p>Use the low-ops capabilities of the Google Cloud services to create a set of highly scalable features. You can separate the solution into two main components: the service layer and the content ingestion pipeline. The service layer acts as the entry point for document ingestion and user queries. It’s a simple set of REST resources exposed through Cloud Run and implemented by using &lt;a href="https://quarkus.io/">Quarkus&lt;/a> and the client libraries to access other services (Vertex AI models, Cloud Bigtable and Pub/Sub). The content ingestion pipeline includes the following components:&lt;/p>
&lt;ul>
&lt;li>A streaming pipeline that captures user content from wherever it resides.&lt;/li>
&lt;li>A process that extracts meaning from this content as a set of multi-dimensional vectors (text embeddings).&lt;/li>
&lt;li>A storage system that simplifies context matching between knowledge content and user inquiries (a Vector Database).&lt;/li>
&lt;li>Another storage system that maps knowledge representation with the actual content, forming the aggregated context of the inquiry.&lt;/li>
&lt;li>A model capable of understanding the aggregated context and, through prompt engineering, delivering meaningful answers.&lt;/li>
&lt;li>HTTP and gRPC-based services.&lt;/li>
&lt;/ul>
&lt;p>Together, these components provide a comprehensive and simple implementation for a content discovery platform.&lt;/p>
&lt;h2 id="workflow-architecture">Workflow Architecture&lt;/h2>
&lt;p>This section explains how the different components interact.&lt;/p>
&lt;h3 id="dependencies-of-the-components">Dependencies of the components&lt;/h3>
&lt;p>The following diagram shows all of the components that the platform integrates with. It also shows all of the dependencies that exist between the components of the solution and the Google Cloud services.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-arch.png"
alt="Content Discovery Platform Interactions">&lt;/p>
&lt;p>As seen in the diagram, the context-extraction component is the central aspect in charge of retrieving the document’s content, also their semantic meaning from the embedding’s model and storing the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. PubSub resources are the glue between the streaming pipeline and the asynchronous processing, capturing the user ingestion requests, retries from potential errors from the ingestion pipeline (like the cases on where documents have been sent for ingestion but the permission has not been granted yet, triggering a retry after some minutes) and content refresh events (periodically the pipeline will scan the ingested documents, review the latest editions and define if a content refresh should be triggered).&lt;/p>
&lt;p>The context-extraction component retrieves the content of the documents, diving it in chunks. It also computes embeddings, using the LLM interaction, from the extracted content. Then it stores the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. Pub/Sub resources connect the streaming pipeline and the asynchronous processing, capturing the following actions:&lt;/p>
&lt;ul>
&lt;li>user ingestion requests&lt;/li>
&lt;li>retries from errors from the ingestion pipeline, such as when documents are sent for ingestion but access permissions are missing&lt;/li>
&lt;li>content refresh events (periodically the pipeline scans the ingested documents, reviews the latest editions, and decides whether to trigger a content refresh)&lt;/li>
&lt;/ul>
&lt;p>Also, CloudRun plays an important role exposing the services, interacting with many Google Cloud services to resolve the user query or ingestion requests. For example, while resolving a query request the service will:&lt;/p>
&lt;ul>
&lt;li>Request the computation of embeddings from the user’s query by interacting with the embeddings model&lt;/li>
&lt;li>Find near neighbor matches from the Vertex AI Vector Search (formerly Matching Engine) using the query embeddings representation&lt;/li>
&lt;li>Retrieve the text content from BigTable for those matched vectors, using their identifier, in order contextualize a LLM prompt&lt;/li>
&lt;li>And finally create a request to the VertexAI Chat-Bison model, generating the response the system will delivery to the user’s query.&lt;/li>
&lt;/ul>
&lt;h3 id="google-cloud-products">Google Cloud products&lt;/h3>
&lt;p>This section describes the Google Cloud products and services used in the solution and what purpose they serve.&lt;/p>
&lt;p>&lt;strong>Cloud Build:&lt;/strong> All container images, including services and pipelines, are built directly from source code by using Cloud Build. Using Cloud Build simplifies code distribution during the deployment of the solution.&lt;/p>
&lt;p>&lt;strong>CloudRun:&lt;/strong> The solution&amp;rsquo;s service entry points are deployed and automatically scaled by CloudRun.&lt;/p>
&lt;p>&lt;strong>Pub/Sub:&lt;/strong> A Pub/Sub topic and subscription queue all of the ingestion requests for Google Drive or self-contained content and deliver the requests to the pipeline.&lt;/p>
&lt;p>&lt;strong>Dataflow:&lt;/strong> A multi-language, streaming Apache Beam pipeline processes the ingestion requests. These requests are sent to the pipeline from the Pub/Sub subscription. The pipeline extracts content from Google Docs, Google Drive URLs, and self-contained binary encoded text content. It then produces content chunks. These chunks are sent to one of the Vertex AI foundational models for the embedding representation. The embeddings and chunks from the documents are sent to Vertex AI Vector Search and to Cloud Bigtable for indexing and rapid access. Finally, the ingested documentation is stored in Google Cloud Storage in JSON-L format, which can be used to fine-tune the Vertex AI models. By using Dataflow to run the Apache Beam streaming pipeline, you minimize the ops needed to scale resources. If you have a burst on ingestion requests, Dataflow can keep the latency less than a minute.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Vector Search:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/matching-engine/overview">Vector Search&lt;/a> is a high-performance, low-latency vector database. These vector databases are often called vector similarity search or approximate nearest neighbor (ANN) services. We use a Vector Search Index to store all the ingested documents embeddings as a meaning representation. These embeddings are indexed by chunk and document id. Later on, these identifiers can be used to contextualize the user queries and enrich the requests made to a LLM by providing knowledge extracted directly from the document’s content mappings stored on BigTable (using the same chunk-document identifiers).&lt;/p>
&lt;p>&lt;strong>Cloud BigTable:&lt;/strong> This storage system provides a low latency search by identifier at a predictable scale. Is a perfect fit, given the low latency of the requests resolution, for online exchanges between user queries and the platform component interactions. It used to store the content extracted from the documents since it&amp;rsquo;s indexed by chunk and document identifier. Every time a user makes a request to the query service, and after the query text embeddings are resolved and matched with the existing context, the document and chunk ids are used to retrieve the document’s content that will be used as context to request an answer to the LLM in use. Also, BigTable is used to keep track of the conversational exchanges between users and the platform, furthermore enriching the context included on the requests sent to the LLMs (embeddings, summarization, chat Q&amp;amp;A).&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Embedding Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings">Text embeddings&lt;/a> are a condensed vector (numeric) representation of a piece of text. If two pieces of text are semantically similar, their corresponding embeddings will be located close together in the embedding vector space. For more details please see &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings">get text embeddings&lt;/a>. These embeddings are directly used by the ingestion pipeline when processing the document’s content and the query service as an input to match the users query semantic with existing content indexed in Vector Search.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Summarization Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text">Text-bison&lt;/a> is the name of the PaLM 2 LLM that understands, summarizes and generates text. The types of content that text-bison can create include document summaries, answers to questions, and labels that classify the provided input content. We used this LLM to summarize the previously maintained conversation with the goal of enriching the user’s queries and better embedding matching. In summary, the user does not have to include all the context of his question, we extract and summarize it from the conversation history.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Chat Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat">Chat-bison&lt;/a> is the PaLM 2 LLM that excels at language understanding, language generation, and conversations. This chat model is fine-tuned to conduct natural multi-turn conversations, and is ideal for text tasks about code that require back-and-forth interactions. We use this LLM to provide answers to the queries made by users of the solution, including the conversation history between both parties and enriching the model’s context with the content stored in the solution.&lt;/p>
&lt;h3 id="extraction-pipeline">Extraction Pipeline&lt;/h3>
&lt;p>The content extraction pipeline is the platform&amp;rsquo;s centerpiece. It takes care of handling content ingestion requests, extracting documents content and computing embeddings from that content, to finally store the data in specialized storage systems that will be used in the query service components for rapid access.&lt;/p>
&lt;h4 id="high-level-view">High Level View&lt;/h4>
&lt;p>As previously mentioned the pipeline is implemented using Apache Beam framework and runs in streaming fashion on GCP&amp;rsquo;s &lt;a href="https://cloud.google.com/dataflow">Dataflow&lt;/a> service.&lt;/p>
&lt;p>By using Apache Beam and Dataflow we can ensure minimal latency (sub minute processing times), low ops (no need to manually scale up or down the pipeline when traffic spikes occur with time, worker recycle, updates, etc.) and with high level of observability (clear and abundant performance metrics are available).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-1.png"
alt="Apache Beam Pipeline">&lt;/p>
&lt;p>On a high level, the pipeline separates the extraction, computing, error handling and storage responsibilities on different components or PTransforms. As seen in the diagram, the messages are read from a PubSub subscription and immediately afterwards are included in the window definition before the content extraction.&lt;/p>
&lt;p>Each of those PTransforms can be expanded to reveal more details regarding the underlying stages for the implementation. We will dive into each in the following sections.&lt;/p>
&lt;p>The pipeline was implemented using a multi-language approach, with the main components written in the Java language (JDK version 17) and those related with the embeddings computations implemented in Python (version 3.11) since the Vertex AI API clients are available for this language.&lt;/p>
&lt;h4 id="content-extraction">Content Extraction&lt;/h4>
&lt;p>The content extraction component is in charge of reviewing the ingestion request payload and deciding (given the event properties) if it will need to retrieve the content from the event itself (self-contained content, text based document binary encoded) or retrieve it from Google Drive.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-2-extractcontent.png"
alt="Pipeline's Content Extraction">&lt;/p>
&lt;p>In case of a self-contained document, the pipeline will extract the document id and format the document in paragraphs for later embedding processing.&lt;/p>
&lt;p>When in need of retrieval from Google Drive, the pipeline will inspect if the provided URL in the event refers to a Google Drive folder or a single file format (supported formats are Documents, Spreadsheets and Presentations). In the case of a folder, the pipeline will crawl the folder’s content recursively extracting all the files for the supported formats, in case of a single document will just return that one.&lt;/p>
&lt;p>Finally, with all the file references retrieved from the ingestion request, textual content is extracted from the files (no image support implemented for this PoC). That content will also be passed to the embedding processing stages including the document’s identifier and the content as paragraphs.&lt;/p>
&lt;h4 id="error-handling">Error Handling&lt;/h4>
&lt;p>On every stage of the content extraction process multiple errors can be encountered, malformed ingestion requests, non-conformant URLs, lack of permissions for Drive resources, lack of permissions for File data retrieval.&lt;/p>
&lt;p>In all those cases a dedicated component will capture those potential errors and define, given the nature of the error, if the event should be retried or sent to a dead letter GCS bucket for later inspection.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-3-errorhandling.png"
alt="Pipeline's Error Handling">&lt;/p>
&lt;p>The final errors, or those which won’t be retried, are those errors related with bad request formats (the event itself or the properties content, like malformed or wrong URLs, etc.).&lt;/p>
&lt;p>The retryable errors are those related with content access and lack of permissions. A request may have been resolved faster than the manual process of providing the right permissions to the Service Account that runs the pipeline to access the resources included in the ingestion request (Google Drive folders or files). In case of detecting a retryable error, the pipeline will hold the retry for 10 minutes before re-sending the message to the upstream PubSub topic; each error is retried at most 5 times before being sent to the dead letter GCS bucket.&lt;/p>
&lt;p>In all cases of events ending on the dead letter destination, the inspection and re-processing must be done in a manual process.&lt;/p>
&lt;h4 id="process-embeddings">Process Embeddings&lt;/h4>
&lt;p>Once the content has been extracted from the request, or captured from Google Drive files, the pipeline will trigger the embeddings computation process. As previously mentioned the interactions with the Vertex AI Foundational Models API is implemented in Python language. For this reason we need to format the extracted content in Java types that have a direct translation to those existing in the Python world. Those are key-values (in Python those are 2-element tuples), Strings (available in both languages), and iterables (also available in both languages). We could have implemented coders in both languages to support custom transport types, but we opted out of that in favor of clarity and simplicity.&lt;/p>
&lt;p>Before computing the content’s embeddings we decided to introduce a Reshuffle step, making the output consistent to downstream stages, with the idea of avoiding the content extraction step being repeated in case of errors. This should avoid putting pressure on existing access quotas on Google Drive related APIs.&lt;/p>
&lt;p>The pipeline will then chunk the content in configurable sizes and also configurable overlapping, good parameters are hard to get for generic effective data extraction, so we opted to use smaller chunks with small overlapping factor as the default settings to favor diversity on the document results (at least that’s what we see from the empirical results obtained).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings1.png"
alt="Embeddings Processing">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings2.png"
alt="Embeddings Processing">
&lt;/p>
&lt;p>Once the embeddings vectors are retrieved from the embeddings Vertex AI LLM, we will consolidate them again avoiding repetition of this step in case of downstream errors.&lt;/p>
&lt;p>Worth to notice that this pipeline is interacting directly with Vertex AI models using the client SDKs, Apache Beam already provides supports for this interactions through the RunInference PTransform (see an example &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/vertex_ai_llm_text_classification.py">here&lt;/a>).&lt;/p>
&lt;h4 id="content-storage">Content Storage&lt;/h4>
&lt;p>Once the embeddings are computed for the content chunks extracted from the ingested documents, we need to store the vectors in a searchable storage and also the textual content that correlates with those embeddings. We will be using the embeddings vectors as a semantic match later from the query service, and the textual content that corresponds to those embeddings for LLM context as a way to improve and guide the response expectations.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-5-storecontent.png"
alt="Content Storage">&lt;/p>
&lt;p>With that in mind is that in mind we split the consolidated embeddings into 3 paths, one that stores the vectors into Vertex AI Vector Search (using simple REST calls), another storing the textual content into BigTable (for low latency retrieval after semantic matching) and the final one as a potential clean up of content refresh or re ingestion (more on that later). The three paths are using the ingested document identifier as the correlating data on the actions, this key is formed by the document name (in case of available), the document identifier and the chunk sequence number. The reason for using identifiers for the chunk comes behind the idea of subsequent updates. An increase in the content will generate a larger number of chunks, and upserting all the chunks will enable always fresh data; on the contrary, a decrease in content will generate a smaller chunk count for the document’s content, this number difference can be used to delete the remaining orphan indexed chunks (from content no longer existing in the latest version of the document).&lt;/p>
&lt;h4 id="content-refresh">Content Refresh&lt;/h4>
&lt;p>The last pipeline component is the simplest, at least conceptually. After the documents from Google Drive gets ingested, an external user can produce updates in them, causing the indexed content to become out of date. We implemented a simple periodic process, inside the same streaming pipeline, that will take care of the review of already ingested documents and see if there are content updates needed. We use a GenerateSequence transform to produce a periodic impulse (every 6 hours by default), that will trigger a scan on BigTable retrieving all the ingested document identifiers. Given those identifiers we can then query Google Drive for the latest update timestamp of each document and use that marker to decide if an update is needed.&lt;/p>
&lt;p>In case of needing to update the document’s content, we can simply send an ingestion request to the upstream PubSub topic and let the pipeline run its course for this new event. Since we are taking care of upserting embeddings and cleaning up those that no longer exist, we should be capable of taking care of the majority of the additions (as long those are text updates, image based content is not being processed as of now).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh1.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh2.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh3.png"
alt="Content Refresh">
&lt;/p>
&lt;p>This task could be performed as a separate job, possibly one that is periodically scheduled in batch form. This would result in lower costs, a separate error domain, and more predictable auto scaling behavior. However, for the purposes of this demonstration, it is simpler to have a single job.&lt;/p>
&lt;p>Next, we will be focusing on how the solution interacts with external clients for ingestion and content discovery use cases.&lt;/p>
&lt;h2 id="interaction-design">Interaction Design&lt;/h2>
&lt;p>The solution aims to make the interactions for ingesting and querying the platform as simple as possible. Also, since the ingestion part may imply interacting with several services and imply retries or content refresh, we decided to make both separated and asynchronous, freeing the external users of blocking themselves while waiting for requests resolutions.&lt;/p>
&lt;h3 id="example-interactions">Example Interactions&lt;/h3>
&lt;p>Once the platform is deployed in a GCP project, a simple way to interact with the services is through the use of a web client, curl is a good example. Also, since the endpoints are authenticated, a client needs to include its credentials in the request header to have its access granted.&lt;/p>
&lt;p>Here is an example of an interaction for content ingestion:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST -H &amp;#34;Content-Type: application/json&amp;#34; -H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; https://&amp;lt;service-address&amp;gt;/ingest/content/gdrive -d $&amp;#39;{&amp;#34;url&amp;#34;:&amp;#34;https://drive.google.com/drive/folders/somefolderid&amp;#34;}&amp;#39; | jq .
# response from service
{
&amp;#34;status&amp;#34;: &amp;#34;Ingestion trace id: &amp;lt;some identifier&amp;gt;&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>In this case, after the ingestion request has been sent to the PubSub topic for processing, the service will return the tracking identifier, which maps with the PubSub message identifier. Note the provided URL can be one of a Google Doc or a Google Drive folder, in the later case the ingestion process will crawl the folder’s content recursively to retrieve all the contained documents and their contents.&lt;/p>
&lt;p>Next, an example of a content query interaction, very similar to the previous one:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundation Models are a set of pre-trained models that can be used to accelerate the development of machine learning applications. They are available for a variety of tasks, including natural language processing, computer vision, and recommendation systems.\n\nVertexAI Foundation Models can be used to improve the performance of Generative AI applications by providing a starting point for model development. They can also be used to reduce the amount of time and effort required to train a model.\n\nIn addition, VertexAI Foundation Models can be used to improve the accuracy and robustness of Generative AI applications. This is because they are trained on large datasets and are subject to rigorous quality control.\n\nOverall, VertexAI Foundation Models can be a valuable resource for developers who are building Generative AI applications. They can help to accelerate the development process, reduce the cost of development, and improve the performance and accuracy of applications.&amp;#34;,
&amp;#34;previousConversationSummary&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;sourceLinks&amp;#34;: [
{
&amp;#34;link&amp;#34;: &amp;#34;&amp;lt;possibly some ingested doc url/id&amp;gt;&amp;#34;,
&amp;#34;distance&amp;#34;: 0.7233397960662842
}
],
&amp;#34;citationMetadata&amp;#34;: [
{
&amp;#34;citations&amp;#34;: []
}
],
&amp;#34;safetyAttributes&amp;#34;: [
{
&amp;#34;categories&amp;#34;: [],
&amp;#34;scores&amp;#34;: [],
&amp;#34;blocked&amp;#34;: false
}
]
}
&lt;/code>&lt;/pre>&lt;p>The platform will answer the request with a textual response from the LLM and include as well information about the categorization, citation metadata and source links (if available) of the content used to generate the response (this are for example, Google Docs links of the documents previously ingested by the platform).&lt;/p>
&lt;p>When interacting with the services, a good query will generally return good results, the clearer the query the easier it will be to contextualize its meaning and more accurate information will be sent to the LLMs to retrieve answers. But having to include all the details of the query context in a phrase on every exchange with the service can be very cumbersome and difficult. For that case the platform can use a provided session identifier that will be used to store all the previous exchanges between a user and the platform. This should help the implementation to better contextualize the initial query embeddings matching and even provide more concise contextual information in the model requests. Here is an example of a contextual exchange:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundational Models are a suite of pre-trained models that can be used to accelerate the development of Generative AI applications. These models are available in a variety of languages and domains, and they can be used to generate text, images, audio, and other types of content.\n\nUsing VertexAI Foundational Models can help you to:\n\n* Reduce the time and effort required to develop Generative AI applications\n* Improve the accuracy and quality of your models\n* Access the latest research and development in Generative AI\n\nVertexAI Foundational Models are a powerful tool for developers who want to create innovative and engaging Generative AI applications.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;describe the available LLM models?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models suite includes a variety of LLM models, including:\n\n* Text-to-text LLMs: These models can generate text based on a given prompt. They can be used for tasks such as summarization, translation, and question answering.\n* Image-to-text LLMs: These models can generate text based on an image. They can be used for tasks such as image captioning and description generation.\n* Audio-to-text LLMs: These models can generate text based on an audio clip. They can be used for tasks such as speech recognition and transcription.\n\nThese models are available in a variety of languages, including English, Spanish, French, German, and Japanese. They can be used to create a wide range of Generative AI applications, such as chatbots, customer service applications, and creative writing tools.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;do rate limit apply for those LLMs?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;Yes, there are rate limits for the VertexAI Foundational Models. The rate limits are based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models documentation](https://cloud.google.com/vertex-ai/docs/foundational-models#rate-limits).&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;care to share the price?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models are priced based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models pricing page](https://cloud.google.com/vertex-ai/pricing#foundational-models).&amp;#34;,
…
}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Usage Tip:&lt;/strong> in case of abruptly changing topics, sometimes is better to use a new session identifier.&lt;/p>
&lt;h3 id="deployment">Deployment&lt;/h3>
&lt;p>As part of the platform solution, there are a set of scripts that help with the deployment of all the different components. By running the &lt;code>start.sh&lt;/code> and setting the right parameters (GCP project, terraform state bucket and name for the platform instance) the script will take care of building the code, deploying the needed containers (service endpoint container and Dataflow python custom container), deploying all the GCP resources using Terraform and finally deploying the pipeline. There is also the possibility of modifying the pipeline’s execution by passing an extra parameter to the startup script, for example: &lt;code>start.sh &amp;lt;gcp project&amp;gt; &amp;lt;state-bucket-name&amp;gt; &amp;lt;a run name&amp;gt; &amp;quot;--update&amp;quot;&lt;/code> will update the content extraction pipeline in-place.&lt;/p>
&lt;p>Also, in case of wanting to focus only on the deployment of specific components other scripts have been included to help with those specific tasks (build the solution, deploy the infrastructure, deploy the pipeline, deploy the services, etc.).&lt;/p>
&lt;h3 id="solutions-notes">Solution&amp;rsquo;s Notes&lt;/h3>
&lt;p>This solution is designed to serve as an example for learning purposes. Many of the configuration values for the extraction pipeline and security restrictions are provided only as examples. The solution doesn&amp;rsquo;t propagate the existing access control lists (ACLs) of the ingested content. As a result, all users that have access to the service endpoints have access to summarizations of the ingested content from those original documents.&lt;/p>
&lt;h3 id="notes-about-the-source-code">Notes about the source code&lt;/h3>
&lt;p>The source code for the content discovery platform is available in &lt;a href="https://github.com/prodriguezdefino/content-dicovery-platform-gcp">Github&lt;/a>. You can run it in any Google Cloud project. The repository includes the source code for the integration services, the multi-language ingestion pipeline, and the deployment automation through Terraform. If you deploy this example, it might take up to 90 minutes to create and configure all the needed resources. The README file contains additional documentation about the deployment prerequisites and example REST interactions.&lt;/p></description></item><item><title>Blog: Apache Beam 2.50.0</title><link>/blog/beam-2.50.0/</link><pubDate>Wed, 30 Aug 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.50.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.50.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2500-2023-08-30">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.50.0, check out the &lt;a href="https://github.com/apache/beam/milestone/14">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark 3.2.2 is used as default version for Spark runner (&lt;a href="https://github.com/apache/beam/issues/23804">#23804&lt;/a>).&lt;/li>
&lt;li>The Go SDK has a new default local runner, called Prism (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java KafkaIO now supports picking up topics via topicPattern (&lt;a href="https://github.com/apache/beam/pull/26948">#26948&lt;/a>)&lt;/li>
&lt;li>Support for read from Cosmos DB Core SQL API (&lt;a href="https://github.com/apache/beam/issues/23604">#23604&lt;/a>)&lt;/li>
&lt;li>Upgraded to HBase 2.5.5 for HBaseIO. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">#27711&lt;/a>)&lt;/li>
&lt;li>Added support for GoogleAdsIO source (Java) (&lt;a href="https://github.com/apache/beam/pull/27681">#27681&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Go SDK now requires Go 1.20 to build. (&lt;a href="https://github.com/apache/beam/issues/27558">#27558&lt;/a>)&lt;/li>
&lt;li>The Go SDK has a new default local runner, Prism. (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).
&lt;ul>
&lt;li>Prism is a portable runner that executes each transform independantly, ensuring coders.&lt;/li>
&lt;li>At this point it supercedes the Go direct runner in functionality. The Go direct runner is now deprecated.&lt;/li>
&lt;li>See &lt;a href="https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md">https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md&lt;/a> for the goals and features of Prism.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hugging Face Model Handler for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/26632">#26632&lt;/a>)&lt;/li>
&lt;li>Hugging Face Pipelines support for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/27399">#27399&lt;/a>)&lt;/li>
&lt;li>Vertex AI Model Handler for RunInference now supports private endpoints (&lt;a href="https://github.com/apache/beam/pull/27696">#27696&lt;/a>)&lt;/li>
&lt;li>MLTransform transform added with support for common ML pre/postprocessing operations (&lt;a href="https://github.com/apache/beam/pull/26795">#26795&lt;/a>)&lt;/li>
&lt;li>Upgraded the Kryo extension for the Java SDK to Kryo 5.5.0. This brings in bug fixes, performance improvements, and serialization of Java 14 records. (&lt;a href="https://github.com/apache/beam/issues/27635">#27635&lt;/a>)&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures. (&lt;a href="https://github.com/apache/beam/issues/27674">#27674&lt;/a>). The multi-arch container images include:
&lt;ul>
&lt;li>All versions of Go, Python, Java and Typescript SDK containers.&lt;/li>
&lt;li>All versions of Flink job server containers.&lt;/li>
&lt;li>Java and Python expansion service containers.&lt;/li>
&lt;li>Transform service controller container.&lt;/li>
&lt;li>Spark3 job server container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for batched writes to AWS SQS for improved throughput (Java, AWS 2).(&lt;a href="https://github.com/apache/beam/issues/21429">#21429&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python SDK: Legacy runner support removed from Dataflow, all pipelines must use runner v2.&lt;/li>
&lt;li>Python SDK: Dataflow Runner will no longer stage Beam SDK from PyPI in the &lt;code>--staging_location&lt;/code> at pipeline submission. Custom container images that are not based on Beam&amp;rsquo;s default image must include Apache Beam installation.(&lt;a href="https://github.com/apache/beam/issues/26996">#26996&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>The Go Direct Runner is now Deprecated. It remains available to reduce migration churn.
&lt;ul>
&lt;li>Tests can be set back to the direct runner by overriding TestMain: &lt;code>func TestMain(m *testing.M) { ptest.MainWithDefault(m, &amp;quot;direct&amp;quot;) }&lt;/code>&lt;/li>
&lt;li>It&amp;rsquo;s recommended to fix issues seen in tests using Prism, as they can also happen on any portable runner.&lt;/li>
&lt;li>Use the generic register package for your pipeline DoFns to ensure pipelines function on portable runners, like prism.&lt;/li>
&lt;li>Do not rely on closures or using package globals for DoFn configuration. They don&amp;rsquo;t function on portable runners.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed DirectRunner bug in Python SDK where GroupByKey gets empty PCollection and fails when pipeline option &lt;code>direct_num_workers!=1&lt;/code>.(&lt;a href="https://github.com/apache/beam/pull/27373">#27373&lt;/a>)&lt;/li>
&lt;li>Fixed BigQuery I/O bug when estimating size on queries that utilize row-level security (&lt;a href="https://github.com/apache/beam/pull/27474">#27474&lt;/a>)&lt;/li>
&lt;li>Beam Python containers rely on a version of Debian/aom that has several security vulnerabilities: &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python Pipelines using BigQuery IO or &lt;code>orjson&lt;/code> dependency might experience segmentation faults or get stuck: &lt;a href="https://github.com/apache/beam/issues/28318">#28318&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>acejune&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>al97&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anton Shalkovich&lt;/p>
&lt;p>ArjunGHUB&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brett Morgan&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Connor Brett&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Hansen&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmytro Sadovnychyi&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Gabriel Lacroix&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hong Liang Teoh&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jon esperanza&lt;/p>
&lt;p>Josef Šimánek&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>Mahmud Ridwan&lt;/p>
&lt;p>Manav Garg&lt;/p>
&lt;p>Marco Vela&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>Peter Sobot&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Saba Sathya&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Steven Niemitz&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p></description></item><item><title>Blog: Apache Beam 2.49.0</title><link>/blog/beam-2.49.0/</link><pubDate>Mon, 17 Jul 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.49.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.49.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2490-2023-07-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.49.0, check out the &lt;a href="https://github.com/apache/beam/milestone/13">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable Change Streams added in Java &lt;code>BigtableIO.ReadChangeStream&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/27183">#27183&lt;/a>).&lt;/li>
&lt;li>Added Bigtable Read and Write cross-language transforms to Python SDK ((&lt;a href="https://github.com/apache/beam/issues/26593">#26593&lt;/a>), (&lt;a href="https://github.com/apache/beam/issues/27146">#27146&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow prebuilding large images when using &lt;code>--prebuild_sdk_container_engine=cloud_build&lt;/code>, like images depending on &lt;code>tensorflow&lt;/code> or &lt;code>torch&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/27023">#27023&lt;/a>).&lt;/li>
&lt;li>Disabled &lt;code>pip&lt;/code> cache when installing packages on the workers. This reduces the size of prebuilt Python container images (&lt;a href="https://github.com/apache/beam/pull/27035">#27035&lt;/a>).&lt;/li>
&lt;li>Select dedicated avro datum reader and writer (Java) (&lt;a href="https://github.com/apache/beam/issues/18874">#18874&lt;/a>).&lt;/li>
&lt;li>Timer API for the Go SDK (Go) (&lt;a href="https://github.com/apache/beam/issues/22737">#22737&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Python 3.7 support. (&lt;a href="https://github.com/apache/beam/issues/26447">#26447&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed KinesisIO &lt;code>NullPointerException&lt;/code> when a progress check is made before the reader is started (IO) (&lt;a href="https://github.com/apache/beam/issues/23868">#23868&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;li>Python pipelines using the &lt;code>--impersonate_service_account&lt;/code> option with BigQuery IOs might fail on Dataflow (&lt;a href="https://github.com/apache/beam/issues/32030">#32030&lt;/a>). This is fixed in 2.59.0 release.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.49.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Dopierała&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Gavin McDonald&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>John Gill&lt;/p>
&lt;p>Joseph Crowley&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Galloway&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Masato Nakamura&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nathaniel Young&lt;/p>
&lt;p>Nelson Osacky&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Smeet nagda&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>WuA&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Xianhua Liu&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zachary Houfek&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>bigduu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>jubebo&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>sultanalieva-s&lt;/p>
&lt;p>vitaly.terentyev&lt;/p></description></item><item><title>Blog: Managing Beam dependencies in Java</title><link>/blog/managing-beam-dependencies-in-java/</link><pubDate>Fri, 23 Jun 2023 09:00:00 -0700</pubDate><guid>/blog/managing-beam-dependencies-in-java/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Managing your Java dependencies can be challenging, and if not done correctly,
it may cause a variety of problems, as incompatibilities may arise when using
specific and previously untested combinations.&lt;/p>
&lt;p>To make that process easier, Beam now
provides &lt;a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#bill-of-materials-bom-poms">Bill of Materials (BOM)&lt;/a>
artifacts that will help dependency management tools to select compatible
combinations.&lt;/p>
&lt;p>We hope this will make it easier for you to use Apache Beam, and have a simpler
transition when upgrading to newer versions.&lt;/p>
&lt;p>When bringing incompatible classes and libraries, the code is susceptible to
errors such
as &lt;code>NoClassDefFoundError&lt;/code>, &lt;code>NoSuchMethodError&lt;/code>, &lt;code>NoSuchFieldError&lt;/code>, &lt;code>FATAL ERROR in native method&lt;/code>.&lt;/p>
&lt;p>When importing Apache Beam, the recommended way is to use Bill of Materials
(BOMs). The way BOMs work is by providing hints to the dependency management
resolution tool, so when a project imports unspecified or ambiguous dependencies,
it will know what version to use.&lt;/p>
&lt;p>There are currently two BOMs provided by Beam:&lt;/p>
&lt;ul>
&lt;li>&lt;code>beam-sdks-java-bom&lt;/code>, which manages what dependencies of Beam will be used, so
you can specify the version only once.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>, a more comprehensive list,
which manages Beam, along with GCP client and third-party dependencies.&lt;/li>
&lt;/ul>
&lt;p>Since errors are more likely to arise when using third-party dependencies,
that&amp;rsquo;s the one that is recommended to use to minimize any conflicts.&lt;/p>
&lt;p>In order to use BOM, the artifact has to be imported to your Maven or Gradle
dependency configurations. For example, to
use &lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>,
the following changes have to be done (and make sure that &lt;em>BEAM_VERSION&lt;/em> is
replaced by a valid version):&lt;/p>
&lt;p>&lt;strong>Maven&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-google-cloud-platform-bom&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;version&amp;gt;&lt;/span>BEAM_VERSION&lt;span class="nt">&amp;lt;/version&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;type&amp;gt;&lt;/span>pom&lt;span class="nt">&amp;lt;/type&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;scope&amp;gt;&lt;/span>import&lt;span class="nt">&amp;lt;/scope&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Gradle&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>dependencies {
implementation(platform(&amp;#34;org.apache.beam:beam-sdks-java-google-cloud-platform-bom:BEAM_VERSION&amp;#34;))
}
&lt;/code>&lt;/pre>&lt;p>After importing the BOM, specific version pinning of dependencies, for example,
anything for &lt;code>org.apache.beam&lt;/code>, &lt;code>io.grpc&lt;/code>, &lt;code>com.google.cloud&lt;/code> (
including &lt;code>libraries-bom&lt;/code>) may be removed.&lt;/p>
&lt;p>Do not entirely remove the dependencies, as they are not automatically imported
by the BOM. It is important to keep the dependency without specifying a version.
For example, in Maven:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-core&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or Gradle:&lt;/p>
&lt;pre tabindex="0">&lt;code>implementation(&amp;#34;org.apache.beam:beam-sdks-java-core&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>For a full list of dependency versions that are managed by a specific BOM, the
Maven tool &lt;code>help:effective-pom&lt;/code> can be used. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">mvn help:effective-pom -f ~/.m2/repository/org/apache/beam/beam-sdks-java-google-cloud-platform-bom/BEAM_VERSION/beam-sdks-java-google-cloud-platform-bom-BEAM_VERSION.pom
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The third-party
website &lt;a href="https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-google-cloud-platform-bom/">mvnrepository.com&lt;/a>
can also be used to display such version information.&lt;/p>
&lt;p>We hope you find this
useful. &lt;a href="https://beam.apache.org/community/contact-us/">Feedback&lt;/a> and
contributions are always welcome! So feel free to create a GitHub issue, or open
a Pull Request if you encounter any problem when using those artifacts.&lt;/p></description></item><item><title>Blog: Getting started with Apache Beam: An open source proficiency credential sponsored by Google Cloud</title><link>/blog/beamquest/</link><pubDate>Tue, 06 Jun 2023 00:00:01 -0800</pubDate><guid>/blog/beamquest/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-badge-image-scaled.png"
alt="Quest image">&lt;/p>
&lt;p>We’re excited to announce the release of the &lt;a href="https://www.cloudskillsboost.google/course_templates/724">“Getting Started with Apache Beam” quest&lt;/a>, a series of four online labs that venture into different Apache Beam concepts. When you complete all four labs, you’ll earn a Google Cloud badge that you can share on platforms like LinkedIn. Earning this badge should take less than seven hours total, and signing up for the quest costs $20 (there are often free specials for people who attend Beam events, such as &lt;a href="https://www.meetup.com/topics/apache-beam/">Meetups&lt;/a>, &lt;a href="https://beamsummit.org/">Beam Summit&lt;/a>, and &lt;a href="https://beamcollege.dev/">Beam College&lt;/a>).&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>&amp;ldquo;I really like the workshop to be honest. I learnt a lot from doing those labs. I would suggest we offer that for any new members to Beam&amp;rdquo;&lt;/em> &amp;ndash; Shunping Huang, Software Engineer&lt;/p>
&lt;/blockquote>
&lt;p>Beam is one of the largest big data open source projects actively in development. Over the past six years, the Apache Beam community has seen tremendous growth in the number of contributors, committers, and users. If you’re a long time Beam user, you can now earn a badge to show your skills to potential employers. If you’re new to Beam, you can begin your learning journey with this quest. To attempt this quest, you don’t need any prior knowledge of data processing or distributed systems. All you need is elementary knowledge about programming.&lt;/p>
&lt;p>Individuals aren’t the only ones who can benefit from completing this quest - organizations can too! Because earning this badge represents deep knowledge of an industry leading big data library, having the badge validates your organization’s understanding of Beam. In addition, you can run the Beam library on a wide variety of runners, including Google Cloud Dataflow, Flink, Spark, and more, making knowledge about this library highly transferable. Finally, your organization can use this quest as onboarding material for new hires on big data teams, allowing teams and organizations to get their newest employees up-to-date on the latest and greatest that Apache Beam has to offer.&lt;/p>
&lt;p>Data Processing is a key part of AI/ML workflows. Given the recent advancements in artificial intelligence, now’s the time to jump into the world of data processing! Get started on your journey &lt;a href="https://www.cloudskillsboost.google/quests/310">here&lt;/a>.&lt;/p>
&lt;p>We are currently offering this quest &lt;strong>FREE OF CHARGE&lt;/strong>. To obtain your badge for &lt;strong>FREE&lt;/strong>, use the &lt;a href="https://www.cloudskillsboost.google/catalog?qlcampaign=1h-swiss-19">Access Code&lt;/a>, create an account, and search &lt;a href="https://www.cloudskillsboost.google/course_templates/724">&amp;ldquo;Getting Started with Apache Beam&amp;rdquo;&lt;/a>. If the code does not work, please email &lt;a href="dev@beam.apache.org">dev@beam.apache.org&lt;/a> to obtain a free code.&lt;/p>
&lt;p>PS: Once you earn your badge, please &lt;a href="https://support.google.com/qwiklabs/answer/9222527?hl=en&amp;amp;sjid=14905615709060962899-NA">share it on social media&lt;/a>!&lt;/p></description></item><item><title>Blog: Apache Beam 2.48.0</title><link>/blog/beam-2.48.0/</link><pubDate>Wed, 31 May 2023 11:30:00 -0400</pubDate><guid>/blog/beam-2.48.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.48.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2480-2023-05-31">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.48.0, check out the &lt;a href="https://github.com/apache/beam/milestone/12">detailed release notes&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Note: The release tag for Go SDK for this release is sdks/v2.48.2 instead of sdks/v2.48.0 because of incorrect commit attached to the release tag sdks/v2.48.0.&lt;/strong>&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Experimental&amp;rdquo; annotation cleanup: the annotation and concept have been removed from Beam to avoid
the misperception of code as &amp;ldquo;not ready&amp;rdquo;. Any proposed breaking changes will be subject to
case-by-case pro/con decision making (and generally avoided) rather than using the &amp;ldquo;Experimental&amp;rdquo;
to allow them.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added rename for GCS and copy for local filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/26064">#25779&lt;/a>).&lt;/li>
&lt;li>Added support for enhanced fan-out in KinesisIO.Read (Java) (&lt;a href="https://github.com/apache/beam/issues/19967">#19967&lt;/a>).
&lt;ul>
&lt;li>This change is not compatible with Flink savepoints created by Beam 2.46.0 applications which had KinesisIO sources.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added textio.ReadWithFilename transform (Go) (&lt;a href="https://github.com/apache/beam/issues/25812">#25812&lt;/a>).&lt;/li>
&lt;li>Added fileio.MatchContinuously transform (Go) (&lt;a href="https://github.com/apache/beam/issues/26186">#26186&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow passing service name for google-cloud-profiler (Python) (&lt;a href="https://github.com/apache/beam/issues/26280">#26280&lt;/a>).&lt;/li>
&lt;li>Dead letter queue support added to RunInference in Python (&lt;a href="https://github.com/apache/beam/issues/24209">#24209&lt;/a>).&lt;/li>
&lt;li>Support added for defining pre/postprocessing operations on the RunInference transform (&lt;a href="https://github.com/apache/beam/issues/26308">#26308&lt;/a>)&lt;/li>
&lt;li>Adds a Docker Compose based transform service that can be used to discover and use portable Beam transforms (&lt;a href="https://github.com/apache/beam/pull/26023">#26023&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Passing a tag into MultiProcessShared is now required in the Python SDK (&lt;a href="https://github.com/apache/beam/issues/26168">#26168&lt;/a>).&lt;/li>
&lt;li>CloudDebuggerOptions is removed (deprecated in Beam v2.47.0) for Dataflow runner as the Google Cloud Debugger service is &lt;a href="https://cloud.google.com/debugger/docs/deprecations">shutting down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;li>AWS 2 client providers (deprecated in Beam &lt;a href="#2380---2022-04-20">v2.38.0&lt;/a>) are finally removed (&lt;a href="https://github.com/apache/beam/issues/26681">#26681&lt;/a>).&lt;/li>
&lt;li>AWS 2 SnsIO.writeAsync (deprecated in Beam v2.37.0 due to risk of data loss) was finally removed (&lt;a href="https://github.com/apache/beam/issues/26710">#26710&lt;/a>).&lt;/li>
&lt;li>AWS 2 coders (deprecated in Beam v2.43.0 when adding Schema support for AWS Sdk Pojos) are finally removed (&lt;a href="https://github.com/apache/beam/issues/23315">#23315&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java bootloader failing with Too Long Args due to long classpaths, with a pathing jar. (Java) (&lt;a href="https://github.com/apache/beam/issues/25582">#25582&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>PubsubIO writes will throw &lt;em>SizeLimitExceededException&lt;/em> for any message above 100 bytes, when used in batch (bounded) mode. (Java) (&lt;a href="https://github.com/apache/beam/issues/27000">#27000&lt;/a>).&lt;/li>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.48.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bazyli Polednia&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Arn&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Novitskiy&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Kuba Rauch&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Pranjal Joshi&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>RuiLong J&lt;/p>
&lt;p>RyujiTamaki&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vishal Bhise&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>kellen&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>mokamoka03210120&lt;/p>
&lt;p>psolomin&lt;/p></description></item><item><title>Blog: Apache Beam 2.47.0</title><link>/blog/beam-2.47.0/</link><pubDate>Wed, 10 May 2023 12:00:00 -0500</pubDate><guid>/blog/beam-2.47.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.47.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2470-2023-05-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.47.0, check out the &lt;a href="https://github.com/apache/beam/milestone/10">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam adds Python 3.11 support (&lt;a href="https://github.com/apache/beam/issues/23848">#23848&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>BigQuery Storage Write API is now available in Python SDK via cross-language (&lt;a href="https://github.com/apache/beam/issues/21961">#21961&lt;/a>).&lt;/li>
&lt;li>Added HbaseIO support for writing RowMutations (ordered by rowkey) to Hbase (Java) (&lt;a href="https://github.com/apache/beam/issues/25830">#25830&lt;/a>).&lt;/li>
&lt;li>Added fileio transforms MatchFiles, MatchAll and ReadMatches (Go) (&lt;a href="https://github.com/apache/beam/issues/25779">#25779&lt;/a>).&lt;/li>
&lt;li>Add integration test for JmsIO + fix issue with multiple connections (Java) (&lt;a href="https://github.com/apache/beam/issues/25887">#25887&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Flink runner now supports Flink 1.16.x (&lt;a href="https://github.com/apache/beam/issues/25046">#25046&lt;/a>).&lt;/li>
&lt;li>Schema&amp;rsquo;d PTransforms can now be directly applied to Beam dataframes just like PCollections.
(Note that when doing multiple operations, it may be more efficient to explicitly chain the operations
like &lt;code>df | (Transform1 | Transform2 | ...)&lt;/code> to avoid excessive conversions.)&lt;/li>
&lt;li>The Go SDK adds new transforms periodic.Impulse and periodic.Sequence that extends support
for slowly updating side input patterns. (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>Several Google client libraries in Python SDK dependency chain were updated to latest available major versions. (&lt;a href="https://github.com/apache/beam/pull/24599">#24599&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>If a main session fails to load, the pipeline will now fail at worker startup. (&lt;a href="https://github.com/apache/beam/issues/25401">#25401&lt;/a>).&lt;/li>
&lt;li>Python pipeline options will now ignore unparsed command line flags prefixed with a single dash. (&lt;a href="https://github.com/apache/beam/issues/25943">#25943&lt;/a>).&lt;/li>
&lt;li>The SmallestPerKey combiner now requires keyword-only arguments for specifying optional parameters, such as &lt;code>key&lt;/code> and &lt;code>reverse&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/25888">#25888&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Cloud Debugger support and its pipeline options are deprecated and will be removed in the next Beam version,
in response to the Google Cloud Debugger service &lt;a href="https://cloud.google.com/debugger/docs/deprecations">turning down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>BigQuery sink in STORAGE_WRITE_API mode in batch pipelines might result in data consistency issues during the handling of other unrelated transient errors for Beam SDKs 2.35.0 - 2.46.0 (inclusive). For more details see: &lt;a href="https://github.com/apache/beam/issues/26521">https://github.com/apache/beam/issues/26521&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>BigQueryIO Storage API write with autoUpdateSchema may cause data corruption for Beam SDKs 2.45.0 - 2.47.0 (inclusive) (&lt;a href="https://github.com/apache/beam/issues/26789">#26789&lt;/a>)&lt;/li>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.47.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amir Fayazi&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Andrey Kot&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>ChangyuLi28&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Ma&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Jiangjie (Becket) Qin&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Weaver&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Reza Rokni&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Saifuddin53&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Thomas Gaddy&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yanan Hao&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yuvi Panda&lt;/p>
&lt;p>andres-vv&lt;/p>
&lt;p>bochap&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>hnnsgstfssn&lt;/p>
&lt;p>jrmccluskey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xianhualiu&lt;/p>
&lt;p>zhangskz&lt;/p></description></item><item><title>Blog: Apache Beam 2.46.0</title><link>/blog/beam-2.46.0/</link><pubDate>Fri, 10 Mar 2023 13:00:00 -0500</pubDate><guid>/blog/beam-2.46.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.46.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2460-2023-03-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.46.0, check out the &lt;a href="https://github.com/apache/beam/milestone/9?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java SDK containers migrated to &lt;a href="https://hub.docker.com/_/eclipse-temurin">Eclipse Temurin&lt;/a>
as a base. This change migrates away from the deprecated &lt;a href="https://hub.docker.com/_/openjdk">OpenJDK&lt;/a>
container. Eclipse Temurin is currently based upon Ubuntu 22.04 while the OpenJDK
container was based upon Debian 11.&lt;/li>
&lt;li>RunInference PTransform will accept model paths as SideInputs in Python SDK. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>RunInference supports ONNX runtime in Python SDK (&lt;a href="https://github.com/apache/beam/issues/22972">#22972&lt;/a>)&lt;/li>
&lt;li>Tensorflow Model Handler for RunInference in Python SDK (&lt;a href="https://github.com/apache/beam/issues/25366">#25366&lt;/a>)&lt;/li>
&lt;li>Java SDK modules migrated to use &lt;code>:sdks:java:extensions:avro&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/24748">#24748&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added in JmsIO a retry policy for failed publications (Java) (&lt;a href="https://github.com/apache/beam/issues/24971">#24971&lt;/a>).&lt;/li>
&lt;li>Support for &lt;code>LZMA&lt;/code> compression/decompression of text files added to the Python SDK (&lt;a href="https://github.com/apache/beam/issues/25316">#25316&lt;/a>)&lt;/li>
&lt;li>Added ReadFrom/WriteTo Csv/Json as top-level transforms to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;li>Option for SparkRunner to avoid the need of SDF output to fit in memory (&lt;a href="https://github.com/apache/beam/issues/23852">#23852&lt;/a>).
This helps e.g. with ParquetIO reads. Turn the feature on by adding experiment &lt;code>use_bounded_concurrent_output_for_sdf&lt;/code>.&lt;/li>
&lt;li>Add &lt;code>WatchFilePattern&lt;/code> transform, which can be used as a side input to the RunInference PTransfrom to watch for model updates using a file pattern. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>Add support for loading TorchScript models with &lt;code>PytorchModelHandler&lt;/code>. The TorchScript model path can be
passed to PytorchModelHandler using &lt;code>torch_script_model_path=&amp;lt;path_to_model&amp;gt;&lt;/code>. (&lt;a href="https://github.com/apache/beam/pull/25321">#25321&lt;/a>)&lt;/li>
&lt;li>The Go SDK now requires Go 1.19 to build. (&lt;a href="https://github.com/apache/beam/pull/25545">#25545&lt;/a>)&lt;/li>
&lt;li>The Go SDK now has an initial native Go implementation of a portable Beam Runner called Prism. (&lt;a href="https://github.com/apache/beam/pull/24789">#24789&lt;/a>)
&lt;ul>
&lt;li>For more details and current state see &lt;a href="https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism">https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The deprecated SparkRunner for Spark 2 (see &lt;a href="#2410---2022-08-23">2.41.0&lt;/a>) was removed (&lt;a href="https://github.com/apache/beam/pull/25263">#25263&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s BatchElements performs more aggressive batching in some cases,
capping at 10 second rather than 1 second batches by default and excluding
fixed cost in this computation to better handle cases where the fixed cost
is larger than a single second. To get the old behavior, one can pass
&lt;code>target_batch_duration_secs_including_fixed_cost=1&lt;/code> to BatchElements.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Avro related classes are deprecated in module &lt;code>beam-sdks-java-core&lt;/code> and will be eventually removed. Please, migrate to a new module &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead by importing the classes from &lt;code>org.apache.beam.sdk.extensions.avro&lt;/code> package.
For the sake of migration simplicity, the relative package path and the whole class hierarchy of Avro related classes in new module is preserved the same as it was before.
For example, import &lt;code>org.apache.beam.sdk.extensions.avro.coders.AvroCoder&lt;/code> class instead of&lt;code>org.apache.beam.sdk.coders.AvroCoder&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/24749">#24749&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.46.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Katz&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Egbert van der Wal&lt;/p>
&lt;p>Elizaveta Lomteva&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Herman Mak&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo E&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruslan Altynnikov&lt;/p>
&lt;p>Ryan Zhang&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sam sam&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shivam&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vachan&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vincent Devillers&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>William Ross Morrow&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>Ziqi Ma&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>aliftadvantage&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kamrankoupayi&lt;/p>
&lt;p>kileys&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>nickuncaged1201&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p></description></item><item><title>Blog: Apache Beam 2.45.0</title><link>/blog/beam-2.45.0/</link><pubDate>Wed, 15 Feb 2023 09:00:00 -0700</pubDate><guid>/blog/beam-2.45.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.45.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2023-01-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.45.0, check out the &lt;a href="https://github.com/apache/beam/milestone/8?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>MongoDB IO connector added (Go) (&lt;a href="https://github.com/apache/beam/issues/24575">#24575&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>RunInference Wrapper with Sklearn Model Handler support added in Go SDK (&lt;a href="https://github.com/apache/beam/issues/23382">#24497&lt;/a>).&lt;/li>
&lt;li>Adding override of allowed TLS algorithms (Java), now maintaining the disabled/legacy algorithms
present in 2.43.0 (up to 1.8.0_342, 11.0.16, 17.0.2 for respective Java versions). This is accompanied
by an explicit re-enabling of TLSv1 and TLSv1.1 for Java 8 and Java 11.&lt;/li>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Portable Java pipelines, Go pipelines, Python streaming pipelines, and portable Python batch
pipelines on Dataflow are required to use Runner V2. The &lt;code>disable_runner_v2&lt;/code>,
&lt;code>disable_runner_v2_until_2023&lt;/code>, &lt;code>disable_prime_runner_v2&lt;/code> experiments will raise an error during
pipeline construction. You can no longer specify the Dataflow worker jar override. Note that
non-portable Java jobs and non-portable Python batch jobs are not impacted. (&lt;a href="https://github.com/apache/beam/issues/24515">#24515&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Avoids Cassandra syntax error when user-defined query has no where clause in it (Java) (&lt;a href="https://github.com/apache/beam/issues/24829">#24829&lt;/a>).&lt;/li>
&lt;li>Fixed JDBC connection failures (Java) during handshake due to deprecated TLSv1(.1) protocol for the JDK. (&lt;a href="https://github.com/apache/beam/issues/24623">#24623&lt;/a>)&lt;/li>
&lt;li>Fixed Python BigQuery Batch Load write may truncate valid data when deposition sets to WRITE_TRUNCATE and incoming data is large (Python) (&lt;a href="https://github.com/apache/beam/issues/24535">#24623&lt;/a>).&lt;/li>
&lt;li>Fixed Kafka watermark issue with sparse data on many partitions (&lt;a href="https://github.com/apache/beam/pull/24205">#24205&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.45.0 release. Thank you to all contributors!&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrea Nardelli&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Benjamin Gonzalez&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniela Martín&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dejan Spasic&lt;/p>
&lt;p>Diego Gomez&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Elias Segundo Antonio&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Evgeny Antyshev&lt;/p>
&lt;p>Fernando Morales&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Lucas Marques&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Marco Robles&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>Melanie&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Philippe Moussalli&lt;/p>
&lt;p>Piyush Sagar&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Rick Viscomi&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Seung Jin An&lt;/p>
&lt;p>Shane Hansen&lt;/p>
&lt;p>Sho Nakatani&lt;/p>
&lt;p>Shunya Ueta&lt;/p>
&lt;p>Siddharth Agrawal&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinbin Huang&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Xinyue Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>andoni-guzman&lt;/p>
&lt;p>andthezhang&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>camphillips22&lt;/p>
&lt;p>gabihodoroaga&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>yyy1000&lt;/p>
&lt;p>zhengbuqian&lt;/p></description></item><item><title>Blog: Apache Beam 2.44.0</title><link>/blog/beam-2.44.0/</link><pubDate>Tue, 17 Jan 2023 09:00:00 -0700</pubDate><guid>/blog/beam-2.44.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.44.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2023-01-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.44.0, check out the &lt;a href="https://github.com/apache/beam/milestone/7?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable sink (Write and WriteBatch) added (Go) (&lt;a href="https://github.com/apache/beam/issues/23324">#23324&lt;/a>).&lt;/li>
&lt;li>S3 implementation of the Beam filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/23991">#23991&lt;/a>).&lt;/li>
&lt;li>Support for SingleStoreDB source and sink added (Java) (&lt;a href="https://github.com/apache/beam/issues/22617">#22617&lt;/a>).&lt;/li>
&lt;li>Added support for DefaultAzureCredential authentication in Azure Filesystem (Python) (&lt;a href="https://github.com/apache/beam/issues/24210">#24210&lt;/a>).&lt;/li>
&lt;li>Added new CdapIO for CDAP Batch and Streaming Source/Sinks (Java) (&lt;a href="https://github.com/apache/beam/issues/24961">#24961&lt;/a>).&lt;/li>
&lt;li>Added new SparkReceiverIO for Spark Receivers 2.4.* (Java) (&lt;a href="https://github.com/apache/beam/issues/24960">#24960&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Beam now provides a portable &amp;ldquo;runner&amp;rdquo; that can render pipeline graphs with
graphviz. See &lt;code>python -m apache_beam.runners.render --help&lt;/code> for more details.&lt;/li>
&lt;li>Local packages can now be used as dependencies in the requirements.txt file, rather
than requiring them to be passed separately via the &lt;code>--extra_package&lt;/code> option
(Python) (&lt;a href="https://github.com/apache/beam/pull/23684">#23684&lt;/a>).&lt;/li>
&lt;li>Pipeline Resource Hints now supported via &lt;code>--resource_hints&lt;/code> flag (Go) (&lt;a href="https://github.com/apache/beam/pull/23990">#23990&lt;/a>).&lt;/li>
&lt;li>Make Python SDK containers reusable on portable runners by installing dependencies to temporary venvs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12792">BEAM-12792&lt;/a>).&lt;/li>
&lt;li>RunInference model handlers now support the specification of a custom inference function in Python (&lt;a href="https://github.com/apache/beam/issues/22572">#22572&lt;/a>)&lt;/li>
&lt;li>Support for &lt;code>map_windows&lt;/code> urn added to Go SDK (&lt;a href="https://github.apache/beam/pull/24307">#24307&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>ParquetIO.withSplit&lt;/code> was removed since splittable reading has been the default behavior since 2.35.0. The effect of
this change is to drop support for non-splittable reading (Java)(&lt;a href="https://github.com/apache/beam/issues/23832">#23832&lt;/a>).&lt;/li>
&lt;li>&lt;code>beam-sdks-java-extensions-google-cloud-platform-core&lt;/code> is no longer a
dependency of the Java SDK Harness. Some users of a portable runner (such as Dataflow Runner v2)
may have an undeclared dependency on this package (for example using GCS with
TextIO) and will now need to declare the dependency.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-core&lt;/code> is no longer a dependency of the Java SDK Harness. Users of a portable
runner (such as Dataflow Runner v2) will need to provide this package and its dependencies.&lt;/li>
&lt;li>Slices now use the Beam Iterable Coder. This enables cross language use, but breaks pipeline updates
if a Slice type is used as a PCollection element or State API element. (Go)&lt;a href="https://github.com/apache/beam/issues/24339">#24339&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed JmsIO acknowledgment issue (Java) (&lt;a href="https://github.com/apache/beam/issues/20814">#20814&lt;/a>)&lt;/li>
&lt;li>Fixed Beam SQL CalciteUtils (Java) and Cross-language JdbcIO (Python) did not support JDBC CHAR/VARCHAR, BINARY/VARBINARY logical types (&lt;a href="https://github.com/apache/beam/issues/23747">#23747&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/23526">#23526&lt;/a>).&lt;/li>
&lt;li>Ensure iterated and emitted types are used with the generic register package are registered with the type and schema registries.(Go) (&lt;a href="https://github.com/apache/beam/pull/23889">#23889&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.44.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alex Merose&lt;/p>
&lt;p>Alexey Inkin&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrej Galad&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Ayush Sharma&lt;/p>
&lt;p>Benjamin Gonzalez&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat Safiullin&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Danielle Syse&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Elias Segundo Antonio&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Evgeny Antyshev&lt;/p>
&lt;p>Heejong Lee&lt;/p>
&lt;p>Henrik Heggelund-Berg&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Janek Bevendorff&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John J. Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nancy Xu&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Nivaldo Tokuda&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Philippe Moussalli&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Rick Smit&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ryan Thompson&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Scott Strong&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Thiago Nunes&lt;/p>
&lt;p>Tianyang Hu&lt;/p>
&lt;p>Trevor Gevers&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vladislav Chunikhin&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>agvdndor&lt;/p>
&lt;p>andremissaglia&lt;/p>
&lt;p>arne-alex&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>camphillips22&lt;/p>
&lt;p>capthiron&lt;/p>
&lt;p>creste&lt;/p>
&lt;p>fab-jul&lt;/p>
&lt;p>illoise&lt;/p>
&lt;p>kn1kn1&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>peridotml&lt;/p>
&lt;p>shinannegans&lt;/p>
&lt;p>smeet07&lt;/p></description></item><item><title>Blog: Apache Beam Playground: An interactive environment to try transforms and examples</title><link>/blog/apacheplayground/</link><pubDate>Wed, 30 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/apacheplayground/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h3 id="what-is-apache-beam-playground">&lt;strong>What is Apache Beam Playground?&lt;/strong>&lt;/h3>
&lt;p>&lt;a href="https://play.beam.apache.org/">Apache Beam Playground&lt;/a> is an interactive environment to try Apache Beam transforms and examples without requiring to install or set up a Beam environment.&lt;/p>
&lt;h3 id="apache-beam-playground-features">&lt;strong>Apache Beam Playground Features&lt;/strong>&lt;/h3>
&lt;p>&lt;img class="center-block"
src="/images/blog/BeamPlayground.gif"
alt="Apache Beam Playground">&lt;/p>
&lt;ul>
&lt;li>Discover transform examples that you can try right away by browsing or searching Catalog that is sourced from Apache Beam GitHub&lt;/li>
&lt;li>Supports Java, Python, Go SDKs, and Scio to execute the example in Beam Direct Runner&lt;/li>
&lt;li>Displays pipeline execution graph (DAG)&lt;/li>
&lt;li>Code editor to modify examples or try your own custom pipeline with a Direct Runner&lt;/li>
&lt;li>Code editor with code highlighting, flexible layout, color schemes, and other features to provide responsive UX in desktop browsers&lt;/li>
&lt;li>Embedding a Playground example on a web page prompts the web page readers to try the example pipeline in the Playground - e.g., &lt;a href="/get-started/try-beam-playground/">Playground Quickstart&lt;/a> page&lt;/li>
&lt;/ul>
&lt;h3 id="whats-next">&lt;strong>What’s Next&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Try examples in &lt;a href="https://play.beam.apache.org/">Apache Beam Playground&lt;/a>&lt;/li>
&lt;li>Submit your feedback using “Enjoying Playground?” in Apache Beam Playground or via &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSd5_5XeOwwW2yjEVHUXmiBad8Lxk-4OtNcgG45pbyAZzd4EbA/viewform?usp=pp_url">this form&lt;/a>&lt;/li>
&lt;li>Join the Beam &lt;a href="/community/contact-us">users@&lt;/a> mailing list&lt;/li>
&lt;li>Contribute to the Apache Beam Playground codebase by following a few steps in this &lt;a href="/contribute">Contribution Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Please &lt;a href="/community/contact-us">reach out&lt;/a> if you have any feedback or encounter any issues!&lt;/p></description></item><item><title>Blog: Apache Beam 2.43.0</title><link>/blog/beam-2.43.0/</link><pubDate>Thu, 17 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.43.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.43.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2022-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.43.0, check out the &lt;a href="https://github.com/apache/beam/milestone/5?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python 3.10 support in Apache Beam (&lt;a href="https://github.com/apache/beam/issues/21458">#21458&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. Try it out and give us feedback! (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Decreased TextSource CPU utilization by 2.3x (Java) (&lt;a href="https://github.com/apache/beam/issues/23193">#23193&lt;/a>).&lt;/li>
&lt;li>Fixed bug when using SpannerIO with RuntimeValueProvider options (Java) (&lt;a href="https://github.com/apache/beam/issues/22146">#22146&lt;/a>).&lt;/li>
&lt;li>Fixed issue for unicode rendering on WriteToBigQuery (&lt;a href="https://github.com/apache/beam/issues/22312">#22312&lt;/a>)&lt;/li>
&lt;li>Remove obsolete variants of BigQuery Read and Write, always using Beam-native variant
(&lt;a href="https://github.com/apache/beam/issues/23564">#23564&lt;/a> and &lt;a href="https://github.com/apache/beam/issues/23559">#23559&lt;/a>).&lt;/li>
&lt;li>Bumped google-cloud-spanner dependency version to 3.x for Python SDK (&lt;a href="https://github.com/apache/beam/issues/21198">#21198&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Dataframe wrapper added in Go SDK via Cross-Language (with automatic expansion service). (Go) (&lt;a href="https://github.com/apache/beam/issues/23384">#23384&lt;/a>).&lt;/li>
&lt;li>Name all Java threads to aid in debugging (&lt;a href="https://github.com/apache/beam/issues/23049">#23049&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;li>Allow configuring GCP OAuth scopes via pipeline options. This unblocks usages of Beam IOs that require additional scopes.
For example, this feature makes it possible to access Google Drive backed tables in BigQuery (&lt;a href="https://github.com/apache/beam/issues/23290">#23290&lt;/a>).&lt;/li>
&lt;li>An example for using Python RunInference from Java (&lt;a href="https://github.com/apache/beam/pull/23619">#23290&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>CoGroupByKey transform in Python SDK has changed the output typehint. The typehint component representing grouped values changed from List to Iterable,
which more accurately reflects the nature of the arbitrarily large output collection. &lt;a href="https://github.com/apache/beam/issues/21556">#21556&lt;/a> Beam users may see an error on transforms downstream from CoGroupByKey. Users must change methods expecting a List to expect an Iterable going forward. See &lt;a href="https://docs.google.com/document/d/1RIzm8-g-0CyVsPb6yasjwokJQFoKHG4NjRUcKHKINu0">document&lt;/a> for information and fixes.&lt;/li>
&lt;li>The PortableRunner for Spark assumes Spark 3 as default Spark major version unless configured otherwise using &lt;code>--spark_version&lt;/code>.
Spark 2 support is deprecated and will be removed soon (&lt;a href="https://github.com/apache/beam/issues/23728">#23728&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Numeric/Decimal type values (&lt;a href="https://github.com/apache/beam/issues/19817">#19817&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.43.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
AlexZMLyu
Alexey Romanenko
Anand Inguva
Andrew Pilloud
Andy Ye
Arnout Engelen
Benjamin Gonzalez
Bharath Kumarasubramanian
BjornPrime
Brian Hulette
Bruno Volpato
Chamikara Jayalath
Colin Versteeg
Damon
Daniel Smilkov
Daniela Martín
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Denis Pyshev
Dmitry Repin
Evan Galpin
Evgeny Antyshev
Fernando Morales
Geddy05
Harshit Mehrotra
Iñigo San Jose Visiers
Ismaël Mejía
Israel Herraiz
Jan Lukavský
Juta Staes
Kanishk Karanawat
Kenneth Knowles
KevinGG
Kiley Sok
Liam Miller-Cushon
Luke Cwik
Mc
Melissa Pashniak
Moritz Mack
Ning Kang
Pablo Estrada
Philippe Moussalli
Pranav Bhandari
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Ryohei Nagao
Sam Rohde
Sam Whittle
Sanil Jain
Seunghwan Hong
Shane Hansen
Shubham Krishna
Shunsuke Otani
Steve Niemitz
Steven van Rossum
Svetak Sundhar
Thiago Nunes
Toran Sahu
Veronica Wasson
Vitaly Terentyev
Vladislav Chunikhin
Xinyu Liu
Yi Hu
Yixiao Shen
alexeyinkin
arne-alex
azhurkevich
bulat safiullin
bullet03
coldWater
dpcollins-google
egalpin
johnjcasey
liferoad
rvballada
shaojwu
tvalentyn&lt;/p></description></item><item><title>Blog: New Resources Available for Beam ML</title><link>/blog/ml-resources/</link><pubDate>Wed, 09 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/ml-resources/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>If you&amp;rsquo;ve been paying attention, over the past year you&amp;rsquo;ve noticed that
Beam has released a number of features designed to make Machine Learning
easy. Ranging from things like the introduction of the &lt;code>RunInference&lt;/code>
transform to the continued refining of &lt;code>Beam Dataframes&lt;/code>, this has been
an area where we&amp;rsquo;ve seen Beam make huge strides. While development has
advanced quickly, however, until recently there has been a lack of
resources to help people discover and use these new features.&lt;/p>
&lt;p>Over the past several months, we&amp;rsquo;ve been hard at work building out
documentation and notebooks to make it easier to use these new features
and to show how Beam can be used to solve common Machine Learning problems.
We&amp;rsquo;re now happy to present this new and improved Beam ML experience!&lt;/p>
&lt;p>To get started, we encourage you to visit Beam&amp;rsquo;s new &lt;a href="/documentation/ml/overview/">AI/ML landing page&lt;/a>.
We&amp;rsquo;ve got plenty of content on things like &lt;a href="/documentation/ml/multi-model-pipelines/">multi-model pipelines&lt;/a>,
&lt;a href="/documentation/ml/runinference-metrics/">performing inference with metrics&lt;/a>,
&lt;a href="/documentation/ml/online-clustering/">online training&lt;/a>, and much more.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ml-landing.png"
alt="ML landing page">&lt;/p>
&lt;p>We&amp;rsquo;ve also introduced a number of example &lt;a href="https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml">Jupyter Notebooks&lt;/a>
showing how to use built in beam transforms like &lt;code>RunInference&lt;/code> and &lt;code>Beam Dataframes&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ensemble-model-notebook.png"
alt="Example ensemble notebook with RunInference">&lt;/p>
&lt;p>Adding more examples and notebooks will be a point of emphasis going forward.
For our next round of improvements, we are planning on adding examples of
using RunInference with &amp;gt;30GB models, with multi-language pipelines, with
common Beam concepts, and with TensorRT. We will also add examples showing
other pieces of the Machine Learning lifecycle like model evaluation with TFMA,
per-entity training, and more online training.&lt;/p>
&lt;p>We hope you find this useful! As always, if you see any areas for improvement, please &lt;a href="https://github.com/apache/beam/issues/new/choose">open an issue&lt;/a>
or a &lt;a href="https://github.com/apache/beam/pulls">pull request&lt;/a>!&lt;/p></description></item><item><title>Blog: Beam starter projects</title><link>/blog/beam-starter-projects/</link><pubDate>Thu, 03 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-starter-projects/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re happy to announce that we&amp;rsquo;re providing new Beam starter projects! 🎉&lt;/p>
&lt;p>Setting up and configuring a new project can be time consuming, and varies in different languages. We hope this will make it easier for you to get started in creating new Apache Beam projects and pipelines.&lt;/p>
&lt;p>All the starter projects come in their own GitHub repository, so you can simply clone a repo and you&amp;rsquo;re ready to go. Each project comes with a README with how to use it, a simple &amp;ldquo;Hello World&amp;rdquo; pipeline, and a test for the pipeline. The GitHub repositories come pre-configured with GitHub Actions to automatically run tests when pull requests are opened or modified, and Dependabot is enabled to make sure all the dependencies are up to date. This all comes out of the box, so you can start playing with your Beam pipeline without a hassle.&lt;/p>
&lt;p>For example, here&amp;rsquo;s how to get started with Java:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/apache/beam-starter-java
cd beam-starter-java
# Install Java and Gradle with sdkman.
curl -s &amp;#34;https://get.sdkman.io&amp;#34; | bash
sdk install java 11.0.12-tem
sdk install gradle
# To run the pipeline.
gradle run
# To run the tests.
gradle test
&lt;/code>&lt;/pre>&lt;p>And here&amp;rsquo;s how to get started with Python:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/apache/beam-starter-python
cd beam-starter-python
# Set up a virtual environment with the dependencies.
python -m venv env
source env/bin/activate
pip install -r requirements.txt
# To run the pipeline.
python main.py
# To run the tests.
python -m unittest
&lt;/code>&lt;/pre>&lt;p>Here are the starter projects; you can choose your favorite language:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[Java]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-java">github.com/apache/beam-starter-java&lt;/a> – Includes both Gradle and Maven configurations.&lt;/li>
&lt;li>&lt;strong>[Python]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-python">github.com/apache/beam-starter-python&lt;/a> – Includes a setup.py file to allow multiple files in your pipeline.&lt;/li>
&lt;li>&lt;strong>[Go]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-go">github.com/apache/beam-starter-go&lt;/a> – Includes how to register different types of functions for ParDo.&lt;/li>
&lt;li>&lt;strong>[Kotlin]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-kotlin">github.com/apache/beam-starter-kotlin&lt;/a> – Adapted to idiomatic Kotlin&lt;/li>
&lt;li>&lt;strong>[Scala]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-scala">github.com/apache/beam-starter-scala&lt;/a> – Coming soon!&lt;/li>
&lt;/ul>
&lt;p>We have updated the &lt;a href="/get-started/quickstart/java/">Java quickstart&lt;/a> to use the new starter project, and we&amp;rsquo;re working on updating the Python and Go quickstarts as well.&lt;/p>
&lt;p>We hope you find this useful. Feedback and contributions are always welcome! So feel free to create a GitHub issue, or open a Pull Request to any of the starter project repositories.&lt;/p></description></item><item><title>Blog: Apache Beam 2.42.0</title><link>/blog/beam-2.42.0/</link><pubDate>Mon, 17 Oct 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.42.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.42.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2420-2022-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.42.0, check out the &lt;a href="https://github.com/apache/beam/milestone/4?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;li>Added support for &lt;a href="/documentation/programming-guide/#batched-dofns">Batched
DoFns&lt;/a>
to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for Zstd compression to the Python SDK.&lt;/li>
&lt;li>Added support for Google Cloud Profiler to the Go SDK.&lt;/li>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go SDK&amp;rsquo;s Row Coder now uses a different single-precision float encoding for float32 types to match Java&amp;rsquo;s behavior (&lt;a href="https://github.com/apache/beam/issues/22629">#22629&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Timestamp type values &lt;a href="https://github.com/apache/beam/issues/19817">19817&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Go SDK doesn&amp;rsquo;t yet support Slowly Changing Side Input pattern (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/4">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.42.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abirdcfly
Ahmed Abualsaud
Alexander Zhuravlev
Alexey Inkin
Alexey Romanenko
Anand Inguva
Andrej Galad
Andrew Pilloud
Andy Ye
Balázs Németh
Brian Hulette
Bruno Volpato
bulat safiullin
bullet03
Chamikara Jayalath
ChangyuLi28
Clément Guillaume
Damon
Danny McCormick
Darkhan Nausharipov
David Huntsperger
dpcollins-google
Evgeny Antyshev
grufino
Heejong Lee
Ismaël Mejía
Jack McCluskey
johnjcasey
Jonathan Shen
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
liferoad
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manit Gupta
masahitojp
Michael Hu
Michel Davit
Moritz Mack
Naireen Hussain
nancyxu123
Nikhil Nadig
oborysevych
Pablo Estrada
Pranav Bhandari
Rajat Bhatta
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sergey Pronin
Shivam
Shunsuke Otani
Shunya Ueta
Steven Niemitz
Stuart
Svetak Sundhar
Valentyn Tymofieiev
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yixiao Shen&lt;/p></description></item><item><title>Blog: Apache Hop web version with Cloud Dataflow</title><link>/blog/hop-web-cloud/</link><pubDate>Sat, 15 Oct 2022 00:00:01 -0800</pubDate><guid>/blog/hop-web-cloud/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Hop is a codeless visual development environment for Apache Beam pipelines that
can run jobs in any Beam runner, such as Dataflow, Flink or Spark. &lt;a href="/blog/apache-hop-with-dataflow/">In a
previous post&lt;/a>, we
introduced the desktop version of Apache Hop. Hop also has a web environment,
Hop Web, that you can run from a container, so you don&amp;rsquo;t have to install
anything on your computer to use it.&lt;/p>
&lt;p>In this detailed tutorial, you access Hop through the internet using a web
browser and point to a container running in a virtual machine on Google
Cloud. That container will launch jobs in Dataflow and report back the results
of those jobs. Because we don&amp;rsquo;t want just anyone to access your Hop instance,
we’re going to secure it so that only you can access that virtual machine. The
following diagram illustrates the setup:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image2.png" alt="Architecture deployed with this tutorial">&lt;/p>
&lt;p>We will show how to do the deployment described previously, creating a web and
visual development environment that builds Beam pipelines using just a web
browser. When complete, you will have a secure web environment that you can use
to create pipelines with your web browser and launch them using Google Cloud
Dataflow.&lt;/p>
&lt;h2 id="what-do-you-need-to-run-this-example">What do you need to run this example?&lt;/h2>
&lt;p>We are using Google Cloud, so the first thing you need is a Google Cloud
project. If needed, you can sign up for the free trial of Google Cloud at
&lt;a href="https://cloud.google.com/free">https://cloud.google.com/free&lt;/a>.&lt;/p>
&lt;p>When you have a project, you can use &lt;a href="https://cloud.google.com/shell">Cloud
Shell&lt;/a> in your web browser with no additional
setup. In Cloud Shell, the Google Cloud SDK is automatically configured for your
project and credentials. That&amp;rsquo;s the option we use here. Alternatively, you can
configure the Google Cloud SDK in your local computer. For instructions, see
&lt;a href="https://cloud.google.com/sdk/docs/install">https://cloud.google.com/sdk/docs/install&lt;/a>.&lt;/p>
&lt;p>To open Cloud Shell, go to the [Google Cloud console]
(&lt;a href="http://console.cloud.google.com">http://console.cloud.google.com&lt;/a>), make sure your project is selected, and click
the Cloud Shell button &lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image1.png" alt="Cloud Shellbutton">. Cloud Shell opens,
and you can use it to run the commands shown in this post.&lt;/p>
&lt;p>The commands that we are going to use in the next steps are &lt;a href="https://gist.github.com/iht/6219b227424ada477462c7b9d9d93c57">available in a Gist
in Github&lt;/a>, just
in case you prefer to run that script instead of copying the commands from this
tutorial.&lt;/p>
&lt;h2 id="permissions-and-accounts">Permissions and accounts&lt;/h2>
&lt;p>When we run a Dataflow pipeline, we can use our personal Google Cloud
credentials to run the job. But Hop web will be running in a virtual machine,
and in Google Cloud, virtual machines run using service accounts as
credentials. So we need to make sure that we have a service account that has
permission to run Dataflow jobs.&lt;/p>
&lt;p>By default, virtual machines use the service account called &lt;em>Compute Engine
default service account&lt;/em>. For the sake of simplicity, we will use this
account. Still, we need to add some permissions to run Dataflow jobs with that
service account.&lt;/p>
&lt;p>First, let&amp;rsquo;s make sure that you have enabled all the required Google Cloud
APIs. &lt;a href="https://console.cloud.google.com/flows/enableapi?apiid=dataflow,compute_component,logging,storage_component,storage_api,bigquery,pubsub">Click this link to enable Dataflow, BigQuery and
Pub/Sub&lt;/a>,
which we’ll use in this workflow. The link takes you to your project in the
Google Cloud console, where you can enable the APIs.&lt;/p>
&lt;p>Let&amp;rsquo;s now give permissions to the VM account. First, find the ID of the service
account. Open Cloud Shell, and run the following command.&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud iam service-accounts list | grep compute
&lt;/code>&lt;/pre>&lt;p>The output is similar to the following, with &lt;code>&amp;lt;PROJECT_NUMBER&amp;gt;&lt;/code> replaced by your
project number:&lt;/p>
&lt;pre tabindex="0">&lt;code>EMAIL: &amp;lt;PROJECT_NUMBER&amp;gt;-compute@developer.gserviceaccount.com
&lt;/code>&lt;/pre>&lt;p>Copy that service account ID, because we use it in the next step. Run the
following command to grant the &lt;a href="https://cloud.google.com/dataflow/docs/concepts/access-control">Dataflow Admin
role&lt;/a> to the
service account. This role is required to run jobs:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member=&amp;#34;serviceAccount:&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&amp;#34; --role=&amp;#34;roles/dataflow.admin&amp;#34;
&lt;/code>&lt;/pre>&lt;p>where &lt;code>&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&lt;/code> is the ID that you retrieved previously. If you are
running these commands in Cloud Shell, the environment variable
&lt;code>GOOGLE_CLOUD_PROJECT&lt;/code> is already set to your project ID. If you are running
this from any other place, set the &lt;code>$GOOGLE_CLOUD_PROJECT&lt;/code> variable with the ID
of your project.&lt;/p>
&lt;p>Now your &amp;ldquo;user&amp;rdquo; for Dataflow is that service account. If your jobs are accessing
data in BigQuery, Cloud Storage, Pub/Sub, and so on, you also need to grant
roles for those services to the service account.&lt;/p>
&lt;h2 id="disk-and-virtual-machine">Disk and virtual machine&lt;/h2>
&lt;p>Let&amp;rsquo;s create a virtual machine (VM) in Compute Engine to run the Docker
container of Apache Hop.&lt;/p>
&lt;p>In Compute Engine, it is possible to run a container directly in a VM. There are
other options to run containers in Google Cloud, but a VM is probably the
simplest and most straightforward. The full details are in the &lt;a href="https://cloud.google.com/compute/docs/containers/deploying-containers">Deploying
containers on VMs and
MIGs&lt;/a>
page of the Google Cloud documentation.&lt;/p>
&lt;p>In this tutorial, we will always be working in the zone &lt;code>europe-west1-b&lt;/code>, so you
will see that zone in a lot of the commands. However, you can choose any Google
Cloud zone; just remember to use the value for your zone instead of
&lt;code>europe-west1-b&lt;/code>. Always use the same zone for all the resources, such as disks
and VMs. To minimize the latency when using Hop web, choose a zone that is
geographically close to your location. Let&amp;rsquo;s define the zone now and use this
variable for the rest of the commands:&lt;/p>
&lt;pre tabindex="0">&lt;code>ZONE=europe-west1-b
&lt;/code>&lt;/pre>&lt;p>Containers have ephemeral storage: when you restart the container, the disk of
the container returns to its original state. Therefore, if we restart the Hop
web container, we lose all our precious pipelines. To avoid that, we are going
to create a persistent disk, where we will store all our work with Hop web. Run
the following command to create the disk:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute disks create my-hop-disk \
--type=pd-balanced \
--size=10GB \
--zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Thanks to this disk, we’re able to stop the virtual machine and still keep all
our personal files in Hop web intact.&lt;/p>
&lt;p>Let&amp;rsquo;s now create the VM. For the VM, we need to select the network (&lt;code>default&lt;/code> in
the, well, default case) so the VM will not have a public IP address. This is
important for security reasons, but it won’t stop us from using the VM from our
web browser thanks to the Identity Aware Proxy. More on this later; for now
let&amp;rsquo;s create the VM:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute instances create-with-container my-hop-vm \
--zone=$ZONE \
--network-interface=subnet=default,no-address \
--scopes=https://www.googleapis.com/auth/cloud-platform \
--tags=http-server,https-server,ssh \
--container-image=apache/hop-web:2.0.1 \
--container-restart-policy=on-failure \
--container-mount-disk=mode=rw,mount-path=/root,name=my-hop-disk,partition=0 \
--disk=boot=no,device-name=my-hop-disk,mode=rw,name=my-hop-disk
&lt;/code>&lt;/pre>&lt;p>You might be wondering what those additional options are. They are required for
the VM to work properly with Hop web. For instance, the &lt;code>scopes&lt;/code> option is what
allows the VM to use Dataflow, and the &lt;code>tags&lt;/code> option lets your browser reach the
Hop web address through the network firewall.&lt;/p>
&lt;p>Apache Hop listens on port 8080 for HTTP connections, so if you have additional
custom firewall rules in your project, make sure you are not stopping TCP
traffic on port 8080.&lt;/p>
&lt;p>But wait a minute; we have created a machine with only private IPs. How can we
reach Hop web from the web browser on our computer? Don&amp;rsquo;t we need a public IP
address for that?&lt;/p>
&lt;p>Google Cloud has a feature called the Identity Aware Proxy (IAP) that can be
used to wrap services with an authorization layer, allowing connections to
resources with only internal IPs.&lt;/p>
&lt;p>We can use the IAP to wrap our Apache Hop web server. With the following
command, we create a tunnel listening on local port 8080 that connects to port
8080 on the VM:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>To keep the tunnel open, leave that command running. If the command fails right
after creating the VM, wait a few seconds and try again; the container might
still be booting up.&lt;/p>
&lt;p>We now have a tunnel that we can connect to using our web browser. If you’re
running these commands on your local computer and not in Cloud Shell, point your
browser to &lt;code>localhost:8080&lt;/code>. The Hop UI should load.&lt;/p>
&lt;p>If you are running these command in Cloud Shell, where do we point the browser
to? Cloud Shell comes with an utility for situations like this one. In Cloud
Shell, locate the &lt;strong>Web Preview&lt;/strong> button:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image3.png" alt="Web preview options">&lt;/p>
&lt;p>If the preview isn’t using port 8080, click &lt;strong>Change port&lt;/strong>, and switch to
port 8080. When you click &lt;strong>Preview on port&lt;/strong>, Cloud Shell opens a new tab in
your browser that points to the tunnel address.&lt;/p>
&lt;p>The &lt;strong>Identity Aware Proxy&lt;/strong> will ask you to identify yourself using your Google
account.&lt;/p>
&lt;p>After that, the Apache Hop web interface loads:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image4.png" alt="Hop web UI">&lt;/p>
&lt;p>That URL is authenticated using your Google account, the same one that you are
using for Google Cloud (the one you are authenticated with in the Google Cloud
SDK). So even if another person gets that URL address, they won’t be able to
access your Apache Hop instance.&lt;/p>
&lt;p>You are now ready to use Apache Hop in a web browser!&lt;/p>
&lt;p>You can try to replicate the example that was given &lt;a href="/blog/apache-hop-with-dataflow/">in a previous
post&lt;/a> using Hop web, or
just try to launch any other project from the samples included with Hop:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image5.png" alt="Sample projects in Hop">&lt;/p>
&lt;h2 id="where-should-i-store-my-stuff">Where should I store my stuff?&lt;/h2>
&lt;p>The directories in the file system of a container are ephemeral. How can you be
sure that you store your pipelines and JARs in a persistent location?&lt;/p>
&lt;p>The home directory container is &lt;code>/root&lt;/code>, and it is the only &lt;strong>persistent&lt;/strong>
directory in the container (thanks to the disk we created previously). When you
restart the VM for whatever reason, any file included in that directory is
retained. But the rest of the directories reset to their original state. So make
sure you save your stuff, such as your pipelines, the fat JAR generated for
Dataflow, and so on, in the &lt;code>/root&lt;/code> directory or its subdirectories.&lt;/p>
&lt;p>In the Hop file dialogs, when you click the home icon, you are directed to the
&lt;code>/root&lt;/code> directory, so it is very straightforward to use it to store
everything. In the example in the picture, we clicked the &lt;strong>Home&lt;/strong> button and
are storing a JAR in that persistent directory:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image6.png" alt="Hop file dialog">&lt;/p>
&lt;h2 id="turning-off-the-virtual-machine">Turning off the virtual machine&lt;/h2>
&lt;p>If you want to save some money when you are not using the virtual machine, stop
the VM and launch it again when needed. The content of the &lt;em>/root&lt;/em> directory is
saved when you stop the virtual machine.&lt;/p>
&lt;p>To stop the VM, run the following command (or in the console, on the Compute
Engine VM page, click &lt;strong>Stop&lt;/strong>):&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute instances stop my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>And to start it again, run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute instances start my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Remember that you need to have the Identity Aware Proxy running in order to
access Hop web, so after starting the VM, don&amp;rsquo;t forget to run the command to
start the Identity Aware Proxy (and if it fails right after starting, wait a few
seconds and run it again):&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>This post has shown that all that you need to run Hop is a web browser. And,
well, a Google Cloud project too.&lt;/p>
&lt;p>We deployed the container to a virtual machine in Google Cloud, so you can
access Hop from anywhere, and we created a persistent disk, so you can have
permanent storage for your pipelines. Now you can use your web browser to create
your pipelines and to run Dataflow jobs without having to install anything
locally in your computer: not Java, not Docker, not the Google Cloud SDK;
nothing, just your favourite web browser.&lt;/p>
&lt;p>If you followed the instructions in this post, head over to the post &lt;a href="/blog/apache-hop-with-dataflow/">Running
Apache Hop visual pipelines with Google Cloud
Dataflow&lt;/a> to run a
Dataflow pipeline right from your web browser!&lt;/p></description></item><item><title>Blog: Apache Beam 2.41.0</title><link>/blog/beam-2.41.0/</link><pubDate>Tue, 23 Aug 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.41.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.41.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2410-2022-08-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.41.0, check out the &lt;a href="https://github.com/apache/beam/milestone/3?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer is now on by default for streaming, matching the behavior of batch pipelines since 2.38.0. If you encounter a bug with the optimizer, please file an issue and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Previously available in Java sdk, Python sdk now also supports logging level overrides per module. (&lt;a href="https://github.com/apache/beam/issues/18222">#18222&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer may break Dataflow upgrade compatibility for optimized pipelines when it removes unused fields. If you need to upgrade and encounter a compatibility issue, disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Support for Spark 2.4.x is deprecated and will be dropped with the release of Beam 2.44.0 or soon after (Spark runner) (&lt;a href="https://github.com/apache/beam/issues/22094">#22094&lt;/a>).&lt;/li>
&lt;li>The modules &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services">amazon-web-services&lt;/a> and
&lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/kinesis">kinesis&lt;/a> for AWS Java SDK v1 are deprecated
in favor of &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services2">amazon-web-services2&lt;/a>
and will be eventually removed after a few Beam releases (Java) (&lt;a href="https://github.com/apache/beam/issues/21249">#21249&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed a condition where retrying queries would yield an incorrect cursor in the Java SDK Firestore Connector (&lt;a href="https://github.com/apache/beam/issues/22089">#22089&lt;/a>).&lt;/li>
&lt;li>Fixed plumbing allowed lateness in Go SDK. It was ignoring the user set value earlier and always used to set to 0. (&lt;a href="https://github.com/apache/beam/issues/22474">#22474&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/3">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.41.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
akashorabek
Alexey Inkin
Alexey Romanenko
Anand Inguva
andoni-guzman
Andrew Pilloud
Andrey
Andy Ye
Balázs Németh
Benjamin Gonzalez
BjornPrime
Brian Hulette
bulat safiullin
bullet03
Byron Ellis
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Daniel Thevessen
Danny McCormick
David Huntsperger
Dheeraj Gharde
Etienne Chauchot
Evan Galpin
Fernando Morales
Heejong Lee
Jack McCluskey
johnjcasey
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manu Zhang
Minbo Bae
Moritz Mack
Naireen Hussain
Ning Kang
Oleh Borysevych
Pablo Estrada
pablo rodriguez defino
Pranav Bhandari
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Steven Niemitz
Valentyn Tymofieiev
Vincent Marquez
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
yirutang
Yixiao Shen
Yu Feng&lt;/p></description></item><item><title>Blog: Big Improvements in Beam Go's 2.40 Release</title><link>/blog/go-2.40/</link><pubDate>Wed, 06 Jul 2022 00:00:01 -0800</pubDate><guid>/blog/go-2.40/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The 2.40 release is one of Beam Go&amp;rsquo;s biggest yet, and we wanted to highlight
some of the biggest changes coming with this important release!&lt;/p>
&lt;h1 id="native-streaming-support">Native Streaming Support&lt;/h1>
&lt;p>2.40 marks the release of one of our most anticipated feature sets yet:
native streaming Go pipelines. This includes adding support for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="/documentation/programming-guide/#user-initiated-checkpoint">Self Checkpointing&lt;/a>&lt;/li>
&lt;li>&lt;a href="/documentation/programming-guide/#watermark-estimation">Watermark Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="/documentation/programming-guide/#truncating-during-drain">Pipeline Drain/Truncation&lt;/a>&lt;/li>
&lt;li>&lt;a href="/documentation/programming-guide/#bundle-finalization">Bundle Finalization&lt;/a> (added in 2.39)&lt;/li>
&lt;/ul>
&lt;p>With all of these features, it is now possible to write your own streaming
pipeline source DoFns in Go without relying on cross-language transforms
from Java or Python. We encourage you to try out all of these new features
in your streaming pipelines! The &lt;a href="/documentation/programming-guide/#splittable-dofns">programming guide&lt;/a>
has additional information on getting started with native Go streaming DoFns.&lt;/p>
&lt;h1 id="generic-registration-make-your-pipelines-3x-faster">Generic Registration (Make Your Pipelines 3x Faster)&lt;/h1>
&lt;p>The release of &lt;a href="https://go.dev/blog/intro-generics">Go Generics&lt;/a> in Go 1.18
unlocked significant performance improvements for Beam Go. With generics,
we were able to add simple registration functions that can massively reduce
your pipeline&amp;rsquo;s runtime and resource consumption. For example, registering
the ParDo&amp;rsquo;s in our load tests which are designed to simulate a basic pipeline
reduced execution time from around 25 minutes to around 7 minutes on average&lt;/p>
&lt;ul>
&lt;li>an over 70% reduction!&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/go-registration.png"
alt="Beam Registration Load Tests ParDo Improvements">&lt;/p>
&lt;p>To get started with registering your own DoFns and unlocking these performance
gains, check out the &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/go/pkg/beam/register">registration doc page&lt;/a>.&lt;/p>
&lt;h1 id="whats-next">What&amp;rsquo;s Next?&lt;/h1>
&lt;p>Moving forward, we remain focused on improving the streaming experience and
leveraging generics to improve the SDK. Specific improvements we are considering
include adding &lt;a href="/documentation/programming-guide/#state-and-timers">State &amp;amp; Timers&lt;/a>
support, introducing a Go expansion service so that Go DoFns can be used in other
languages, and wrapping more Java and Python IOs so that they can be easily used
in Go. As always, please let us know what changes you would like to see by
&lt;a href="https://github.com/apache/beam/issues/new/choose">filing an issue&lt;/a>,
&lt;a href="dev@beam.apache.org">emailing the dev list&lt;/a>, or starting a &lt;a href="https://app.slack.com/client/T4S1WH2J3/C9H0YNP3P">slack thread&lt;/a>!&lt;/p></description></item><item><title>Blog: Apache Beam 2.40.0</title><link>/blog/beam-2.40.0/</link><pubDate>Sat, 25 Jun 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.40.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.40.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2400-2022-06-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.40.0 check out the &lt;a href="https://github.com/apache/beam/releases/tag/v2.40.0">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added &lt;a href="https://s.apache.org/inference-sklearn-pytorch">RunInference&lt;/a> API, a framework agnostic transform for inference. With this release, PyTorch and Scikit-learn are supported by the transform.
See also example at apache_beam/examples/inference/pytorch_image_classification.py&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Upgraded to Hive 3.1.3 for HCatalogIO. Users can still provide their own version of Hive. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">Issue-19554&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Go SDK users can now use generic registration functions to optimize their DoFn execution. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>)&lt;/li>
&lt;li>Go SDK users may now write self-checkpointing Splittable DoFns to read from streaming sources. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11104">BEAM-11104&lt;/a>)&lt;/li>
&lt;li>Go SDK textio Reads have been moved to Splittable DoFns exclusively. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14489">BEAM-14489&lt;/a>)&lt;/li>
&lt;li>Pipeline drain support added for Go SDK has now been tested. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>)&lt;/li>
&lt;li>Go SDK users can now see heap usage, sideinput cache stats, and active process bundle stats in Worker Status. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13829">BEAM-13829&lt;/a>)&lt;/li>
&lt;li>The serialization (pickling) library for Python is dill==0.3.1.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11167">BEAM-11167&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go Sdk now requires a minimum version of 1.18 in order to support generics (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>).&lt;/li>
&lt;li>synthetic.SourceConfig field types have changed to int64 from int for better compatibility with Flink&amp;rsquo;s use of Logical types in Schemas (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14173">BEAM-14173&lt;/a>)&lt;/li>
&lt;li>Default coder updated to compress sources used with &lt;code>BoundedSourceAsSDFWrapperFn&lt;/code> and &lt;code>UnboundedSourceAsSDFWrapper&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java expansion service to allow specific files to stage (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14160">BEAM-14160&lt;/a>).&lt;/li>
&lt;li>Fixed Elasticsearch connection when using both ssl and username/password (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14000">BEAM-14000&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python&amp;rsquo;s &lt;code>beam.FlatMap&lt;/code> will raise &lt;code>AttributeError: 'builtin_function_or_method' object has no attribute '__func__'&lt;/code> when
constructed with some
&lt;a href="https://docs.python.org/3/library/functions.html">built-ins&lt;/a>, like &lt;code>sum&lt;/code>
and &lt;code>len&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/22091">#22091&lt;/a>).&lt;/li>
&lt;li>Java&amp;rsquo;s &lt;code>BigQueryIO.Write&lt;/code> can have an exception where it attempts to output a timestamp beyond the max timestamp range
&lt;code>Cannot output with timestamp 294247-01-10T04:00:54.776Z. Output timestamps must be no earlier than the timestamp of the current input or timer (294247-01-10T04:00:54.776Z) minus the allowed skew (0 milliseconds) and no later than 294247-01-10T04:00:54.775Z. See the DoFn#getAllowedTimestampSkew() Javadoc for details on changing the allowed skew.&lt;/code>
This happens when a sink is idle, causing the idle timeout to trigger, or when a specific table is idle long enough when using dynamic destinations.
When this happens, the job is no longer able to be drained. This has been fixed for the 2.41 release.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.40.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
Aizhamal Nurmamat kyzy
Alejandro Rodriguez-Morantes
Alexander Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
Andy Ye
Balázs Németh
Benjamin Gonzalez
Brian Hulette
bulat safiullin
bullet03
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Diego Gomez
dpcollins-google
Ekaterina Tatanova
Elias Segundo
Etienne Chauchot
Evan Galpin
fbeevikm
Fernando Morales
Heejong Lee
Igor Krasavin
Ilion Beyst
Israel Herraiz
Jack McCluskey
Jan Kuehle
Jan Lukavský
johnjcasey
Jonathan Lui
jrmccluskey
Julien Tournay
Kenneth Knowles
Kerry Donny-Clark
Kevin Puthusseri
Kiley Sok
Kyle Weaver
kynx
Lucas Nogueira
Luke Cwik
LuNing Wang
Marco Robles
masahitojp
Minbo Bae
Moritz Mack
Naireen Hussain
Nancy Xu
Niel Markwick
Ning Kang
nishant jain
nishantjain91
Oskar Firlej
Pablo Estrada
pablo rodriguez defino
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Thiago Nunes
Tom Stepp
vachan-shetty
Valentyn Tymofieiev
vikash2310
Vitaly Terentyev
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yiru Tang
yixiaoshen
zwestrick&lt;/p></description></item><item><title>Blog: Apache Beam 2.39.0</title><link>/blog/beam-2.39.0/</link><pubDate>Wed, 25 May 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.39.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.39.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2390-2022-05-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.39.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ConfigureReleaseNote.jspa?projectId=12319527&amp;amp;version=12351170">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>JmsIO gains the ability to map any kind of input to any subclass of &lt;code>javax.jms.Message&lt;/code> (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).&lt;/li>
&lt;li>JmsIO introduces the ability to write to dynamic topics (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).
&lt;ul>
&lt;li>A &lt;code>topicNameMapper&lt;/code> must be set to extract the topic name from the input value.&lt;/li>
&lt;li>A &lt;code>valueMapper&lt;/code> must be set to convert the input value to JMS message.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reduce number of threads spawned by BigqueryIO StreamingInserts (
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14283">BEAM-14283&lt;/a>).&lt;/li>
&lt;li>Implemented Apache PulsarIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8218">BEAM-8218&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for flink scala 2.12, because most of the libraries support version 2.12 onwards. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14386">beam-14386&lt;/a>)&lt;/li>
&lt;li>&amp;lsquo;Manage Clusters&amp;rsquo; JupyterLab extension added for users to configure usage of Dataproc clusters managed by Interactive Beam (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>Pipeline drain support added for Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>). &lt;strong>Note: this feature is not yet fully validated and should be treated as experimental in this release.&lt;/strong>&lt;/li>
&lt;li>&lt;code>DataFrame.unstack()&lt;/code>, &lt;code>DataFrame.pivot() &lt;/code> and &lt;code>Series.unstack()&lt;/code>
implemented for DataFrame API (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13948">BEAM-13948&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13966">BEAM-13966&lt;/a>).&lt;/li>
&lt;li>Support for impersonation credentials added to dataflow runner in the Java and Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14014">BEAM-14014&lt;/a>).&lt;/li>
&lt;li>Implemented Jupyterlab extension for managing Dataproc clusters (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>ExternalPythonTransform API added for easily invoking Python transforms from
Java (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14143">BEAM-14143&lt;/a>).&lt;/li>
&lt;li>Added Add support for Elasticsearch 8.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14003">BEAM-14003&lt;/a>).&lt;/li>
&lt;li>Shard aware Kinesis record aggregation (AWS Sdk v2), (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14104">BEAM-14104&lt;/a>).&lt;/li>
&lt;li>Upgrade to ZetaSQL 2022.04.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14348">BEAM-14348&lt;/a>).&lt;/li>
&lt;li>Fixed ReadFromBigQuery cannot be used with the interactive runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14112">BEAM-14112&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Unused functions &lt;code>ShallowCloneParDoPayload()&lt;/code>, &lt;code>ShallowCloneSideInput()&lt;/code>, and &lt;code>ShallowCloneFunctionSpec()&lt;/code> have been removed from the Go SDK&amp;rsquo;s pipelinex package (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13739">BEAM-13739&lt;/a>).&lt;/li>
&lt;li>JmsIO requires an explicit &lt;code>valueMapper&lt;/code> to be set (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>). You can use the &lt;code>TextMessageMapper&lt;/code> to convert &lt;code>String&lt;/code> inputs to JMS &lt;code>TestMessage&lt;/code>s:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl"> &lt;span class="n">JmsIO&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="na">withConnectionFactory&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">jmsConnectionFactory&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="na">withValueMapper&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">TextMessageMapper&lt;/span>&lt;span class="o">());&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Coders in Python are expected to inherit from Coder. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14351">BEAM-14351&lt;/a>).&lt;/li>
&lt;li>New abstract method &lt;code>metadata()&lt;/code> added to io.filesystem.FileSystem in the
Python SDK. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14314">BEAM-14314&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Flink 1.11 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14139">BEAM-14139&lt;/a>).&lt;/li>
&lt;li>Python 3.6 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13657">BEAM-13657&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java Spanner IO NPE when ProjectID not specified in template executions (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14405">BEAM-14405&lt;/a>).&lt;/li>
&lt;li>Fixed potential NPE in BigQueryServicesImpl.getErrorInfo (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14133">BEAM-14133&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/browse/BEAM-14412?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.39.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.39.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alexander Zhuravlev,
Alexey Romanenko,
Anand Inguva,
Andrei Gurau,
Andrew Pilloud,
Andy Ye,
Arun Pandian,
Arwin Tio,
Aydar Farrakhov,
Aydar Zainutdinov,
AydarZaynutdinov,
Balázs Németh,
Benjamin Gonzalez,
Brian Hulette,
Buqian Zheng,
Chamikara Jayalath,
Chun Yang,
Daniel Oliveira,
Daniela Martín,
Danny McCormick,
David Huntsperger,
Deepak Nagaraj,
Denise Case,
Esun Kim,
Etienne Chauchot,
Evan Galpin,
Hector Miuler Malpica Gallegos,
Heejong Lee,
Hengfeng Li,
Ilango Rajagopal,
Ilion Beyst,
Israel Herraiz,
Jack McCluskey,
Kamil Bregula,
Kamil Breguła,
Ke Wu,
Kenneth Knowles,
KevinGG,
Kiley,
Kiley Sok,
Kyle Weaver,
Liam Miller-Cushon,
Luke Cwik,
Marco Robles,
Matt Casters,
Michael Li,
MiguelAnzoWizeline,
Milan Patel,
Minbo Bae,
Moritz Mack,
Nick Caballero,
Niel Markwick,
Ning Kang,
Oskar Firlej,
Pablo Estrada,
Pavel Avilov,
Reuven Lax,
Reza Rokni,
Ritesh Ghorse,
Robert Bradshaw,
Robert Burke,
Ryan Thompson,
Sam Whittle,
Steven Niemitz,
Thiago Nunes,
Tomo Suzuki,
Valentyn Tymofieiev,
Victor,
Yi Hu,
Yichi Zhang,
Yiru Tang,
ahmedabu98,
andoni-guzman,
brachipa,
bulat safiullin,
bullet03,
dannymartinm,
daria.malkova,
dpcollins-google,
egalpin,
emily,
fbeevikm,
johnjcasey,
kileys,
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>,
nguyennk92,
pablo rodriguez defino,
rszper,
rvballada,
sachinag,
tvalentyn,
vachan-shetty,
yirutang&lt;/p></description></item><item><title>Blog: Running Beam SQL in notebooks</title><link>/blog/beam-sql-with-notebooks/</link><pubDate>Thu, 28 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/beam-sql-with-notebooks/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>&lt;a href="/documentation/dsls/sql/overview/">Beam SQL&lt;/a> allows a
Beam user to query PCollections with SQL statements.
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/runners/interactive#interactive-beam">Interactive Beam&lt;/a>
provides an integration between Apache Beam and
&lt;a href="https://docs.jupyter.org/en/latest/">Jupyter Notebooks&lt;/a> (formerly known as
IPython Notebooks) to make pipeline prototyping and data exploration much faster
and easier.
You can set up your own notebook user interface (for example,
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">JupyterLab&lt;/a>
or classic &lt;a href="https://docs.jupyter.org/en/latest/install.html">Jupyter Notebooks&lt;/a>)
on your own device following their documentations. Alternatively, you can
choose a hosted solution that does everything for you. You are free to select
whichever notebook user interface you prefer. For simplicity, this
post does not go through the notebook environment setup and uses
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development">Apache Beam Notebooks&lt;/a>
that provides a cloud-hosted
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab&lt;/a> environment and lets
a Beam user iteratively develop pipelines, inspect pipeline graphs, and parse
individual PCollections in a read-eval-print-loop (REPL) workflow.&lt;/p>
&lt;p>In this post, you will see how to use &lt;code>beam_sql&lt;/code>, a notebook
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">magic&lt;/a>, to
execute Beam SQL in notebooks and inspect the results.&lt;/p>
&lt;p>By the end of the post, it also demonstrates how to use the &lt;code>beam_sql&lt;/code> magic
with a production environment, such as running it as a one-shot job on
Dataflow. It&amp;rsquo;s optional. To follow those steps, you should have a project in
Google Cloud Platform with
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#before_you_begin">necessary APIs enabled&lt;/a>
, and you should have enough permissions to create a Google Cloud Storage bucket
(or to use an existing one), query a public Google Cloud BigQuery dataset, and
run Dataflow jobs.&lt;/p>
&lt;p>If you choose to use the cloud hosted notebook solution, once you have your
Google Cloud project ready, you will need to create an Apache Beam Notebooks
instance and open the JupyterLab web interface. Please follow the instructions
given at:
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance">https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance&lt;/a>&lt;/p>
&lt;h2 id="getting-familiar-with-the-environment">Getting familiar with the environment&lt;/h2>
&lt;h3 id="landing-page">Landing page&lt;/h3>
&lt;p>After starting your own notebook user interface: for example, if using Apche
Beam Notebooks, after clicking the &lt;code>OPEN JUPYTERLAB&lt;/code> link, you will land on
the default launcher page of the notebook environment.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image1.png"
alt="Beam SQL in Notebooks: landing page">&lt;/p>
&lt;p>On the left side, there is a file explorer to view examples, tutorials and
assets on the notebook instance. To easily navigate the files, you may
double-click the &lt;code>00-Start_Here.md&lt;/code> (#1 in the screenshot) file to view detailed
information about the files.&lt;/p>
&lt;p>On the right side, it displays the default launcher page of JupyterLab. To
create and open a completely new notebook file and code with a selected version
of Apache Beam, click one of (#2) the items with Apache Beam &amp;gt;=2.34.0 (because
&lt;code>beam_sql&lt;/code> was introduced in 2.34.0) installed.&lt;/p>
&lt;h3 id="createopen-a-notebook">Create/open a notebook&lt;/h3>
&lt;p>For example, if you clicked the image button with Apache Beam 2.36.0, you would
see an &lt;code>Untitled.ipynb&lt;/code> file created and opened.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image2.png"
alt="Beam SQL in Notebooks: create/open a notebook ">&lt;/p>
&lt;p>In the file explorer, your new notebook file has been created as
&lt;code>Untitled.ipynb&lt;/code>.&lt;/p>
&lt;p>On the right side, in the opened notebook, there are 4 buttons on top that you
may interact most frequently with:&lt;/p>
&lt;ul>
&lt;li>#1: insert an empty code block after the selected / highlighted code block&lt;/li>
&lt;li>#2: execute the code in the block that is selected / highlighted&lt;/li>
&lt;li>#3: interrupt code execution if your code execution is stuck&lt;/li>
&lt;li>#4: “Restart the kernel”: clear all states from code executions and start
from fresh&lt;/li>
&lt;/ul>
&lt;p>There is a button on the top-right (#5) for you to choose a different Apache
Beam version if needed, so it’s not set in stone.&lt;/p>
&lt;p>You can always double-click a file from the file explorer to open it without
creating a new one.&lt;/p>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;h3 id="beam_sql-magic">&lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>&lt;code>beam_sql&lt;/code> is an IPython
&lt;a href="https://ipython.readthedocs.io/en/stable/config/custommagics.html">custom magic&lt;/a>.
If you&amp;rsquo;re not familiar with magics, here are some
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">built-in examples&lt;/a>.
It&amp;rsquo;s a convenient way to validate your queries locally against known/test data
sources when prototyping a Beam pipeline with SQL, before productionizing it on
remote cluster/services.&lt;/p>
&lt;p>The Apache Beam Notebooks environment has preloaded the &lt;code>beam_sql&lt;/code> magic and
basic &lt;code>apache-beam&lt;/code> modules so you can directly use them without additional
imports. You can also explicitly load the magic via
&lt;code>%load_ext apache_beam.runners.interactive.sql.beam_sql_magics&lt;/code> and
&lt;code>apache-beam&lt;/code> modules if you set up your own notebook elsewhere.&lt;/p>
&lt;p>You can type:&lt;/p>
&lt;pre tabindex="0">&lt;code>%beam_sql -h
&lt;/code>&lt;/pre>&lt;p>and then execute the code to learn how to use the magic:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image3.png"
alt="Beam SQL in Notebooks: beam_sql magic help message ">&lt;/p>
&lt;p>The selected/highlighted block is called a notebook cell. It mainly has 3
components:&lt;/p>
&lt;ul>
&lt;li>#1: The execution count. &lt;code>[1]&lt;/code> indicates this block is the first executed
code. It increases by 1 for each piece of code you execute even if you
re-execute the same piece of code. &lt;code>[ ]&lt;/code> indicates this block is not
executed.&lt;/li>
&lt;li>#2: The cell input: the code gets executed.&lt;/li>
&lt;li>#3: The cell output: the output of the code execution. Here it contains the
help documentation of the &lt;code>beam_sql&lt;/code> magic.&lt;/li>
&lt;/ul>
&lt;h3 id="create-a-pcollection">Create a PCollection&lt;/h3>
&lt;p>There are 3 scenarios for Beam SQL when creating a PCollection:&lt;/p>
&lt;ol>
&lt;li>Use Beam SQL to create a PCollection from constant values&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o pcoll
SELECT CAST(1 AS INT) AS id, CAST(&amp;#39;foo&amp;#39; AS VARCHAR) AS str, CAST(3.14 AS DOUBLE) AS flt
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image4.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from raw values.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic creates and outputs a PCollection named &lt;code>pcoll&lt;/code> with
element_type like &lt;code>BeamSchema_...(id: int32, str: str, flt: float64)&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> that you have &lt;strong>not&lt;/strong> explicitly created a Beam pipeline. You get a
PCollection because the &lt;code>beam_sql&lt;/code> magic always implicitly creates a pipeline to
execute your SQL query. To hold the elements with each field&amp;rsquo;s type info, Beam
automatically creates a
&lt;a href="/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>
as the &lt;code>element_type&lt;/code> for the created PCollection. You will learn more about
schema-aware PCollections later.&lt;/p>
&lt;ol start="2">
&lt;li>Use Beam SQL to query a PCollection&lt;/li>
&lt;/ol>
&lt;p>You can chain another SQL using the output from a previous SQL (or any
schema-aware PCollection produced by any normal Beam PTransforms) as the input
to produce a new PCollection.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: if you name the output PCollection, make sure that it’s unique in your
notebook to avoid overwriting a different PCollection.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o id_pcoll
SELECT id FROM pcoll
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image5.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from another.">&lt;/p>
&lt;ol start="3">
&lt;li>Use Beam SQL to join multiple PCollections&lt;/li>
&lt;/ol>
&lt;p>You can query multiple PCollections from a single query.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o str_with_same_id
SELECT id, str FROM pcoll JOIN id_pcoll USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image6.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from multiple PCollections.">&lt;/p>
&lt;p>Now you have learned how to use the &lt;code>beam_sql&lt;/code> magic to create PCollections and
inspect their results.&lt;/p>
&lt;p>&lt;strong>Tip&lt;/strong>: if you accidentally delete some of the notebook cell outputs, you can
always check the content of a PCollection by invoking &lt;code>ib.show(pcoll_name)&lt;/code> or
&lt;code>ib.collect(pcoll_name)&lt;/code> where &lt;code>ib&lt;/code> stands for “Interactive Beam”
(&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#reading_and_visualizing_the_data">learn more&lt;/a>).&lt;/p>
&lt;h3 id="schema-aware-pcollections">Schema-aware PCollections&lt;/h3>
&lt;p>The &lt;code>beam_sql&lt;/code> magic provides the flexibility to seamlessly mix SQL and non-SQL
Beam statements to build pipelines and even run them on Dataflow. However, each
PCollection queried by Beam SQL needs to have a
&lt;a href="/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>.
For the &lt;code>beam_sql&lt;/code> magic, it’s recommended to use &lt;code>typing.NamedTuple&lt;/code> when a
schema is desired. You can go through the below example to learn more details
about schema-aware PCollections.&lt;/p>
&lt;h4 id="setup">Setup&lt;/h4>
&lt;p>In the setup of this example, you will:&lt;/p>
&lt;ul>
&lt;li>Install PyPI package &lt;code>names&lt;/code> using the built-in &lt;code>%pip&lt;/code> magic: you will use
the module to generate some random English names as the raw data input.&lt;/li>
&lt;li>Define a schema with &lt;code>NamedTuple&lt;/code> that has 2 attributes: &lt;code>id&lt;/code> - an unique
numeric identifier of a person; &lt;code>name&lt;/code> - a string name of a person.&lt;/li>
&lt;li>Define a pipeline with an &lt;code>InteractiveRunner&lt;/code> to utilize notebook related
features of Apache Beam.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">%&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">names&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">names&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is no visible output for the code execution.&lt;/p>
&lt;h4 id="create-schema-aware-pcollections-without-using-sql">Create schema-aware PCollections without using SQL&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">persons&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image7.png"
alt="Beam SQL in Notebooks: create a schema-aware PCollection without SQL.">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">persons_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">15&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons_2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image8.png"
alt="Beam SQL in Notebooks: create another schema-aware PCollection without SQL.">&lt;/p>
&lt;p>Now you have 2 PCollections both with the same schema defined by the &lt;code>Person&lt;/code>
class:&lt;/p>
&lt;ul>
&lt;li>&lt;code>persons&lt;/code> contains 10 records for 10 persons with ids ranging from 0 to 9,&lt;/li>
&lt;li>&lt;code>persons_2&lt;/code> contains another 10 records for 10 persons with ids ranging from
5 to 14.&lt;/li>
&lt;/ul>
&lt;h4 id="encode-and-decode-of-schema-aware-pcollections">Encode and Decode of schema-aware PCollections&lt;/h4>
&lt;p>For this example, you still need one more piece of data from the first &lt;code>pcoll&lt;/code>
that you have created with instructions in this post.&lt;/p>
&lt;p>You can use the original &lt;code>pcoll&lt;/code>. Optionally, if you want to exercise using
coders explicitly with schema-aware PCollections, you can add a Text I/O into
the mix: write the content of &lt;code>pcoll&lt;/code> into a text file retaining its schema
information, then read the file back into a new schema-aware PCollection called
&lt;code>pcoll_in_file&lt;/code>, and use the new PCollection to join &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>
to find names with the common id in all three of them.&lt;/p>
&lt;p>To encode &lt;code>pcoll&lt;/code> into a file, execute:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pcoll&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;/tmp/pcoll&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pipeline&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait_until_finish&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">!&lt;/span>&lt;span class="n">cat&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">tmp&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">*&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image9.png"
alt="Beam SQL in Notebooks: write a schema-aware PCollection into a text file.">&lt;/p>
&lt;p>The above code execution writes the PCollection &lt;code>pcoll&lt;/code> (basically
&lt;code>{id: 1, str: foo, flt: 3.14}&lt;/code>) into a text file using the coder assigned by
Beam. As you can see, the file content is recorded in a binary non
human-readable format, and that’s normal.&lt;/p>
&lt;p>To decode the file content into a new PCollection, execute:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">pcoll_in_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;/tmp/pcoll*&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll_in_file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image10.png"
alt="Beam SQL in Notebooks: read a schema-aware PCollection from a text file.">&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> you have to use the same coder during encoding and decoding, and
furthermore you may assign the schema explicitly to the new PCollection through
&lt;code>with_output_types()&lt;/code>.&lt;/p>
&lt;p>Reading out the encoded binary content from the text file and decoding it with
the correct coder, the content of &lt;code>pcoll&lt;/code> is recovered into &lt;code>pcoll_in_file&lt;/code>. You
can use this technique to save and share your data through any Beam I/O (not
necessarily a text file) with collaborators who work on their own pipelines (not
just in your notebook session or pipelines).&lt;/p>
&lt;h4 id="schema-in-beam_sql-magic">Schema in &lt;code>beam_sql&lt;/code> magic&lt;/h4>
&lt;p>The &lt;code>beam_sql&lt;/code> magic automatically registers a &lt;code>RowCoder&lt;/code> for your &lt;code>NamedTuple&lt;/code>
schema so that you only need to focus on preparing your data for query without
worrying about coders. To see more verbose details of what the &lt;code>beam_sql&lt;/code> magic
does behind the scenes, you can use the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>For example, you can look for all elements with &lt;code>id &amp;lt; 5&lt;/code> in &lt;code>persons&lt;/code> with the
below query and assign the output to &lt;code>persons_id_lt_5&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o persons_id_lt_5 -v
SELECT * FROM persons WHERE id &amp;lt; 5
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image11.png"
alt="Beam SQL in Notebooks: beam_sql registers a schema for a PCollection.">&lt;/p>
&lt;p>Since this is the first time running this query, you might see a warning message
about:&lt;/p>
&lt;blockquote>
&lt;p>Schema Person has not been registered to use a RowCoder. Automatically
registering it by running:
beam.coders.registry.register_coder(Person, beam.coders.RowCoder)&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>beam_sql&lt;/code> magic helps registering a &lt;code>RowCoder&lt;/code> for each schema you define
and use whenever it finds one. You can also explicitly run the same code to do
so.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> the output element type is &lt;code>Person(id: int, name: str)&lt;/code> instead of
&lt;code>BeamSchema_…&lt;/code> because you have selected all the fields from a single
PCollection of the known type &lt;code>Person(id: int, name: str)&lt;/code>.&lt;/p>
&lt;p>Another example, you can query for all names from &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code> with
the same ids and assign the output to &lt;code>persons_with_common_id&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o persons_with_common_id -v
SELECT * FROM persons JOIN persons_2 USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image12.png"
alt="Beam SQL in Notebooks: beam_sql creates a schema for a query.">&lt;/p>
&lt;p>Note the output element type is now some
&lt;code>BeamSchema_...(id: int64, name: str, name0: str)&lt;/code>. Because you have selected
columns from both PCollections, there is no known schema to hold the result.
Beam automatically creates a schema and differentiates the conflicted field
&lt;code>name&lt;/code> by suffixing 0 to one of them.&lt;/p>
&lt;p>And since &lt;code>Person&lt;/code> is already previously registered with a &lt;code>RowCoder&lt;/code>, there is
no more warning about registering it even with the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>Additionally, you can do a join with &lt;code>pcoll_in_file&lt;/code>, &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o entry_with_common_id
SELECT pcoll_in_file.id, persons.name AS name_1, persons_2.name AS name_2
FROM pcoll_in_file JOIN persons ON pcoll_in_file.id = persons.id
JOIN persons_2 ON pcoll_in_file.id = persons_2.id
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image13.png"
alt="Beam SQL in Notebooks: rename fields in a query.">&lt;/p>
&lt;p>The schema generated reflects the column renaming you have done in the SQL.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>You will go through an example to find out the US state with the most COVID
positive cases on a specific day with data provided by the
&lt;a href="https://covidtracking.com/">covid tracking project&lt;/a>.&lt;/p>
&lt;h3 id="get-the-data">Get the data&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">requests&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The covidtracking project has stopped collecting new data, current data ends on 2021-03-07&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">json_current&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;https://api.covidtracking.com/v1/states/current.json&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">requests&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">loads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">current_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">json_current&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">current_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image14.png"
alt="Beam SQL in Notebooks: preview example data.">&lt;/p>
&lt;p>The data is dated as 2021-03-07. It contains many details about COVID cases for
different states in the US. &lt;code>current_data[0]&lt;/code> is just one of the data points.&lt;/p>
&lt;p>You can get rid of most of the columns of the data. For example, just focus on
“date”, “state”, “positive” and “negative”, and then define a schema
&lt;code>UsCovidData&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="c1"># Remember to str(e[&amp;#39;date&amp;#39;]).&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positive&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">negative&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>date&lt;/code> is a keyword in (Calcite)SQL, use a different field name such as
&lt;code>partition_date&lt;/code>;&lt;/li>
&lt;li>&lt;code>date&lt;/code> from the data is an &lt;code>int&lt;/code> type, not &lt;code>str&lt;/code>. Make sure you convert the
data using &lt;code>str()&lt;/code> or use &lt;code>date: int&lt;/code>.&lt;/li>
&lt;li>&lt;code>negative&lt;/code> has missing values and the default is &lt;code>None&lt;/code>. So instead of
&lt;code>negative: int&lt;/code>, it should be &lt;code>negative: Optional[int]&lt;/code>. Or you can convert
&lt;code>None&lt;/code> into 0 when using the schema.&lt;/li>
&lt;/ul>
&lt;p>Then parse the json data into a PCollection with the schema:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_sql&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_sql&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Create PCollection from json&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positive&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;positive&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">negative&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;negative&amp;#39;&lt;/span>&lt;span class="p">]))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">covid_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image15.png"
alt="Beam SQL in Notebooks: parse example data with a schema.">&lt;/p>
&lt;h3 id="query">Query&lt;/h3>
&lt;p>You can now find the biggest positive on the “current day” (2021-03-07).&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o max_positive
SELECT partition_date, MAX(positive) AS positive
FROM covid_data
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image16.png"
alt="Beam SQL in Notebooks: find the biggest positive from the data.">&lt;/p>
&lt;p>However, this is just the positive number. You cannot observe the state that has
this maximum number nor the negative case number for the state.&lt;/p>
&lt;p>To enrich your result, you have to join this data back to the original data set
you have parsed.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o entry_with_max_positive
SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS negative
FROM covid_data JOIN max_positive
USING (partition_date, positive)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image17.png"
alt="Beam SQL in Notebooks: enriched data with biggest positive.">&lt;/p>
&lt;p>Now you can see all columns of the data with the maximum positive case on
2021-03-07.
&lt;strong>Note&lt;/strong>: to handle missing values of the negative column in the original data,
you can use &lt;code>{fn IFNULL(covid_data.negative, 0)}&lt;/code> to set null values to 0.&lt;/p>
&lt;p>When you&amp;rsquo;re ready to scale up, you can translate the SQLs into a pipeline with
&lt;code>SqlTransform&lt;/code>s and run your pipeline on a distributed runner like Flink or
Spark. This post demonstrates it by launching a one-shot job on Dataflow from
the notebook with the help of &lt;code>beam_sql&lt;/code> magic.&lt;/p>
&lt;h3 id="run-on-dataflow">Run on Dataflow&lt;/h3>
&lt;p>Now that you have a pipeline that parses US COVID data from json to find
positive/negative/state information for the state with the most positive cases
on each day, you can try applying it to all historical daily data and running it
on Dataflow.&lt;/p>
&lt;p>The new data source you will use is a public dataset from USAFacts US
Coronavirus Database that contains all historical daily summary of COVID cases
in the US.&lt;/p>
&lt;p>The schema of data is very similar to what the covid tracking project website
provides. The fields you will query are: &lt;code>date&lt;/code>, &lt;code>state&lt;/code>, &lt;code>confirmed_cases&lt;/code>, and
&lt;code>deaths&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image18.png"
alt="Beam SQL in Notebooks: schema of cloud data.">&lt;/p>
&lt;p>A preview of the data looks like below (you may skip the inspection in BigQuery
and just take a look at the screenshot):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image19.png"
alt="Beam SQL in Notebooks: preview of cloud data.">&lt;/p>
&lt;p>The format of the data is &lt;strong>slightly different&lt;/strong> from the json data you parsed
in the previous pipeline because the numbers are grouped by counties instead of
states, thus some additional aggregations need to be done in the SQLs.&lt;/p>
&lt;p>If you need a fresh execution, you may click the “Restart the kernel” button on
the top menu.&lt;/p>
&lt;p>Full code is as below, on-top of the original pipeline and queries:&lt;/p>
&lt;ul>
&lt;li>It changes the source from a single-day data to a more complete historical
data;&lt;/li>
&lt;li>It changes the I/O and schema to accommodate the new dataset;&lt;/li>
&lt;li>It changes the SQLs to include more aggregations to accommodate the new
format of the dataset.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prepare the data with schema&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Public BQ dataset.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;bigquery-public-data:covid19_usafacts.summary&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replace with your project.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;YOUR-PROJECT-NAME-HERE&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replace with your GCS bucket.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gcs_location&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;gs://YOUR_GCS_BUCKET_HERE&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">deaths&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_on_dataflow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_on_dataflow&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Read dataset&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">project&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gcs_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">confirmed_cases&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">deaths&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">])))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Run on Dataflow&lt;/strong>&lt;/p>
&lt;p>To run SQL on Dataflow is very simple, you just need to add the option
&lt;code>-r DataflowRunner&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o data_by_state -r DataflowRunner
SELECT partition_date, state, SUM(confirmed_cases) as confirmed_cases, SUM(deaths) as deaths
FROM covid_data
GROUP BY partition_date, state
&lt;/code>&lt;/pre>&lt;p>Different from previous &lt;code>beam_sql&lt;/code> magic executions, you won’t see the result
immediately. Instead, a form like below is printed in the notebook cell output:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image20.png"
alt="Beam SQL in Notebooks: empty run-on-dataflow form.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic tries its best to guess your project id and preferred cloud
region. You still have to input additional information necessary to submit a
Dataflow job, such as a GCS bucket to stage the Dataflow job and any additional
Python dependencies the job needs.&lt;/p>
&lt;p>For now, ignore the form in the cell output, because you still need 2 more SQLs
to: 1) find the maximum confirmed cases on each day; 2) join the maximum case
data with the full data_by_state. The &lt;code>beam_sql&lt;/code> magic allows you to chain SQLs,
so chain 2 more by executing:&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o max_cases -r DataflowRunner
SELECT partition_date, MAX(confirmed_cases) as confirmed_cases
FROM data_by_state
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>And&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o data_with_max_cases -r DataflowRunner
SELECT data_by_state.partition_date, data_by_state.state, data_by_state.confirmed_cases, data_by_state.deaths
FROM data_by_state JOIN max_cases
USING (partition_date, confirmed_cases)
&lt;/code>&lt;/pre>&lt;p>By default, when running &lt;code>beam_sql&lt;/code> on Dataflow, the output PCollection will be
written to a text file on GCS. The “write” is automatically provided by
&lt;code>beam_sql&lt;/code> and mainly for your inspection of the output data for this one-shot
Dataflow job. It’s lightweight and does not encode elements for further
development. To save the output and share it with others, you can add more Beam
I/Os into the mix.&lt;/p>
&lt;p>For example, you can appropriately encode elements into text files using the
technique described in the above schema-aware PCollections example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">GoogleCloudOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">coder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">max_data_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcs_location&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s1">&amp;#39;/encoded_max_data&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">data_with_max_cases&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_data_file&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, you can create a new BQ dataset in your own project to store the
processed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image21.png"
alt="Beam SQL in Notebooks: create a new BQ dataset.">&lt;/p>
&lt;p>You have to select the same data location as the public BigQuery data you are
reading. In this case, “us (multiple regions in United States)”.&lt;/p>
&lt;p>Once you finish creating an empty dataset, you can execute below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">output_table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s1">:covid_data.max_analysis&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">bq_schema&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;fields&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">}]}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;To json-like&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">partition_date&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deaths&lt;/span>&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">output_table&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">schema&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bq_schema&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;STREAMING_INSERTS&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">custom_gcs_temp_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now back in the form of the last SQL cell output, you may fill in necessary
information to run the pipeline on Dataflow. An example input looks like below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image22.png"
alt="Beam SQL in Notebooks: fill in the run-on-Dataflow form.">&lt;/p>
&lt;p>Because this pipeline doesn’t use any additional Python dependency, “Additional
Packages” is left empty. In the previous example where you have installed a
package called &lt;code>names&lt;/code>, to run that pipeline on Dataflow, you have to put
&lt;code>names&lt;/code> in this field.&lt;/p>
&lt;p>Once you finish updating your inputs, you can click the &lt;code>Show Options&lt;/code> button to
view what pipeline options have been configured based on your inputs. A variable
&lt;code>options_[YOUR_OUTPUT_PCOLL_NAME]&lt;/code> is generated, and you can supply more
pipeline options to it if the form is not enough for your execution.&lt;/p>
&lt;p>Once you are ready to submit the Dataflow job, click the &lt;code>Run on Dataflow&lt;/code>
button. It tells you where the default output would be written, and after a
while, a line with:&lt;/p>
&lt;blockquote>
&lt;p>Click here for the details of your Dataflow job.&lt;/p>
&lt;/blockquote>
&lt;p>would be displayed. You can click on the hyperlink to go to your Dataflow job
page. (Optionally, you can ignore the form and continue development to extend
your pipeline. Once you are satisfied with the state of your pipeline, you can
come back to the form and submit the job to Dataflow.)&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image23.png"
alt="Beam SQL in Notebooks: a Dataflow job graph.">&lt;/p>
&lt;p>As you can see, each transform name of the generated Dataflow job is prefixed
with a string &lt;code>[number]: &lt;/code>. This is to distinguish re-executed codes in
notebooks because Beam requires each transform to have a distinct name. Under
the hood, the &lt;code>beam_sql&lt;/code> magic also stages your schema information to Dataflow,
so you might see transforms named as &lt;code>schema_loaded_beam_sql_…&lt;/code>. This is because
the &lt;code>NamedTuple&lt;/code> defined in the notebook is likely in the &lt;code>__main__&lt;/code> scope and
Dataflow is not aware of them at all. To minimize user intervention and avoid
pickling the whole main session (and it’s infeasible to pickle the main session
when it contains unpickle-able attributes), the &lt;code>beam_sql&lt;/code> magic optimizes the
staging process by serializing your schemas, staging them to Dataflow, and then
deserialize/load them for job execution.&lt;/p>
&lt;p>Once the job succeeds, the result of the output PCollection would be written to
places instructed by your I/O transforms. &lt;strong>Note&lt;/strong>: running &lt;code>beam_sql&lt;/code> on
Dataflow generates a one-shot job and it’s not interactive.&lt;/p>
&lt;p>A simple inspection of the data from the default output location:&lt;/p>
&lt;pre tabindex="0">&lt;code>!gsutil cat &amp;#39;gs://ningk-so-test/bq/staging/data_with_max_cases*&amp;#39;
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image24.png"
alt="Beam SQL in Notebooks: inspect the default output file.">&lt;/p>
&lt;p>The text file with encoded binary data written by your &lt;code>WriteToText&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>!gsutil cat &amp;#39;gs://ningk-so-test/bq/encoded_max_data*&amp;#39;
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image25.png"
alt="Beam SQL in Notebooks: inspect the user-defined output file.">&lt;/p>
&lt;p>The table &lt;code>YOUR-PROJECT:covid_data.max_analysis&lt;/code> created by your
&lt;code>WriteToBigQuery&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image26.png"
alt="Beam SQL in Notebooks: inspect the output BQ dataset.">&lt;/p>
&lt;h3 id="run-on-other-oss-runners-directly-with-the-beam_sql-magic">Run on other OSS runners directly with the &lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>On the day this blog is posted, the &lt;code>beam_sql&lt;/code> magic only supports DirectRunner
(interactive) and DataflowRunner (one-shot). It&amp;rsquo;s a simple wrapper on top of
the &lt;code>SqlTransform&lt;/code> with interactive input widgets implemented by
&lt;a href="https://ipywidgets.readthedocs.io/en/stable/">ipywidgets&lt;/a>. You can implement
your own runner support or utilities by following the
&lt;a href="https://lists.apache.org/thread/psrx1xhbyjcqbhxx6trf5nvh66c6pk3y">instructions&lt;/a>.&lt;/p>
&lt;p>Additionally, support for other OSS runners are WIP, for example,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14373">support using FlinkRunner with the &lt;code>beam_sql&lt;/code> magic&lt;/a>.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>The &lt;code>beam_sql&lt;/code> magic and Apache Beam Notebooks combined is a convenient tool for
you to learn Beam SQL and mix Beam SQL into prototyping and productionizing (
e.g., to Dataflow) your Beam pipelines with minimum setups.&lt;/p>
&lt;p>For more details about the Beam SQL syntax, check out the Beam Calcite SQL
&lt;a href="/documentation/dsls/sql/calcite/overview/">compatibility&lt;/a>
and the Apache Calcite SQL
&lt;a href="https://calcite.apache.org/docs/reference.html">syntax&lt;/a>.&lt;/p></description></item><item><title>Blog: Running Apache Hop visual pipelines with Google Cloud Dataflow</title><link>/blog/apache-hop-with-dataflow/</link><pubDate>Fri, 22 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/apache-hop-with-dataflow/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Apache Hop (&lt;a href="https://hop.apache.org/">https://hop.apache.org/&lt;/a>) is a visual development environment for creating data pipelines using Apache Beam. You can run your Hop pipelines in Spark, Flink or Google Cloud Dataflow.&lt;/p>
&lt;p>In this post, we will see how to install Hop, and we will run a sample pipeline in the cloud with Dataflow. To follow the steps given in this post, you should have a project in Google Cloud Platform, and you should have enough permissions to create a Google Cloud Storage bucket (or to use an existing one), as well as to run Dataflow jobs.&lt;/p>
&lt;p>Once you have your Google Cloud project ready, you will need to &lt;a href="https://cloud.google.com/sdk/docs/install">install the Google Cloud SDK&lt;/a> to trigger the Dataflow pipeline.&lt;/p>
&lt;p>Also, don&amp;rsquo;t forget to configure the Google Cloud SDK to use your account and your project.&lt;/p>
&lt;h2 id="setup-and-local-execution">Setup and local execution&lt;/h2>
&lt;p>You can run Apache Hop as a local application, or use &lt;a href="https://hop.incubator.apache.org/manual/latest/hop-gui/hop-web.html">the Hop web version&lt;/a> from a Docker container. The instructions given in this post will work for the local application, as the authentication for Cloud Dataflow would be different if Hop is running in a container. All the rest of the instructions remain valid. The UI of Hop is exactly the same either running as a local app or in the web version.&lt;/p>
&lt;p>Now it&amp;rsquo;s time to download and install Apache Hop, following these &lt;a href="https://hop.apache.org/manual/latest/getting-started/hop-download-install.html">instructions&lt;/a>.&lt;/p>
&lt;p>For this post, I have used the binaries in the apache-hop-client package, version 1.2.0, released on March 7th, 2022.&lt;/p>
&lt;p>After installing Hop, we are ready to start.&lt;/p>
&lt;p>The Zip file contains a directory &lt;code>config&lt;/code> where you will find some sample projects and some pipeline run configuration for Dataflow and other runners.&lt;/p>
&lt;p>For this example, we are going to use the pipeline located in &lt;code>config/projects/samples/beam/pipelines/input-process-output.hpl.&lt;/code>&lt;/p>
&lt;p>Let&amp;rsquo;s start by opening Apache Hop. In the directory where you have unzipped the client, run&lt;/p>
&lt;p>&lt;code>./hop/hop-gui.sh&lt;/code>&lt;/p>
&lt;p>(or &lt;code>./hop/hop-gui.bat&lt;/code> if you are on Windows).&lt;/p>
&lt;p>Once we are in Hop, let&amp;rsquo;s open the pipeline.&lt;/p>
&lt;p>We first switch from the project &lt;code>default&lt;/code> to the project &lt;code>samples&lt;/code>. Locate the &lt;code>projects&lt;/code> box in the top left corner of the window, and select the project &lt;code>samples&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image18.png"
alt="Apache Hop projects">&lt;/p>
&lt;p>Now we click the open button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image4.png"
alt="Apache Hop open project">&lt;/p>
&lt;p>Select the pipeline &lt;code>input-process-output.hpl&lt;/code> in the &lt;code>beam/pipelines&lt;/code> subdirectory:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image12.png"
alt="Apache Hop select pipeline">&lt;/p>
&lt;p>You should see a graph like the following in the main window of Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image17.png"
alt="Apache Hop main window">&lt;/p>
&lt;p>This pipeline takes some customer data from a CSV file and filters out everything but the records with the column &lt;code>stateCode&lt;/code> equal to &lt;code>CA.&lt;/code>&lt;/p>
&lt;p>Then we select only some of the columns of the file, and the result is written to Google Cloud Storage.&lt;/p>
&lt;p>It is always a good idea to test the pipeline locally before submitting it to Dataflow. In Apache Hop, you can preview the output of each transform. Let&amp;rsquo;s have a look at the input &lt;code>Customers&lt;/code>.&lt;/p>
&lt;p>Click in the &lt;code>Customers&lt;/code> input transform and then in &lt;em>Preview Output&lt;/em> in the dialog box that opens after selecting the transform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image10.png"
alt="Apache Hop Customers preview">&lt;/p>
&lt;p>Now select the option &lt;em>Quick launch&lt;/em> and you will see some of the input data:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image24.png"
alt="Apache Hop input data">&lt;/p>
&lt;p>Click &lt;em>Stop&lt;/em> when you finish reviewing the data.&lt;/p>
&lt;p>If we repeat the process right after the &lt;code>Only CA&lt;/code> transform, we will see that all the rows have the &lt;code>stateCode&lt;/code> column equal to &lt;code>CA&lt;/code>.&lt;/p>
&lt;p>The next transform selects only some of the columns of the input data and reorders the columns. Let&amp;rsquo;s have a look. Click the transform and then &lt;em>Preview Output&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image15.png"
alt="Apache Hop preview output">&lt;/p>
&lt;p>Then click _Quick Launch _again, and you should see output like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image8.png"
alt="Apache Hop output">&lt;/p>
&lt;p>The column &lt;code>id&lt;/code> is now the first, and we see only a subset of the input columns. This is how the data will look once the pipeline finishes writing the full output.&lt;/p>
&lt;h2 id="using-the-beam-direct-runner">Using the Beam Direct Runner&lt;/h2>
&lt;p>Let&amp;rsquo;s run the pipeline. To run the pipeline, we need to specify a runner configuration. This is done through the Metadata tool of Apache Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image6.png"
alt="Apache Hop runner configuration">&lt;/p>
&lt;p>In the &lt;code>samples&lt;/code> project, there are already several configurations created:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image9.png"
alt="Apache Hop configurations">&lt;/p>
&lt;p>The &lt;code>local&lt;/code> configuration is the one used to run the pipeline using Hop. For instance, this is the configuration that we used when we examined the previews of the output of different steps.&lt;/p>
&lt;p>The &lt;code>Direct&lt;/code> configuration uses the direct runner of Apache Beam. Let&amp;rsquo;s examine what it looks like. There are two tabs in the Pipeline Run Configurations: main and variables.&lt;/p>
&lt;p>For the direct runner, the main tab has the following options:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image28.png"
alt="Apache Hop direct runner">&lt;/p>
&lt;p>We can change the number of workers settings to match our number of CPUs, or even limit it just to 1 so the pipeline does not consume a lot of resources.&lt;/p>
&lt;p>In the variables tab, we find the configuration parameters for the pipeline itself (not for the runner): \&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image14.png"
alt="Apache Hop variables tab">&lt;/p>
&lt;p>For this pipeline, only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables are used. The &lt;code>STATE_INPUT&lt;/code> is used in a different example.&lt;/p>
&lt;p>If you go to the Beam transforms in the input and output nodes of the pipeline, you will see how these variables are used there:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image29.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image11.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>Since those variables are correctly set up to point to the location of data in the samples project folder, let&amp;rsquo;s try to run the pipeline using the Beam Direct Runner.&lt;/p>
&lt;p>For that, we need to go back to the pipeline view (arrow button just above the Metadata tool), and click the run button (the small &amp;ldquo;play&amp;rdquo; button in the toolbar). Then choose the Direct pipeline run configuration, and click the &lt;em>Launch&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image20.png"
alt="Apache Hop launch">&lt;/p>
&lt;p>How do you know if the job has finished or not? You can check the logs at the bottom of the main window for that. You should see something like this:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image19.png"
alt="Apache Hop completed job">&lt;/p>
&lt;p>If we go to the location set by &lt;code>DATA_OUTPUT&lt;/code>, in our case &lt;code>config/projects/samples/beam/output&lt;/code>, we should see some output files there. In my case, I see these files:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image26.png"
alt="Apache Hop output files">&lt;/p>
&lt;p>The number of files depends on the number of workers that you have set in the run configuration.&lt;/p>
&lt;p>Great, so the pipeline works locally. It is time to run it in the cloud!&lt;/p>
&lt;h2 id="running-at-cloud-scale-with-dataflow">Running at cloud scale with Dataflow&lt;/h2>
&lt;p>Let&amp;rsquo;s have a look at the Dataflow Pipeline Run Configuration. Go to the metadata tool, then to Pipeline Run Configuration and select Dataflow:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image30.png"
alt="Apache Hop Pipeline Run Configuration">&lt;/p>
&lt;p>We have again the Main and the Variables tab. We will need to change some values in both. Let&amp;rsquo;s start with the Variables. Click the Variables tab, and you should see the following values:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image3.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Those are Google Cloud Storage (GCS) locations that belong to the author of that sample project. We need to change them to point to our own GCS bucket.&lt;/p>
&lt;h2 id="project-setup-in-google-cloud">Project setup in Google Cloud&lt;/h2>
&lt;p>But for that, we will have to create a bucket. For the next step, you need to make sure that you have configured gcloud (the Google Cloud SDK), and that you have managed to authenticate.&lt;/p>
&lt;p>To double check, run the command &lt;code>gcloud config list&lt;/code> and check if the account and the project look correct. If they do, let&amp;rsquo;s triple check and run &lt;code>gcloud auth login&lt;/code>. That should open a tab in your web browser, to do the authentication process. Once you have done that, you can interact with your project using the SDK.&lt;/p>
&lt;p>For this example, I will use the region europe-west1 of GCP. Let&amp;rsquo;s create a regional bucket there. In my case, I am using the name &lt;code>ihr-apache-hop-blog&lt;/code> for the bucket name. Choose a different name for your bucket!&lt;/p>
&lt;pre tabindex="0">&lt;code>gsutil mb -c regional -l europe-west1 gs://ihr-apache-hop-blog
&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s upload the sample data to the GCS bucket, to test how the pipeline would run in Dataflow. Go to the same directory where you have all the hop files (the same directory that &lt;code>hop-gui.sh&lt;/code> is in), and let&amp;rsquo;s copy the data to GCS:&lt;/p>
&lt;pre tabindex="0">&lt;code> gsutil cp config/projects/samples/beam/input/customers-noheader-1k.txt gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>Notice the final slash &lt;code>/&lt;/code> in the path, indicating that you want to create a directory of name &lt;code>data&lt;/code>, with all the contents.&lt;/p>
&lt;p>To make sure that you have uploaded the data correctly, check the contents of that location:&lt;/p>
&lt;pre tabindex="0">&lt;code>gsutil ls gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>You should see the file &lt;code>customer-noheader-1k.txt&lt;/code> in that location.&lt;/p>
&lt;p>Before we continue, make sure that Dataflow is enabled in your project, and that you have a service account ready to be used with Hop. Please check the instructions given at the documentation of Dataflow, in the &lt;em>&lt;a href="https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-java#before-you-begin">Before you begin section&lt;/a>&lt;/em> to see how to enable the API for Dataflow.&lt;/p>
&lt;p>Now we need to make sure that Hop can use the necessary credentials for accessing Dataflow. In the Hop documentation, you will find that it recommends creating a service account, exporting a key for that service account, and setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. This is also the method given in the above link.&lt;/p>
&lt;p>Exporting the key of a service account is potentially dangerous, so we are going to use a different method, by leveraging the Google Cloud SDK. Run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud auth application-default login
&lt;/code>&lt;/pre>&lt;p>That will open a tab in your web browser asking to confirm the authentication. Once you have confirmed, any application in your system that needs to access Google Cloud Platform will use those credentials for that access.&lt;/p>
&lt;p>We need also to create a service account for the Dataflow job, with certain permissions. Create the service account with&lt;/p>
&lt;pre tabindex="0">&lt;code>​​gcloud iam service-accounts create dataflow-hop-sa
&lt;/code>&lt;/pre>&lt;p>And now we give permissions to this service account for Dataflow:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;#34;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;#34;\
--role=&amp;#34;roles/dataflow.worker&amp;#34;
&lt;/code>&lt;/pre>&lt;p>We also need to give additional permissions for Google Cloud Storage:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;#34;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;#34;\
--role=&amp;#34;roles/storage.admin&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Make sure that you change the project id &lt;code>ihr-hop-playground&lt;/code> to your own project id.&lt;/p>
&lt;p>Now let&amp;rsquo;s give permissions to our user to impersonate that service account. For that, go to &lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">Service Accounts in the Google Cloud Console&lt;/a> in your project, and click on the service account we have just created.&lt;/p>
&lt;p>Click on the &lt;em>Permissions&lt;/em> tab and then in the &lt;em>Grant Access&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image21.png"
alt="Apache Hop Permissions">&lt;/p>
&lt;p>Give your user the role &lt;em>Service Account User&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image13.png"
alt="Apache Hop Service Account User">&lt;/p>
&lt;p>You are now all set to be able to run Dataflow with that service account and your user.&lt;/p>
&lt;h2 id="updating-the-pipeline-run-configuration">Updating the Pipeline Run Configuration&lt;/h2>
&lt;p>Before we can run a pipeline in Dataflow, we need to generate the JAR package for the pipeline code. For that, you have to go to the &lt;em>Tools&lt;/em> menu (in the menu bar), and choose the option &lt;em>Generate a Hop fat jar&lt;/em>. Click ok in the dialog, and then select a location and filename for the jar, and click &lt;em>Save&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image5.png"
alt="Apache Hop Tools menu">&lt;/p>
&lt;p>It will take some minutes to generate the file:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image22.png"
alt="Apache Hop generate file">&lt;/p>
&lt;p>We are ready to run the pipeline in Dataflow. Or almost :).&lt;/p>
&lt;p>Go the pipeline editor, click the play button, and select &lt;em>DataFlow&lt;/em> as Pipeline run configuration, and then click the play button on the right side:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image7.png"
alt="Apache Hop pipeline editor">&lt;/p>
&lt;p>That will open the Dataflow Pipeline Run Configuration, where we can change the input variables, and other Dataflow settings.&lt;/p>
&lt;p>Click on the &lt;em>Variables&lt;/em> tab and modify only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image2.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Notice that we also need to change the filename.&lt;/p>
&lt;p>Let&amp;rsquo;s go now to the &lt;em>Main&lt;/em> tab, because there are some other options that we need to change there. We need to update:&lt;/p>
&lt;ul>
&lt;li>Project id&lt;/li>
&lt;li>Service account&lt;/li>
&lt;li>Staging location&lt;/li>
&lt;li>Region&lt;/li>
&lt;li>Temp location&lt;/li>
&lt;li>Fat jar file location&lt;/li>
&lt;/ul>
&lt;p>For project id, set your project id (the same one you see when you run &lt;code>gcloud config list&lt;/code>).&lt;/p>
&lt;p>For service account, use the address of the Service Account we have created. If you don&amp;rsquo;t remember, you can find it under S&lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">ervice Accounts in the Google Cloud Console&lt;/a>.&lt;/p>
&lt;p>For staging and temp locations, use the same bucket that we have just created. Change the bucket address in the paths, and leave the same &amp;ldquo;binaries&amp;rdquo; and &amp;ldquo;tmp&amp;rdquo; locations that are already set in the configuration.&lt;/p>
&lt;p>For region, in this example we are using &lt;code>europe-west1&lt;/code>.&lt;/p>
&lt;p>Also, depending on your network configuration, you may want to check the box of &amp;ldquo;Use Public IPs?&amp;rdquo;, or alternatively leave it unchecked but enable Google Private Access in the regional subnetwork for europe-west1 in your project (for more details, please see &lt;a href="https://cloud.google.com/vpc/docs/configure-private-google-access#enabling-pga">Configuring Private Google Access | VPC&lt;/a>). In this example, I will check the box for simplicity.&lt;/p>
&lt;p>For the fat jar location, use the _Browse _button on the right side, and locate the JAR that we generated above. In summary, my &lt;em>Main&lt;/em> options look like these (your project id and locations will be different):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image27.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>You may, of course, change any other option, depending on the specific settings that might be required for your project.&lt;/p>
&lt;p>When you are ready, click on the _Ok _button and then &lt;em>Launch&lt;/em> to trigger the pipeline.&lt;/p>
&lt;p>In the logging window, you should see a line like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image16.png"
alt="Apache Hop logging window">&lt;/p>
&lt;h2 id="checking-the-job-in-dataflow">Checking the job in Dataflow&lt;/h2>
&lt;p>If everything has gone well, you should now see a job running at &lt;a href="https://console.cloud.google.com/dataflow/jobs">https://console.cloud.google.com/dataflow/jobs&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image1.png"
alt="Dataflow job list">&lt;/p>
&lt;p>If for some reason the job has failed, open the failed job page, check the _Logs _at the bottom, and click the error icon to find why the pipeline has failed. It is normally because we have set some wrong option in your configuration:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image25.png"
alt="Dataflow Logs">&lt;/p>
&lt;p>When the pipeline starts running, you should see the graph of the pipeline in the job page:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image23.png"
alt="Dataflow pipeline graph">&lt;/p>
&lt;p>When the job finishes, there should be a file in the output location. You can check it out with &lt;code>gsutil&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>% gsutil ls gs://ihr-apache-hop-blog/output
gs://ihr-apache-hop-blog/output/input-process-output-00000-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00001-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00002-of-00003.csv
&lt;/code>&lt;/pre>&lt;p>In my case, the job has generated three files, but the actual number will vary from run to run.&lt;/p>
&lt;p>Let&amp;rsquo;s explore the first lines of those files:&lt;/p>
&lt;pre tabindex="0">&lt;code>gsutil cat &amp;#34;gs://ihr-apache-hop-blog/output/*csv&amp;#34;| head
12,wha-firstname,vnaov-name,egm-city,CALIFORNIA
25,ayl-firstname,bwkoe-name,rtw-city,CALIFORNIA
26,zio-firstname,rezku-name,nvt-city,CALIFORNIA
44,rgh-firstname,wzkjq-name,hkm-city,CALIFORNIA
135,ttv-firstname,eqley-name,trs-city,CALIFORNIA
177,ahc-firstname,nltvw-name,uxf-city,CALIFORNIA
181,kxv-firstname,bxerk-name,sek-city,CALIFORNIA
272,wpy-firstname,qxjcn-name,rew-city,CALIFORNIA
304,skq-firstname,cqapx-name,akw-city,CALIFORNIA
308,sfu-firstname,ibfdt-name,kqf-city,CALIFORNIA
&lt;/code>&lt;/pre>&lt;p>We can see that all the rows have CALIFORNIA as the state, that the output contains only the columns that we selected, and that the user id is the first column. The actual output you get will probably be different, as the order in which data is processed will not be the same in each run.&lt;/p>
&lt;p>We have run this job with a small data sample, but we could have run the same job with an arbitrarily large input CSV. Dataflow would parallelize and process the data in chunks.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>Apache Hop is a visual development environment for Beam pipelines, that allows us to run the pipelines locally, inspect the data, debug, unit test and many other capabilities. Once we are happy with a pipeline that has run locally, we can deploy the same visual pipeline in the cloud by just setting the necessary parameters for using Dataflow.&lt;/p>
&lt;p>If you want to know more about Apache Hop, don&amp;rsquo;t miss &lt;a href="https://www.youtube.com/watch?v=sZSIbcPtebI">the Beam Summit talk delivered by the author of Hop&lt;/a>, and don&amp;rsquo;t forget to check out the &lt;a href="https://hop.apache.org/manual/latest/getting-started/index.html">getting started guide&lt;/a>.&lt;/p></description></item><item><title>Blog: Apache Beam 2.38.0</title><link>/blog/beam-2.38.0/</link><pubDate>Wed, 20 Apr 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.38.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.38.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2380-2022-04-20">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.38.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351169">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Introduce projection pushdown optimizer to the Java SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12976">BEAM-12976&lt;/a>). The optimizer currently only works on the &lt;a href="/documentation/io/built-in/google-bigquery/#storage-api">BigQuery Storage API&lt;/a>, but more I/Os will be added in future releases. If you encounter a bug with the optimizer, please file a JIRA and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;li>A new IO for Neo4j graph databases was added. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1857">BEAM-1857&lt;/a>) It has the ability to update nodes and relationships using UNWIND statements and to read data using cypher statements with parameters.&lt;/li>
&lt;li>&lt;code>amazon-web-services2&lt;/code> has reached feature parity and is finally recommended over the earlier &lt;code>amazon-web-services&lt;/code> and &lt;code>kinesis&lt;/code> modules (Java). These will be deprecated in one of the next releases (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13174">BEAM-13174&lt;/a>).
&lt;ul>
&lt;li>Long outstanding write support for &lt;code>Kinesis&lt;/code> was added (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13175">BEAM-13175&lt;/a>).&lt;/li>
&lt;li>Configuration was simplified and made consistent across all IOs, including the usage of &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13663">BEAM-13663&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13587">BEAM-13587&lt;/a>).&lt;/li>
&lt;li>Additionally, there&amp;rsquo;s a long list of recent improvements and fixes to
&lt;code>S3&lt;/code> Filesystem (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13245">BEAM-13245&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13246">BEAM-13246&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13441">BEAM-13441&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13445">BEAM-13445&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>),
&lt;code>DynamoDB&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13209&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13209">BEAM-13209&lt;/a>),
&lt;code>SQS&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13631">BEAM-13631&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13510">BEAM-13510&lt;/a>) and others.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Pipeline dependencies supplied through &lt;code>--requirements_file&lt;/code> will now be staged to the runner using binary distributions (wheels) of the PyPI packages for linux_x86_64 platform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-4032">BEAM-4032&lt;/a>). To restore the behavior to use source distributions, set pipeline option &lt;code>--requirements_cache_only_sources&lt;/code>. To skip staging the packages at submission time, set pipeline option &lt;code>--requirements_cache=skip&lt;/code> (Python).&lt;/li>
&lt;li>The Flink runner now supports Flink 1.14.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13106">BEAM-13106&lt;/a>).&lt;/li>
&lt;li>Interactive Beam now supports remotely executing Flink pipelines on Dataproc (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14071">BEAM-14071&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>(Python) Previously &lt;code>DoFn.infer_output_types&lt;/code> was expected to return &lt;code>Iterable[element_type]&lt;/code> where &lt;code>element_type&lt;/code> is the PCollection elemnt type. It is now expected to return &lt;code>element_type&lt;/code>. Take care if you have overriden &lt;code>infer_output_type&lt;/code> in a &lt;code>DoFn&lt;/code> (this is not common). See &lt;a href="https://issues.apache.org/jira/browse/BEAM-13860">BEAM-13860&lt;/a>.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) The types of &lt;code>awsRegion&lt;/code> / &lt;code>endpoint&lt;/code> in &lt;code>AwsOptions&lt;/code> changed from String to &lt;code>Region&lt;/code> / &lt;code>URI&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Beam 2.38.0 will be the last minor release to support Flink 1.11.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) Client providers (&lt;code>withXYZClientProvider()&lt;/code>) as well as IO specific &lt;code>RetryConfiguration&lt;/code>s are deprecated, instead use &lt;code>withClientConfiguration()&lt;/code> or &lt;code>AwsOptions&lt;/code> to configure AWS IOs / clients.
Custom implementations of client providers shall be replaced with a respective &lt;code>ClientBuilderFactory&lt;/code> and configured through &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fix S3 copy for large objects (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>)&lt;/li>
&lt;li>Fix quadratic behavior of pipeline canonicalization (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14128">BEAM-14128&lt;/a>)
&lt;ul>
&lt;li>This caused unnecessarily long pre-processing times before job submission for large complex pipelines.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Fix &lt;code>pyarrow&lt;/code> version parsing (Python)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-14235">BEAM-14235&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.38.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.38.0 release. Thank you to all contributors!&lt;/p>
&lt;p>abhijeet-lele
Ahmet Altay
akustov
Alexander
Alexander Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Ankur Goenka
ansh0l
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
Brian Hulette
brucearctor
bulat safiullin
bullet03
Carl Mastrangelo
Chamikara Jayalath
Chun Yang
Daniela Martín
Daniel Oliveira
Danny McCormick
daria.malkova
David Cavazos
David Huntsperger
dmitryor
Dmytro Sadovnychyi
dpcollins-google
egalpin
Elias Segundo Antonio
emily
Etienne Chauchot
Hengfeng Li
Ismaël Mejía
Israel Herraiz
Jack McCluskey
Jakub Kukul
Janek Bevendorff
Jeff Klukas
Johan Sternby
Kamil Breguła
Kenneth Knowles
Ke Wu
Kiley
Kyle Weaver
laraschmidt
Lara Schmidt
LE QUELLEC Olivier
Luka Kalinovcic
Luke Cwik
Marcin Kuthan
masahitojp
Masato Nakamura
Matt Casters
Melissa Pashniak
Michael Li
Miguel Hernandez
Moritz Mack
mosche
nancyxu123
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
paul-tlh
Pavel Avilov
Rahul Iyer
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Skraba
Ryan Thompson
Sam Whittle
Seth Vargo
sp029619
Steven Niemitz
Thiago Nunes
Udi Meiri
Valentyn Tymofieiev
Victor
vitaly.terentyev
Yichi Zhang
Yi Hu
yirutang
Zachary Houfek
Zoe&lt;/p></description></item><item><title>Blog: Apache Beam 2.37.0</title><link>/blog/beam-2.37.0/</link><pubDate>Fri, 04 Mar 2022 08:30:00 -0800</pubDate><guid>/blog/beam-2.37.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.37.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2370-2022-03-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.37.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351168">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java 17 support for Dataflow (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12240">BEAM-12240&lt;/a>).
&lt;ul>
&lt;li>Users using Dataflow Runner V2 may see issues with state cache due to inaccurate object sizes (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13695">BEAM-13695&lt;/a>).&lt;/li>
&lt;li>ZetaSql is currently unsupported (&lt;a href="https://github.com/google/zetasql/issues/89">issue&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Python 3.9 support in Apache Beam (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12000">BEAM-12000&lt;/a>).
&lt;ul>
&lt;li>Dataflow support for Python 3.9 is expected to be available with 2.37.0,
but may not be fully available yet when the release is announced (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13864">BEAM-13864&lt;/a>).&lt;/li>
&lt;li>Users of Dataflow Runner V2 can run Python 3.9 pipelines with 2.37.0 release right away.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Go SDK now has wrappers for the following Cross Language Transforms from Java, along with automatic expansion service startup for each.
&lt;ul>
&lt;li>JDBCIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13293">BEAM-13293&lt;/a>).&lt;/li>
&lt;li>Debezium (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13761">BEAM-13761&lt;/a>).&lt;/li>
&lt;li>BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13683">BEAM-13683&lt;/a>).&lt;/li>
&lt;li>BiqQuery (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13732">BEAM-13732&lt;/a>).&lt;/li>
&lt;li>KafkaIO now also has automatic expansion service startup. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13821">BEAM-13821&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>DataFrame API now supports pandas 1.4.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13605">BEAM-13605&lt;/a>).&lt;/li>
&lt;li>Go SDK DoFns can now observe trigger panes directly (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13757">BEAM-13757&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.37.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.37.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aizhamal Nurmamat kyzy
Alexander
Alexander Chermenin
Alexandr Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
AydarZaynutdinov
Benjamin Gonzalez
Brian Hulette
Chamikara Jayalath
Daniel Oliveira
Danny McCormick
daria-malkova
daria.malkova
darshan-sj
David Huntsperger
dprieto91
emily
Etienne Chauchot
Fernando Morales
Heejong Lee
Ismaël Mejía
Jack McCluskey
Jan Lukavský
johnjcasey
Kamil Breguła
kellen
Kenneth Knowles
kileys
Kyle Weaver
Luke Cwik
Marcin Kuthan
Marco Robles
Matt Rudary
Miguel Hernandez
Milena Bukal
Moritz Mack
Mostafa Aghajani
Ning Kang
Pablo Estrada
Pavel Avilov
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Sam Whittle
Sandy Chapman
Sergey Kalinin
Thiago Nunes
thorbjorn444
Tim Robertson
Tomo Suzuki
Valentyn Tymofieiev
Victor
Victor Chen
Vitaly Ivanov
Yichi Zhang&lt;/p></description></item><item><title>Blog: Upcoming Events for Beam in 2022</title><link>/blog/upcoming-events-for-beam-in-2022/</link><pubDate>Mon, 28 Feb 2022 00:00:01 -0800</pubDate><guid>/blog/upcoming-events-for-beam-in-2022/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are so excited to announce the upcoming Beam events for this year! We believe that events are an important mechanism to foster the community around Apache Beam as an Open Source Project. Our events are focused on a developer experience by giving spaces for the community to connect, facilitate collaboration, and enable knowledge sharing.&lt;/p>
&lt;p>Here is an overview of some upcoming events and ways for everyone to help foster additional community growth:&lt;/p>
&lt;h2 id="beam-summit">Beam Summit&lt;/h2>
&lt;p>The &lt;strong>&lt;a href="https://2022.beamsummit.org/">Beam Summit 2022&lt;/a>&lt;/strong> is approaching! The event will be in a hybrid in-person and virtual format from Austin, TX on July 18-20, 2022. The conference will include three full days of lightning talks, roadmap updates, use cases, demos, and workshops for Beam users of all levels. This is a great opportunity to collaborate, share ideas, and work together in the improvement of the project.&lt;/p>
&lt;p>Check out talks from prior editions of Beam Summit &lt;strong>&lt;a href="https://www.youtube.com/watch?v=jses0W4Zalc&amp;amp;list=PL4dEBWmGSIU8vLWF56shrSuTsLXvO6Ex3">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="the-experience">The Experience&lt;/h3>
&lt;p>We are so excited to see some of you in person again and the rest of the community online! The &lt;strong>&lt;a href="https://2022.beamsummit.org/team/">Beam Summit Steering Committee&lt;/a>&lt;/strong> in partnership with an event production company is working hard to ensure that we provide the community with the best possible experience, no matter which format you choose to attend in.&lt;/p>
&lt;p>If you have any ideas on how we can make this year’s event better, please &lt;strong>&lt;a href="mailto:contact@beamsummit.org">reach out to us&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="ways-to-help--participate">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>&lt;a href="https://sessionize.com/beam-summit-2022">Submit a proposal&lt;/a>&lt;/strong> to talk! The deadline for submissions is &lt;em>March 15th&lt;/em>.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://2022.beamsummit.org/tickets/">Register&lt;/a>&lt;/strong> to join as an attendee in person or online.&lt;/li>
&lt;li>Consider sponsoring the event. If your company is interested in engaging with members of the community, please check out the &lt;strong>&lt;a href="https://2022.beamsummit.org/sponsors/">sponsoring prospectus&lt;/a>.&lt;/strong>&lt;/li>
&lt;li>Help us get the word out. Please make sure to let your colleagues and friends know about the Beam Summit.&lt;/li>
&lt;/ol>
&lt;p>Don’t forget to follow our Beam Summit &lt;strong>&lt;a href="https://twitter.com/BeamSummit?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://www.linkedin.com/company/beam-summit/?viewAsMember=true">LinkedIn&lt;/a>&lt;/strong> pages to receive event updates!&lt;/p>
&lt;h2 id="beam-college">Beam College&lt;/h2>
&lt;p>&lt;strong>&lt;a href="https://beamcollege.dev/">Beam College 2022&lt;/a>&lt;/strong> is around the corner for the second season of training! The event will be hosted virtually from May 10-13, 2022. The training is focused on providing more hands-on experience around end-to-end code samples in an interactive environment, and helping attendees see the applications of concepts covered in other venues, such as the Beam Summit.&lt;/p>
&lt;p>Check out talks from prior editions of Beam College &lt;strong>&lt;a href="https://www.youtube.com/playlist?list=PLjYq1UNvv2UcrfapfgKrnLXtYpkvHmpIh">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;p>This year, the training will consist of learning modules such as:&lt;/p>
&lt;ul>
&lt;li>The Data movement ecosystem and distributed processing the Beam way&lt;/li>
&lt;li>Scaling, productionalizing, and developing your Beam pipelines&lt;/li>
&lt;li>Use Cases&lt;/li>
&lt;li>Beam ML Use Cases&lt;/li>
&lt;/ul>
&lt;p>Be sure to check out our &lt;strong>&lt;a href="https://beamcollege.dev/">website&lt;/a>&lt;/strong> as we continue updating the schedule and follow our &lt;strong>&lt;a href="https://twitter.com/beam_college?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://www.linkedin.com/showcase/beam-college/">LinkedIn&lt;/a>&lt;/strong> pages to receive event updates!&lt;/p>
&lt;h3 id="ways-to-help--participate-1">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>Interested in instructing? Submit a &lt;strong>&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSct6RCrKtgsvxlgngKUGwKoB_iOKihXi1OadKyBQIsi00p3cQ/viewform?usp=sf_link">proposal&lt;/a>&lt;/strong>! The deadline is: &lt;em>February 28th.&lt;/em>&lt;/li>
&lt;li>Enroll in Beam College. Registration is now open on the &lt;strong>&lt;a href="https://beamcollege.dev/step/2022/">registration page&lt;/a>&lt;/strong>.&lt;/li>
&lt;li>Consider partnering with the event. If your company is interested in helping to promote the event and being a part of the branding, please fill out this &lt;strong>form&lt;/strong>.&lt;/li>
&lt;li>Help us get the word out by letting your network know about this exciting opportunity to help users uplevel data processing skills, solve complex data applications, and optimize data pipelines!&lt;/li>
&lt;/ol>
&lt;h2 id="beam-meetups">Beam Meetups&lt;/h2>
&lt;p>In partnership with an event production company, Beam will be hosting an average of one virtual Meetup per month. These Meetups will be relaxed presentations on topics or demos followed by a Q&amp;amp;A session. The objective of our virtual meetups is to give the community an update on the most recent Beam features launched within the past six months. These meetups are free and open to the public.&lt;/p>
&lt;p>Check out recordings from previous Meetups &lt;strong>&lt;a href="https://www.youtube.com/watch?v=8fNEs7SbefM&amp;amp;list=PL4dEBWmGSIU-cQSpYP7R1lSC6e2K_pTf1">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="ways-to-help--participate-2">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>Are you interested in sharing a feature launch or sharing a step-by-step use case for Beam? Submit a &lt;strong>&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLScFg7fmOFc7fTvnJL_dmdhia4HDesW4HYxJsDeulnsHzIzqCg/viewform">talk idea&lt;/a>&lt;/strong>!&lt;/li>
&lt;li>Register for the events. Registration is now open on the &lt;strong>&lt;a href="https://clowder.space/projects/apache-beam/">registration page&lt;/a>&lt;/strong>.&lt;/li>
&lt;li>Help us get the word out by spreading the word throughout the community to enable more knowledge sharing and collaboration!&lt;/li>
&lt;/ol></description></item><item><title>Blog: Apache Beam 2.36.0</title><link>/blog/beam-2.36.0/</link><pubDate>Mon, 07 Feb 2022 10:11:00 -0800</pubDate><guid>/blog/beam-2.36.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.36.0 release of Apache Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2360-2022-02-07">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.36.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350407">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for stopReadTime on KafkaIO SDF (Java).(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13171">BEAM-13171&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>💻 Support for ARM64 / Mac M1 out of the box. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11703">BEAM-11703&lt;/a>).&lt;/li>
&lt;li>Added support for cloudpickle as a pickling library for Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8123">BEAM-8123&lt;/a>). To use cloudpickle, set pipeline option: &amp;ndash;pickle_library=cloudpickle&lt;/li>
&lt;li>Added option to specify triggering frequency when streaming to BigQuery (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12865">BEAM-12865&lt;/a>).&lt;/li>
&lt;li>Added option to enable caching uploaded artifacts across job runs for Python Dataflow jobs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13459">BEAM-13459&lt;/a>). To enable, set pipeline option: &amp;ndash;enable_artifact_caching, this will be enabled by default in a future release.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Updated the jedis from 3.x to 4.x to Java RedisIO. If you are using RedisIO and using jedis directly, please refer to &lt;a href="https://github.com/redis/jedis/blob/v4.0.0/docs/3to4.md">this page&lt;/a> to update it. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12092">BEAM-12092&lt;/a>).&lt;/li>
&lt;li>Datatype of timestamp fields in &lt;code>SqsMessage&lt;/code> for AWS IOs for SDK v2 was changed from &lt;code>String&lt;/code> to &lt;code>long&lt;/code>, visibility of all fields was fixed from &lt;code>package private&lt;/code> to &lt;code>public&lt;/code> &lt;a href="https://issues.apache.org/jira/browse/BEAM-13638">BEAM-13638&lt;/a>.&lt;/li>
&lt;li>Properly check output timestamps on elements output from DoFns, timers, and onWindowExpiration in Java &lt;a href="https://issues.apache.org/jira/browse/BEAM-12931">BEAM-12931&lt;/a>.&lt;/li>
&lt;li>Fixed a bug with DeferredDataFrame.xs when used with a non-tuple key
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13421%5D">BEAM-13421&lt;/a>).&lt;/li>
&lt;li>Beam Python now requires &lt;code>google-cloud-pubsub&amp;gt;=2.1.0&lt;/code>. The API surface for &lt;code>apache_beam.io.gcp.pubsub&lt;/code> has not changed, but code that uses the PubSub client directly may need to be updated.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Users may encounter an unexpected java.lang.ArithmeticException when outputting a timestamp
for an element further than allowedSkew from an allowed DoFN skew set to a value more than
Integer.MAX_VALUE.&lt;/li>
&lt;li>S3 object metadata retrieval broken in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13980">BEAM-13980&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.36.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.36.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ada Wong
Ahmet Altay
Alexander
Alexander Dahl
Alexandr Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
Andrew Pilloud
Andy Ye
Arkadiusz Gasiński
Artur Khanin
Arun Pandian
Aydar Farrakhov
Aydar Zainutdinov
AydarZaynutdinov
Benjamin Gonzalez
Brian Hulette
Chamikara Jayalath
Daniel Collins
Daniel Oliveira
Daniel Thevessen
Daniela Martín
David Hinkes
David Huntsperger
Emily Ye
Etienne Chauchot
Evan Galpin
Heejong Lee
Ilya
Ilya Kozyrev
In-Ho Yi
Jack McCluskey
Janek Bevendorff
Jarek Potiuk
Ke Wu
KevinGG
Kyle Hersey
Kyle Weaver
Luís Bianchin
Luke Cwik
Masato Nakamura
Matthias Baetens
Mehdi Drissi
Melissa Pashniak
Michel Davit
Miguel Hernandez
MiguelAnzoWizeline
Milena Bukal
Moritz Mack
Mostafa Aghajani
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
Pavel Avilov
Quentin Sommer
Reuben van Ammers
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sayat
Sergei Lebedev
Sergey Kalinin
Steve Niemitz
Talat Uyarer
Thiago Nunes
Tianyang Hu
Tim Robertson
Valentyn Tymofieiev
Vitaly Ivanov
Yichi Zhang
Yiru Tang
Yu Feng
Yu ISHIKAWA
Zachary Houfek
blais
daria-malkova
daria.malkova
darshan-sj
dpcollins-google
emily
ewianda
johnjcasey
kileys
lam206
laraschmidt
mosche
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>
tvalentyn&lt;/p></description></item><item><title>Blog: Apache Beam 2.35.0</title><link>/blog/beam-2.35.0/</link><pubDate>Wed, 29 Dec 2021 10:11:00 -0800</pubDate><guid>/blog/beam-2.35.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.35.0 release of Apache Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2350-2021-12-29">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.35.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350406">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>MultiMap side inputs are now supported by the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3293">BEAM-3293&lt;/a>).&lt;/li>
&lt;li>Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12522">BEAM-12522&lt;/a>).&lt;/li>
&lt;li>Upgrades Log4j version used in test suites (Apache Beam testing environment only, not for end user consumption) to 2.17.0(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13434">BEAM-13434&lt;/a>).
Note that Apache Beam versions do not depend on the Log4j 2 dependency (log4j-core) impacted by &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228">CVE-2021-44228&lt;/a>.
However we urge users to update direct and indirect dependencies (if any) on Log4j 2 to the latest version by updating their build configuration and redeploying impacted pipelines.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>We changed the data type for ranges in &lt;code>JdbcIO.readWithPartitions&lt;/code> from &lt;code>int&lt;/code> to &lt;code>long&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13149">BEAM-13149&lt;/a>).
This is a relatively minor breaking change, which we&amp;rsquo;re implementing to improve the usability of the transform without increasing cruft.
This transform is relatively new, so we may implement other breaking changes in the future to improve its usability.&lt;/li>
&lt;li>Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12522">BEAM-12522&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added custom delimiters to Python TextIO reads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12730">BEAM-12730&lt;/a>).&lt;/li>
&lt;li>Added escapechar parameter to Python TextIO reads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13189">BEAM-13189&lt;/a>).&lt;/li>
&lt;li>Splittable reading is enabled by default while reading data with ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12070">BEAM-12070&lt;/a>).&lt;/li>
&lt;li>DoFn Execution Time metrics added to Go (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13001">BEAM-13001&lt;/a>).&lt;/li>
&lt;li>Cross-bundle side input caching is now available in the Go SDK for runners that support the feature by setting the EnableSideInputCache hook (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11097">BEAM-11097&lt;/a>).&lt;/li>
&lt;li>Upgraded the GCP Libraries BOM version to 24.0.0 and associated dependencies (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11205">BEAM-11205&lt;/a>). For Google Cloud client library versions set by this BOM,
see &lt;a href="https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/24.0.0/artifact_details.html">this table&lt;/a>.&lt;/li>
&lt;li>Removed avro-python3 dependency in AvroIO. Fastavro has already been our Avro library of choice on Python 3. Boolean use_fastavro is left for api compatibility, but will have no effect.(&lt;a href="https://github.com/apache/beam/pull/15900">BEAM-13016&lt;/a>).&lt;/li>
&lt;li>MultiMap side inputs are now supported by the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3293">BEAM-3293&lt;/a>).&lt;/li>
&lt;li>Remote packages can now be downloaded from locations supported by apache_beam.io.filesystems. The files will be downloaded on Stager and uploaded to staging location. For more information, see &lt;a href="https://issues.apache.org/jira/browse/BEAM-11275">BEAM-11275&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>A new URN convention was adopted for cross-language transforms and existing URNs were updated. This may break advanced use-cases, for example, if a custom expansion service is used to connect diffrent Beam Java and Python versions. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12047">BEAM-12047&lt;/a>).&lt;/li>
&lt;li>The upgrade to Calcite 1.28.0 introduces a breaking change in the SUBSTRING function in SqlTransform, when used with the Calcite dialect (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13099">BEAM-13099&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/CALCITE-4427">CALCITE-4427&lt;/a>).&lt;/li>
&lt;li>ListShards (with DescribeStreamSummary) is used instead of DescribeStream to list shards in Kinesis streams (AWS SDK v2). Due to this change, as mentioned in &lt;a href="https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html">AWS documentation&lt;/a>, for fine-grained IAM policies it is required to update them to allow calls to ListShards and DescribeStreamSummary APIs. For more information, see &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html">Controlling Access to Amazon Kinesis Data Streams&lt;/a> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13233">BEAM-13233&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Non-splittable reading is deprecated while reading data with ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12070">BEAM-12070&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Properly map main input windows to side input windows by default (Go)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11087">BEAM-11087&lt;/a>).&lt;/li>
&lt;li>Fixed data loss when writing to DynamoDB without setting deduplication key names (Java)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13009&lt;/a>).&lt;/li>
&lt;li>Go SDK Examples now have types and functions registered. (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5378">BEAM-5378&lt;/a>)&lt;/li>
&lt;li>Fixed data loss when using Python WriteToFiles in streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12950">BEAM-12950&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Users of beam-sdks-java-io-hcatalog (and beam-sdks-java-extensions-sql-hcatalog) must take care to override the transitive log4j dependency when they add a hive dependency (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13499">BEAM-13499&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.35.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay
Alexandr Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
Andrew Pilloud
Ankur Goenka
Anthony Sottile
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
brachipa
Brian Hulette
Calvin Leung
Chamikara Jayalath
Chris Gray
Damon Douglas
Daniel Collins
Daniel Oliveira
daria.malkova
darshan-sj
David Huntsperger
David Prieto Rivera
Dmitrii Kuzin
dpcollins-google
dprieto
egalpin
Etienne Chauchot
Eugene Nikolaiev
Fernando Morales
Hector Lagos
Heejong Lee
Ilya Kozyrev
Iñigo San Jose Visiers
Jack McCluskey
Jiayang Wu
jrhy
Kenneth Knowles
KevinGG
kileys
klmilam
Kyle Weaver
Luís Bianchin
Luke Cwik
Melissa Pashniak
Michael Luckey
Miguel Hernandez
Milena Bukal
Minbo Bae
minherz
Moritz Mack
mosche
Natalie
Ning Kang
Pablo Estrada
Pavel Avilov
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Rogan Morrow
Ruslan Altynnikov
Sam Whittle
Sergey Kalinin
Slava Chernyak
Svetak Sundhar
Tianyang Hu
Tim Robertson
Tomo Suzuki
tuorhador
Udi Meiri
vachan-shetty
Valentyn Tymofieiev
Yichi Zhang
zhoufek&lt;/p></description></item><item><title>Blog: Apache Beam 2.34.0</title><link>/blog/beam-2.34.0/</link><pubDate>Thu, 11 Nov 2021 00:11:00 -0800</pubDate><guid>/blog/beam-2.34.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.34.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2340-2021-11-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.34.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350405">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>The Beam Java API for Calcite SqlTransform is no longer experimental (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12680">BEAM-12680&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s ParDo (Map, FlatMap, etc.) transforms now suport a &lt;code>with_exception_handling&lt;/code> option for easily ignoring bad records and implementing the dead letter pattern.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>&lt;code>ReadFromBigQuery&lt;/code> and &lt;code>ReadAllFromBigQuery&lt;/code> now run queries with BATCH priority by default. The &lt;code>query_priority&lt;/code> parameter is introduced to the same transforms to allow configuring the query priority (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12913">BEAM-12913&lt;/a>).&lt;/li>
&lt;li>[EXPERIMENTAL] Support for &lt;a href="https://cloud.google.com/bigquery/docs/reference/storage">BigQuery Storage Read API&lt;/a> added to &lt;code>ReadFromBigQuery&lt;/code>. The newly introduced &lt;code>method&lt;/code> parameter can be set as &lt;code>DIRECT_READ&lt;/code> to use the Storage Read API. The default is &lt;code>EXPORT&lt;/code> which invokes a BigQuery export request. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10917">BEAM-10917&lt;/a>).&lt;/li>
&lt;li>[EXPERIMENTAL] Added &lt;code>use_native_datetime&lt;/code> parameter to &lt;code>ReadFromBigQuery&lt;/code> to configure the return type of &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#datetime_type">DATETIME&lt;/a> fields when using &lt;code>ReadFromBigQuery&lt;/code>. This parameter can &lt;em>only&lt;/em> be used when &lt;code>method = DIRECT_READ&lt;/code>(Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10917">BEAM-10917&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Upgrade to Calcite 1.26.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9379">BEAM-9379&lt;/a>).&lt;/li>
&lt;li>Added a new &lt;code>dataframe&lt;/code> extra to the Python SDK that tracks &lt;code>pandas&lt;/code> versions
we&amp;rsquo;ve verified compatibility with. We now recommend installing Beam with &lt;code>pip install apache-beam[dataframe]&lt;/code> when you intend to use the DataFrame API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12906">BEAM-12906&lt;/a>).&lt;/li>
&lt;li>Add an &lt;a href="https://github.com/cometta/python-apache-beam-spark">example&lt;/a> of deploying Python Apache Beam job with Spark Cluster&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>SQL Rows are no longer flattened (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5505">BEAM-5505&lt;/a>).&lt;/li>
&lt;li>[Go SDK] beam.TryCrossLanguage&amp;rsquo;s signature now matches beam.CrossLanguage. Like other Try functions it returns an error instead of panicking. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9918">BEAM-9918&lt;/a>).&lt;/li>
&lt;li>&lt;a href="https://jira.apache.org/jira/browse/BEAM-12925">BEAM-12925&lt;/a> was fixed. It used to silently pass incorrect null data read from JdbcIO. Pipelines affected by this will now start throwing failures instead of silently passing incorrect data.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed error while writing multiple DeferredFrames to csv (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12701">BEAM-12701&lt;/a>).&lt;/li>
&lt;li>Fixed error when importing the DataFrame API with pandas 1.0.x installed (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12945">BEAM-12945&lt;/a>).&lt;/li>
&lt;li>Fixed top.SmallestPerKey implementation in the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12946">BEAM-12946&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Large Java BigQueryIO writes with the FILE_LOADS method will fail in batch mode (specifically, when copy jobs are used).
This results in the error message: &lt;code>IllegalArgumentException: Attempting to access unknown side input&lt;/code>.
Please upgrade to a newer version (&amp;gt; 2.34.0) or use another write method (e.g. &lt;code>STORAGE_WRITE_API&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.34.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alex Amato,
Alexander Chermenin,
Alexey Romanenko,
AlikRodriguez,
Andrew Pilloud,
Andy Xu,
Ankur Goenka,
Aydar Farrakhov,
Aydar Zainutdinov,
Aydar Zaynutdinov,
AydarZaynutdinov,
Benjamin Gonzalez,
BenWhitehead,
Brachi Packter,
Brian Hulette,
Bu Sun Kim,
Chamikara Jayalath,
Chris Gray,
Chuck Yang,
Chun Yang,
Claire McGinty,
comet,
Daniel Collins,
Daniel Oliveira,
Daniel Thevessen,
daria.malkova,
David Cavazos,
David Huntsperger,
Dmytro Kozhevin,
dpcollins-google,
Eduardo Sánchez López,
Elias Djurfeldt,
emily,
Emily Ye,
Enis Sert,
Etienne Chauchot,
Fernando Morales,
Heejong Lee,
Ihor Indyk,
Ismaël Mejía,
Israel Herraiz,
Jack McCluskey,
Jonathan Hourany,
Judah Rand,
Kenneth Knowles,
KevinGG,
Ke Wu,
kileys,
Kyle Weaver,
Luke Cwik,
masahitojp,
MiguelAnzoWizeline,
Minbo Bae,
Niels Basjes,
Ning Kang,
Pablo Estrada,
pareshsarafmdb,
Paul Féraud,
Piotr Szczepanik,
Reuven Lax,
Ritesh Ghorse,
R. Miles McCain,
Robert Bradshaw,
Robert Burke,
Rogan Morrow,
Ruwan Lambrichts,
rvballada,
Ryan Thompson,
Sam Rohde,
Sam Whittle,
Ștefan Istrate,
Steve Niemitz,
Thomas Li Fredriksen,
Tomo Suzuki,
tvalentyn,
Udi Meiri,
Vachan,
Valentyn Tymofieiev,
Vincent Marquez,
WinsonT,
Yichi Zhang,
Yifan Mai,
Yilei &amp;ldquo;Dolee&amp;rdquo; Yang,
zhoufek&lt;/p></description></item><item><title>Blog: Go SDK Exits Experimental in Apache Beam 2.33.0</title><link>/blog/go-sdk-release/</link><pubDate>Thu, 04 Nov 2021 00:00:01 -0800</pubDate><guid>/blog/go-sdk-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam’s latest release, version &lt;a href="/get-started/downloads/">2.33.0&lt;/a>, is the first official release of the long experimental Go SDK.
Built with the &lt;a href="https://golang.org/">Go Programming Language&lt;/a>, the Go SDK joins the Java and Python SDKs as the third implementation of the Beam programming model.&lt;/p>
&lt;h2 id="using-the-new-go-sdk">Using the new Go SDK.&lt;/h2>
&lt;p>New users of the Go SDK can start using it in their Go programs by importing the main beam package:&lt;/p>
&lt;pre tabindex="0">&lt;code>import &amp;#34;github.com/apache/beam/sdks/v2/go/pkg/beam&amp;#34;
&lt;/code>&lt;/pre>&lt;p>The next run of &lt;code>go mod tidy&lt;/code> will fetch the latest stable version of the module.
Alternatively executing &lt;code>go get github.com/apache/beam/sdks/v2/go/pkg/beam&lt;/code> will download it to the local module cache immeadiately, and add it to your &lt;code>go.mod&lt;/code> file.&lt;/p>
&lt;p>Existing users of the experimental Go SDK need to update to new &lt;code>v2&lt;/code> import paths to start using the latest versions of the SDK.
This can be done by adding &lt;code>v2&lt;/code> to the import paths, changing &lt;code>github.com/apache/beam/sdks/go/&lt;/code>&amp;hellip; to &lt;code>github.com/apache/beam/sdks/v2/go/&lt;/code>&amp;hellip; where applicable, and then running &lt;code>go mod tidy&lt;/code>.&lt;/p>
&lt;p>Further documentation on using the SDK is available in the &lt;a href="/documentation/programming-guide/">Beam Programming Guide&lt;/a>, and in the package &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam">Go Doc&lt;/a>.&lt;/p>
&lt;h2 id="feature-support">Feature Support&lt;/h2>
&lt;p>At time of writing, the Go SDK is currently &amp;ldquo;Batteries Not Included&amp;rdquo;.
This means that there are gaps or edge cases in supported IOs and transforms.
That said, the core of the SDK enables a great deal of the Beam Model for
custom user use, supporting the following features:&lt;/p>
&lt;ul>
&lt;li>PTransforms
&lt;ul>
&lt;li>Impulse&lt;/li>
&lt;li>Create&lt;/li>
&lt;li>ParDo with user DoFns
&lt;ul>
&lt;li>Iterable side inputs&lt;/li>
&lt;li>Multiple output emitters&lt;/li>
&lt;li>Receive and return key-value pairs&lt;/li>
&lt;li>SplittableDoFns&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>GroupByKey and CoGroupByKey&lt;/li>
&lt;li>Combine and CombinePerKey with user CombineFns&lt;/li>
&lt;li>Flatten&lt;/li>
&lt;li>Partition&lt;/li>
&lt;li>Composite transforms&lt;/li>
&lt;li>Cross language transforms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Event time windowing
&lt;ul>
&lt;li>Global, Interval, Sliding, and Session windows&lt;/li>
&lt;li>Aggregating over windowed PCollections with GroupByKeys or Combines&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Coders
&lt;ul>
&lt;li>Primitive Go types (ints, string, []bytes, and more)&lt;/li>
&lt;li>Beam Schemas for Go struct types (including struct, slice, and map fields)&lt;/li>
&lt;li>Registering custom coders&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Metrics
&lt;ul>
&lt;li>PCollection metrics (element counts, size estimates)&lt;/li>
&lt;li>Custom user metrics&lt;/li>
&lt;li>Post job user metrics querying (coming in 2.34.0)&lt;/li>
&lt;li>DoFn profiling metrics (coming in 2.35.0)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Built-in transforms
&lt;ul>
&lt;li>Sum, count, min, max, top, filter&lt;/li>
&lt;li>Scalable TextIO reading&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Upcoming feature roadmap, and known issues are discussed below.
In particular, we plan to support a much richer set of IO connectors via Beam&amp;rsquo;s cross-language capabilities.&lt;/p>
&lt;h2 id="releases">Releases&lt;/h2>
&lt;p>With this release, the Go SDK now uses &lt;a href="https://golang.org/ref/mod">Go Modules&lt;/a> for dependency management.
This makes it so users, SDK authors, and the testing infrastructure can all rely on the same versions of dependencies, making builds reproducible.
This also makes &lt;a href="/blog/validate-beam-release/#configuring-a-go-build-to-validate-a-beam-release-candidate">validating Go SDK Release Candidates simple&lt;/a>.&lt;/p>
&lt;p>Versioned SDK worker containers are now built and &lt;a href="https://hub.docker.com/r/apache/beam_go_sdk/tags?page=1&amp;amp;ordering=last_updated">published&lt;/a>, with the SDK using matching tagged versions.
User jobs no longer need to specify a container to use, except when using custom containers.&lt;/p>
&lt;h2 id="compatibility">Compatibility&lt;/h2>
&lt;p>The Go SDK will largely follow suit with the Go notion of compatibility.
Some concessions are made to keep all SDKs together on the same release cycle.&lt;/p>
&lt;h3 id="language-compatibility">Language Compatibility&lt;/h3>
&lt;p>The SDK will be tested at a minimum &lt;a href="https://golang.org/doc/devel/release">Go Programming Language version of 1.16&lt;/a>, and use available language features and standard library packages accordingly.
To maintain a broad compatibility, the Go SDK will not require the latest major version of Go.
We expect to follow the 2nd newest supported release of the language, with a possible exception when Go 1.18 is released, in order to begin experimenting with &lt;a href="https://go.dev/blog/generics-proposal">Go Generics&lt;/a> in the SDK.
Release notes will call out when the minimum version of the language changes.&lt;/p>
&lt;h3 id="package-compatibility">Package Compatibility&lt;/h3>
&lt;p>The primary user packages will avoid changing in backwards incompatible ways for core features.
This is to be inline with Go&amp;rsquo;s notion of the &lt;a href="https://research.swtch.com/vgo-import">&lt;code>import compatibility rule&lt;/code>&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>If an old package and a new package have the same import path,
the new package must be backwards compatible with the old package.&lt;/p>
&lt;/blockquote>
&lt;p>Exceptions to this policy are around newer, experimental, or in development features and are subject to change.
Such features will have a doc comment noting the experimental status.
Major changes will be mentioned in the release notes.
For example, using &lt;code>beam.WindowInto&lt;/code> with Triggers is currently experimental and may have the API changed in a future release.&lt;/p>
&lt;p>Primary user packages include:&lt;/p>
&lt;ul>
&lt;li>The main beam package &lt;code>github.com/apache/beam/sdks/v2/go/pkg/beam&lt;/code>&lt;/li>
&lt;li>Sub packages under &lt;code>.../transforms&lt;/code>, &lt;code>.../io&lt;/code>, &lt;code>.../runners&lt;/code>, and &lt;code>.../testing&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Generally, packages in the module other than the primary user packages are for framework use and are at risk of changing.&lt;/p>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;h4 id="batteries-not-included">Batteries not included.&lt;/h4>
&lt;ul>
&lt;li>Current native transforms are undertested&lt;/li>
&lt;li>IOs may not be written to scale&lt;/li>
&lt;li>Go Direct Runner is incomplete and is not portable, prefer using the Python Portable runner, or Flink
&lt;ul>
&lt;li>Doesn&amp;rsquo;t support side input windowing. &lt;a href="https://issues.apache.org/jira/browse/BEAM-13075">BEAM-13075&lt;/a>&lt;/li>
&lt;li>Doesn&amp;rsquo;t serialize data, making it unlikely to catch coder issues &lt;a href="https://issues.apache.org/jira/browse/BEAM-6372">BEAM-6372&lt;/a>&lt;/li>
&lt;li>Can use other general improvements, and become portable &lt;a href="https://issues.apache.org/jira/browse/BEAM-11076">BEAM-11076&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Current Trigger API is under iteration and subject to change &lt;a href="https://issues.apache.org/jira/browse/BEAM-3304">BEAM-3304&lt;/a>
&lt;ul>
&lt;li>API has a possible breaking change between 2.33.0 and 2.34.0, and may change again&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Support of the SDK on services, like Google Cloud Dataflow, remains at the service owner&amp;rsquo;s discretion&lt;/li>
&lt;li>Need something?
&lt;ul>
&lt;li>File a ticket in the &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20component%20%3D%20sdk-go">Beam JIRA&lt;/a> and,&lt;/li>
&lt;li>Email the &lt;a href="mailto:dev@beam.apache.org?subject=%5BGo%20SDK%20Feature%5D">dev@beam.apache.org&lt;/a> list!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="fixed-in-2340">Fixed in 2.34.0&lt;/h4>
&lt;ul>
&lt;li>&lt;code>top.SmallestPerKey&lt;/code> was broken &lt;a href="https://issues.apache.org/jira/browse/BEAM-12946">BEAM-12946&lt;/a>&lt;/li>
&lt;li>&lt;code>beam.TryCrossLanguage&lt;/code> API didn&amp;rsquo;t match non-Try version &lt;a href="https://issues.apache.org/jira/browse/BEAM-9918">BEAM-9918&lt;/a>
&lt;ul>
&lt;li>This is a breaking change if one was calling &lt;code>beam.TryCrossLanguage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="fixed-in-2350">Fixed in 2.35.0&lt;/h4>
&lt;ul>
&lt;li>Non-global window side inputs don&amp;rsquo;t match (correctness bug) &lt;a href="https://issues.apache.org/jira/browse/BEAM-11087">BEAM-11087&lt;/a>
&lt;ul>
&lt;li>Until 2.35.0 it&amp;rsquo;s not recommended to use side inputs that are not using the global window.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DoFns using side inputs accumulate memory over bundles, causing out of memory issues &lt;a href="https://issues.apache.org/jira/browse/BEAM-13130">BEAM-13130&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="roadmap">Roadmap&lt;/h2>
&lt;p>The &lt;a href="/roadmap/go-sdk/">SDK roadmap&lt;/a> has been updated.
Ongoing focus is to bolster streaming focused features, improve existing connectors, and make connectors easier to implement.&lt;/p>
&lt;p>In the nearer term this comes in the form of improvements to side inputs, and providing wrappers and improving ease-of-use for cross language transforms from Java.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We hope you find the SDK useful, and it&amp;rsquo;s still early days.
If you make something with the Go SDK, consider &lt;a href="/community/contact-us/">sharing it with us&lt;/a>.
And remember, &lt;a href="/contribute/">contributions&lt;/a> are always welcome.&lt;/p></description></item></channel></rss>