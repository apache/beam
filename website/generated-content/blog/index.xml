<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – Blogs</title><link>/blog/</link><description>Recent content in Blogs on Apache Beam</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 29 Jul 2020 00:00:01 -0800</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Apache Beam 2.23.0</title><link>/blog/beam-2.23.0/</link><pubDate>Wed, 29 Jul 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.23.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.23.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2230-2020-07-29">download page&lt;/a> for this release.
For more information on changes in 2.23.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347145">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Twister2 Runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7304">BEAM-7304&lt;/a>).&lt;/li>
&lt;li>Python 3.8 support (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8494">BEAM-8494&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for reading from Snowflake added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9722">BEAM-9722&lt;/a>).&lt;/li>
&lt;li>Support for writing to Splunk added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8596">BEAM-8596&lt;/a>).&lt;/li>
&lt;li>Support for assume role added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10335">BEAM-10335&lt;/a>).&lt;/li>
&lt;li>A new transform to read from BigQuery has been added: &lt;code>apache_beam.io.gcp.bigquery.ReadFromBigQuery&lt;/code>. This transform
is experimental. It reads data from BigQuery by exporting data to Avro files, and reading those files. It also supports
reading data by exporting to JSON files. This has small differences in behavior for Time and Date-related fields. See
Pydoc for more information.&lt;/li>
&lt;li>Add dispositions for SnowflakeIO.write (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10343">BEAM-10343&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Update Snowflake JDBC dependency and add application=beam to connection URL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10383">BEAM-10383&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>RowJson.RowJsonDeserializer&lt;/code>, &lt;code>JsonToRow&lt;/code>, and &lt;code>PubsubJsonTableProvider&lt;/code> now accept &amp;ldquo;implicit
nulls&amp;rdquo; by default when deserializing JSON (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10220">BEAM-10220&lt;/a>).
Previously nulls could only be represented with explicit null values, as in
&lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;, &amp;quot;baz&amp;quot;: null}&lt;/code>, whereas an implicit null like &lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;}&lt;/code> would raise an
exception. Now both JSON strings will yield the same result by default. This behavior can be
overridden with &lt;code>RowJson.RowJsonDeserializer#withNullBehavior&lt;/code>.&lt;/li>
&lt;li>Fixed a bug in &lt;code>GroupIntoBatches&lt;/code> experimental transform in Python to actually group batches by key.
This changes the output type for this transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6696">BEAM-6696&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Gearpump runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>Remove Apex runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>RedisIO.readAll() is deprecated and will be removed in 2 versions, users must use RedisIO.readKeyPatterns() as a replacement (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9747">BEAM-9747&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.23.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron, Abhishek Yadav, Ahmet Altay, aiyangar, Aizhamal Nurmamat kyzy, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alex Amato, Alexey Romanenko, Allen Pradeep Xavier, Andrew Crites, Andrew Pilloud, Ankur Goenka, Anna Qin, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Brian Michalski, brucearctor, Chamikara Jayalath, chi-chi weng, Chuck Yang, Chun Yang, Colm O hEigeartaigh, Corvin Deboeser, Craig Chambers, Damian Gadomski, Damon Douglas, Daniel Oliveira, Dariusz Aniszewski, darshanj, darshan jani, David Cavazos, David Moravek, David Yan, Esun Kim, Etienne Chauchot, Filipe Regadas, fuyuwei, Graeme Morgan, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, InigoSJ, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, Jie Fan, John Mora, Jozef Vilcek, Julien Phalip, Justine Koa, Kamil Gabryjelski, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, kevingg, Kevin Sijo Puthusseri, kshivvy, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Mark-Zeng, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Nathan Fisher, Niel Markwick, Ning Kang, Omar Ismail, Pablo Estrada, paul fisher, Pawel Pasterz, perkss, Piotr Szuberski, pulasthi, purbanow, Rahul Patwari, Rajat Mittal, Rehman, Rehman Murad Ali, Reuben van Ammers, Reuven Lax, Reza Rokni, Rion Williams, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, sabhyankar, Sam Rohde, Sam Whittle, sclukas77, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Ted Romer, Tesio, Thinh Ha, Thomas Weise, Tobias Kaymak, tobiaslieber-cognitedata, Tobiasz Kędzierski, Tomo Suzuki, Tudor Marian, tvs, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, Yixing Zhang, yoshiki.obata, Yueyang Qiu, Yu Feng, Yuwei Fu, Zhuo Peng, ZijieSong946.&lt;/p></description></item><item><title>Blog: Apache Beam 2.22.0</title><link>/blog/beam-2.22.0/</link><pubDate>Mon, 08 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.22.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.22.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2220-2020-06-08">download page&lt;/a> for this release.
For more information on changes in 2.22.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347144">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Basic Kafka read/write support for DataflowRunner (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8019">BEAM-8019&lt;/a>).&lt;/li>
&lt;li>Sources and sinks for Google Healthcare APIs (Java)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9468">BEAM-9468&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>&lt;code>--workerCacheMB&lt;/code> flag is supported in Dataflow streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9964">BEAM-9964&lt;/a>)&lt;/li>
&lt;li>&lt;code>--direct_num_workers=0&lt;/code> is supported for FnApi runner. It will set the number of threads/subprocesses to number of cores of the machine executing the pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9443">BEAM-9443&lt;/a>).&lt;/li>
&lt;li>Python SDK now has experimental support for SqlTransform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8603">BEAM-8603&lt;/a>).&lt;/li>
&lt;li>Add OnWindowExpiration method to Stateful DoFn (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1589">BEAM-1589&lt;/a>).&lt;/li>
&lt;li>Added PTransforms for Google Cloud DLP (Data Loss Prevention) services integration (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9723">BEAM-9723&lt;/a>):
&lt;ul>
&lt;li>Inspection of data,&lt;/li>
&lt;li>Deidentification of data,&lt;/li>
&lt;li>Reidentification of data.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Add a more complete I/O support matrix in the documentation site (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9916">BEAM-9916&lt;/a>).&lt;/li>
&lt;li>Upgrade Sphinx to 3.0.3 for building PyDoc.&lt;/li>
&lt;li>Added a PTransform for image annotation using Google Cloud AI image processing service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9646">BEAM-9646&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now requires &lt;code>--job_endpoint&lt;/code> to be set when using &lt;code>--runner=PortableRunner&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9860">BEAM-9860&lt;/a>). Users seeking the old default behavior should set &lt;code>--runner=FlinkRunner&lt;/code> instead.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.22.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, aiyangar, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alexey Romanenko, Allen Pradeep Xavier, amaliujia, Andrew Pilloud, Ankur Goenka, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Colm O hEigeartaigh, Craig Chambers, Damon Douglas, Daniel Oliveira, David Cavazos, David Moravek, Esun Kim, Etienne Chauchot, Filipe Regadas, Graeme Morgan, Hannah Jiang, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, John Mora, Kamil Wasilewski, Kenneth Jung, Kenneth Knowles, kevingg, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Niel Markwick, Ning Kang, Omar Ismail, omarismail94, Pablo Estrada, paul fisher, pawelpasterz, Pawel Pasterz, Piotr Szuberski, Rahul Patwari, rarokni, Rehman, Rehman Murad Ali, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Sam Rohde, Sam Whittle, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Tesio, Thomas Weise, tobiaslieber-cognitedata, Tomo Suzuki, Tudor Marian, tvalentyn, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, Zhuo Peng&lt;/p></description></item><item><title>Blog: Announcing Beam Katas for Kotlin</title><link>/blog/beam-katas-kotlin-release/</link><pubDate>Mon, 01 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-katas-kotlin-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Today, we are happy to announce a new addition to the Beam Katas family: Kotlin!&lt;/p>
&lt;p>&lt;img src="/images/blog/beam-katas-kotlin-release/beam-and-kotlin.png" alt="Apache Beam and Kotlin Shaking Hands" height="330" width="800" >&lt;/p>
&lt;p>You may remember &lt;a href="https://beam.apache.org/blog/beam-kata-release">a post from last year&lt;/a> that informed everyone of the wonderful Beam Katas available on &lt;a href="https://stepik.org">Stepik&lt;/a>
for learning more about writing Apache Beam applications, working with its various APIs and programming model
hands-on, all from the comfort of your favorite IDEs. As of today, you can now work through all of the progressive
exercises to learn about the fundamentals of Beam in Kotlin.&lt;/p>
&lt;p>&lt;a href="https://kotlinlang.org">Kotlin&lt;/a> is a modern, open-source, statically typed language that targets the JVM. It is most commonly used by
Android developers, however it has recently risen in popularity due to its extensive feature set that enables
more concise and cleaner code than Java, without sacrificing performance or type safety. It recently was &lt;a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved">ranked
as one of the most beloved programming languages in the annual Stack Overflow Developer Survey&lt;/a>, so don&amp;rsquo;t take
just our word for it.&lt;/p>
&lt;p>The relationship between Apache Beam and Kotlin isn&amp;rsquo;t a new one. You can find examples scattered across the web
of engineering teams embracing the two technologies including &lt;a href="https://beam.apache.org/blog/beam-kotlin/">a series of samples announced on this very blog&lt;/a>.
If you are new to Beam or are an experienced veteran looking for a change of pace, we&amp;rsquo;d encourage you to give
Kotlin a try.&lt;/p>
&lt;p>You can find the Kotlin and the other excellent Beam Katas below (or by just searching for &amp;ldquo;Beam Katas&amp;rdquo; within
&lt;a href="https://www.jetbrains.com/education/download/#section=idea">IntelliJ&lt;/a> or &lt;a href="https://www.jetbrains.com/education/download/#section=pycharm-edu">PyCharm&lt;/a> through &lt;a href="https://plugins.jetbrains.com/plugin/10081-edutools">the EduTools plugin&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stepik.org/course/72488">&lt;strong>Kotlin&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54530">&lt;strong>Java&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54532">&lt;strong>Python&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/70387">&lt;strong>Go (in development)&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;d like to extend a very special thanks to &lt;a href="https://twitter.com/henry_ken">Henry Suryawirawan&lt;/a> for his creation of the original series of Katas
and his support during the review process and making this effort a reality.&lt;/p>
&lt;p>&lt;br />&lt;/p>
&lt;p>&lt;img src="/images/blog/beam-katas-kotlin-release/beam-katas-in-edutools.png" alt="Access Beam Katas Kotlin through a JetBrains Educational Product" height="252" width="800" >&lt;/p></description></item><item><title>Blog: Python SDK Typing Changes</title><link>/blog/python-typing/</link><pubDate>Thu, 28 May 2020 00:00:01 -0800</pubDate><guid>/blog/python-typing/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Beam Python has recently increased its support and integration of Python 3 type
annotations for improved code clarity and type correctness checks.
Read on to find out what&amp;rsquo;s new.&lt;/p>
&lt;p>Python supports type annotations on functions (PEP 484). Static type checkers,
such as mypy, are used to verify adherence to these types.
For example:&lt;/p>
&lt;pre>&lt;code>def f(v: int) -&amp;gt; int:
return v[0]
&lt;/code>&lt;/pre>&lt;p>Running mypy on the above code will give the error:
&lt;code>Value of type &amp;quot;int&amp;quot; is not indexable&lt;/code>.&lt;/p>
&lt;p>We&amp;rsquo;ve recently made changes to Beam in 2 areas:&lt;/p>
&lt;p>Adding type annotations throughout Beam. Type annotations make a large and
sophisticated codebase like Beam easier to comprehend and navigate in your
favorite IDE.&lt;/p>
&lt;p>Second, we&amp;rsquo;ve added support for Python 3 type annotations. This allows SDK
users to specify a DoFn&amp;rsquo;s type hints in one place.
We&amp;rsquo;ve also expanded Beam&amp;rsquo;s support of &lt;code>typing&lt;/code> module types.&lt;/p>
&lt;p>For more background see:
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;h1 id="beam-is-typed">Beam Is Typed&lt;/h1>
&lt;p>In tandem with the new type annotation support within DoFns, we&amp;rsquo;ve invested a
great deal of time adding type annotations to the Beam python code itself.
With this in place, we have begun using mypy, a static type
checker, as part of Beam&amp;rsquo;s code review process, which ensures higher quality
contributions and fewer bugs.
The added context and insight that type annotations add throughout Beam is
useful for all Beam developers, contributors and end users alike, but
it is especially beneficial for developers who are new to the project.
If you use an IDE that understands type annotations, it will provide richer
type completions and warnings than before.
You&amp;rsquo;ll also be able to use your IDE to inspect the types of Beam functions and
transforms to better understand how they work, which will ease your own
development.
Finally, once Beam is fully annotated, end users will be able to benefit from
the use of static type analysis on their own pipelines and custom transforms.&lt;/p>
&lt;h1 id="new-ways-to-annotate">New Ways to Annotate&lt;/h1>
&lt;h2 id="python-3-syntax-annotations">Python 3 Syntax Annotations&lt;/h2>
&lt;p>Coming in Beam 2.21 (BEAM-8280), you will be able to use Python annotation
syntax to specify input and output types.&lt;/p>
&lt;p>For example, this new form:&lt;/p>
&lt;pre>&lt;code>class MyDoFn(beam.DoFn):
def process(self, element: int) -&amp;gt; typing.Text:
yield str(element)
&lt;/code>&lt;/pre>&lt;p>is equivalent to this:&lt;/p>
&lt;pre>&lt;code>@apache_beam.typehints.with_input_types(int)
@apache_beam.typehints.with_output_types(typing.Text)
class MyDoFn(beam.DoFn):
def process(self, element):
yield str(element)
&lt;/code>&lt;/pre>&lt;p>One of the advantages of the new form is that you may already be using it
in tandem with a static type checker such as mypy, thus getting additional
runtime type checking for free.&lt;/p>
&lt;p>This feature will be enabled by default, and there will be 2 mechanisms in
place to disable it:&lt;/p>
&lt;ol>
&lt;li>Calling &lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline
construction will disable the new feature completely.&lt;/li>
&lt;li>Decorating a function with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will
tell Beam to ignore annotations for it.&lt;/li>
&lt;/ol>
&lt;p>Uses of Beam&amp;rsquo;s &lt;code>with_input_type&lt;/code>, &lt;code>with_output_type&lt;/code> methods and decorators will
still work and take precedence over annotations.&lt;/p>
&lt;h3 id="sidebar">Sidebar&lt;/h3>
&lt;p>You might ask: couldn&amp;rsquo;t we use mypy to type check Beam pipelines?
There are several reasons why this is not the case.&lt;/p>
&lt;ul>
&lt;li>Pipelines are constructed at runtime and may depend on information that is
only known at that time, such as a config file or database table schema.&lt;/li>
&lt;li>PCollections don&amp;rsquo;t have the necessary type information, so mypy sees them as
effectively containing any element type.
This may change in in the future.&lt;/li>
&lt;li>Transforms using lambdas (ex: &lt;code>beam.Map(lambda x: (1, x)&lt;/code>) cannot be
annotated properly using PEP 484.
However, Beam does a best-effort attempt to analyze the output type
from the bytecode.&lt;/li>
&lt;/ul>
&lt;h2 id="typing-module-support">Typing Module Support&lt;/h2>
&lt;p>Python&amp;rsquo;s &lt;a href="https://docs.python.org/3/library/typing.html">typing&lt;/a> module defines
types used in type annotations. This is what we call &amp;ldquo;native&amp;rdquo; types.
While Beam has its own typing types, it also supports native types.
While both Beam and native types are supported, for new code we encourage using
native typing types. Native types have as these are supported by additional tools.&lt;/p>
&lt;p>While working on Python 3 annotations syntax support, we&amp;rsquo;ve also discovered and
fixed issues with native type support. There may still be bugs and unsupported
native types. Please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a> if you encounter
issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.21.0</title><link>/blog/beam-2.21.0/</link><pubDate>Wed, 27 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.21.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.21.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2210-2020-05-27">download page&lt;/a> for this release.
For more information on changes in 2.21.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347143">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Python: Deprecated module &lt;code>apache_beam.io.gcp.datastore.v1&lt;/code> has been removed
as the client it uses is out of date and does not support Python 3
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9529">BEAM-9529&lt;/a>).
Please migrate your code to use
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.datastore.v1new.datastoreio.html">apache_beam.io.gcp.datastore.&lt;strong>v1new&lt;/strong>&lt;/a>.
See the updated
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py">datastore_wordcount&lt;/a>
for example usage.&lt;/li>
&lt;li>Python SDK: Added integration tests and updated batch write functionality for Google Cloud Spanner transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8949">BEAM-8949&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Python SDK will now use Python 3 type annotations as pipeline type hints.
(&lt;a href="https://github.com/apache/beam/pull/10717">#10717&lt;/a>)&lt;/p>
&lt;p>If you suspect that this feature is causing your pipeline to fail, calling
&lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline creation
will disable is completely, and decorating specific functions (such as
&lt;code>process()&lt;/code>) with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will disable it
for that function.&lt;/p>
&lt;p>More details can be found in
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>
and the Python SDK Typing Changes
&lt;a href="https://beam.apache.org/blog/python-typing/">blog post&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: Introducing the concept of options in Beam Schema’s. These options add extra
context to fields and schemas. This replaces the current Beam metadata that is present
in a FieldType only, options are available in fields and row schemas. Schema options are
fully typed and can contain complex rows. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9035">BEAM-9035&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: The protobuf extension is fully schema aware and also includes protobuf option
conversion to beam schema options. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9044">BEAM-9044&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added ability to write to BigQuery via Avro file loads (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a>)&lt;/p>
&lt;p>By default, file loads will be done using JSON, but it is possible to
specify the temp_file_format parameter to perform file exports with AVRO.
AVRO-based file loads work by exporting Python types into Avro types, so
to switch to Avro-based loads, you will need to change your data types
from Json-compatible types (string-type dates and timestamp, long numeric
values as strings) into Python native types that are written to Avro
(Python&amp;rsquo;s date, datetime types, decimal, etc). For more information
see &lt;a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI VideoIntelligence service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9147">BEAM-9147&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI natural language processing API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9634">BEAM-9634&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker-pull-licenses&lt;/code> tag was introduced. Licenses/notices of third party dependencies will be added to the docker images when &lt;code>docker-pull-licenses&lt;/code> was set.
The files are added to &lt;code>/opt/apache/beam/third_party_licenses/&lt;/code>.
By default, no licenses/notices are added to the docker images. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9136">BEAM-9136&lt;/a>)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Dataflow runner now requires the &lt;code>--region&lt;/code> option to be set, unless a default value is set in the environment (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9199">BEAM-9199&lt;/a>). See &lt;a href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints">here&lt;/a> for more details.&lt;/li>
&lt;li>HBaseIO.ReadAll now requires a PCollection of HBaseIO.Read objects instead of HBaseQuery objects (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9279">BEAM-9279&lt;/a>).&lt;/li>
&lt;li>ProcessContext.updateWatermark has been removed in favor of using a WatermarkEstimator (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9430">BEAM-9430&lt;/a>).&lt;/li>
&lt;li>Coder inference for PCollection of Row objects has been disabled (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9569">BEAM-9569&lt;/a>).&lt;/li>
&lt;li>Go SDK docker images are no longer released until further notice.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Java SDK: Beam Schema FieldType.getMetadata is now deprecated and is replaced by the Beam
Schema Options, it will be removed in version &lt;code>2.23.0&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9704">BEAM-9704&lt;/a>)&lt;/li>
&lt;li>The &lt;code>--zone&lt;/code> option in the Dataflow runner is now deprecated. Please use &lt;code>--worker_zone&lt;/code> instead. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9716">BEAM-9716&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.21.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Meihm, Adrian Eka, Ahmet Altay, AldairCoronel, Alex Van Boxel, Alexey Romanenko, Andrew Crites, Andrew Pilloud, Ankur Goenka, Badrul (Taki) Chowdhury, Bartok Jozsef, Boyuan Zhang, Brian Hulette, brucearctor, bumblebee-coming, Chad Dombrova, Chamikara Jayalath, Chie Hayashida, Chris Gorgolewski, Chuck Yang, Colm O hEigeartaigh, Curtis &amp;ldquo;Fjord&amp;rdquo; Hawthorne, Daniel Mills, Daniel Oliveira, David Yan, Elias Djurfeldt, Emiliano Capoccia, Etienne Chauchot, Fernando Diaz, Filipe Regadas, Gleb Kanterov, Hai Lu, Hannah Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Hk-tang, Ismaël Mejía, Jacoby, Jan Lukavský, Jeroen Van Goey, jfarr, Jozef Vilcek, Kai Jiang, Kamil Wasilewski, Kenneth Knowles, KevinGG, Kyle Weaver, Kyoungha Min, Luke Cwik, Maximilian Michels, Michal Walenia, Ning Kang, Pablo Estrada, paul fisher, Piotr Szuberski, Reuven Lax, Robert Bradshaw, Robert Burke, Rose Nguyen, Rui Wang, Sam Rohde, Sam Whittle, Spoorti Kundargi, Steve Koonce, sunjincheng121, Ted Yun, Tesio, Thomas Weise, Tomo Suzuki, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, Yichi Zhang, yoshiki.obata, Yueyang Qiu&lt;/p></description></item><item><title>Blog: Beam Summit Digital Is Coming - Register Now!</title><link>/blog/beam-summit-digital-2020/</link><pubDate>Fri, 08 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-summit-digital-2020/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>As some of you are already aware, the 2020 edition of the Beam Summit will be completely &lt;strong>digital and free&lt;/strong>. Beam Summit Digital will take place from &lt;strong>August 24th to 28th&lt;/strong>. The conference will be spread across the course of one week with a couple of hours of program each day.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beamsummit/beamsummit-digital-2020.png"
alt="Beam Summit Digital 2020, August 24-28">&lt;/p>
&lt;p>While we would have loved to see all of you in person, we have to accept that 2020 will not be the year for that. So, we are looking at this as an opportunity to have a bigger and more inclusive event, where people who would normally not be able to travel to the summit will now be able to join, learn and share with the rest of the community.&lt;/p>
&lt;h2 id="providing-you-the-best-experience-possible">Providing you the best experience possible&lt;/h2>
&lt;p>We are going to great lengths to ensure that we provide the Beam community with the best possible experience in an online event. From audio/video quality, to an adequate schedule for our community, to making it as easy as possible to register to the event and join the sessions, to setting up ways for the community to interact and network with each other. The team behind the organization of the Beam Summit has been working on these things, and we are also teaming up with an event production company with experience in online events who are bringing in their knowledge.&lt;/p>
&lt;p>So, what we want to say with this is: We will have a great event! And if you have any ideas on how to make it better, please let us know.&lt;/p>
&lt;h2 id="ways-to-participate-and-help">Ways to participate and help&lt;/h2>
&lt;p>As all things Beam, this is a community effort. The door is open for participation:&lt;/p>
&lt;ol>
&lt;li>Submit a proposal to talk. Please check out the &lt;strong>&lt;a href="https://sessionize.com/beam-digital-summit-2020/">Call for Papers&lt;/a>&lt;/strong> and submit a talk. The deadline for submissions is &lt;em>June 15th&lt;/em>!&lt;/li>
&lt;li>Register to join as an attendee. Registration is now open at the &lt;strong>&lt;a href="https://crowdcast.io/e/beamsummit">registration page&lt;/a>&lt;/strong>. Registration is free!&lt;/li>
&lt;li>Consider sponsoring the event. If your company is interested in engaging with members of the community please check out our &lt;a href="https://drive.google.com/open?id=1EbijvZKpkWwWyMryLY9sJfyZzZk1k44v">sponsoring prospectus&lt;/a>.&lt;/li>
&lt;li>Help us get the word out. Please make sure to let your colleagues and friends in the data engineering field (and beyond!) know about the Beam Summit.&lt;/li>
&lt;/ol>
&lt;h2 id="follow-up-and-more-information">Follow up and more information&lt;/h2>
&lt;p>While we will use the Crowdcast platform to broadcast the event, we will still have a full event website at &lt;a href="https://beamsummit.org">beamsummit.org&lt;/a> with details about the schedule, speakers, FAQ and everything else you need from an event. We are currently working on updating the website and will publish all event details in the next couple of weeks.&lt;/p>
&lt;p>Please also follow us on &lt;a href="https://twitter.com/beamsummit">Twitter&lt;/a> or &lt;a href="https://www.linkedin.com/company/beam-summit/">LinkedIn&lt;/a> to get event updates.&lt;/p></description></item><item><title>Blog: Apache Beam 2.20.0</title><link>/blog/beam-2.20.0/</link><pubDate>Wed, 15 Apr 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.20.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.20.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2190-2020-02-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.20.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12346780">detailed release notes&lt;/a>.&lt;/p>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;p>Python SDK: . (#10223).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8561">BEAM-8561&lt;/a> Adds support for Thrift encoded data via ThriftIO&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7310">BEAM-7310&lt;/a> KafkaIO supports schema resolution using Confluent Schema Registry&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7246">BEAM-7246&lt;/a> Support for Google Cloud Spanner. This is an experimental module for reading and writing data from Google Cloud Spanner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8399">BEAM-8399&lt;/a> Adds support for standard HDFS URLs (with server name)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9146">BEAM-9146&lt;/a> New AnnotateVideo &amp;amp; AnnotateVideoWithContext PTransform&amp;rsquo;s that integrates GCP Video Intelligence functionality&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9247">BEAM-9247&lt;/a> New AnnotateImage &amp;amp; AnnotateImageWithContext PTransform&amp;rsquo;s for element-wise &amp;amp; batch image annotation using Google Cloud Vision API&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9258">BEAM-9258&lt;/a> Added a PTransform for inspection and deidentification of text using Google Cloud DLP&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9248">BEAM-9248&lt;/a> New AnnotateText PTransform that integrates Google Cloud Natural Language functionality&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9305">BEAM-9305&lt;/a> ReadFromBigQuery now supports value providers for the query string&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a> Added ability to write to BigQuery via Avro file loads&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9228">BEAM-9228&lt;/a> Direct runner for FnApi supports further parallelism&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8550">BEAM-8550&lt;/a> Support for @RequiresTimeSortedInput in Flink and Spark&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6857">BEAM-6857&lt;/a> Added support for dynamic timers&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-3453">BEAM-3453&lt;/a> Backwards incompatible change in ReadFromPubSub(topic=) in Python&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9310">BEAM-9310&lt;/a> SpannerAccessor in Java is now package-private to reduce API surface&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8616">BEAM-8616&lt;/a> ParquetIO hadoop dependency should be now provided by the users&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9063">BEAM-9063&lt;/a> Docker images will be deployed to apache/beam repositories from 2.20&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9579">BEAM-9579&lt;/a> Fixed numpy operators in ApproximateQuantiles&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9277">BEAM-9277&lt;/a> Fixed exception when running in IPython notebook&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-1833">BEAM-1833&lt;/a> Restructure Python pipeline construction to better follow the Runner API&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9225">BEAM-9225&lt;/a> Fixed Flink uberjar job termination bug&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9503">BEAM-9503&lt;/a> Fixed SyntaxError in process worker startup&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9322">BEAM-9322&lt;/a> Python SDK ignores manually set PCollection tags&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9445">BEAM-9445&lt;/a> Python SDK pre_optimize=all experiment may cause error&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9725">BEAM-9725&lt;/a> Python SDK performance regression for reshuffle transform&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.20.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath
, Charles Chen, Craig Chambers, Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov, Hai Lu, Heejong Lee
, Ismaël Mejía, J Ross Thomson, Jan Lukavský, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, João Cabrita, Juan Rael, Juta, Kasia Kucharczyk
, Kengo Seki, Kenneth Jung, Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Marek Simunek, Mark Liu, Maximilian Michels, Melissa Pashniak
, Michael Luckey, Michal Walenia, Mike Pedersen, Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Rehman Murad Ali, Reuven Lax, Rob, Robbe Sneyders
, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle, Scott Wegner, Shoaib Zafar, Thomas Weise, Tianyang Hu, Tyler Akidau
, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, XuMingmin, ttanay, tvalentyn, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Apache Beam 2.19.0</title><link>/blog/beam-2.19.0/</link><pubDate>Tue, 04 Feb 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.19.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.19.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2190-2020-02-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.19.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12346582">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Multiple improvements made into Python SDK harness:
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8624">BEAM-8624&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8623">BEAM-8623&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-7949">BEAM-7949&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8935">BEAM-8935&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8816">BEAM-8816&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-1440">BEAM-1440&lt;/a> Create a BigQuery source (that implements iobase.BoundedSource) for Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-2572">BEAM-2572&lt;/a> Implement an S3 filesystem for Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5192">BEAM-5192&lt;/a> Support Elasticsearch 7.x&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8745">BEAM-8745&lt;/a> More fine-grained controls for the size of a BigQuery Load job&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8801">BEAM-8801&lt;/a> PubsubMessageToRow should not check useFlatSchema() in processElement&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8953">BEAM-8953&lt;/a> Extend ParquetIO.Read/ReadFiles.Builder to support Avro GenericData model&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8946">BEAM-8946&lt;/a> Report collection size from MongoDBIOIT&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8978">BEAM-8978&lt;/a> Report saved data size from HadoopFormatIOIT&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6008">BEAM-6008&lt;/a> Improve error reporting in Java/Python PortableRunner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8296">BEAM-8296&lt;/a> Containerize the Spark job server&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8746">BEAM-8746&lt;/a> Allow the local job service to work from inside docker&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8837">BEAM-8837&lt;/a> PCollectionVisualizationTest: possible bug&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8139">BEAM-8139&lt;/a> Execute portable Spark application jar&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9019">BEAM-9019&lt;/a> Improve Spark Encoders (wrappers of beam coders)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9053">BEAM-9053&lt;/a> Improve error message when unable to get the correct filesystem for specified path in Python SDK) Improve error message when unable to get the correct filesystem for specified path in Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9055">BEAM-9055&lt;/a> Unify the config names of Fn Data API across languages&lt;/li>
&lt;/ul>
&lt;h3 id="sql">SQL&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5690">BEAM-5690&lt;/a> Issue with GroupByKey in BeamSql using SparkRunner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8993">BEAM-8993&lt;/a> [SQL] MongoDb should use predicate push-down&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8844">BEAM-8844&lt;/a> [SQL] Create performance tests for BigQueryTable&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9023">BEAM-9023&lt;/a> Upgrade to ZetaSQL 2019.12.1&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8989">BEAM-8989&lt;/a> Backwards incompatible change in ParDo.getSideInputs (caught by failure when running Apache Nemo quickstart)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8402">BEAM-8402&lt;/a> Backwards incompatible change related to how Environments are represented in Python &lt;code>DirectRunner&lt;/code>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9218">BEAM-9218&lt;/a> Template staging broken on Beam 2.18.0&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8696">BEAM-8696&lt;/a> Beam Dependency Update Request: com.google.protobuf:protobuf-java&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8701">BEAM-8701&lt;/a> Beam Dependency Update Request: commons-io:commons-io&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8716">BEAM-8716&lt;/a> Beam Dependency Update Request: org.apache.commons:commons-csv&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8717">BEAM-8717&lt;/a> Beam Dependency Update Request: org.apache.commons:commons-lang3&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8749">BEAM-8749&lt;/a> Beam Dependency Update Request: com.datastax.cassandra:cassandra-driver-mapping&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5546">BEAM-5546&lt;/a> Beam Dependency Update Request: commons-codec:commons-codec&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9123">BEAM-9123&lt;/a> HadoopResourceId returns wrong directory name&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8962">BEAM-8962&lt;/a> FlinkMetricContainer causes churn in the JobManager and lets the web frontend malfunction&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5495">BEAM-5495&lt;/a> PipelineResources algorithm is not working in most environments&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8025">BEAM-8025&lt;/a> Cassandra IO classMethod test is flaky&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8577">BEAM-8577&lt;/a> FileSystems may have not be initialized during ResourceId deserialization&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8582">BEAM-8582&lt;/a> Python SDK emits duplicate records for Default and AfterWatermark triggers&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8943">BEAM-8943&lt;/a> SDK harness servers don&amp;rsquo;t shut down properly when SDK harness environment cleanup fails&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8995">BEAM-8995&lt;/a> apache_beam.io.gcp.bigquery_read_it_test failing on Py3.5 PC with: TypeError: the JSON object must be str, not &amp;lsquo;bytes&amp;rsquo;&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8999">BEAM-8999&lt;/a> PGBKCVOperation does not respect timestamp combiners&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9050">BEAM-9050&lt;/a> Beam pickler doesn&amp;rsquo;t pickle classes that have &lt;strong>module&lt;/strong> set to None.&lt;/li>
&lt;li>&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.19.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath, Charles Chen, Craig Chambers, Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov, Hai Lu, Heejong Lee, Ismaël Mejía, Jan Lukavský, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, João Cabrita, J Ross Thomson, Juan Rael, Juta, Kasia Kucharczyk, Kengo Seki, Kenneth Jung, Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Łukasz Gajowy, Marek Simunek, Mark Liu, Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Pedersen, Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Reuven Lax, Rob, Robbe Sneyders, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle, Scott Wegner, Thomas Weise, Tianyang Hu, ttanay, tvalentyn, Tyler Akidau, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, XuMingmin&lt;/p></description></item><item><title>Blog: Apache Beam 2.18.0</title><link>/blog/beam-2.18.0/</link><pubDate>Thu, 23 Jan 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.18.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.18.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2180-2020-01-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.18.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12346383&amp;amp;projectId=12319527">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8470">BEAM-8470&lt;/a> - Create a new Spark runner based on Spark Structured streaming framework&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7636">BEAM-7636&lt;/a> - Added SqsIO v2 support.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8513">BEAM-8513&lt;/a> - RabbitMqIO: Allow reads from exchange-bound queue without declaring the exchange.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8540">BEAM-8540&lt;/a> - Fix CSVSink example in FileIO docs&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5878">BEAM-5878&lt;/a> - Added support DoFns with Keyword-only arguments in Python 3.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6756">BEAM-6756&lt;/a> - Improved support for lazy iterables in schemas (Java).&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-4776">BEAM-4776&lt;/a> AND &lt;a href="https://issues.apache.org/jira/browse/BEAM-4777">BEAM-4777&lt;/a> - Added metrics supports to portable runners.&lt;/li>
&lt;li>Various improvements to Interactive Beam: &lt;a href="https://issues.apache.org/jira/browse/BEAM-7760">BEAM-7760&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8379">BEAM-8379&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8016">BEAM-8016&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8016">BEAM-8016&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8658">BEAM-8658&lt;/a> - Optionally set artifact staging port in FlinkUberJarJobServer.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8660">BEAM-8660&lt;/a> - Override returned artifact staging endpoint&lt;/li>
&lt;/ul>
&lt;h3 id="sql">SQL&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8343">BEAM-8343&lt;/a> - [SQL] Add means for IO APIs to support predicate and/or project push-down when running SQL pipelines. And &lt;a href="https://issues.apache.org/jira/browse/BEAM-8468">BEAM-8468&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8365">BEAM-8365&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8508">BEAM-8508&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8427">BEAM-8427&lt;/a> - [SQL] Add support for MongoDB source.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8456">BEAM-8456&lt;/a> - Add pipeline option to control truncate of BigQuery data processed by Beam SQL.&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8814">BEAM-8814&lt;/a> - &amp;ndash;no_auth flag changed to boolean type.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8252">BEAM-8252&lt;/a> AND &lt;a href="https://issues.apache.org/jira/browse/BEAM-8254">BEAM-8254&lt;/a> Add worker_region and worker_zone options. Deprecated &amp;ndash;zone flag and &amp;ndash;worker_region experiment argument.&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7078">BEAM-7078&lt;/a> - com.amazonaws:amazon-kinesis-client updated to 1.13.0.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8822">BEAM-8822&lt;/a> - Upgrade Hadoop dependencies to version 2.8.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7917">BEAM-7917&lt;/a> - Python datastore v1new fails on retry.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7981">BEAM-7981&lt;/a> - ParDo function wrapper doesn&amp;rsquo;t support Iterable output types.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8146">BEAM-8146&lt;/a> - SchemaCoder/RowCoder have no equals() function.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8347">BEAM-8347&lt;/a> - UnboundedRabbitMqReader can fail to advance watermark if no new data comes in.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8352">BEAM-8352&lt;/a> - Reading records in background may lead to OOM errors&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8480">BEAM-8480&lt;/a> - Explicitly set restriction coder for bounded reader wrapper SDF.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8515">BEAM-8515&lt;/a> - Ensure that ValueProvider types have equals/hashCode implemented for comparison reasons.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8579">BEAM-8579&lt;/a> - Strip UTF-8 BOM bytes (if present) in TextSource.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8657">BEAM-8657&lt;/a> - Not doing Combiner lifting for data-driven triggers.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8663">BEAM-8663&lt;/a> - BundleBasedRunner Stacked Bundles don&amp;rsquo;t respect PaneInfo.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8667">BEAM-8667&lt;/a> - Data channel should to avoid unlimited buffering in Python SDK.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8802">BEAM-8802&lt;/a> - Timestamp combiner not respected across bundles in streaming mode.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8803">BEAM-8803&lt;/a> - Default behaviour for Python BQ Streaming inserts sink should be to retry always.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8825">BEAM-8825&lt;/a> - OOM when writing large numbers of &amp;lsquo;narrow&amp;rsquo; rows.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8835">BEAM-8835&lt;/a> - Artifact retrieval fails with FlinkUberJarJobServer&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8836">BEAM-8836&lt;/a> - ExternalTransform is not providing a unique name&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8884">BEAM-8884&lt;/a> - Python MongoDBIO TypeError when splitting.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9041">BEAM-9041&lt;/a> - SchemaCoder equals should not rely on from/toRowFunction equality.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9042">BEAM-9042&lt;/a> - AvroUtils.schemaCoder(schema) produces a not serializable SchemaCoder.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9065">BEAM-9065&lt;/a> - Spark runner accumulates metrics (incorrectly) between runs.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6303">BEAM-6303&lt;/a> - Add .parquet extension to files in ParquetIO.&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8882">BEAM-8882&lt;/a> - Python: &lt;code>beam.Create&lt;/code> no longer preserves order unless &lt;code>reshuffle=False&lt;/code> is passed in as an argument.&lt;/p>
&lt;p>You may encounter this issue when using DirectRunner.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9065">BEAM-9065&lt;/a> - Spark runner accumulates metrics (incorrectly) between runs&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9123">BEAM-9123&lt;/a> - HadoopResourceId returns wrong directory name&lt;/p>
&lt;/li>
&lt;li>
&lt;p>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.18.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9144">BEAM-9144&lt;/a> - If you are using Avro 1.9.x with Beam you should not upgrade to this version. There is an issue with timestamp conversions. A fix will be available in the next release.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.18.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Aizhamal Nurmamat kyzy, Alan Myrvold, Alexey Romanenko, Alex Van Boxel, Andre Araujo, Andrew Crites, Andrew Pilloud, Aryan Naraghi, Boyuan Zhang, Brian Hulette, bumblebee-coming, Cerny Ondrej, Chad Dombrova, Chamikara Jayalath, Changming Ma, Chun Yang, cmachgodaddy, Colm O hEigeartaigh, Craig Chambers, Daniel Oliveira, Daniel Robert, David Cavazos, David Moravek, David Song, dependabot[bot], Derek, Dmytro Sadovnychyi, Elliotte Rusty Harold, Etienne Chauchot, Hai Lu, Henry Suryawirawan, Ismaël Mejía, Jack Whelpton, Jan Lukavský, Jean-Baptiste Onofré, Jeff Klukas, Jincheng Sun, Jing, Jing Chen, Joe Tsai, Jonathan Alvarez-Gutierrez, Kamil Wasilewski, KangZhiDong, Kasia Kucharczyk, Kenneth Knowles, kirillkozlov, Kirill Kozlov, Kyle Weaver, liumomo315, lostluck, Łukasz Gajowy, Luke Cwik, Mark Liu, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Niel Markwick, Ning Kang, nlofeudo, pabloem, Pablo Estrada, Pankaj Gudlani, Piotr Szczepanik, Primevenn, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, RusOr10n, Ryan Skraba, Saikat Maitra, sambvfx, Sam Rohde, Samuel Husso, Stefano, Steve Koonce, Steve Niemitz, sunjincheng121, Thomas Weise, Tianyang Hu, Tim Robertson, Tomo Suzuki, tvalentyn, Udi Meiri, Valentyn Tymofieiev, Viola Lyu, Wenjia Liu, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, ziel, 康智冬&lt;/p></description></item><item><title>Blog: Apache Beam 2.17.0</title><link>/blog/beam-2.17.0/</link><pubDate>Mon, 06 Jan 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.17.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.17.0 release of Beam. This release includes both improvements and new functionality.
Users of the MongoDbIO connector are encouraged to upgrade to this release to address a &lt;a href="/security/CVE-2020-1929/">security vulnerability&lt;/a>.&lt;/p>
&lt;p>See the &lt;a href="/get-started/downloads/#2170-2020-01-06">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.17.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12345970&amp;amp;projectId=12319527">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7962">BEAM-7962&lt;/a> - Drop support for Flink 1.5 and 1.6&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7635">BEAM-7635&lt;/a> - Migrate SnsIO to AWS SDK for Java 2&lt;/li>
&lt;li>Improved usability for portable Flink Runner
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8183">BEAM-8183&lt;/a> - Optionally bundle multiple pipelines into a single Flink jar.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8372">BEAM-8372&lt;/a> - Allow submission of Flink UberJar directly to flink cluster.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8471">BEAM-8471&lt;/a> - Flink native job submission for portable pipelines.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8312">BEAM-8312&lt;/a> - Flink portable pipeline jars do not need to stage artifacts remotely.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7730">BEAM-7730&lt;/a> - Add Flink 1.9 build target and Make FlinkRunner compatible with Flink 1.9.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7990">BEAM-7990&lt;/a> - Add ability to read parquet files into PCollection of pyarrow.Table.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8355">BEAM-8355&lt;/a> - Make BooleanCoder a standard coder.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8394">BEAM-8394&lt;/a> - Add withDataSourceConfiguration() method in JdbcIO.ReadRows class.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5428">BEAM-5428&lt;/a> - Implement cross-bundle state caching.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5967">BEAM-5967&lt;/a> - Add handling of DynamicMessage in ProtoCoder.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7473">BEAM-7473&lt;/a> - Update RestrictionTracker within Python to not be required to be thread safe.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7920">BEAM-7920&lt;/a> - Added AvroTableProvider to Beam SQL.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8098">BEAM-8098&lt;/a> - Improve documentation on BigQueryIO.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8100">BEAM-8100&lt;/a> - Add exception handling to Json transforms in Java SDK.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8306">BEAM-8306&lt;/a> - Improve estimation of data byte size reading from source in ElasticsearchIO.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8351">BEAM-8351&lt;/a> - Support passing in arbitrary KV pairs to sdk worker via external environment config.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8396">BEAM-8396&lt;/a> - Default to LOOPBACK mode for local flink (spark, &amp;hellip;) runner.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8410">BEAM-8410&lt;/a> - JdbcIO should support setConnectionInitSqls in its DataSource.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8609">BEAM-8609&lt;/a> - Add HllCount to Java transform catalog.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8861">BEAM-8861&lt;/a> - Disallow self-signed certificates by default in ElasticsearchIO.&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8285">BEAM-8285&lt;/a> - Upgrade ZetaSQL to 2019.09.1.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8392">BEAM-8392&lt;/a> - Upgrade pyarrow version bounds: 0.15.1&amp;lt;= to &amp;lt;0.16.0.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5895">BEAM-5895&lt;/a> - Upgrade com.rabbitmq:amqp-client to 5.7.3.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6896">BEAM-6896&lt;/a> - Upgrade PyYAML version bounds: 3.12&amp;lt;= to &amp;lt;6.0.0.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>[BEAM-8819] - AvroCoder for SpecificRecords is not serialized correctly since 2.13.0&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8989">BEAM-8989&lt;/a> Apache Nemo
runner broken due to backwards incompatible change since 2.16.0.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.17.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alexey Romanenko, Andre-Philippe Paquet, Andrew
Pilloud, angulartist, Ankit Jhalaria, Ankur Goenka, Anton Kedin, Aryan Naraghi,
Aurélien Geron, B M VISHWAS, Bartok Jozsef, Boyuan Zhang, Brian Hulette, Cerny
Ondrej, Chad Dombrova, Chamikara Jayalath, ChethanU, cmach, Colm O hEigeartaigh,
Cyrus Maden, Daniel Oliveira, Daniel Robert, Dante, David Cavazos, David
Moravek, David Yan, Enrico Canzonieri, Etienne Chauchot, gxercavins, Hai Lu,
Hannah Jiang, Ian Lance Taylor, Ismaël Mejía, Israel Herraiz, James Wen, Jan
Lukavský, Jean-Baptiste Onofré, Jeff Klukas, jesusrv1103, Jofre, Kai Jiang,
Kamil Wasilewski, Kasia Kucharczyk, Kenneth Knowles, Kirill Kozlov,
kirillkozlov, Kohki YAMAGIWA, Kyle Weaver, Leonardo Alves Miguel, lloigor,
lostluck, Luis Enrique Ortíz Ramirez, Luke Cwik, Mark Liu, Maximilian Michels,
Michal Walenia, Mikhail Gryzykhin, mrociorg, Nicolas Delsaux, Ning Kang, NING
KANG, Pablo Estrada, pabloem, Piotr Szczepanik, rahul8383, Rakesh Kumar, Renat
Nasyrov, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruslan Altynnikov,
Ryan Skraba, Salman Raza, Saul Chavez, Sebastian Jambor, sunjincheng121, Tatu
Saloranta, tchiarato, Thomas Weise, Tomo Suzuki, Tudor Marian, tvalentyn, Udi
Meiri, Valentyn Tymofieiev, Viola Lyu, Vishwas, Yichi Zhang, Yifan Zou, Yueyang
Qiu, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Apache Beam 2.16.0</title><link>/blog/beam-2.16.0/</link><pubDate>Mon, 07 Oct 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.16.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.16.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2160-2019-10-07">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.16.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345494">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Customizable Docker container images released and supported by Beam portable runners on Python 2.7, 3.5, 3.6, 3.7. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7907">BEAM-7907&lt;/a>)&lt;/li>
&lt;li>Integration improvements for Python Streaming on Dataflow including service features like autoscaling, drain, update, streaming engine and counter updates.&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>A new count distinct transform based on BigQuery compatible HyperLogLog++ implementation. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7013">BEAM-7013&lt;/a>)&lt;/li>
&lt;li>Element counters in the Web UI graph representations for transforms for Python streaming jobs in Google Cloud Dataflow. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7045">BEAM-7045&lt;/a>)&lt;/li>
&lt;li>Add SetState in Python sdk. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7741">BEAM-7741&lt;/a>)&lt;/li>
&lt;li>Add hot key detection to Dataflow Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7820">BEAM-7820&lt;/a>)&lt;/li>
&lt;li>Add ability to get the list of submitted jobs from gRPC JobService. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7927">BEAM-7927&lt;/a>)&lt;/li>
&lt;li>Portable Flink pipelines can now be bundled into executable jars. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7966">BEAM-7966&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7967">BEAM-7967&lt;/a>)&lt;/li>
&lt;li>SQL join selection should be done in planner, not in expansion to PTransform. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6114">BEAM-6114&lt;/a>)&lt;/li>
&lt;li>A Python Sink for BigQuery with File Loads in Streaming. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6611">BEAM-6611&lt;/a>)&lt;/li>
&lt;li>Python BigQuery sink should be able to handle 15TB load job quota. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7588">BEAM-7588&lt;/a>)&lt;/li>
&lt;li>Spark portable runner: reuse SDK harness. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7600">BEAM-7600&lt;/a>)&lt;/li>
&lt;li>BigQuery File Loads to work well with load job size limits. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7742">BEAM-7742&lt;/a>)&lt;/li>
&lt;li>External environment with containerized worker pool. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7980">BEAM-7980&lt;/a>)&lt;/li>
&lt;li>Use OffsetRange as restriction for OffsetRestrictionTracker. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8014">BEAM-8014&lt;/a>)&lt;/li>
&lt;li>Get logs for SDK worker Docker containers. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8015">BEAM-8015&lt;/a>)&lt;/li>
&lt;li>PCollection boundedness is tracked and propagated in python sdk. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8088">BEAM-8088&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>Upgrade &amp;ldquo;com.amazonaws:amazon-kinesis-producer&amp;rdquo; to version 0.13.1. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7894">BEAM-7894&lt;/a>)&lt;/li>
&lt;li>Upgrade to joda time 2.10.3 to get updated TZDB. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8161">BEAM-8161&lt;/a>)&lt;/li>
&lt;li>Upgrade Jackson to version 2.9.10. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8299">BEAM-8299&lt;/a>)&lt;/li>
&lt;li>Upgrade grpcio minimum required version to 1.12.1. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7986">BEAM-7986&lt;/a>)&lt;/li>
&lt;li>Upgrade funcsigs minimum required version to 1.0.2 in Python2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7060">BEAM-7060&lt;/a>)&lt;/li>
&lt;li>Upgrade google-cloud-pubsub maximum required version to 1.0.0. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5539">BEAM-5539&lt;/a>)&lt;/li>
&lt;li>Upgrade google-cloud-bigtable maximum required version to 1.0.0. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5539">BEAM-5539&lt;/a>)&lt;/li>
&lt;li>Upgrade dill version to 0.3.0. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8324">BEAM-8324&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Given that Python 2 will reach EOL on Jan 1 2020, Python 2 users of Beam will now receive a warning that new releases of Apache Beam will soon support Python 3 only.&lt;/li>
&lt;li>Filesystems not properly registered using FileIO.write in FlinkRunner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8303">BEAM-8303&lt;/a>)&lt;/li>
&lt;li>Performance regression in Java DirectRunner in streaming mode. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8363">BEAM-8363&lt;/a>)&lt;/li>
&lt;li>Can&amp;rsquo;t install the Python SDK on macOS 10.15. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8368">BEAM-8368&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.16.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Van Boxel, Alexey Romanenko, Alexey Strokach, Alireza Samadian,
Andre-Philippe Paquet, Andrew Pilloud, Ankur Goenka, Anton Kedin, Aryan Naraghi,
B M VISHWAS, Bartok Jozsef, Bill Neubauer, Boyuan Zhang, Brian Hulette, Bruno Volpato,
Chad Dombrova, Chamikara Jayalath, Charith Ellawala, Charles Chen, Claire McGinty,
Cyrus Maden, Daniel Oliveira, Dante, David Cavazos, David Moravek, David Yan,
Dominic Mitchell, Elias Djurfeldt, Enrico Canzonieri, Etienne Chauchot, Gleb Kanterov,
Hai Lu, Hannah Jiang, Heejong Lee, Ian Lance Taylor, Ismaël Mejía, Jack Whelpton,
James Wen, Jan Lukavský, Jean-Baptiste Onofré, Jofre, Kai Jiang, Kamil Wasilewski,
Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, Kirill Kozlov, Kohki YAMAGIWA,
Kyle Weaver, Kyle Winkelman, Ludovic Post, Luis Enrique Ortíz Ramirez, Luke Cwik,
Mark Liu, Maximilian Michels, Michal Walenia, Mike Kaplinskiy, Mikhail Gryzykhin,
NING KANG, Oliver Henlich, Pablo Estrada, Rakesh Kumar, Renat Nasyrov, Reuven Lax,
Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Skraba, Sahith Nallapareddy,
Salman Raza, Sam Rohde, Saul Chavez, Shoaib, Shoaib Zafar, Slava Chernyak, Tanay Tummalapalli,
Thinh Ha, Thomas Weise, Tianzi Cai, Tim van der Lippe, Tomer Zeltzer, Tudor Marian,
Udi Meiri, Valentyn Tymofieiev, Yichi Zhang, Yifan Zou, Yueyang Qiu, gxercavins,
jesusrv1103, lostluck, matt-darwin, mrociorg, ostrokach, parahul, rahul8383, rosetn,
sunjincheng121, the1plummie, ttanay, tvalentyn, venn001, yoshiki.obata, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Google Summer of Code '19</title><link>/blog/gsoc-19/</link><pubDate>Wed, 04 Sep 2019 00:00:01 -0800</pubDate><guid>/blog/gsoc-19/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Google Summer of Code was an amazing learning experience for me.
I contributed to open source, learned about Apache Beam’s internals and worked with the best engineers in the world.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Two of my friends had participated in GSoC in 2018. I was intrigued by their experience.
The idea of working on open-source software that could potentially be used by developers across the world, while being mentored by the best people in a field was exciting!
So, I decided to give Google Summer of Code a shot this year.&lt;/p>
&lt;h2 id="what-is-google-summer-of-code">What is Google Summer of Code?&lt;/h2>
&lt;p>&lt;a href="https://summerofcode.withgoogle.com/">Google Summer of Code&lt;/a> is a global program hosted by Google focused on introducing students to open source software development.
Students work on a 3 month programming project with an open source organization during their break from university.&lt;/p>
&lt;h2 id="why-apache-beam">Why Apache Beam?&lt;/h2>
&lt;p>While interning at &lt;a href="https://atlan.com/">Atlan&lt;/a>, I discovered the field of Data Engineering. I found the challenges and the discussions of the engineers there interesting. While researching for my internship project, I came across the Streaming Systems book. It introduced me to the unified model of Apache Beam for Batch and Streaming Systems, which I was fascinated by.
I wanted to explore Data Engineering, so for GSoC, I wanted to work on a project in that field. Towards the end of my internship, I started contributing to Apache Airflow(very cool project) and Apache Beam, hoping one of them would participate in GSoC. I got lucky!&lt;/p>
&lt;p>&lt;a href="https://youtu.be/U2eWLb-LD44">Also, Spotify’s Discover Weekly uses Apache Beam!&lt;/a>&lt;/p>
&lt;h2 id="preparation">Preparation&lt;/h2>
&lt;p>I had already read the &lt;a href="http://streamingsystems.net/">Streaming Systems book&lt;/a>. So, I had an idea of the concepts that Beam is built on, but had never actually used Beam.
Before actually submitting a proposal, I went through a bunch of resources to make sure I had a concrete understanding of Beam.
I read the &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Streaming 101&lt;/a> and &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102">Streaming 102&lt;/a> blogs by Tyler Akidau. They are the perfect introduction to Beam’s unified model for Batch and Streaming.
In addition, I watched all Beam talks on YouTube. You can find them on the &lt;a href="https://beam.apache.org/documentation/resources/videos-and-podcasts/">Beam Website&lt;/a>.
Beam has really good documentation. The &lt;a href="https://beam.apache.org/documentation/programming-guide/">Programming Guide&lt;/a> lays out all of Beam’s concepts really well. &lt;a href="https://beam.apache.org/documentation/runtime/model">Beam’s execution model&lt;/a> is also documented well and is a must-read to understand how Beam processes data.
&lt;a href="https://www.waitingforcode.com/apache-beam">waitingforcode.com&lt;/a> also has good blog posts about Beam concepts.
To get a better sense of the Beam codebase, I played around with it and worked on some PRs to understand Beam better and got familiar with the test suite and workflows.&lt;/p>
&lt;h2 id="gsoc-journey">GSoC Journey&lt;/h2>
&lt;p>GSoC has 2 phases. The first is the Community Bonding period in which students get familiar with the project and the community. The other is the actual Coding Period in which students work on their projects. Since the Coding Period has three evaluations spaced out by a month, I divided my project into three parts focusing on the implementation, tests, and documentation or improvements.&lt;/p>
&lt;h3 id="project">Project&lt;/h3>
&lt;p>My project(&lt;a href="https://issues.apache.org/jira/browse/BEAM-6611">BEAM-6611&lt;/a>) added support for File Loads method of inserting data into BigQuery for streaming pipelines. It builds on PR - &lt;a href="https://github.com/apache/beam/pull/7655">#7655&lt;/a> for &lt;a href="https://issues.apache.org/jira/browse/BEAM-6553">BEAM-6553&lt;/a> that added support in the Python SDK for writing to BigQuery using File Loads method for Batch pipelines. Streaming pipelines with non-default Windowing, Triggering and Accumulation mode can write data to BigQuery using file loads method. In case of failure, the pipeline will fail atomically. This means that each record will be loaded into BigQuery at-most-once.
You can find my proposal &lt;a href="https://docs.google.com/document/d/15Peyd3Z_wu5rvGWw8lMLpZuTyyreM_JOAEFFWvF97YY/edit?usp=sharing">here&lt;/a>.&lt;/p>
&lt;h3 id="community-bonding">Community Bonding&lt;/h3>
&lt;p>When GSoC started, my semester end exams had not yet finished. As a result, I couldn’t get much done. I worked on three PTransforms for the Python SDK - Latest, WithKeys and Reify.&lt;/p>
&lt;h3 id="coding-period-i">Coding Period I&lt;/h3>
&lt;p>In this period, I wrote some Integration Tests for the BigQuery sink using Streaming Inserts in streaming mode. I worked on a failing integration test for my project. I also finished the implementation of my project. But, one PostCommit test didn’t pass. I realized that the matcher for the Integration Test that queried BigQuery for the results was intended to be used in Batch mode. So, I wrote a version of the matcher to work in streaming mode.&lt;/p>
&lt;h3 id="coding-period-ii">Coding Period II&lt;/h3>
&lt;p>Even after I had added the matcher for streaming mode, the PostComit tests did not pass. A test was being run even though it was not specified. I isolated the failure to a &lt;a href="https://nose.readthedocs.io/en/latest/doc_tests/test_multiprocess/multiprocess.html#other-differences-in-test-running">limitation&lt;/a> of the multiprocess plugin for &lt;a href="https://nose.readthedocs.io/en/latest/">nose(a Python test framework)&lt;/a> due to which it found more tests than had been specified. It took me a while to figure this out. In this period, changes for my project got merged.
I also worked on small issues related to testing.&lt;/p>
&lt;p>This period was marked by a few exciting events:&lt;/p>
&lt;ul>
&lt;li>Ending up in the top #100 contributors to apache/beam.&lt;/li>
&lt;li>My first ever PR Review on an open source project.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pbs.twimg.com/media/D_XNSC-UIAUmswG?format=png&amp;name=small" alt="Weird flex but ok" />&lt;/p>
&lt;h3 id="coding-period-iii">Coding Period III&lt;/h3>
&lt;p>This was the final coding period before the program ended. Since my project was merged earlier than expected, my mentor suggested another issue(&lt;a href="https://issues.apache.org/jira/browse/BEAM-7742">BEAM-7742&lt;/a>) in the same area - BigQueryIO, that I found interesting. So, I worked on partitioning written files in BigQuery to ensure that all load jobs triggered adhere to the load job size limitations specified for BigQuery.
While working on my project, I was using a pipeline that uses PubSub as a source and BigQuery as a sink to validate my changes. My mentor suggested we add them to the Beam test suite as it would be the ultimate test for BigQueryIO. I also worked on adding this test to Beam.&lt;/p>
&lt;p>You can find the list of PRs I worked on &lt;a href="https://github.com/apache/beam/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Apr+author%3Attanay">here&lt;/a>.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>GSoC has been a lesson in discipline and goal-setting for me. Deciding what I wanted to work on and how much I wanted to get done each week was an important lesson.
I had never worked remotely, so this was a new experience. Although I struggled with it initially, I appreciate the flexibility that it comes with.
I also had a lot of fun learning about Apache Beam’s internals, and other tools in the same ecosystem.
This was also the first time I had written code with a test-first approach.&lt;/p>
&lt;p>I thank my mentor - Pablo Estrada, Apache Beam, The Apache Software Foundation and Google Summer of Code for this opportunity. I am also grateful to my mentor for helping me with everything I needed and more, and the Apache Beam community for being supportive and encouraging.&lt;/p>
&lt;p>With the right effort, perseverance, conviction, and a plan, anything is possible. Anything.&lt;/p></description></item><item><title>Blog: Apache Beam 2.15.0</title><link>/blog/beam-2.15.0/</link><pubDate>Thu, 22 Aug 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.15.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.15.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2150-2019-08-22">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.15.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345489">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Vendored Guava was upgraded to version 26.0.&lt;/li>
&lt;li>Support multi-process execution on the FnApiRunner for Python. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3645">BEAM-3645&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Add AvroIO.sink for IndexedRecord (FileIO compatible). (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6480">BEAM-6480&lt;/a>)&lt;/li>
&lt;li>Add support for writing to BigQuery clustered tables. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5191">BEAM-5191&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Support ParquetTable in SQL. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7728">BEAM-7728&lt;/a>)&lt;/li>
&lt;li>Add hot key detection to Dataflow Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7820">BEAM-7820&lt;/a>)&lt;/li>
&lt;li>Support schemas in the JDBC sink. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6675">BEAM-6675&lt;/a>)&lt;/li>
&lt;li>Report GCS throttling time to Dataflow autoscaler for better autoscaling. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7667">BEAM-7667&lt;/a>)&lt;/li>
&lt;li>Support transform_name_mapping option in Python SDK for &lt;code>--update&lt;/code> use. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7761">BEAM-7761&lt;/a>)&lt;/li>
&lt;li>Dependency: Upgrade Jackson databind to version 2.9.9.3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7880">BEAM-7880&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7616">BEAM-7616&lt;/a> urlopen calls may get stuck. (Regression from 2.14.0)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8111">BEAM-8111&lt;/a> SchemaCoder fails on Dataflow, preventing the use of SqlTransform and schema-aware transforms. (Regression from 2.14.0)&lt;/li>
&lt;li>(&lt;a href="https://issues.apache.org/jira/browse/BEAM-8368">BEAM-8368&lt;/a>) Can&amp;rsquo;t install the Python SDK on macOS 10.15.&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;code>--region&lt;/code> flag will be a required flag in the future for Dataflow. A warning is added to warn for this future change. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7833">BEAM-7833&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.15.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alexey Romanenko, Alex Goos, Alireza Samadian, Andrew Pilloud, Ankur Goenka,
Anton Kedin, Aryan Naraghi, Bartok Jozsef, bmv126, B M VISHWAS, Boyuan Zhang,
Brian Hulette, brucearctor, Cade Markegard, Cam Mach, Chad Dombrova,
Chaim Turkel, Chamikara Jayalath, Charith Ellawala, Claire McGinty, Craig Chambers,
Daniel Oliveira, David Cavazos, David Moravek, Dominic Mitchell, Dustin Rhodes,
Etienne Chauchot, Filipe Regadas, Gleb Kanterov, Gunnar Schulze, Hannah Jiang,
Heejong Lee, Henry Suryawirawan, Ismaël Mejía, Ivo Galic, Jan Lukavský,
Jawad, Juta, Juta Staes, Kai Jiang, Kamil Wasilewski, Kasia Kucharczyk,
Kenneth Jung, Kenneth Knowles, Kyle Weaver, Lily Li, Logan HAUSPIE, lostluck,
Łukasz Gajowy, Luke Cwik, Mark Liu, Matt Helm, Maximilian Michels,
Michael Luckey, Mikhail Gryzykhin, Neville Li, Nicholas Rucci, pabloem,
Pablo Estrada, Paul King, Paul Suganthan, Raheel Khan, Rakesh Kumar,
Reza Rokni, Robert Bradshaw, Robert Burke, rosetn, Rui Wang, Ryan Skraba, RyanSkraba,
Sahith Nallapareddy, Sam Rohde, Sam Whittle, Steve Niemitz, Tanay Tummalapalli, Thomas Weise,
Tianyang Hu, ttanay, tvalentyn, Udi Meiri, Valentyn Tymofieiev, Wout Scheepers,
yanzhi, Yekut, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, Yunqing Zhou&lt;/p></description></item><item><title>Blog: Apache Beam 2.14.0</title><link>/blog/beam-2.14.0/</link><pubDate>Wed, 31 Jul 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.14.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.14.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2140-2019-08-01">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.14.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345431">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python 3 support is extended to Python 3.6 and 3.7; in addition to various other Python 3 &lt;a href="https://issues.apache.org/jira/browse/BEAM-1251?focusedCommentId=16890504&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16890504">improvements&lt;/a>.&lt;/li>
&lt;li>Spark portable runner (batch) now &lt;a href="https://lists.apache.org/thread.html/c43678fc24c9a1dc9f48c51c51950aedcb9bc0fd3b633df16c3d595a@%3Cuser.beam.apache.org%3E">available&lt;/a> for Java, Python, Go.&lt;/li>
&lt;li>Added new runner: Hazelcast Jet Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7305">BEAM-7305&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Schema support added to BigQuery reads. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6673">BEAM-6673&lt;/a>)&lt;/li>
&lt;li>Schema support added to JDBC source. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6674">BEAM-6674&lt;/a>)&lt;/li>
&lt;li>BigQuery support for &lt;code>bytes&lt;/code> is fixed. (Python 3) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6769">BEAM-6769&lt;/a>)&lt;/li>
&lt;li>Added DynamoDB IO. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7043">BEAM-7043&lt;/a>)&lt;/li>
&lt;li>Added support unbounded reads with HCatalogIO (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7450">BEAM-7450&lt;/a>)&lt;/li>
&lt;li>Added BoundedSource wrapper for SDF. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7443">BEAM-7443&lt;/a>)&lt;/li>
&lt;li>Added support for INCRBY/DECRBY operations in RedisIO. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7286">BEAM-7286&lt;/a>)&lt;/li>
&lt;li>Added Support for ValueProvider defined GCS Location for WriteToBigQuery with File Loads. (Java) ((&lt;a href="https://issues.apache.org/jira/browse/BEAM-7603">BEAM-7603&lt;/a>))&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Python SDK add support for DoFn &lt;code>setup&lt;/code> and &lt;code>teardown&lt;/code> methods. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-562">BEAM-562&lt;/a>)&lt;/li>
&lt;li>Python SDK adds new transforms: &lt;a href="https://issues.apache.org/jira/browse/BEAM-6693">ApproximateUnique&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-6695">Latest&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7019">Reify&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7021">ToString&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7023">WithKeys&lt;/a>.&lt;/li>
&lt;li>Added hook for user-defined JVM initialization in workers. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6872">BEAM-6872&lt;/a>)&lt;/li>
&lt;li>Added support for SQL Row Estimation for BigQueryTable. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7513">BEAM-7513&lt;/a>)&lt;/li>
&lt;li>Auto sharding of streaming sinks in FlinkRunner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5865">BEAM-5865&lt;/a>)&lt;/li>
&lt;li>Removed the Hadoop dependency from the external sorter. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7268">BEAM-7268&lt;/a>)&lt;/li>
&lt;li>Added option to expire portable SDK worker environments. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7348">BEAM-7348&lt;/a>)&lt;/li>
&lt;li>Beam does not relocate Guava anymore and depends only on its own vendored version of Guava. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6620">BEAM-6620&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Deprecated set/getClientConfiguration in Jdbc IO. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7263">BEAM-7263&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Fixed reading of concatenated compressed files. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6952">BEAM-6952&lt;/a>)&lt;/li>
&lt;li>Fixed re-scaling issues on Flink &amp;gt;= 1.6 versions. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7144">BEAM-7144&lt;/a>)&lt;/li>
&lt;li>Fixed SQL EXCEPT DISTINCT behavior. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7194">BEAM-7194&lt;/a>)&lt;/li>
&lt;li>Fixed OOM issues with bounded Reads for Flink Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7442">BEAM-7442&lt;/a>)&lt;/li>
&lt;li>Fixed HdfsFileSystem to correctly match directories. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7561">BEAM-7561&lt;/a>)&lt;/li>
&lt;li>Upgraded Spark runner to use spark version 2.4.3. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7265">BEAM-7265&lt;/a>)&lt;/li>
&lt;li>Upgraded Jackson to version 2.9.9. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7465">BEAM-7465&lt;/a>)&lt;/li>
&lt;li>Various other bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Do &lt;strong>NOT&lt;/strong> use Python MongoDB source in this release. Python MongoDB source &lt;a href="https://issues.apache.org/jira/browse/BEAM-5148">added&lt;/a> in this release has a known issue that can result in data loss. See (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7866">BEAM-7866&lt;/a>) for details.&lt;/li>
&lt;li>Can&amp;rsquo;t install the Python SDK on macOS 10.15. See (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8368">BEAM-8368&lt;/a>) for details.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.14.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Aizhamal Nurmamat kyzy, Ajo Thomas, Alex Amato, Alexey Romanenko,
Alexey Strokach, Alex Van Boxel, Alireza Samadian, Andrew Pilloud,
Ankit Jhalaria, Ankur Goenka, Anton Kedin, Aryan Naraghi, Bartok Jozsef,
Bora Kaplan, Boyuan Zhang, Brian Hulette, Cam Mach, Chamikara Jayalath,
Charith Ellawala, Charles Chen, Colm O hEigeartaigh, Cyrus Maden,
Daniel Mills, Daniel Oliveira, David Cavazos, David Moravek, David Yan,
Daniel Lescohier, Elwin Arens, Etienne Chauchot, Fábio Franco Uechi,
Finch Keung, Frederik Bode, Gregory Kovelman, Graham Polley, Hai Lu, Hannah Jiang,
Harshit Dwivedi, Harsh Vardhan, Heejong Lee, Henry Suryawirawan,
Ismaël Mejía, Jan Lukavský, Jean-Baptiste Onofré, Jozef Vilcek, Juta, Kai Jiang,
Kamil Wu, Kasia Kucharczyk, Kenneth Knowles, Kyle Weaver, Lara Schmidt,
Łukasz Gajowy, Luke Cwik, Manu Zhang, Mark Liu, Matthias Baetens,
Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia,
Mikhail Gryzykhin, Ming Liang, Neville Li, Pablo Estrada, Paul Suganthan,
Peter Backx, Rakesh Kumar, Rasmi Elasmar, Reuven Lax, Reza Rokni, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Rose Nguyen, Rui Wang, Ruoyun Huang,
Shoaib Zafar, Slava Chernyak, Steve Niemitz, Tanay Tummalapalli, Thomas Weise,
Tim Robertson, Tim van der Lippe, Udi Meiri, Valentyn Tymofieiev, Varun Dhussa,
Viktor Gerdin, Yichi Zhang, Yifan Mai, Yifan Zou, Yueyang Qiu.&lt;/p></description></item><item><title>Blog: Looping timers in Apache Beam</title><link>/blog/looping-timers/</link><pubDate>Tue, 11 Jun 2019 00:00:01 -0800</pubDate><guid>/blog/looping-timers/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam’s primitives let you build expressive data pipelines, suitable for a
variety of use cases. One specific use case is the analysis of time series data
in which continuous sequences across window boundaries are important. A few fun
challenges arise as you tackle this type of data and in this blog we will
explore one of those in more detail and make use of the Timer API
(&lt;a href="/blog/2017/08/28/timely-processing.html">blog post&lt;/a>)
using the &amp;ldquo;looping timer&amp;rdquo; pattern.&lt;/p>
&lt;p>With Beam in streaming mode, you can take streams of data and build analytical
transforms to produce results on the data. But for time series data, the absence
of data is useful information. So how can we produce results in the absence of
data?&lt;/p>
&lt;p>Let&amp;rsquo;s use a more concrete example to illustrate the requirement. Imagine you
have a simple pipeline that sums the number of events coming from an IoT device
every minute. We would like to produce the value 0 when no data has been seen
within a specific time interval. So why can this get tricky? Well it is easy to
build a simple pipeline that counts events as they arrive, but when there is no
event, there is nothing to count!&lt;/p>
&lt;p>Let&amp;rsquo;s build a simple pipeline to work with:&lt;/p>
&lt;pre>&lt;code> // We will start our timer at 1 sec from the fixed upper boundary of our
// minute window
Instant now = Instant.parse(&amp;quot;2000-01-01T00:00:59Z&amp;quot;);
// ----- Create some dummy data
// Create 3 elements, incrementing by 1 minute and leaving a time gap between
// element 2 and element 3
TimestampedValue&amp;lt;KV&amp;lt;String, Integer&amp;gt;&amp;gt; time_1 =
TimestampedValue.of(KV.of(&amp;quot;Key_A&amp;quot;, 1), now);
TimestampedValue&amp;lt;KV&amp;lt;String, Integer&amp;gt;&amp;gt; time_2 =
TimestampedValue.of(KV.of(&amp;quot;Key_A&amp;quot;, 2),
now.plus(Duration.standardMinutes(1)));
// No Value for start time + 2 mins
TimestampedValue&amp;lt;KV&amp;lt;String, Integer&amp;gt;&amp;gt; time_3 =
TimestampedValue.of(KV.of(&amp;quot;Key_A&amp;quot;, 3),
now.plus(Duration.standardMinutes(3)));
// Create pipeline
PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation()
.as(PipelineOptions.class);
Pipeline p = Pipeline.create(options);
// Apply a fixed window of duration 1 min and Sum the results
p.apply(Create.timestamped(time_1, time_2, time_3))
.apply(
Window.&amp;lt;KV&amp;lt;String,Integer&amp;gt;&amp;gt;into(
FixedWindows.&amp;lt;Integer&amp;gt;of(Duration.standardMinutes(1))))
.apply(Sum.integersPerKey())
.apply(ParDo.of(new DoFn&amp;lt;KV&amp;lt;String, Integer&amp;gt;, KV&amp;lt;String, Integer&amp;gt;&amp;gt;() {
@ProcessElement public void process(ProcessContext c) {
LOG.info(&amp;quot;Value is {} timestamp is {}&amp;quot;, c.element(), c.timestamp());
}
}));
p.run();
&lt;/code>&lt;/pre>&lt;p>Running that pipeline will result in the following output:&lt;/p>
&lt;pre>&lt;code>INFO LoopingTimer - Value is KV{Key_A, 1} timestamp is 2000-01-01T00:00:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 3} timestamp is 2000-01-01T00:03:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 2} timestamp is 2000-01-01T00:01:59.999Z
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>Note: The lack of order in the output should be expected, however the
key-window tuple is correctly computed.&lt;/p>
&lt;/blockquote>
&lt;p>As expected, we see output in each of the interval windows which had a data
point with a timestamp between the minimum and maximum value of the window.
There was a data point at timestamps 00:00:59, 00:01:59 and 00:03:59, which
fell into the following interval windows.&lt;/p>
&lt;ul>
&lt;li>[00:00:00, 00:00:59.999)&lt;/li>
&lt;li>[00:01:00, 00:01:59.999)&lt;/li>
&lt;li>[00:03:00, 00:03:59.999)&lt;/li>
&lt;/ul>
&lt;p>But as there was no data between 00:02:00 and 00:02:59, no value is produced
for interval window [00:02:00,00:02:59.999).&lt;/p>
&lt;p>How can we get Beam to output values for that missing window? First, let’s walk
through some options that do not make use of the Timer API.&lt;/p>
&lt;h2 id="option-1-external-heartbeat">Option 1: External heartbeat&lt;/h2>
&lt;p>We can use an external system to emit a value for each time interval and inject
it into the stream of data that Beam consumes. This simple option moves any
complexity out of the Beam pipeline. But using an external system means we need
to monitor this system and perform other maintenance tasks in tandem with the
Beam pipeline.&lt;/p>
&lt;h2 id="option-2-use-a-generated-source-in-the-beam-pipeline">Option 2: Use a generated source in the Beam pipeline&lt;/h2>
&lt;p>We can use a generating source to emit the value using this code snippet:&lt;/p>
&lt;pre>&lt;code>pipeline.apply(GenerateSequence.
from(0).withRate(1,Duration.standardSeconds(1L)))
&lt;/code>&lt;/pre>&lt;p>We can then:&lt;/p>
&lt;ol>
&lt;li>Use a DoFn to convert the value to zero.&lt;/li>
&lt;li>Flatten this value with the real source.&lt;/li>
&lt;li>Produce a PCollection which has ticks in every time interval.&lt;/li>
&lt;/ol>
&lt;p>This is also a simple way of producing a value in each time interval.&lt;/p>
&lt;h2 id="option-1--2-the-problem-with-multiple-keys">Option 1 &amp;amp; 2 The problem with multiple keys&lt;/h2>
&lt;p>Both options 1 and 2 work well for the case where there the pipeline processes a
single key. Let’s now deal with the case where instead of 1 IoT device, there
are 1000s or 100,000s of these devices, each with a unique key. To make option 1
or option 2 work in this scenario, we need to carry out an extra step: creating
a FanOut DoFn. Each tick needs to be distributed to all the potential keys, so
we need to create a FanOut DoFn that takes the dummy value and generates a
key-value pair for every available key.&lt;/p>
&lt;p>For example, let&amp;rsquo;s assume we have 3 keys for 3 IoT devices, {key1,key2,key3}.
Using the method we outlined in Option 2 when we get the first element from
GenerateSequence, we need to create a loop in the DoFn that generates 3
key-value pairs. These pairs become the heartbeat value for each of the IoT
devices.&lt;/p>
&lt;p>And things get a lot more fun when we need to deal with lots of IoT devices,
with a list of keys that are dynamically changing. We would need to add a
transform that does a Distinct operation and feed the data produced as a
side-input into the FanOut DoFn.&lt;/p>
&lt;h2 id="option-3-implementing-a-heartbeat-using-beam-timers">Option 3: Implementing a heartbeat using Beam timers&lt;/h2>
&lt;p>So how do timers help? Well let&amp;rsquo;s have a look at a new transform:&lt;/p>
&lt;p>Edit: Looping Timer State changed from Boolean to Long to allow for min value check.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">LoopingStatefulTimer&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">Instant&lt;/span> &lt;span class="n">stopTimerTime&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">LoopingStatefulTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Instant&lt;/span> &lt;span class="n">stopTime&lt;/span>&lt;span class="o">){&lt;/span>
&lt;span class="k">this&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stopTimerTime&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">stopTime&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimerTime&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">loopingTimerTime&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigEndianLongCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;key&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">StringUtf8Coder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">TimerSpec&lt;/span> &lt;span class="n">loopingTimer&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">TimerSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">EVENT_TIME&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;key&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimerTime&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">loopingTimerTime&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// If the timer has been set already, or if the value is smaller than
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="c1">// the current element + window duration, do not set
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">Long&lt;/span> &lt;span class="n">currentTimerValue&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loopingTimerTime&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Instant&lt;/span> &lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">currentTimerValue&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">null&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">currentTimerValue&lt;/span> &lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getMillis&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">loopingTimerTime&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getMillis&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="c1">// We need this value so that we can output a value for the correct key in OnTimer
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">null&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">key&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@OnTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">onTimer&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">OnTimerContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;key&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">LOG&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">info&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;Timer @ {} fired&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="c1">// If we do not put in a “time to live” value, then the timer would loop forever
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">Instant&lt;/span> &lt;span class="n">nextTimer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">nextTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">isBefore&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">stopTimerTime&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nextTimer&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">LOG&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">info&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="s">&amp;#34;Timer not being set as exceeded Stop Timer value {} &amp;#34;&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">stopTimerTime&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>There are two data values that the state API needs to keep:&lt;/p>
&lt;ol>
&lt;li>A boolean &lt;code>timeRunning&lt;/code> value used to avoid resetting the timer if it’s
already running.&lt;/li>
&lt;li>A &amp;ldquo;&lt;em>key&lt;/em>&amp;rdquo; state object value that allows us to store the key that we are
working with. This information will be needed in the &lt;code>OnTimer&lt;/code> event later.&lt;/li>
&lt;/ol>
&lt;p>We also have a Timer with the ID &lt;code>**loopingTimer**&lt;/code> that acts as our per
interval alarm clock. Note that the timer is an &lt;em>event timer&lt;/em>. It fires based on
the watermark, not on the passage of time as the pipeline runs.&lt;/p>
&lt;p>Next, let&amp;rsquo;s unpack what&amp;rsquo;s happening in the @ProcessElement block:&lt;/p>
&lt;p>The first element to come to this block will:&lt;/p>
&lt;ol>
&lt;li>Set the state of the &lt;code>timerRunner&lt;/code> to True.&lt;/li>
&lt;li>Write the value of the key from the key-value pair into the key StateValue.&lt;/li>
&lt;li>The code sets the value of the timer to fire one minute after the elements
timestamp. Note that the maximum value allowed for this timestamp is
XX:XX:59.999. This places the maximum alarm value at the upper boundary of
the next time interval.&lt;/li>
&lt;li>Finally, we output the data from the &lt;code>@ProcessElement&lt;/code> block using
&lt;code>c.output&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>In the @OnTimer block, the following occurs:&lt;/p>
&lt;ol>
&lt;li>The code emits a value with the key pulled from our key StateValue and a
value of 0. The timestamp of the event corresponds to the event time of the
timer firing.&lt;/li>
&lt;li>We set a new timer for one minute from now, unless we are past the
&lt;code>stopTimerTime&lt;/code> value. Your use case will normally have more complex stopping
conditions, but we use a simple condition here to allow us to keep the
illustrated code simple. The topic of stopping conditions is discussed in
more detail later.&lt;/li>
&lt;/ol>
&lt;p>And that&amp;rsquo;s it, let&amp;rsquo;s add our transform back into the pipeline:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="c1">// Apply a fixed window of duration 1 min and Sum the results
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamped&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">time_1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">time_2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">time_3&lt;/span>&lt;span class="o">)).&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">Window&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="n">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">FixedWindows&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="c1">// We use a combiner to reduce the number of calls in keyed state
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="c1">// from all elements to 1 per FixedWindow
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Sum&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">integersPerKey&lt;/span>&lt;span class="o">())&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GlobalWindows&lt;/span>&lt;span class="o">()))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">LoopingStatefulTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Instant&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">parse&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;2000-01-01T00:04:00Z&amp;#34;&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">FixedWindows&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Sum&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">integersPerKey&lt;/span>&lt;span class="o">())&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">LOG&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">info&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;Value is {} timestamp is {}&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;ol>
&lt;li>In the first part of the pipeline we create FixedWindows and reduce the value
per key down to a single Sum.&lt;/li>
&lt;li>Next we re-window the output into a GlobalWindow. Since state and timers are
per window, they must be set within the window boundary. We want the looping
timer to span all the fixed windows, so we set it up in the global window.&lt;/li>
&lt;li>We then add our LoopingStatefulTimer DoFn.&lt;/li>
&lt;li>Finally, we reapply the FixedWindows and Sum our values.&lt;/li>
&lt;/ol>
&lt;p>This pipeline ensures that a value of zero exists for each interval window, even
if the Source of the pipeline emitted a value in the minimum and maximum
boundaries of the interval window. This means that we can mark the absence of
data.&lt;/p>
&lt;p>You might question why we use two reducers with multiple &lt;code>Sum.integersPerKey&lt;/code>.
Why not just use one? Functionally, using one would also produce the correct
result. However, putting two &lt;code>Sum.integersPerKey&lt;/code> gives us a nice performance
advantage. It reduces the number of elements from many to just one per time
interval. This can reduce the number of reads of the State API during the
&lt;code>@ProcessElement&lt;/code> calls.&lt;/p>
&lt;p>Here is the logging output of running our modified pipeline:&lt;/p>
&lt;pre>&lt;code>INFO LoopingTimer - Timer @ 2000-01-01T00:01:59.999Z fired
INFO LoopingTimer - Timer @ 2000-01-01T00:02:59.999Z fired
INFO LoopingTimer - Timer @ 2000-01-01T00:03:59.999Z fired
INFO LoopingTimer - Timer not being set as exceeded Stop Timer value 2000-01-01T00:04:00.000Z
INFO LoopingTimer - Value is KV{Key_A, 1} timestamp is 2000-01-01T00:00:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 0} timestamp is 2000-01-01T00:02:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 2} timestamp is 2000-01-01T00:01:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 3} timestamp is 2000-01-01T00:03:59.999Z
&lt;/code>&lt;/pre>&lt;p>Yay! We now have output from the time interval [00:01:00, 00:01:59.999), even
though the source dataset has no elements in that interval.&lt;/p>
&lt;p>In this blog, we covered one of the fun areas around time series use cases and
worked through several options, including an advanced use case of the Timer API.
Happy looping everyone!&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Looping timers is an interesting new use case for the Timer API and
runners will need to add support for it with all of their more advanced
feature sets. You can experiment with this pattern today using the
DirectRunner. For other runners, please look out for their release notes on
support for dealing with this use case in production.&lt;/p>
&lt;p>(&lt;a href="/documentation/runners/capability-matrix/">Capability Matrix&lt;/a>)&lt;/p>
&lt;p>Runner specific notes:
Google Cloud Dataflow Runners Drain feature does not support looping timers (Link to matrix)&lt;/p></description></item><item><title>Blog: Apache Beam 2.13.0</title><link>/blog/beam-2.13.0/</link><pubDate>Fri, 07 Jun 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.13.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.13.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2130-2019-05-21">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.13.0, check out the
&lt;a href="https://jira.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345166">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Support reading query results with the BigQuery storage API.&lt;/li>
&lt;li>Support KafkaIO to be configured externally for use with other SDKs.&lt;/li>
&lt;li>BigQuery IO now supports BYTES datatype on Python 3.&lt;/li>
&lt;li>Avro IO support enabled on Python 3.&lt;/li>
&lt;li>For Python 3 pipelines, the default Avro library used by Beam AvroIO and Dataflow workers was switched from avro-python3 to fastavro.&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Flink 1.8 support added.&lt;/li>
&lt;li>Support to run word count on Portable Spark runner.&lt;/li>
&lt;li>ElementCount metrics in FnApi Dataflow Runner.&lt;/li>
&lt;li>Support to create BinaryCombineFn from lambdas.&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>When writing BYTES Datatype into Bigquery with Beam Bigquery IO on Python DirectRunner, users need to base64-encode bytes values before passing them to Bigquery IO. Accordingly, when reading bytes data from BigQuery, the IO will also return base64-encoded bytes. This change only affects Bigquery IO on Python DirectRunner. New DirectRunner behavior is consistent with treatment of Bytes by Beam Java Bigquery IO, and Python Dataflow Runner.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.13.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Li, Ahmet Altay, Aizhamal Nurmamat kyzy, Alex Amato, Alexey Romanenko,
Andrew Pilloud, Ankur Goenka, Anton Kedin, apstndb, Boyuan Zhang, Brian Hulette,
Brian Quinlan, Chamikara Jayalath, Cyrus Maden, Daniel Chen, Daniel Oliveira,
David Cavazos, David Moravek, David Yan, EdgarLGB, Etienne Chauchot, frederik2,
Gleb Kanterov, Harshit Dwivedi, Harsh Vardhan, Heejong Lee, Hennadiy Leontyev,
Henri-Mayeul de Benque, Ismaël Mejía, Jae-woo Kim, Jamie Kirkpatrick, Jan Lukavský,
Jason Kuster, Jean-Baptiste Onofré, JohnZZGithub, Jozef Vilcek, Juta, Kenneth Jung,
Kenneth Knowles, Kyle Weaver, Łukasz Gajowy, Luke Cwik, Mark Liu, Mathieu Blanchard,
Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Kaplinskiy,
Mike Pedersen, Mikhail Gryzykhin, Mikhail-Ivanov, Niklas Hansson, pabloem,
Pablo Estrada, Pranay Nanda, Reuven Lax, Richard Moorhead, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Roman van der Krogt, rosetn, Rui Wang, Ryan Yuan,
Sam Whittle, sudhan499, Sylwester Kardziejonek, Ted, Thomas Weise, Tim Robertson,
ttanay, tvalentyn, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, Yifan Zou,
yoshiki.obata, Yueyang Qiu&lt;/p></description></item><item><title>Blog: Adding new Data Sources to Beam SQL CLI</title><link>/blog/adding-data-sources-to-sql/</link><pubDate>Tue, 04 Jun 2019 00:00:01 -0800</pubDate><guid>/blog/adding-data-sources-to-sql/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>A new, exciting feature that came to Apache Beam is the ability to use
SQL in your pipelines. This is done using Beam&amp;rsquo;s
&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/extensions/sql/SqlTransform.html">&lt;code>SqlTransform&lt;/code>&lt;/a>
in Java pipelines.&lt;/p>
&lt;p>Beam also has a fancy new SQL command line that you can use to query your
data interactively, be it Batch or Streaming. If you haven&amp;rsquo;t tried it, check out
&lt;a href="https://bit.ly/ExploreBeamSQL">http://bit.ly/ExploreBeamSQL&lt;/a>.&lt;/p>
&lt;p>A nice feature of the SQL CLI is that you can use &lt;code>CREATE EXTERNAL TABLE&lt;/code>
commands to &lt;em>add&lt;/em> data sources to be accessed in the CLI. Currently, the CLI
supports creating tables from BigQuery, PubSub, Kafka, and text files. In this
post, we explore how to add new data sources, so that you will be able to
consume data from other Beam sources.&lt;/p>
&lt;p>The table provider we will be implementing in this post will be generating a
continuous unbounded stream of integers. It will be based on the
&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/GenerateSequence.html">&lt;code>GenerateSequence&lt;/code> PTransform&lt;/a>
from the Beam SDK. In the end will be able to define and use the sequence generator
in SQL like this:&lt;/p>
&lt;pre>&lt;code>CREATE EXTERNAL TABLE -- all tables in Beam are external, they are not persisted
sequenceTable -- table alias that will be used in queries
(
sequence BIGINT, -- sequence number
event_timestamp TIMESTAMP -- timestamp of the generated event
)
TYPE sequence -- type identifies the table provider
TBLPROPERTIES '{ elementsPerSecond : 12 }' -- optional rate at which events are generated
&lt;/code>&lt;/pre>&lt;p>And we&amp;rsquo;ll be able to use it in queries like so:&lt;/p>
&lt;pre>&lt;code>SELECT sequence FROM sequenceTable;
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s dive in!&lt;/p>
&lt;h3 id="implementing-a-tableprovider">Implementing a &lt;code>TableProvider&lt;/code>&lt;/h3>
&lt;p>Beam&amp;rsquo;s &lt;code>SqlTransform&lt;/code> works by relying on &lt;code>TableProvider&lt;/code>s, which it uses when
one uses a &lt;code>CREATE EXTERNAL TABLE&lt;/code> statement. If you are looking to add a new
data source to the Beam SQL CLI, then you will want to add a &lt;code>TableProvider&lt;/code> to
do it. In this post, I will show what steps are necessary to create a new table
provider for the
&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/GenerateSequence.html">&lt;code>GenerateSequence&lt;/code> transform&lt;/a> available in the Java SDK.&lt;/p>
&lt;p>The &lt;code>TableProvider&lt;/code> classes are under
&lt;a href="https://github.com/apache/beam/tree/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider">&lt;code>sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/&lt;/code>&lt;/a>. If you look in there, you can find providers, and their implementations, for all available data sources. So, you just need to add the one you want, along with an implementation of &lt;code>BaseBeamTable&lt;/code>.&lt;/p>
&lt;h3 id="the-generatesequencetableprovider">The GenerateSequenceTableProvider&lt;/h3>
&lt;p>Our table provider looks like this:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@AutoService&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TableProvider&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">GenerateSequenceTableProvider&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">InMemoryMetaTableProvider&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">String&lt;/span> &lt;span class="nf">getTableType&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="s">&amp;#34;sequence&amp;#34;&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">BeamSqlTable&lt;/span> &lt;span class="nf">buildBeamSqlTable&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Table&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">GenerateSequenceTable&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>All it does is give a type to the table - and it implements the
&lt;code>buildBeamSqlTable&lt;/code> method, which simply returns a &lt;code>BeamSqlTable&lt;/code> defined by
our &lt;code>GenerateSequenceTable&lt;/code> implementation.&lt;/p>
&lt;h3 id="the-generatesequencetable">The GenerateSequenceTable&lt;/h3>
&lt;p>We want a table implementation that supports streaming properly, so we will
allow users to define the number of elements to be emitted per second. We will
define a simple table that emits sequential integers in a streaming fashion.
This looks like so:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">class&lt;/span> &lt;span class="nc">GenerateSequenceTable&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">BaseBeamTable&lt;/span> &lt;span class="kd">implements&lt;/span> &lt;span class="n">Serializable&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">Schema&lt;/span> &lt;span class="n">TABLE_SCHEMA&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">Schema&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Field&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sequence&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">FieldType&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">INT64&lt;/span>&lt;span class="o">),&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;event_time&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">FieldType&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">DATETIME&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">Integer&lt;/span> &lt;span class="n">elementsPerSecond&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">5&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">GenerateSequenceTable&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Table&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">super&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TABLE_SCHEMA&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getProperties&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">containsKey&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;elementsPerSecond&amp;#34;&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">elementsPerSecond&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getProperties&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getInteger&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;elementsPerSecond&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">PCollection&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">IsBounded&lt;/span> &lt;span class="nf">isBounded&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">IsBounded&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">UNBOUNDED&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">buildIOReader&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">PBegin&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">begin&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GenerateSequence&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">from&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">withRate&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">elementsPerSecond&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardSeconds&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">MapElements&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TypeDescriptor&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">via&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">elm&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Row&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">withSchema&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TABLE_SCHEMA&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">addValues&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">elm&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">now&lt;/span>&lt;span class="o">()).&lt;/span>&lt;span class="na">build&lt;/span>&lt;span class="o">()))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">setRowSchema&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">getSchema&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">POutput&lt;/span> &lt;span class="nf">buildIOWriter&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">throw&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">UnsupportedOperationException&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buildIOWriter unsupported!&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h2 id="the-real-fun">The real fun&lt;/h2>
&lt;p>Now that we have implemented the two basic classes (a &lt;code>BaseBeamTable&lt;/code>, and a
&lt;code>TableProvider&lt;/code>), we can start playing with them. After building the
&lt;a href="https://beam.apache.org/documentation/dsls/sql/shell/">SQL CLI&lt;/a>, we
can now perform selections on the table:&lt;/p>
&lt;pre>&lt;code>0: BeamSQL&amp;gt; CREATE EXTERNAL TABLE input_seq (
. . . . . &amp;gt; sequence BIGINT COMMENT 'this is the primary key',
. . . . . &amp;gt; event_time TIMESTAMP COMMENT 'this is the element timestamp'
. . . . . &amp;gt; )
. . . . . &amp;gt; TYPE 'sequence';
No rows affected (0.005 seconds)
&lt;/code>&lt;/pre>&lt;p>And let&amp;rsquo;s select a few rows:&lt;/p>
&lt;pre>&lt;code>0: BeamSQL&amp;gt; SELECT * FROM input_seq LIMIT 5;
+---------------------+------------+
| sequence | event_time |
+---------------------+------------+
| 0 | 2019-05-21 00:36:33 |
| 1 | 2019-05-21 00:36:33 |
| 2 | 2019-05-21 00:36:33 |
| 3 | 2019-05-21 00:36:33 |
| 4 | 2019-05-21 00:36:33 |
+---------------------+------------+
5 rows selected (1.138 seconds)
&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s try something more interesting. Such as grouping. This will also let
us make sure that we&amp;rsquo;re providing the timestamp for each row properly:&lt;/p>
&lt;pre>&lt;code>0: BeamSQL&amp;gt; SELECT
. . . . . &amp;gt; COUNT(sequence) as elements,
. . . . . &amp;gt; TUMBLE_START(event_time, INTERVAL '2' SECOND) as window_start
. . . . . &amp;gt; FROM input_seq
. . . . . &amp;gt; GROUP BY TUMBLE(event_time, INTERVAL '2' SECOND) LIMIT 5;
+---------------------+--------------+
| elements | window_start |
+---------------------+--------------+
| 6 | 2019-06-05 00:39:24 |
| 10 | 2019-06-05 00:39:26 |
| 10 | 2019-06-05 00:39:28 |
| 10 | 2019-06-05 00:39:30 |
| 10 | 2019-06-05 00:39:32 |
+---------------------+--------------+
5 rows selected (10.142 seconds)
&lt;/code>&lt;/pre>&lt;p>And voilà! We can start playing with some interesting streaming queries to our
sequence generator.&lt;/p></description></item><item><title>Blog: Apache Beam Katas</title><link>/blog/beam-kata-release/</link><pubDate>Thu, 30 May 2019 00:00:01 -0800</pubDate><guid>/blog/beam-kata-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to announce
&lt;a href="https://github.com/apache/beam/tree/master/learning/katas">Apache Beam Katas&lt;/a>, a set of
interactive Beam coding exercises (i.e. &lt;a href="http://codekata.com/">code katas&lt;/a>) that can help you in
learning Apache Beam concepts and programming model hands-on.&lt;/p>
&lt;p>Beam Katas objective is to provide a series of structured hands-on learning experiences for learners
to understand about Apache Beam and its SDKs by solving exercises with gradually increasing
complexity. It is built based on
&lt;a href="https://www.jetbrains.com/education/">JetBrains Educational Products&lt;/a>. Beam Katas is available for
both Java and Python SDKs. Currently we have about 20 lessons that cover Apache Beam fundamentals,
such as core transforms, common transforms, and simple use case (word count), with more katas to
be added in the coming future.&lt;/p>
&lt;p>To start with the courses, you can simply download
&lt;a href="https://www.jetbrains.com/education/download/#section=idea">IntelliJ Edu&lt;/a> or
&lt;a href="https://www.jetbrains.com/education/download/#section=pycharm-edu">PyCharm Edu&lt;/a> and then browse
the integrated Stepik courses from the menu. Search for “Beam Katas” and once the course is loaded
on the IDE, you’re good to go.&lt;/p>
&lt;p>We have plans to add more katas covering more topics including some of the intermediate and
advanced ones in the coming future, such as windowing, streaming, and use case patterns. We would
also like to welcome you to &lt;a href="https://github.com/apache/beam">contribute&lt;/a> by building and adding more katas that you think would be
useful for people to learn more about Apache Beam, and eventually become Beam Masters!&lt;/p>
&lt;br/>
&lt;img src="/images/blog/beam-kata/beam-kata-intellij-edu-1.png" alt="Beam Kata - IntelliJ Edu" width="363" height="350">
&lt;img src="/images/blog/beam-kata/beam-kata-intellij-edu-2.png" alt="Beam Kata - IntelliJ Edu" width="455" height="350">
&lt;img src="/images/blog/beam-kata/beam-kata-pycharm-edu-1.png" alt="Beam Kata - PyCharm Edu" width="363" height="350">
&lt;img src="/images/blog/beam-kata/beam-kata-pycharm-edu-2.png" alt="Beam Kata - PyCharm Edu" width="459" height="350"></description></item><item><title>Blog: Beam community update!</title><link>/blog/beam-summit-europe-2019/</link><pubDate>Sat, 11 May 2019 00:00:01 -0800</pubDate><guid>/blog/beam-summit-europe-2019/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="the-apache-beam-community-in-2019">The Apache Beam community in 2019&lt;/h1>
&lt;p>2019 has already been a busy time for the Apache Beam community. The ASF blog featured &lt;a href="https://blogs.apache.org/comdev/date/20190222">our way of community building&lt;/a> and we&amp;rsquo;ve had &lt;a href="https://www.meetup.com/San-Francisco-Apache-Beam/events/257482350">more Beam meetups&lt;/a> around the world. Apache Beam also received the &lt;a href="https://www.infoworld.com/article/3336072/infoworlds-2019-technology-of-the-year-award-winners.html">Technology of the Year Award&lt;/a> from InfoWorld.&lt;/p>
&lt;p>As these events happened, we were building up to the &lt;a href="https://opensource.googleblog.com/2019/03/celebrating-20-years-of-apache.html">20th anniversary of the Apache Software Foundation&lt;/a>. The contributions of the Beam community were a part of Maximilian Michels blog post on the success of the ASF&amp;rsquo;s open source development model:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Success at Apache: What You Need to Know by Maximilian Michels &lt;a href="https://t.co/XjtVYgPAHX">https://t.co/XjtVYgPAHX&lt;/a> &lt;a href="https://twitter.com/hashtag/Apache?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Apache&lt;/a> &lt;a href="https://twitter.com/hashtag/Open?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Open&lt;/a> &lt;a href="https://twitter.com/hashtag/Innovation?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Innovation&lt;/a> &lt;a href="https://twitter.com/hashtag/Community?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Community&lt;/a> &lt;a href="https://twitter.com/hashtag/people?src=hash&amp;amp;ref_src=twsrc%5Etfw">#people&lt;/a> &lt;a href="https://twitter.com/hashtag/processes?src=hash&amp;amp;ref_src=twsrc%5Etfw">#processes&lt;/a> &lt;a href="https://twitter.com/hashtag/JustWorks?src=hash&amp;amp;ref_src=twsrc%5Etfw">#JustWorks&lt;/a> &lt;a href="https://twitter.com/stadtlegende?ref_src=twsrc%5Etfw">@stadtlegende&lt;/a> &lt;a href="https://t.co/xSibnyWAMe">pic.twitter.com/xSibnyWAMe&lt;/a>&lt;/p>&amp;mdash; Apache - The ASF (@TheASF) &lt;a href="https://twitter.com/TheASF/status/1110364656143601664?ref_src=twsrc%5Etfw">26 maart 2019&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>In that spirit, let&amp;rsquo;s have an overview of the things that have happened, what the next few months look like, and how we can foster even more community growth.&lt;/p>
&lt;h2 id="meetups">Meetups&lt;/h2>
&lt;p>We&amp;rsquo;ve had a flurry of activity, with several meetups in the planning process and more popping up globally over time. As diversity of contributors is a core ASF value, this geographic spread is exciting for the community. Here&amp;rsquo;s a picture from the latest Apache Beam meetup organized at Lyft in San Francisco:&lt;/p>
&lt;p>&lt;img src="https://secure.meetupstatic.com/photos/event/8/0/1/2/600_481292786.jpeg" alt="Beam Meetup Bay Area" >&lt;/p>
&lt;p>We have more &lt;a href="https://www.meetup.com/San-Francisco-Apache-Beam">Bay Area meetups&lt;/a> coming soon, and the community is looking into kicking off a meetup in Toronto!&lt;/p>
&lt;p>&lt;a href="https://www.meetup.com/London-Apache-Beam-Meetup">London&lt;/a> had its first meetup of 2019 at the start of April:&lt;/p>
&lt;p>&lt;img src="https://secure.meetupstatic.com/photos/event/4/7/0/e/600_480318190.jpeg" alt="Beam Meetup London" height="360" width="640" >&lt;/p>
&lt;p>and &lt;a href="https://www.meetup.com/Apache-Beam-Stockholm/events/260634514">Stockholm&lt;/a> had its second meetup at the start of May:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en-gb">&lt;p lang="en" dir="ltr">Big audience for the second &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> meetup in Stockholm! Gleb, &lt;a href="https://twitter.com/kanterov?ref_src=twsrc%5Etfw">@kanterov&lt;/a> from &lt;a href="https://twitter.com/SpotifyEng?ref_src=twsrc%5Etfw">@SpotifyEng&lt;/a> kicking off the first talk with Beam SQL.&lt;a href="https://twitter.com/hashtag/ApacheBeamStockholm?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ApacheBeamStockholm&lt;/a> &lt;a href="https://t.co/fDqPPFh2gY">pic.twitter.com/fDqPPFh2gY&lt;/a>&lt;/p>&amp;mdash; Matthias Baetens 🌆 (@matthiasbaetens) &lt;a href="https://twitter.com/matthiasbaetens/status/1125442916711915521?ref_src=twsrc%5Etfw">6 May 2019&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Keep an eye out for a meetup in &lt;a href="https://www.meetup.com/Paris-Apache-Beam-Meetup">Paris&lt;/a>.&lt;/p>
&lt;p>If you are interested in starting your own meetup, feel free &lt;a href="https://beam.apache.org/community/contact-us">to reach out&lt;/a>! Good places to start include our Slack channel, the dev and user mailing lists, or the Apache Beam Twitter.&lt;/p>
&lt;p>Even if you can’t travel to these meetups, you can stay informed on the happenings of the community. The talks and sessions from previous conferences and meetups are archived on the &lt;a href="https://www.youtube.com/c/ApacheBeamYT">Apache Beam YouTube channel&lt;/a>. If you want your session added to the channel, don’t hesitate to get in touch! And in case you want to attend the next Beam event in style, you can also order your swag on the &lt;a href="https://store-beam.myshopify.com">Beam swag store&lt;/a>&lt;/p>
&lt;h2 id="summits">Summits&lt;/h2>
&lt;p>The first summit of the year will be held in Berlin:&lt;/p>
&lt;p>&lt;img src="https://img.evbuc.com/https%3A%2F%2Fcdn.evbuc.com%2Fimages%2F58635346%2F70962106775%2F1%2Foriginal.20190317-212619?w=800&amp;auto=compress&amp;rect=0%2C115%2C2666%2C1333&amp;s=2680f5036dcad9177b027cce026c0224" alt="Beam Summit Europe Banner" >&lt;/p>
&lt;p>You can find more info on the &lt;a href="https://beamsummit.org">website&lt;/a> and read about the inaugural edition of the Beam Summit Europe &lt;a href="https://beam.apache.org/blog/2018/10/31/beam-summit-aftermath.html">here&lt;/a>. At these summits, you have the opportunity to meet with other Apache Beam creators and users, get expert advice, learn from the speaker sessions, and participate in workshops.&lt;/p>
&lt;p>We strongly encourage you to get involved again this year! You can participate in the following ways for the upcoming summit in Europe:&lt;/p>
&lt;p>🎫 If you want to secure your ticket to attend the Beam Summit Europe 2019, check our &lt;a href="https://beam-summit-europe.eventbrite.com">event page&lt;/a>.&lt;/p>
&lt;p>💸 If you want to make the Summit even &lt;strong>more&lt;/strong> awesome, check out our &lt;a href="https://drive.google.com/file/d/1R3vvOHihQbpuzF2aaSV8WYg9YHRmJwxS/view">sponsor booklet&lt;/a>!&lt;/p>
&lt;p>We also launched the CfP for our Beam Summit in North America, which will be held in collaboration with &lt;a href="https://www.apachecon.com">ApacheCon&lt;/a>.&lt;/p>
&lt;p>🎤 If you want to give a talk, take a look at our &lt;a href="https://www.apachecon.com/acna19/cfp.html">CfP&lt;/a>.&lt;/p>
&lt;p>Stay tuned for more information on the summit in North America and Asia.&lt;/p>
&lt;h2 id="why-community-engagement-matters">Why community engagement matters&lt;/h2>
&lt;p>Why we need a strong Apache Beam community:&lt;/p>
&lt;ul>
&lt;li>We&amp;rsquo;re receiving lots of code contributions and need committers to review those and help onboard new contributors to the project.&lt;/li>
&lt;li>We want people to feel a sense of ownership to the project. By fostering this level of engagement, the work becomes even more exciting.&lt;/li>
&lt;li>A healthy community has a further reach and leads to more growth. More hours can be contributed to the project as we can spread the work and ownership.&lt;/li>
&lt;/ul>
&lt;p>Why are we organizing these summits:&lt;/p>
&lt;ul>
&lt;li>We&amp;rsquo;d like to give folks a place to meet, congregate, and share ideas.&lt;/li>
&lt;li>We know that offline interactions often changes the nature of the online ones in a positive manner.&lt;/li>
&lt;li>Building an active and diverse community is part of the Apache Way. These summits provide an opportunity for us to engage people from different locations, companies, and backgrounds.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam + Kotlin = ❤️</title><link>/blog/beam-kotlin/</link><pubDate>Thu, 25 Apr 2019 00:00:01 -0800</pubDate><guid>/blog/beam-kotlin/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam samples are now available in Kotlin!&lt;/p>
&lt;p>&lt;img src="/images/blog/kotlin.png" alt="Kotlin" height="320" width="800" >&lt;/p>
&lt;p>If you are someone who&amp;rsquo;s been working with Java in your professional career; there&amp;rsquo;s a good chance that you&amp;rsquo;ve also heard of &lt;a href="https://kotlinlang.org/">Kotlin&lt;/a>, which is an Open Sourced, statically typed language for JVM and is mostly being favoured by Android Developers due to the many myriad features which enable more concise and cleaner code than Java without sacrificing performance or safety.&lt;/p>
&lt;p>It gives us an immense pleasure to announce that we are also taking a step ahead in the same direction and releasing the samples for the Beam SDK in Kotlin alongside Java!&lt;/p>
&lt;p>(Note : At the time of writing this post, only the WordCount samples have been added in Koltin with more samples underway)&lt;/p>
&lt;h2 id="code-snippets">Code Snippets&lt;/h2>
&lt;p>Here are few brief snippets of code that show how the Kotlin Samples compare to Java&lt;/p>
&lt;h3 id="java">Java&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="n">String&lt;/span> &lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">String&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">format&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="s">&amp;#34;%s-%s-of-%s%s&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">filenamePrefixForWindow&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">intervalWindow&lt;/span>&lt;span class="o">),&lt;/span>
&lt;span class="n">shardNumber&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">numShards&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">outputFileHints&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">suggestedFilenameSuffix&lt;/span>&lt;span class="o">);&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="kotlin">Kotlin&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="c1">// String templating
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">val&lt;/span> &lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;$filenamePrefixForWindow(intervalWindow)-$shardNumber-of-$numShards${outputFileHints.suggestedFilenameSuffix)&amp;#34;&lt;/span> &lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="java-1">Java&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">FormatAsTextFn&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">SimpleFunction&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">String&lt;/span> &lt;span class="nf">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s">&amp;#34;: &amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getValue&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h2 id="kotlin-1">Kotlin&lt;/h2>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">FormatAsTextFn&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">SimpleFunction&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">override&lt;/span> &lt;span class="n">fun&lt;/span> &lt;span class="nf">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;${input.key} : ${input.value}&amp;#34;&lt;/span> &lt;span class="c1">//Single line functions
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="java-2">Java&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">if&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">tableRow&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="kc">null&lt;/span>&lt;span class="o">){&lt;/span>
&lt;span class="n">formatAndInsert&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">tableRow&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="kotlin-2">Kotlin&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">tableRow&lt;/span>&lt;span class="o">?.&lt;/span>&lt;span class="na">let&lt;/span>&lt;span class="o">{&lt;/span>
&lt;span class="n">formatAndInsert&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="c1">// No need for null checks
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="java-3">Java&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">String&lt;/span> &lt;span class="n">tableName&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;testTable&amp;#34;&lt;/span>&lt;span class="o">;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="kotlin-3">Kotlin&lt;/h3>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">val&lt;/span> &lt;span class="n">tableName&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;testTable&amp;#34;&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">Type&lt;/span> &lt;span class="n">inferencing&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h2 id="contributors-welcomed">Contributors Welcomed!&lt;/h2>
&lt;p>While we&amp;rsquo;re still adding more samples and streamlining the current ones, we would love to have your feedback on the code snippets.
You can find them over here : &lt;a href="https://github.com/apache/beam/tree/master/examples/kotlin">https://github.com/apache/beam/tree/master/examples/kotlin&lt;/a>&lt;/p>
&lt;p>If you are using Kotlin with Apache Beam already; we would very much appreciate if you went ahead and help us convert the existing samples from Java into Koltin.&lt;/p>
&lt;p>Thank you, and we are looking forward to feedback from you!&lt;/p></description></item><item><title>Blog: Apache Beam 2.12.0</title><link>/blog/beam-2.12.0/</link><pubDate>Thu, 25 Apr 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.12.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.12.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2120-2019-04-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.12.0, check out the
&lt;a href="https://jira.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344944">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Add support for a BigQuery custom sink for Python SDK.&lt;/li>
&lt;li>Add support to specify a query in CassandraIO for Java SDK.&lt;/li>
&lt;li>Add experimental support for cross-language transforms,
please see &lt;a href="https://issues.apache.org/jira/browse/BEAM-6730">BEAM-6730&lt;/a>&lt;/li>
&lt;li>Add support in the Flink Runner for exactly-once Writes with KafkaIO&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Enable Bundle Finalization in Python SDK for portable runners.&lt;/li>
&lt;li>Add support to the Java SDK harness to merge windows.&lt;/li>
&lt;li>Add Kafka Sink EOS support on Flink runner.&lt;/li>
&lt;li>Added a dead letter queue to Python streaming BigQuery sink.&lt;/li>
&lt;li>Add Experimental Python 3.6 and 3.7 workloads enabled.
Beam 2.12 supports starting Dataflow pipelines under Python 3.6, 3.7, however 3.5 remains the only recommended minor version for Dataflow runner. In addition to announced 2.11 limitations, Beam typehint annotations are currently not supported on Python &amp;gt;= 3.6.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.12.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed El.Hussaini, Ahmet Altay, Alan Myrvold, Alex Amato, Alexander Savchenko,
Alexey Romanenko, Andrew Brampton, Andrew Pilloud, Ankit Jhalaria,
Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Chamikara Jayalath,
Charles Chen, Colm O hEigeartaigh, Craig Chambers, Dan Duong, Daniel Mescheder,
Daniel Oliveira, David Moravek, David Rieber, David Yan, Eric Roshan-Eisner,
Etienne Chauchot, Gleb Kanterov, Heejong Lee, Ho Tien Vu, Ismaël Mejía,
Jan Lukavský, Jean-Baptiste Onofré, Jeff Klukas, Juta, Kasia Kucharczyk,
Kengo Seki, Kenneth Jung, Kenneth Knowles, kevin, Kyle Weaver, Kyle Winkelman,
Łukasz Gajowy, Mark Liu, Mathieu Blanchard, Max Charas, Maximilian Michels,
Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Kaplinskiy,
Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Radoslaw Stankiewicz,
Reuven Lax, Robbe Sneyders, Robert Bradshaw, Robert Burke, Rui Wang,
Ruoyun Huang, Ryan Williams, Slava Chernyak, Shahar Frank, Sunil Pedapudi,
Thomas Weise, Tim Robertson, Tanay Tummalapalli, Udi Meiri,
Valentyn Tymofieiev, Xinyu Liu, Yifan Zou, Yueyang Qiu&lt;/p></description></item><item><title>Blog: Apache Beam is applying to Season of Docs</title><link>/blog/season-of-docs/</link><pubDate>Fri, 19 Apr 2019 00:00:01 -0800</pubDate><guid>/blog/season-of-docs/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The Apache Beam community is thrilled to announce its application to the first edition of Season of Docs 2019!&lt;/p>
&lt;p>&lt;img src="/images/blog/SoD.png" alt="Season of Docs 2019 flyer" height="455" width="640" >&lt;/p>
&lt;p>&lt;a href="https://developers.google.com/season-of-docs/">Season of Docs&lt;/a> is a unique program that pairs technical writers with open source mentors to contribute to open source. This creates an opportunity to introduce the technical writer to an open source community and provide guidance while the writer works on a real world open source project. We, in the Apache Beam community, would love to take this chance and invite technical writers to collaborate with us, and help us improve our documentation in many ways.&lt;/p>
&lt;p>Apache Beam does have help from excellent technical writers, but the documentation needs of the project often exceed their bandwidth. This is why we are excited about this program.&lt;/p>
&lt;p>After discussing ideas in the community, we have been able to find mentors, and frame two ideas that we think would be a great fit for an incoming tech writer to tackle. We hope you will find this opportunity interesting - and if you do, please get in touch by emailing the Apache Beam mailing list at &lt;a href="mailto:dev@beam.apache.org">dev@beam.apache.org&lt;/a> (you will need to subscribe first by emailing to &lt;a href="mailto:dev-subscribe@beam.apache.org">dev-subscribe@beam.apache.org&lt;/a>).&lt;/p>
&lt;p>The project ideas available in Apache Beam are described below. Please take a look and ask any questions that you may have. We will be very happy to help you get onboarded with the project.&lt;/p>
&lt;h3 id="project-ideas">Project ideas&lt;/h3>
&lt;p>&lt;strong>Deployment of Flink and Spark Clusters for use with Portable Beam&lt;/strong>&lt;/p>
&lt;p>The Apache Beam vision has been to provide a framework for users to write and execute pipelines on the programming language of your choice, and the runner of your choice. As the reality of Beam has evolved towards this vision, the way in which Beam is run on top of runners such as Apache Spark and Apache Flink has changed.&lt;/p>
&lt;p>These changes are documented in the wiki and in design documents, and are accessible for Beam contributors; but they are not available in the user-facing documentation. This has been a barrier of adoption for other users of Beam.&lt;/p>
&lt;p>This project involves improving the &lt;a href="https://beam.apache.org/documentation/runners/flink/">Flink Runner page&lt;/a> to include strategies to deploy Beam on a few different environments: A Kubernetes cluster, a Google Cloud Dataproc cluster, and an AWS EMR cluster. There are other places in the documentation that should be updated in this regard, such as the &lt;a href="https://beam.apache.org/documentation/sdks/python-streaming/">Python streaming&lt;/a> section, and the &lt;a href="https://beam.apache.org/documentation/sdks/python-streaming/#unsupported-features">set of supported features&lt;/a>.&lt;/p>
&lt;p>After working on the Flink Runner, then similar updates should be made to the &lt;a href="https://beam.apache.org/documentation/runners/spark/">Spark Runner page&lt;/a>, and the &lt;a href="https://beam.apache.org/get-started/beam-overview/">getting started documentation&lt;/a>.&lt;/p>
&lt;p>&lt;strong>The runner comparison page / capability matrix update&lt;/strong>&lt;/p>
&lt;p>Beam maintains a &lt;a href="https://beam.apache.org/documentation/runners/capability-matrix/">capability matrix&lt;/a> to track which Beam features are supported by which set of language SDKs + Runners.
This project involves a number of &lt;a href="https://issues.apache.org/jira/browse/BEAM-2888">corrections and improvements to the capability matrix&lt;/a>; followed by a few larger set of changes, involving:&lt;/p>
&lt;ul>
&lt;li>Plain english summaries for each runner’s support of the Beam model.&lt;/li>
&lt;li>A paragraph-length description of the production-readiness for each runner.&lt;/li>
&lt;li>Comparisons for non-model differences between runners.&lt;/li>
&lt;li>Comparison for support of the portability framework for each runner.&lt;/li>
&lt;/ul>
&lt;p>Thank you, and we are looking forward to hearing from you!&lt;/p></description></item><item><title>Blog: Announcing Beam Summit Site</title><link>/blog/beam-summit-site/</link><pubDate>Mon, 18 Mar 2019 00:00:01 -0800</pubDate><guid>/blog/beam-summit-site/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are thrilled to announce the launch of our new website dedicated to Beam Summits!&lt;/p>
&lt;p>The &lt;a href="https://beamsummit.org">beamsummit.org&lt;/a> site provides all the information you need towards the upcoming Beam Summits in Europe, Asia and North America in 2019. You will find information about the conference theme, the speakers and sessions, the abstract submission timeline and the registration process, the conference venues and hosting cities - and much more that you will find useful until and during the Beam Summits 2019.&lt;/p>
&lt;p>We are working to make the website easy to use, so that anyone who is organizing a Beam event can rely on it. You can find the &lt;a href="https://github.com/matthiasa4/hoverboard">code for it in Github&lt;/a>.&lt;/p>
&lt;p>The pages will be updated on a regular basis, but we also love hearing thoughts from our community! Let us know if you have any questions, comments or suggestions, and help us improve! Also, if you are thinking of organizing a Beam event, please feel free to reach out for support, and to use the code in GitHub as well.&lt;/p>
&lt;p>We sincerely hope that you like the new Beam Summit website and will find it useful for accessing information. Enjoy browsing around!&lt;/p>
&lt;p>See you in Berlin!&lt;/p>
&lt;p>#beamsummit2019.&lt;/p></description></item><item><title>Blog: Apache Beam 2.11.0</title><link>/blog/beam-2.11.0/</link><pubDate>Tue, 05 Mar 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.11.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.11.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2110-2019-02-26">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.11.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344775">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="dependency-upgradeschanges">Dependency Upgrades/Changes&lt;/h3>
&lt;ul>
&lt;li>Java: antlr: 4.7&lt;/li>
&lt;li>Java: antlr_runtime: 4.7&lt;/li>
&lt;li>Java: bigdataoss_gcsio: 1.9.16&lt;/li>
&lt;li>Java: bigdataoss_util: 1.9.16&lt;/li>
&lt;li>Java: bigtable_client_core: 1.8.0&lt;/li>
&lt;li>Java: cassandra-driver-core: 3.6.0&lt;/li>
&lt;li>Java: cassandra-driver-mapping: 3.6.0&lt;/li>
&lt;li>Java: commons-compress: 1.18&lt;/li>
&lt;li>Java: gax_grpc: 1.38.0&lt;/li>
&lt;li>Java: google_api_common: 1.7.0&lt;/li>
&lt;li>Java: google_api_services_dataflow: v1b3-rev20190126-1.27.0&lt;/li>
&lt;li>Java: google_cloud_bigquery_storage: 0.79.0-alpha&lt;/li>
&lt;li>Java: google_cloud_bigquery_storage_proto: 0.44.0&lt;/li>
&lt;li>Java: google_auth_library_credentials: 0.12.0&lt;/li>
&lt;li>Java: google_auth_library_oauth2_http: 0.12.0&lt;/li>
&lt;li>Java: google_cloud_core: 1.61.0&lt;/li>
&lt;li>Java: google_cloud_core_grpc: 1.61.0&lt;/li>
&lt;li>Java: google_cloud_spanner: 1.6.0&lt;/li>
&lt;li>Java: grpc_all: 1.17.1&lt;/li>
&lt;li>Java: grpc_auth: 1.17.1&lt;/li>
&lt;li>Java: grpc_core: 1.17.1&lt;/li>
&lt;li>Java: grpc_google_cloud_pubsub_v1: 1.17.1&lt;/li>
&lt;li>Java: grpc_protobuf: 1.17.1&lt;/li>
&lt;li>Java: grpc_protobuf_lite: 1.17.1&lt;/li>
&lt;li>Java: grpc_netty: 1.17.1&lt;/li>
&lt;li>Java: grpc_stub: 1.17.1&lt;/li>
&lt;li>Java: netty_handler: 4.1.30.Final&lt;/li>
&lt;li>Java: netty_tcnative_boringssl_static: 2.0.17.Final&lt;/li>
&lt;li>Java: netty_transport_native_epoll: 4.1.30.Final&lt;/li>
&lt;li>Java: proto_google_cloud_spanner_admin_database_v1: 1.6.0&lt;/li>
&lt;li>Java: zstd_jni: 1.3.8-3&lt;/li>
&lt;li>Python: futures&amp;gt;=3.2.0,&amp;lt;4.0.0; python_version &amp;lt; &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: pyvcf&amp;gt;=0.6.8,&amp;lt;0.7.0; python_version &amp;lt; &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: google-apitools&amp;gt;=0.5.26,&amp;lt;0.5.27&lt;/li>
&lt;li>Python: google-cloud-core==0.28.1&lt;/li>
&lt;li>Python: google-cloud-bigtable==0.31.1&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Portable Flink runner support for running cross-language transforms.&lt;/li>
&lt;li>Add Cloud KMS support to GCS copies.&lt;/li>
&lt;li>Add parameters for offsetConsumer in KafkaIO.read().&lt;/li>
&lt;li>Allow setting compression codec in ParquetIO write.&lt;/li>
&lt;li>Add kms_key to BigQuery transforms, pass to Dataflow.&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Python 3 (experimental) suppport for DirectRunner and DataflowRunner.&lt;/li>
&lt;li>Add ZStandard compression support for Java SDK.&lt;/li>
&lt;li>Python: Add CombineFn.compact, similar to Java.&lt;/li>
&lt;li>SparkRunner: GroupByKey optimized for non-merging windows.&lt;/li>
&lt;li>SparkRunner: Add bundleSize parameter to control splitting of Spark sources.&lt;/li>
&lt;li>FlinkRunner: Portable runner savepoint / upgrade support.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Deprecate MongoDb &lt;code>withKeepAlive&lt;/code> because it is deprecated in the Mongo driver.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.11.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato. Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin,
Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath, Charles Chen, Craig Chambers,
Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov,
Hai Lu, Heejong Lee, Ismaël Mejía, J Ross Thomson, Jan Lukavsky, Jason Kuster, Jean-Baptiste Onofré,
Jeff Klukas, João Cabrita, Juan Rael, Juta Staes, Kasia Kucharczyk, Kengo Seki, Kenneth Jung,
Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Marek Simunek, Mark Liu,
Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Pedersen,
Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Reuven Lax, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle,
Scott Wegner, Tanay Tummalapalli, Thomas Weise, Tianyang Hu, Tyler Akidau, Udi Meiri,
Valentyn Tymofieiev, Xinyu Liu, Xu Mingmin, Łukasz Gajowy.&lt;/p></description></item><item><title>Blog: Apache Beam 2.10.0</title><link>/blog/beam-2.10.0/</link><pubDate>Fri, 15 Feb 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.10.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.10.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2100-2019-02-01">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.10.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344540">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="dependency-upgradeschanges">Dependency Upgrades/Changes&lt;/h3>
&lt;ul>
&lt;li>FlinkRunner: Flink 1.5.x/1.6.x/1.7.x&lt;/li>
&lt;li>Java: AutoValue 1.6.3&lt;/li>
&lt;li>Java: Jackson 2.9.8&lt;/li>
&lt;li>Java: google_cloud_bigdataoss 1.9.13&lt;/li>
&lt;li>Java: Apache Commons Codec: 1.10&lt;/li>
&lt;li>Python: avro&amp;gt;=1.8.1,&amp;lt;2.0.0; python_version &amp;lt; &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: avro-python3&amp;gt;=1.8.1,&amp;lt;2.0.0; python_version &amp;gt;= &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: bigdataoss_gcsio 1.9.12&lt;/li>
&lt;li>Python: dill&amp;gt;=0.2.9,&amp;lt;0.2.10&lt;/li>
&lt;li>Python: gcsio 1.9.13&lt;/li>
&lt;li>Python: google-cloud-pubsub 0.39.0&lt;/li>
&lt;li>Python: pytz&amp;gt;=2018.3&lt;/li>
&lt;li>Python: pyyaml&amp;gt;=3.12,&amp;lt;4.0.0&lt;/li>
&lt;li>MongoDbIO: mongo client 3.9.1&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>ParquetIO for Python SDK&lt;/li>
&lt;li>HadoopOutputFormatIO: Add batching support&lt;/li>
&lt;li>HadoopOutputFormatIO: Add streaming support&lt;/li>
&lt;li>MongoDbIO: Add projections&lt;/li>
&lt;li>MongoDbIO: Add support for server with self signed SSL&lt;/li>
&lt;li>MongoDbIO add ordered option (inserts documents even if errors)&lt;/li>
&lt;li>KafkaIO: Add support to write to multiple topics&lt;/li>
&lt;li>KafkaIO: add writing support with ProducerRecord&lt;/li>
&lt;li>CassandraIO: Add ability to delete data&lt;/li>
&lt;li>JdbcIO: Add ValueProvider support for Statement in JdbcIO.write(), so it can be templatized&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>FlinkRunner: support Flink config directory&lt;/li>
&lt;li>FlinkRunner: master url now supports IPv6 addresses&lt;/li>
&lt;li>FlinkRunner: portable runner savepoint / upgrade support&lt;/li>
&lt;li>FlinkRunner: can be built against different Flink versions&lt;/li>
&lt;li>FlinkRunner: Send metrics to Flink in portable runner&lt;/li>
&lt;li>Java: Migrate to vendored gRPC (no conflicts with user gRPC, smaller jars)&lt;/li>
&lt;li>Java: Migrate to vendored Guava (no conflicts with user Guava, smaller jars)&lt;/li>
&lt;li>SQL: support joining unbounded to bounded sources via side input (and is no longer sensitive to left vs right join)&lt;/li>
&lt;li>SQL: support table macro&lt;/li>
&lt;li>Schemas: support for Avro, with automatic schema registration&lt;/li>
&lt;li>Schemas: Automatic schema registration for AutoValue classes&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Watch PTransform fixed (affects FileIO)&lt;/li>
&lt;li>FlinkRunner: no longer fails if GroupByKey contains null values (streaming mode only)&lt;/li>
&lt;li>FlinkRunner: no longer prepares to-be-staged file too late&lt;/li>
&lt;li>FlinkRunner: sets number of shards for writes with runner determined sharding&lt;/li>
&lt;li>FlinkRunner: prevents CheckpointMarks from not getting acknowledged&lt;/li>
&lt;li>Schemas: Generated row object for POJOs, Avros, and JavaBeans should work if the wrapped class is package private&lt;/li>
&lt;li>Schemas: Nested collection types in schemas no longer cause NullPointerException when converting to a POJO&lt;/li>
&lt;li>BigQueryIO: now handles quotaExceeded errors properly&lt;/li>
&lt;li>BigQueryIO: now handles triggering correctly in certain very large load jobs&lt;/li>
&lt;li>FileIO and other file-based IOs: Beam LocalFilesystem now matches glob patterns in windows&lt;/li>
&lt;li>SQL: joins no longer moves timestamps to the end of the window&lt;/li>
&lt;li>SQL: was missing some transitive dependencies&lt;/li>
&lt;li>SQL: JDBC driver no longer breaks interactions with other JDBC sources&lt;/li>
&lt;li>pyarrow supported on Windows Python 2&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Deprecate HadoopInputFormatIO&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.10.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Anton Kedin, Rui Wang,
Andrew Brampton Andrew Pilloud, Ankur Goenka, Antonio D&amp;rsquo;souza, Bingfeng Shu,
Boyuan Zhang, brucearctor, Cade Markegard, Chaim Turkel, Chamikara Jayalath,
Charles Chen, Colm O hEigeartaigh, Cory, Craig Chambers, Cristian, Daniel
Mills, Daniel Oliveira, David Cavazos, David Hrbacek, David Moravek, Dawid
Wysakowicz, djhworld, Dustin Rhodes, Etienne Chauchot, Fabien Rousseau, Garrett
Jones, Gleb Kanterov, Heejong Lee, Ismaël Mejía, Jason Kuster, Jean-Baptiste
Onofré, Jeff Klukas, Joar Wandborg, Jozef Vilcek, Kadir Cetinkaya, Kasia
Kucharczyk, Kengo Seki, Kenneth Knowles, lcaggio, Lukasz Cwik, Łukasz Gajowy,
Manu Zhang, marek.simunek, Mark Daoust, Mark Liu, Maximilian Michels, Melissa
Pashniak, Michael Luckey, Mikhail Gryzykhin, mlotstein, morokosi, Niel
Markwick, Pablo Estrada, Prem Kumar Karunakaran, Reuven Lax, robbe, Robbe
Sneyders, Robert Bradshaw, Robert Burke, Ruoyun Huang, Ryan Williams, Sam
Whittle, Scott Wegner, Slava Chernyak, Theodore Siu, Thomas Weise, Udi Meiri,
&lt;a href="mailto:vaclav.plajt@gmail.com">vaclav.plajt@gmail.com&lt;/a>, Valentyn Tymofieiev, Won Wook SONG, Wout Scheepers,
Xinyu Liu, Yueyang Qiu, Zhuo Peng&lt;/p></description></item><item><title>Blog: Apache Beam 2.9.0</title><link>/blog/beam-2.9.0/</link><pubDate>Thu, 13 Dec 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.9.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.9.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#290-2018-12-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.9.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344258">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h3 id="dependency-upgrades">Dependency Upgrades&lt;/h3>
&lt;ul>
&lt;li>Update google-api-client libraries to 1.27.0.&lt;/li>
&lt;li>Update byte-buddy to 1.9.3&lt;/li>
&lt;li>Update Flink Runner to 1.5.5&lt;/li>
&lt;li>Upgrade google-apitools to 0.5.24&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>Added support for user state and timers to Flink runner.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>I/O connector for RabbitMQ.&lt;/li>
&lt;li>Update SpannerIO to support unbounded writes.&lt;/li>
&lt;li>Add PFADD method to RedisIO.&lt;/li>
&lt;/ul>
&lt;h3 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h3>
&lt;ul>
&lt;li>Dataflow runner was updated to &lt;strong>not&lt;/strong> use &lt;a href="https://github.com/google/conscrypt">Conscrypt&lt;/a> as the default security provider.&lt;/li>
&lt;li>Support set/delete of timers by ID in Flink runner.&lt;/li>
&lt;li>Improvements to stabilize integration tests.&lt;/li>
&lt;li>Updates Spark runner to show Beam metrics in web UI&lt;/li>
&lt;li>Vendor gRPC and Protobuf separately from beam-model-* Java packages&lt;/li>
&lt;li>Avoid reshuffle for zero and one element creates&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.9.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Horky, Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Andrea Foegler, Andrew Fulton, Andrew Pilloud, Ankur Goenka, Anton Kedin, Babu, Ben Song, Bingfeng Shu, Boyuan Zhang, Brian Martin, Brian Quinlan, Chamikara Jayalath, Charles Chen, Christian Schneider, Colm O hEigeartaigh, Cory Brzycki, CraigChambersG, Daniel Oliveira, David Moravek, Dusan Rychnovsky, Etienne Chauchot, Eugene Kirpichov, Fabien Rousseau, Gleb Kanterov, Heejong Lee, Henning Rohde, Ismaël Mejía, Jan Lukavský, Jaromir Vanek, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, Jeroen Steggink, Julien Tournay, Jára Vaněk, Katarzyna Kucharczyk, Keisuke Kondo, Kenneth Knowles, Liam Miller-Cushon, Luke Cwik, Manu Zhang, Mark Liu, Maximilian Michels, Melissa Pashniak, Micah Wylde, Michael Luckey, Mike Pedersen, Mikhail Gryzykhin, Novotnik, Petr, Ondrej Kvasnicka, Pablo Estrada, Pavel Slechta, Raghu Angadi, Reuven Lax, Robbe Sneyders, Robert Bradshaw, Robert Burke, Ruoyu Liu, Ruoyun Huang, Sam Rohde, Sam sam, Scott Wegner, Simon Plovyt, Thomas Weise, Tim Robertson, Tomas Novak, Udi Meiri, Vaclav Plajt, Valentyn Tymofieiev, Varun Dhussa, Vojtech Janota, Wout Scheepers, Xinyu Liu, XuMingmin, Yifan Zou, Yueyang Qiu, akedin, amaliujia, connelloG, flyisland, huygaa11, jasonkuster, jglezt, kkpoon, mareksimunek, matthiasa4, melissa, mingmxu, nielm, reuvenlax, robbe, ruoyu90, splovyt, svXaverius, &lt;a href="mailto:vaclav.plajt@gmail.com">vaclav.plajt@gmail.com&lt;/a>, xinyuiscool, xitep, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Inaugural edition of the Beam Summit Europe 2018 - aftermath</title><link>/blog/beam-summit-aftermath/</link><pubDate>Wed, 31 Oct 2018 00:00:01 -0800</pubDate><guid>/blog/beam-summit-aftermath/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Almost 1 month ago, we had the pleasure to welcome the Beam community at Level39 in London for the inaugural edition of the Beam Summit London Summit.&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en">&lt;p lang="en" dir="ltr">Day 1 of the first Beam Summit London going full speed ahead! Sessions by &lt;a href="https://twitter.com/SkyUK?ref_src=twsrc%5Etfw">@SkyUK&lt;/a> &lt;a href="https://twitter.com/GCPcloud?ref_src=twsrc%5Etfw">@GCPcloud&lt;/a> &lt;a href="https://twitter.com/Talend?ref_src=twsrc%5Etfw">@Talend&lt;/a> &lt;a href="https://twitter.com/PlantixApp?ref_src=twsrc%5Etfw">@PlantixApp&lt;/a> and more! &lt;a href="https://twitter.com/hashtag/ApacheBeam?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ApacheBeam&lt;/a> &lt;a href="https://twitter.com/hashtag/apachebeamlondon?src=hash&amp;amp;ref_src=twsrc%5Etfw">#apachebeamlondon&lt;/a> &lt;a href="https://twitter.com/hashtag/l39?src=hash&amp;amp;ref_src=twsrc%5Etfw">#l39&lt;/a> &lt;a href="https://twitter.com/hashtag/level39?src=hash&amp;amp;ref_src=twsrc%5Etfw">#level39&lt;/a> &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> &lt;a href="https://twitter.com/hashtag/summit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#summit&lt;/a> &lt;a href="https://t.co/aEESFnbgxT">pic.twitter.com/aEESFnbgxT&lt;/a>&lt;/p>&amp;mdash; Matthias Baetens 🌆 (@matthiasbaetens) &lt;a href="https://twitter.com/matthiasbaetens/status/1046756260996149248?ref_src=twsrc%5Etfw">October 1, 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="first-edition">First edition!&lt;/h2>
&lt;p>This first edition of the summit was a free event, with over 125 RSVPs. We had two days of content; day one was focused on the roadmap of the project, the ASF and use cases from companies that use Beam. The second day was divided into tracks (a beginner and an advanced track). Those presentations &amp;amp; workshops were organised for the more than &lt;strong>80 attendees&lt;/strong> - and next to that there were several other activities like discussions, a brainstorm session, a UX booth and a signing session. We were able to offer the attendees &lt;strong>17 sessions&lt;/strong> from a great line-up of companies:
Google, Spotify, Talend, Sky, Amazon, Data Artisans, Datatonic, Vente Exclusive, ML6, Flumaion, Plantix, Polidea, Seznam and more!&lt;/p>
&lt;br/>
#### Topics included using Python to run Beam on Flink:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Don&amp;#39;t miss &lt;a href="https://twitter.com/snntrable?ref_src=twsrc%5Etfw">@snntrable&lt;/a>&amp;#39;s session at Beam Sumit London, Oct. 2, 2018, about &lt;a href="https://twitter.com/hashtag/Python?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Python&lt;/a> Streaming Pipelines with &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> and &lt;a href="https://twitter.com/ApacheFlink?ref_src=twsrc%5Etfw">@ApacheFlink&lt;/a>! Register here: &lt;a href="https://t.co/wblzUeiTIg">https://t.co/wblzUeiTIg&lt;/a> &lt;a href="https://twitter.com/hashtag/streamprocessing?src=hash&amp;amp;ref_src=twsrc%5Etfw">#streamprocessing&lt;/a> &lt;a href="https://twitter.com/hashtag/BigData?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BigData&lt;/a> &lt;a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw">#MachineLearning&lt;/a> &lt;a href="https://t.co/7Ph51WqspW">pic.twitter.com/7Ph51WqspW&lt;/a>&lt;/p>&amp;mdash; data Artisans (@dataArtisans) &lt;a href="https://twitter.com/dataArtisans/status/1044967266817847296?ref_src=twsrc%5Etfw">26 september 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;br/>
#### ML with Beam with the TensorFlow transform integration:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Such a great pleasure to listen to the talk by &lt;a href="https://twitter.com/FsMatt?ref_src=twsrc%5Etfw">@FsMatt&lt;/a> on TensorFlow transform at the &lt;a href="https://twitter.com/hashtag/BeamSummit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BeamSummit&lt;/a>! &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> &lt;a href="https://twitter.com/TensorFlow?ref_src=twsrc%5Etfw">@TensorFlow&lt;/a> &lt;a href="https://twitter.com/Level39CW?ref_src=twsrc%5Etfw">@Level39CW&lt;/a> &lt;br>&lt;br>Check the code for his demo at &lt;a href="https://t.co/8IS41A2aF2">https://t.co/8IS41A2aF2&lt;/a> &lt;a href="https://t.co/UWytiudmRO">https://t.co/UWytiudmRO&lt;/a>&lt;/p>&amp;mdash; datatonic (@teamdatatonic) &lt;a href="https://twitter.com/teamdatatonic/status/1047126173493469184?ref_src=twsrc%5Etfw">2 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;br/>
#### The portability layer was a big topic:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Excellent talk by &lt;a href="https://twitter.com/stadtlegende?ref_src=twsrc%5Etfw">@stadtlegende&lt;/a> on adding portability to &lt;a href="https://twitter.com/hashtag/ApacheBeam?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ApacheBeam&lt;/a>, awesome milestone and next step to make the Apache Beam vision become a reality! &lt;a href="https://t.co/M9jERlTeAE">pic.twitter.com/M9jERlTeAE&lt;/a>&lt;/p>&amp;mdash; Matthias Feys (@FsMatt) &lt;a href="https://twitter.com/FsMatt/status/1047105336841244673?ref_src=twsrc%5Etfw">2 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;br/>
#### As well as a session on how to build your own SDK:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Robert Bredshaw explains how to build a new &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> SDK.&lt;a href="https://twitter.com/hashtag/BeamSummit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BeamSummit&lt;/a> &lt;a href="https://t.co/Bj84GJimdo">pic.twitter.com/Bj84GJimdo&lt;/a>&lt;/p>&amp;mdash; Maximilian Michels 🧗 (@stadtlegende) &lt;a href="https://twitter.com/stadtlegende/status/1047139320195366912?ref_src=twsrc%5Etfw">2 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="presentations">Presentations&lt;/h2>
&lt;p>In the aftermath of the Summit, you can check the presentations of all the sessions.&lt;/p>
&lt;h3 id="day-1-use-cases">Day 1: Use cases&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/open?id=1hyHw7RVpFrFpli3vLt6JGBHrEm4BcgF-5nRdH1ZE8qo">Day 1 - Session 1 - Large scale stream analytics with Apache Beam at Sky&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1MxYrFDVoVFsrzbTtmr18zcbPFUU4nSdi">Day 1 - Session 2 - Running Quantitative Analytics with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSdVBJSnZrbTZjSGFHbnd4cExYOGZQU2hmY3lF">Day 1 - Session 3 - Talend Data Streams: Building Big Data pipelines with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1-GIUVn9QBtg6t-O8uINDkMO4PyZSU_HAEjMWuUHiYY4">Day 1 - Session 4 - Lesson Learned from Migrating to Apache Beam for Geo-Data Visualisation&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="day-2-beginners-track">Day 2: Beginners track&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/open?id=1ntQEDhb8gkxof4uFftxWTOfN39laUDZU7IDHTPKWrcQ">Day 2 - Beginner - Session 1 - Development Environment with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSWWJBWXV3ZTdseWpJN1o5UFdpSzV4Qi1sSGU0">Day 2 - Beginner - Session 2 - Towards Portability and Beyond&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSLTd6TFlYdFZZYjBTOFZQV3MxZzlPLWROWjZv">Day 2 - Beginner - Session 3 - Python Streaming Pipelines with Beam on Flink&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSMEV1a1cwM3ozeWQ4TkxlS0tFcnNtRGNGcjJ3">Day 2 - Beginner - Session 4 - How runners execute a Beam pipeline&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1QyqO8zJ3fIWD5DTnr1JNCEbm2dS15c1c02fI8zD-zqY">Day 2 - Beginner - Session 5 - IO Integration Testing framework in Apache Beam&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="day-2-advanced-track">Day 2: Advanced track&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/open?id=1Kr1skutObtDil2CExSQUb5rCVwZQm1m2lpmuAXFCE5I">Day 2 - Advanced - Session 1 - Pre-processing for TensorFlow pipelines with Apache Beam &amp;amp; tf.Transform&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=11x7gtuAxg76nOQKaB0YOwcvzS4TUeWONTU1ZQK0LsX8">Day 2 - Advanced - Session 2 - Streaming data into BigQuery: schema generation with Protobuf&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1cgQGBIXaACSwbYu_w3AkvvTdsCfeXAS1tBvQ77eVn74">Day 2 - Advanced - Session 3 - Implementing a SplittableParDo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/presentation/d/1F02Lwnqm9H3cGqDQhIZ3gbftyLQSnVMRxX69H_d04OE/edit?usp=sharing">Day 2 - Advanced - Session 4 - Big Data on Google Cloud with Scala and Scio&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1D1ajcKoOR5OzehPwONdHLSzpO4PZOsLk">Day 2 - Advanced - Session 5 - Landuse Classification of Satellite Imagery&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1aFH6lhnVIq4Alu-_HItQ0QOddEPJQRqI5jV_t0o3CYI">Day 2 - Advanced - Session 6 - Java 8 DSL for Beam SDK&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1AkU-QXSflau-RSeolB4TSLy0_mg0xwb398Czw7aqVGw">Day 2 - Advanced - Session 7 - So, You Want to Write a Beam SDK?&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="recordings">Recordings&lt;/h2>
&lt;p>In case you prefer rewatching the recorded talks together with those slides, we are also happy to share the recordings of the majority of the sessions:&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/videoseries?list=PL4dEBWmGSIU_9JTGnkGVg6-BwaV0FMxyJ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;h3 id="day-1-use-cases-1">Day 1: Use cases&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/En0FrjvNr3M">Day 1 - Session 1 - Large scale stream analytics with Apache Beam at Sky&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/6yDEOUophuw">Day 1 - Session 2 - Running Quantitative Analytics with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/1AlEGUtiQek">Day 1 - Session 3 - Talend Data Streams: Building Big Data pipelines with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/GBKqw03doHE">Day 1 - Session 4 - Lesson Learned from Migrating to Apache Beam for Geo-Data Visualisation&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="day-2-advanced-track-1">Day 2: Advanced track&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/L-k6-3ApXR4">Day 2 - Advanced - Session 1 - Pre-processing for TensorFlow pipelines with Apache Beam &amp;amp; tf.Transform&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/ctN5U_Ke8uk">Day 2 - Advanced - Session 2 - Streaming data into BigQuery: schema generation with Protobuf&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/jU6EmPyKefg">Day 2 - Advanced - Session 3 - Implementing a SplittableParDo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/F0n9sqj1_NQ">Day 2 - Advanced - Session 4 - Big Data on Google Cloud with Scala and Scio&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/s-IR2eFe4B4">Day 2 - Advanced - Session 5 - Landuse Classification of Satellite Imagery&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/ott1e_CnZ04">Day 2 - Advanced - Session 6 - Java 8 DSL for Beam SDK&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/VsGQ2LFeTHY">Day 2 - Advanced - Session 7 - So, You Want to Write a Beam SDK?&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>We are also gathering feedback and thoughts on the Summit - please add your thoughts and discussions to the &lt;a href="https://lists.apache.org/thread.html/aa1306da25029dff12a49ba3ce63f2caf6a5f8ba73eda879c8403f3f@%3Cdev.beam.apache.org%3E">topic on the mailing list&lt;/a>.&lt;/p>
&lt;p>Overall, we hope our attendees enjoyed this first edition of our summit and want to thank &lt;strong>our sponsors Google, Datatonic, Vente-Exclusive&lt;/strong> to make this possible.&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Wrapping up the first day of the &lt;a href="https://twitter.com/hashtag/BeamSummit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BeamSummit&lt;/a>. Excellent view from the &lt;a href="https://twitter.com/hashtag/level39?src=hash&amp;amp;ref_src=twsrc%5Etfw">#level39&lt;/a> venue. Very happy with the line up. &lt;a href="https://t.co/7FhokKbQY5">pic.twitter.com/7FhokKbQY5&lt;/a>&lt;/p>&amp;mdash; Alex Van Boxel (@alexvb) &lt;a href="https://twitter.com/alexvb/status/1046803829650608129?ref_src=twsrc%5Etfw">1 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Blog: Apache Beam 2.8.0</title><link>/blog/beam-2.8.0/</link><pubDate>Mon, 29 Oct 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.8.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.8.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#280-2018-10-26">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.8.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343985">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-4783">BEAM-4783&lt;/a> Performance degradations in certain situations when Spark runner is used.&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-upgrades">Dependency Upgrades&lt;/h3>
&lt;ul>
&lt;li>Elastic Search dependency upgraded to 6.3.2&lt;/li>
&lt;li>google-cloud-pubsub dependency upgraded to 0.35.4&lt;/li>
&lt;li>google-api-client dependency upgraded to 1.24.1&lt;/li>
&lt;li>Updated Flink Runner to 1.5.3&lt;/li>
&lt;li>Updated Spark runner to Spark version 2.3.2&lt;/li>
&lt;/ul>
&lt;h3 id="sdks">SDKs&lt;/h3>
&lt;ul>
&lt;li>Python SDK added support for user state and timers.&lt;/li>
&lt;li>Go SDK added support for side inputs.&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/roadmap/portability/#python-on-flink">Python on Flink MVP&lt;/a> completed.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Fixes to RedisIO non-prefix read operations.&lt;/li>
&lt;/ul>
&lt;h2 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h2>
&lt;ul>
&lt;li>Several bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.8.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Horky, Ahmet Altay, Alan Myrvold, Aleksandr Kokhaniukov,
Alex Amato, Alexey Romanenko, Aljoscha Krettek, Andrew Fulton,
Andrew Pilloud, Ankur Goenka, Anton Kedin, Babu, Batkhuyag Batsaikhan, Ben Song,
Bingfeng Shu, Boyuan Zhang, Chamikara Jayalath, Charles Chen,
Christian Schneider, Cody Schroeder, Colm O hEigeartaigh, Daniel Mills,
Daniel Oliveira, Dat Tran, David Moravek, Dusan Rychnovsky, Etienne Chauchot,
Eugene Kirpichov, Gleb Kanterov, Heejong Lee, Henning Rohde, Ismaël Mejía,
Jan Lukavský, Jaromir Vanek, Jean-Baptiste Onofré, Jeff Klukas, Joar Wandborg,
Jozef Vilcek, Julien Phalip, Julien Tournay, Juta, Jára Vaněk,
Katarzyna Kucharczyk, Kengo Seki, Kenneth Knowles, Kevin Si, Kirill Kozlov,
Kyle Winkelman, Logan HAUSPIE, Lukasz Cwik, Manu Zhang, Mark Liu,
Matthias Baetens, Matthias Feys, Maximilian Michels, Melissa Pashniak,
Micah Wylde, Michael Luckey, Mike Pedersen, Mikhail Gryzykhin, Novotnik,
Petr, Ondrej Kvasnicka, Pablo Estrada, PaulVelthuis93, Pavel Slechta,
Rafael Fernández, Raghu Angadi, Renat, Reuven Lax, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Rodrigo Benenson, Rong Ou, Ruoyun Huang,
Ryan Williams, Sam Rohde, Scott Wegner, Shinsuke Sugaya, Shnitz, Simon P,
Sindy Li, Stephen Lumenta, Stijn Decubber, Thomas Weise, Tomas Novak,
Tomas Roos, Udi Meiri, Vaclav Plajt, Valentyn Tymofieiev, Vitalii Tverdokhlib,
Xinyu Liu, XuMingmin, Yifan Zou, Yuan, Yueyang Qiu, aalbatross, amaliujia,
cclauss, connelloG, daidokoro, deepyaman, djhworld, flyisland, huygaa11,
jasonkuster, jglezt, kkpoon, mareksimunek, nielm, svXaverius, timrobertson100,
&lt;a href="mailto:vaclav.plajt@gmail.com">vaclav.plajt@gmail.com&lt;/a>, vitaliytv, vvarma, xiliu, xinyuiscool, xitep,
Łukasz Gajowy.&lt;/p></description></item><item><title>Blog: Apache Beam 2.7.0</title><link>/blog/beam-2.7.0/</link><pubDate>Wed, 03 Oct 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.7.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.7.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#270-lts-2018-10-02">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.7.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343654">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h3 id="new-ios">New I/Os&lt;/h3>
&lt;ul>
&lt;li>KuduIO&lt;/li>
&lt;li>Amazon SNS sink&lt;/li>
&lt;li>Amazon SqsIO&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-upgrades">Dependency Upgrades&lt;/h3>
&lt;ul>
&lt;li>Apache Calcite dependency upgraded to 1.17.0&lt;/li>
&lt;li>Apache Derby dependency upgraded to 10.14.2.0&lt;/li>
&lt;li>Apache HTTP components upgraded (see release notes).&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>Experimental support for Python on local Flink runner for simple
examples, see latest information &lt;a href="/contribute/portability/#status">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>KinesisIO, fixed dependency issue&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following 72 people contributed
to the 2.7.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alexey Romanenko, Aljoscha Krettek,
Andrew Pilloud, Ankit Jhalaria, Ankur Goenka, Anton Kedin, Boyuan
Zhang, Carl McGraw, Carlos Alonso, cclauss, Chamikara Jayalath,
Charles Chen, Cory Brzycki, Daniel Oliveira, Dariusz Aniszewski,
devinduan, Eric Beach, Etienne Chauchot, Eugene Kirpichov, Garrett
Jones, Gene Peters, Gleb Kanterov, Henning Rohde, Henry Suryawirawan,
Holden Karau, Huygaa Batsaikhan, Ismaël Mejía, Jason Kuster, Jean-
Baptiste Onofré, Joachim van der Herten, Jozef Vilcek, jxlewis, Kai
Jiang, Katarzyna Kucharczyk, Kenn Knowles, Krzysztof Trubalski, Kyle
Winkelman, Leen Toelen, Luis Enrique Ortíz Ramirez, Lukasz Cwik,
Łukasz Gajowy, Luke Cwik, Mark Liu, Matthias Feys, Maximilian Michels,
Melissa Pashniak, Mikhail Gryzykhin, Mikhail Sokolov, mingmxu, Norbert
Chen, Pablo Estrada, Prateek Chanda, Raghu Angadi, Ravi Pathak, Reuven
Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ryan Williams, Sindy Li,
Thomas Weise, Tim Robertson, Tormod Haavi, Udi Meiri, Vaclav Plajt,
Valentyn Tymofieiev, xiliu, XuMingmin, Yifan Zou, Yueyang Qiu.&lt;/p></description></item><item><title>Blog: Beam Summit Europe 2018</title><link>/blog/beam-summit-europe/</link><pubDate>Tue, 21 Aug 2018 00:00:01 -0800</pubDate><guid>/blog/beam-summit-europe/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>With a growing community of contributors and users, the Apache Beam project is organising the first European Beam Summit.&lt;/p>
&lt;p>We are happy to invite you to this event, which will take place in &lt;strong>London&lt;/strong> on &lt;strong>October 1st and 2nd of 2018&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="/images/blog/Facebook-AD.png" alt="Beam Summit Europe 2018 flyer" height="360" width="640" >&lt;/p>
&lt;h3 id="what-is-the-beam-summit-2018">What is the Beam Summit 2018?&lt;/h3>
&lt;p>The summit is a 2 day, multi-track event.&lt;/p>
&lt;p>During the first day we’ll host sessions to share use cases from companies using Apache Beam, community driven talks, and a session to discuss the project&amp;rsquo;s roadmap (from the main partners in the project as well as all users planning to contribute to the project and wanting to share their plans). We&amp;rsquo;ll also have break-out sessions that will allow cross team collaboration in multiple sub-topics.&lt;/p>
&lt;p>The second day will be a &amp;ldquo;hands-on&amp;rdquo; day. We will offer an introductory session to Apache Beam. Additionally, we&amp;rsquo;ll host an advanced track for more advanced users with open-table discussions about more complex and newer Apache Beam features.&lt;/p>
&lt;p>The agenda will grow and be communicated in the coming month, keep an eye on the page.&lt;/p>
&lt;h3 id="event-details">Event Details&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Venue&lt;/strong>: &lt;a href="https://goo.gl/maps/LAC4haDzSzR2">Level39, One Canada Square, Canary Wharf, London E14 5AB&lt;/a>&lt;/li>
&lt;li>&lt;strong>Dates&lt;/strong>: 1-2 October 2018&lt;/li>
&lt;/ul>
&lt;h3 id="how-do-i-register">How do I register?&lt;/h3>
&lt;p>You can register for free on the &lt;a href="https://www.eventbrite.com/e/beam-summit-london-2018-tickets-49100625292#tickets">Eventbrite registration page&lt;/a>.&lt;/p>
&lt;h3 id="i-am-interested-in-speaking-how-do-i-propose-my-session">I am interested in speaking, how do I propose my session?&lt;/h3>
&lt;p>With this we are also launching a Call for Papers in case you want to secure a slot for one of the sessions. Please fill out the &lt;a href="https://goo.gl/forms/nrZOCC1JwEfLtKfA2">CfP form&lt;/a>.&lt;/p>
&lt;h3 id="id-love-to-get-involved-as-a-volunteer-or-sponsor">I&amp;rsquo;d love to get involved as a volunteer or sponsor&lt;/h3>
&lt;p>Furthermore, in order to keep this event free, we are looking for people to help out at and/or sponsor some parts of the conference. If you (or your company) are interested to help out, please reach out to: &lt;a href="mailto:baetensmatthias@gmail.com">baetensmatthias@gmail.com&lt;/a> or &lt;a href="mailto:alex@vanboxel.be">alex@vanboxel.be&lt;/a>. You can find more info in the &lt;a href="https://drive.google.com/file/d/1RnZ52rGaB6BR-EKneBcabdMcg9Pl7z9M">sponsor booklet&lt;/a>&lt;/p>
&lt;p>Thanks, and we hope to see you at the event!
The Events &amp;amp; Meetups Group&lt;/p></description></item><item><title>Blog: A review of input streaming connectors</title><link>/blog/review-input-streaming-connectors/</link><pubDate>Mon, 20 Aug 2018 00:00:01 -0800</pubDate><guid>/blog/review-input-streaming-connectors/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this post, you&amp;rsquo;ll learn about the current state of support for input streaming connectors in &lt;a href="/">Apache Beam&lt;/a>. For more context, you&amp;rsquo;ll also learn about the corresponding state of support in &lt;a href="https://spark.apache.org/">Apache Spark&lt;/a>.&lt;/p>
&lt;p>With batch processing, you might load data from any source, including a database system. Even if there are no specific SDKs available for those database systems, you can often resort to using a &lt;a href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC&lt;/a> driver. With streaming, implementing a proper data pipeline is arguably more challenging as generally fewer source types are available. For that reason, this article particularly focuses on the streaming use case.&lt;/p>
&lt;h2 id="connectors-for-java">Connectors for Java&lt;/h2>
&lt;p>Beam has an official &lt;a href="/documentation/sdks/java/">Java SDK&lt;/a> and has several execution engines, called &lt;a href="/documentation/runners/capability-matrix/">runners&lt;/a>. In most cases it is fairly easy to transfer existing Beam pipelines written in Java or Scala to a Spark environment by using the &lt;a href="/documentation/runners/spark/">Spark Runner&lt;/a>.&lt;/p>
&lt;p>Spark is written in Scala and has a &lt;a href="https://spark.apache.org/docs/latest/api/java/">Java API&lt;/a>. Spark&amp;rsquo;s source code compiles to &lt;a href="https://en.wikipedia.org/wiki/Java_(programming_language)#Java_JVM_and_Bytecode">Java bytecode&lt;/a> and the binaries are run by a &lt;a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java Virtual Machine&lt;/a>. Scala code is interoperable with Java and therefore has native compatibility with Java libraries (and vice versa).&lt;/p>
&lt;p>Spark offers two approaches to streaming: &lt;a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Discretized Streaming&lt;/a> (or DStreams) and &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming&lt;/a>. DStreams are a basic abstraction that represents a continuous series of &lt;a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">Resilient Distributed Datasets&lt;/a> (or RDDs). Structured Streaming was introduced more recently (the alpha release came with Spark 2.1.0) and is based on a &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model">model&lt;/a> where live data is continuously appended to a table structure.&lt;/p>
&lt;p>Spark Structured Streaming supports &lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html">file sources&lt;/a> (local filesystems and HDFS-compatible systems like Cloud Storage or S3) and &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html">Kafka&lt;/a> as streaming &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources">inputs&lt;/a>. Spark maintains built-in connectors for DStreams aimed at third-party services, such as Kafka or Flume, while other connectors are available through linking external dependencies, as shown in the table below.&lt;/p>
&lt;p>Below are the main streaming input connectors for available for Beam and Spark DStreams in Java:&lt;/p>
&lt;table class="table table-bordered">
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;td>&lt;strong>Apache Beam&lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Apache Spark DStreams&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >File Systems
&lt;/td>
&lt;td>Local&lt;br>(Using the &lt;code>file://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/TextIO.html">TextIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/StreamingContext.html#textFileStream-java.lang.String-">textFileStream&lt;/a>&lt;br>(Spark treats most Unix systems as HDFS-compatible, but the location should be accessible from all nodes)
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HDFS&lt;br>(Using the &lt;code>hdfs://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/FileIO.html">FileIO&lt;/a> + &lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/hdfs/HadoopFileSystemOptions.html">HadoopFileSystemOptions&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/util/HdfsUtils.html">HdfsUtils&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >Object Stores
&lt;/td>
&lt;td>Cloud Storage&lt;br>(Using the &lt;code>gs://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/FileIO.html">FileIO&lt;/a> + &lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/extensions/gcp/options/GcsOptions.html">GcsOptions&lt;/a>
&lt;/td>
&lt;td rowspan="2" >&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopConfiguration--">hadoopConfiguration&lt;/a>
and &lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/StreamingContext.html#textFileStream-java.lang.String-">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>S3&lt;br>(Using the &lt;code>s3://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/FileIO.html">FileIO&lt;/a> + &lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/aws/options/S3Options.html">S3Options&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="3" >Messaging Queues
&lt;/td>
&lt;td>Kafka
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/kafka/KafkaIO.html">KafkaIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html">spark-streaming-kafka&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kinesis
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/kinesis/KinesisIO.html">KinesisIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/streaming-kinesis-integration.html">spark-streaming-kinesis&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cloud Pub/Sub
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.html">PubsubIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/bahir/tree/master/streaming-pubsub">spark-streaming-pubsub&lt;/a> from &lt;a href="https://bahir.apache.org">Apache Bahir&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other
&lt;/td>
&lt;td>Custom receivers
&lt;/td>
&lt;td>&lt;a href="/documentation/io/developing-io-overview/">Read Transforms&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/streaming-custom-receivers.html">receiverStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h2 id="connectors-for-python">Connectors for Python&lt;/h2>
&lt;p>Beam has an official &lt;a href="/documentation/sdks/python/">Python SDK&lt;/a> that currently supports a subset of the streaming features available in the Java SDK. Active development is underway to bridge the gap between the featuresets in the two SDKs. Currently for Python, the &lt;a href="/documentation/runners/direct/">Direct Runner&lt;/a> and &lt;a href="/documentation/runners/dataflow/">Dataflow Runner&lt;/a> are supported, and &lt;a href="/documentation/sdks/python-streaming/">several streaming options&lt;/a> were introduced in beta in &lt;a href="/blog/2018/06/26/beam-2.5.0.html">version 2.5.0&lt;/a>.&lt;/p>
&lt;p>Spark also has a Python SDK called &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.html">PySpark&lt;/a>. As mentioned earlier, Scala code compiles to a bytecode that is executed by the JVM. PySpark uses &lt;a href="https://www.py4j.org/">Py4J&lt;/a>, a library that enables Python programs to interact with the JVM and therefore access Java libraries, interact with Java objects, and register callbacks from Java. This allows PySpark to access native Spark objects like RDDs. Spark Structured Streaming supports &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">file sources&lt;/a> (local filesystems and HDFS-compatible systems like Cloud Storage or S3) and &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html">Kafka&lt;/a> as streaming inputs.&lt;/p>
&lt;p>Below are the main streaming input connectors for available for Beam and Spark DStreams in Python:&lt;/p>
&lt;table class="table table-bordered">
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;td>&lt;strong>Apache Beam&lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Apache Spark DStreams&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >File Systems
&lt;/td>
&lt;td>Local
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.23.0/apache_beam.io.textio.html">io.textio&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.textFileStream">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HDFS
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.23.0/apache_beam.io.hadoopfilesystem.html">io.hadoopfilesystem&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopConfiguration--">hadoopConfiguration&lt;/a> (Access through &lt;code>sc._jsc&lt;/code> with Py4J)
and &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.textFileStream">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >Object stores
&lt;/td>
&lt;td>Google Cloud Storage
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.23.0/apache_beam.io.gcp.gcsio.html">io.gcp.gcsio&lt;/a>
&lt;/td>
&lt;td rowspan="2" >&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.textFileStream">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>S3
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="3" >Messaging Queues
&lt;/td>
&lt;td>Kafka
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils">KafkaUtils&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kinesis
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#module-pyspark.streaming.kinesis">KinesisUtils&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cloud Pub/Sub
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.23.0/apache_beam.io.gcp.pubsub.html">io.gcp.pubsub&lt;/a>
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other
&lt;/td>
&lt;td>Custom receivers
&lt;/td>
&lt;td>&lt;a href="/documentation/sdks/python-custom-io/">BoundedSource and RangeTracker&lt;/a>
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h2 id="connectors-for-other-languages">Connectors for other languages&lt;/h2>
&lt;h3 id="scala">Scala&lt;/h3>
&lt;p>Since Scala code is interoperable with Java and therefore has native compatibility with Java libraries (and vice versa), you can use the same Java connectors described above in your Scala programs. Apache Beam also has a &lt;a href="https://github.com/spotify/scio">Scala API&lt;/a> open-sourced &lt;a href="https://labs.spotify.com/2017/10/16/big-data-processing-at-spotify-the-road-to-scio-part-1/">by Spotify&lt;/a>.&lt;/p>
&lt;h3 id="go">Go&lt;/h3>
&lt;p>A &lt;a href="/documentation/sdks/go/">Go SDK&lt;/a> for Apache Beam is under active development. It is currently experimental and is not recommended for production. Spark does not have an official Go SDK.&lt;/p>
&lt;h3 id="r">R&lt;/h3>
&lt;p>Apache Beam does not have an official R SDK. Spark Structured Streaming is supported by an &lt;a href="https://spark.apache.org/docs/latest/sparkr.html#structured-streaming">R SDK&lt;/a>, but only for &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources">file sources&lt;/a> as a streaming input.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>We hope this article inspired you to try new and interesting ways of connecting streaming sources to your Beam pipelines!&lt;/p>
&lt;p>Check out the following links for further information:&lt;/p>
&lt;ul>
&lt;li>See a full list of all built-in and in-progress &lt;a href="/documentation/io/built-in/">I/O Transforms&lt;/a> for Apache Beam.&lt;/li>
&lt;li>Learn about some Apache Beam mobile gaming pipeline &lt;a href="/get-started/mobile-gaming-example/">examples&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam 2.6.0</title><link>/blog/beam-2.6.0/</link><pubDate>Fri, 10 Aug 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.6.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are glad to present the new 2.6.0 release of Beam.
This release includes multiple fixes and new functionality, such as new features in SQL and portability.&lt;/p>
&lt;p>We also spent a significant amount of time automating the release and fixing continuous integration. For more information, check the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343392">release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h3 id="grpcprotobuf-shading">gRPC/Protobuf shading&lt;/h3>
&lt;ul>
&lt;li>&lt;code>gRPC/protobuf&lt;/code> is now shaded in the majority of Apache Beam
Java modules. A few modules which expose &lt;code>gRPC/protobuf&lt;/code> on the
API surface still maintain a direct dependency.&lt;/li>
&lt;/ul>
&lt;h3 id="beam-sql">Beam SQL&lt;/h3>
&lt;ul>
&lt;li>Added support for the &lt;code>EXISTS&lt;/code> and &lt;code>LIKE&lt;/code> operators.&lt;/li>
&lt;li>Implemented &lt;code>SUM()&lt;/code> aggregations.&lt;/li>
&lt;li>Fixed issues with the &lt;code>CASE&lt;/code> expression.&lt;/li>
&lt;li>Added support for date comparisons.&lt;/li>
&lt;li>Added unbounded data support to &lt;code>LIMIT&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>Shared libraries for supporting timers and user state
are now available for runner integration.&lt;/li>
&lt;li>Added a Universal Local Runner, which works on a single machine using portability and containerized SDK harnesses.&lt;/li>
&lt;li>The Flink Runner now accepts jobs using the Job API.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">IOs&lt;/h3>
&lt;ul>
&lt;li>Bounded &lt;code>SplittableDoFn&lt;/code> (SDF) support is now available in all
runners (SDF is the new I/O connector API).&lt;/li>
&lt;li>&lt;code>HBaseIO&lt;/code> is the first I/O supporting Bounded SDF (using
&lt;code>readAll&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h3 id="sdks">SDKs&lt;/h3>
&lt;ul>
&lt;li>Improved Python &lt;code>AvroIO&lt;/code> performance.&lt;/li>
&lt;li>Python &lt;code>AvroIO&lt;/code> has a &lt;code>use_fastavro&lt;/code> option that uses
&lt;code>fastavro&lt;/code> instead of &lt;code>apache/avro&lt;/code>, for a
&lt;a href="https://gist.github.com/ryan-williams/ede5ae61605e7ba6aa655071858ef52b">3-6x speedup&lt;/a>!&lt;/li>
&lt;/ul>
&lt;h3 id="other">Other&lt;/h3>
&lt;ul>
&lt;li>Updated various dependency versions.&lt;/li>
&lt;li>Improvements to stability, performance, and documentation.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following 39 people contributed
to the 2.6.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alexey Romanenko, Andrew Pilloud,
Ankur Goenka, Boyuan Zhang, Charles Chen, cclauss,
Daniel Oliveira, Elliott Brossard, Eric Beach,
Etienne Chauchot, Eugene Kirpichov, Henning Rohde,
Ismaël Mejía, Kai Jiang, Kasia, Kenneth Knowles, Luis Osa,
Lukasz Cwik, Maria Garcia Herrero, Mark Liu, Matthias Feys,
Pablo Estrada, Rafael Fernandez, Reuven Lax, Robert Bradshaw,
Robert Burke, Robin Qiu, Ryan Williams, Scott Wegner, Rui Weng,
Sergei Lebedev, Sindy Li, Thomas Weise, Udi Meiri,
Valentyn Tymofieiev, XuMingmin, and Yifan Zou.&lt;/p></description></item><item><title>Blog: Apache Beam 2.5.0</title><link>/blog/beam-2.5.0/</link><pubDate>Tue, 26 Jun 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.5.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are glad to present the new 2.5.0 release of Beam. This release includes
multiple fixes and new functionalities.&lt;/p>
&lt;p>For more information
please check the detailed release notes.&lt;/p>
&lt;h1 id="new-features--improvements">New Features / Improvements&lt;/h1>
&lt;h2 id="go-sdk-support">Go SDK support&lt;/h2>
&lt;p>The Go SDK has been officially accepted into the project, after an incubation period and community effort. Go pipelines run on Dataflow runner. More details are &lt;a href="/documentation/sdks/go/">here&lt;/a>.&lt;/p>
&lt;h2 id="parquet-support">Parquet support&lt;/h2>
&lt;p>Support for Apache Parquet format was added. It uses Parquet 1.10 release which, thanks to AvroParquerWriter&amp;rsquo;s API changes, allows FileIO.Sink implementation.&lt;/p>
&lt;h2 id="performanceintegration-tests">Performance/Integration Tests&lt;/h2>
&lt;ul>
&lt;li>Added new integration tests - HCatalogIOIT (Hive), HBaseIOIT, ParquetIOIT (with the IO itself, local filesystem, HDFS)&lt;/li>
&lt;li>Multinode (3 data node) HDFS cluster is used for running tests on HDFS.&lt;/li>
&lt;li>Several improvements on performance tests running and results analysis.&lt;/li>
&lt;li>Scaled up Kubernetes cluster from 1 to 3 nodes.&lt;/li>
&lt;li>Added metrics in Spark streaming.&lt;/li>
&lt;/ul>
&lt;h2 id="internal-build-system-migrated-to-gradle">Internal Build System: Migrated to Gradle&lt;/h2>
&lt;p>After a months-long community effort, the internal Beam build has been migrated from Maven to Gradle. The new build system was chosen because of dependency-driven build support, incremental build/test, and support for non-Java languages.&lt;/p>
&lt;h2 id="nexmark-improvements">Nexmark Improvements&lt;/h2>
&lt;ul>
&lt;li>Kafka support as a source/sink for events and results.&lt;/li>
&lt;li>Translation of some queries to Beam SQL.&lt;/li>
&lt;/ul>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;ul>
&lt;li>Support for MAP, ROW, ARRAY data types&lt;/li>
&lt;li>Support UNNEST on array fields&lt;/li>
&lt;li>Improved optimizations&lt;/li>
&lt;li>Upgrade Calcite to 1.16&lt;/li>
&lt;li>Support SQL on POJOs via automatic conversion&lt;/li>
&lt;li>Schema moved into core Beam&lt;/li>
&lt;li>UDAFs can be indirect suclasses of CombineFn&lt;/li>
&lt;li>Many other small bugfixes&lt;/li>
&lt;/ul>
&lt;h2 id="portability">Portability&lt;/h2>
&lt;ul>
&lt;li>Common shared code related to supporting portable execution for runners.&lt;/li>
&lt;li>Python SDK supporting side inputs over the portability APIs.&lt;/li>
&lt;/ul>
&lt;h2 id="extract-metrics-in-a-runner-agnostic-way">Extract metrics in a runner agnostic way&lt;/h2>
&lt;p>Metrics are pushed by the runners to configurable sinks (Http REST sink available). It is already enabled in Flink and Spark runner, work is in progress for Dataflow.&lt;/p>
&lt;h1 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h1>
&lt;h2 id="sdks">SDKs&lt;/h2>
&lt;ul>
&lt;li>Implemented HDFS FileSystem for Python SDK.&lt;/li>
&lt;li>Python SDK adds support for side inputs for streaming execution.&lt;/li>
&lt;/ul>
&lt;h2 id="runners">Runners&lt;/h2>
&lt;ul>
&lt;li>Updated Spark runner to Spark version 2.3.1&lt;/li>
&lt;li>Fixed issue with late elements windowed into expired fixed windows get dropped in Directrunner.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">IOs&lt;/h2>
&lt;ul>
&lt;li>CassandraIO gained a better split algorithm based on overlapping regions.&lt;/li>
&lt;li>ElasticsearchIO supports partial updates.&lt;/li>
&lt;li>ElasticsearchIO allows to pass id, type and index per document.&lt;/li>
&lt;li>SolrIO supports a more robust retry on write strategy.&lt;/li>
&lt;li>S3 FileSystem supports encryption (SSE-S3, SSE-C and SSE-KMS).&lt;/li>
&lt;li>Improved connection management in JdbcIO.&lt;/li>
&lt;li>Added support the element timestamps while publishing to Kafka.&lt;/li>
&lt;/ul>
&lt;h2 id="other">Other&lt;/h2>
&lt;ul>
&lt;li>Use Java ErrorProne for static analysis.&lt;/li>
&lt;/ul>
&lt;h1 id="list-of-contributors">List of Contributors&lt;/h1>
&lt;p>According to git shortlog, the following 84 people contributed to the 2.5.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alex Van Boxel, Alexander Dejanovski, Alexey Romanenko, Aljoscha Krettek, ananvay, Andreas Ehrencrona, Andrew Pilloud, Ankur Goenka, Anton Kedin, arkash, Austin Bennett, Axel Magnuson, Ben Chambers, Ben Sidhom, Bill Neubauer, Boyuan Zhang, Braden Bassingthwaite, Cade Markegard, cclauss, Chamikara Jayalath, Charles Chen, Chuan Yu Foo, Cody Schroeder, Colm O hEigeartaigh, Daniel Oliveira, Dariusz Aniszewski, David Cavazos, Dawid Wysakowicz, Eric Roshan-Eisner, Etienne Chauchot, Eugene Kirpichov, Flavio Fiszman, Geet Kumar, GlennAmmons, Grzegorz Kołakowski, Henning Rohde, Innocent Djiofack, Ismaël Mejía, Jack Hsueh, Jason Kuster, Javier Antonio Gonzalez Trejo, Jean-Baptiste Onofré, Kai Jiang, Kamil Szewczyk, Katarzyna Kucharczyk, Kenneth Jung, Kenneth Knowles, Kevin Peterson, Lukasz Cwik, Łukasz Gajowy, Mairbek Khadikov, Manu Zhang, Maria Garcia Herrero, Marian Dvorsky, Mark Liu, Matthias Feys, Matthias Wessendorf, mingmxu, Nathan Howell, Pablo Estrada, Paul Gerver, Raghu Angadi, rarokni, Reuven Lax, Rezan Achmad, Robbe Sneyders, Robert Bradshaw, Robert Burke, Romain Manni-Bucau, Sam Waggoner, Sam Whittle, Scott Wegner, Stephan Hoyer, Thomas Groh, Thomas Weise, Tim Robertson, Udi Meiri, Valentyn Tymofieiev, XuMingmin, Yifan Zou, Yunqing Zhou&lt;/p></description></item><item><title>Blog: Apache Beam 2.3.0</title><link>/blog/beam-2.3.0/</link><pubDate>Mon, 19 Feb 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.3.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are glad to present the new 2.3.0 release of Beam. This release includes
multiple fixes and new functionalities.&lt;/p>
&lt;p>For more information
please check the detailed release notes.&lt;/p>
&lt;h1 id="new-features--improvements">New Features / Improvements&lt;/h1>
&lt;h2 id="beam-moves-to-java-8">Beam moves to Java 8&lt;/h2>
&lt;p>The supported version of Java for Beam is now Java 8. The code and examples have
been refactored to use multiple of the advantages of the language, e.g. lambdas,
streams, improved type inference, etc.&lt;/p>
&lt;h2 id="spark-runner-is-now-based-on-spark-2x">Spark runner is now based on Spark 2.x&lt;/h2>
&lt;p>Spark runner moves forward into the Spark 2.x development line, this would allow
to benefit of improved performance, as well as open the runner for future
compatibility with the Structured Streaming APIs. Notice that support for Spark
1.x is finished with this release.&lt;/p>
&lt;h2 id="amazon-web-services-s3-filesystem-support">Amazon Web Services S3 Filesystem support&lt;/h2>
&lt;p>Beam already supported AWS S3 via HadoopFileSystem, but this version brings a
native implementation with the corresponding performance advantages of the S3
filesystem.&lt;/p>
&lt;h2 id="general-purpose-writing-to-files">General-purpose writing to files&lt;/h2>
&lt;p>This release contains a new transform, FileIO.write() / writeDynamic() that
implements a general-purpose fluent and Java8-friendly API for writing to files
using a FileIO.Sink. This API has similar capabilities to DynamicDestinations
APIs from Beam 2.2 but is much easier to use and extend. The DynamicDestinations
APIs for writing to files are deprecated by it, as is FileBasedSink.&lt;/p>
&lt;h2 id="splittable-dofn-support-on-the-python-sdk">Splittable DoFn support on the Python SDK&lt;/h2>
&lt;p>This release adds the Splittable DoFn API for Python SDK and adds Splittable
DoFn support for Python streaming DirectRunner.&lt;/p>
&lt;h2 id="portability">Portability&lt;/h2>
&lt;p>Progress continues to being able to execute Python on runners other then Google
Cloud Dataflow and the Go SDK on any runner.&lt;/p>
&lt;h1 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h1>
&lt;h2 id="sdks">SDKs&lt;/h2>
&lt;ul>
&lt;li>MapElements and FlatMapElements support using side inputs using the new
interface Contextful.Fn. For library authors, this interface is the
recommended choice for user-code callbacks that may use side inputs.&lt;/li>
&lt;li>Introduces the family of Reify transforms for converting between explicit and
implicit representations of various Beam entities.&lt;/li>
&lt;li>Introduces two transforms for approximate sketching of data: Count-Min Sketch
(approximate element frequency estimation) and HyperLogLog (approximate
cardinality estimation).&lt;/li>
&lt;/ul>
&lt;h2 id="runners">Runners&lt;/h2>
&lt;ul>
&lt;li>Staging files on Dataflow shows progress&lt;/li>
&lt;li>Flink runner is based now on Flink version 1.4.0&lt;/li>
&lt;/ul>
&lt;h2 id="ios">IOs&lt;/h2>
&lt;ul>
&lt;li>BigtableIO now supports ValueProvider configuration&lt;/li>
&lt;li>BigQueryIO supports writing bounded collections to tables with partition
decorators&lt;/li>
&lt;li>KafkaIO moves to version 1.0 (it is still backwards compatible with versions &amp;gt;= 0.9.x.x)&lt;/li>
&lt;li>Added IO source for VCF files (Python)&lt;/li>
&lt;li>Added support for backoff on deadlocks in JdbcIO.write() and connection
improvement&lt;/li>
&lt;li>Improved performance of KinesisIO.read()&lt;/li>
&lt;li>Many improvements to TikaIO&lt;/li>
&lt;/ul>
&lt;h1 id="list-of-contributors">List of Contributors&lt;/h1>
&lt;p>According to git shortlog, the following 78 people contributed to the 2.3.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Ankur Goenka, Anton Kedin, Arnaud Fournier, Asha Rostamianfar, Ben Chambers, Ben Sidhom, Bill Neubauer, Brian Foo, cclauss, Chamikara Jayalath, Charles Chen, Colm O hEigeartaigh, Daniel Oliveira, Dariusz Aniszewski, David Cavazos, David Sabater, David Sabater Dinter, Dawid Wysakowicz, Dmytro Ivanov, Etienne Chauchot, Eugene Kirpichov, Exprosed, Grzegorz Kołakowski, Henning Rohde, Holden Karau, Huygaa Batsaikhan, Ilya Figotin, Innocent Djiofack, Ismaël Mejía, Itamar Ostricher, Jacky, Jacob Marble, James Xu, Jean-Baptiste Onofré, Jeremie Lenfant-Engelmann, Kamil Szewczyk, Kenneth Knowles, Lukasz Cwik, Łukasz Gajowy, Luke Zhu, Mairbek Khadikov, María García Herrero, Marian Dvorsky, Mark Liu, melissa, Miles Saul, mingmxu, Motty Gruda, nerdynick, Neville Li, Nigel Kilmer, Pablo, Pawel Kaczmarczyk, Petr Shevtsov, Rafal Wojdyla, Raghu Angadi, Robert Bradshaw, Robert Burke, Romain Manni-Bucau, Ryan Niemocienski, Ryan Skraba, Sam Whittle, Scott Wegner, Shashank Prabhakara, Solomon Duskis, Thomas Groh, Thomas Weise, Udi Meiri, Valentyn Tymofieiev, wtanaka.com, XuMingmin, zhouhai02, Zohar Yahav, 琨瑜.&lt;/p></description></item><item><title>Blog: Apache Beam: A Look Back at 2017</title><link>/blog/beam-a-look-back/</link><pubDate>Tue, 09 Jan 2018 00:00:01 -0800</pubDate><guid>/blog/beam-a-look-back/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>On January 10, 2017, Apache Beam got &lt;a href="/blog/2017/01/10/beam-graduates.html">promoted&lt;/a>
as a Top-Level Apache Software Foundation project. It was an important milestone
that validated the value of the project, legitimacy of its community, and
heralded its growing adoption. In the past year, Apache Beam has been on a
phenomenal growth trajectory, with significant growth in its community and
feature set. Let us walk you through some of the notable achievements.&lt;/p>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>First, lets take a glimpse at how Beam was used in 2017. Apache Beam being a
unified framework for batch and stream processing, enables a very wide spectrum
of diverse use cases. Here are some use cases that exemplify the versatility of
Beam.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/2017-look-back/timeline.png"
alt="Use Cases"
width="600">&lt;/p>
&lt;h2 id="community-growth">Community growth&lt;/h2>
&lt;p>In 2017, Apache Beam had 174 contributors worldwide, from many different
organizations. As an Apache project, we are proud to count 18 PMC members and
31 committers. The community had 7 releases in 2017, each bringing a rich set of
new features and fixes.&lt;/p>
&lt;p>The most obvious and encouraging sign of the growth of Apache Beam’s community,
and validation of its core value proposition of portability, is the addition of
significant new &lt;a href="/documentation/runners/capability-matrix/">runners&lt;/a>
(i.e. execution engines). We entered 2017 with Apache Flink, Apache Spark 1.x,
Google Cloud Dataflow, Apache Apex, and Apache Gearpump. In 2017, the following
new and updated runners were developed:&lt;/p>
&lt;ul>
&lt;li>Apache Spark 2.x update&lt;/li>
&lt;li>&lt;a href="https://www.ibm.com/blogs/bluemix/2017/10/streaming-analytics-updates-ibm-streams-runner-apache-beam-2-0/">IBM Streams runner&lt;/a>&lt;/li>
&lt;li>MapReduce runner&lt;/li>
&lt;li>&lt;a href="http://jstorm.io/">JStorm runner&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>In addition to runners, Beam added new IO connectors, some notable ones being
the Cassandra, MQTT, AMQP, HBase/HCatalog, JDBC, Solr, Tika, Redis, and
Elasticsearch connectors. Beam’s IO connectors make it possible to read from or
write to data sources/sinks even when they are not natively supported by the
underlying execution engine. Beam also provides fully pluggable filesystem
support, allowing us to support and extend our coverage to HDFS, S3, Azure
Storage, and Google Storage. We continue to add new IO connectors and
filesystems to extend the Beam use cases.&lt;/p>
&lt;p>A particularly telling sign of the maturity of an open source community is when
it is able to collaborate with multiple other open source communities, and
mutually improve the state of the art. Over the past few months, the Beam,
Calcite, and Flink communities have come together to define a robust &lt;a href="https://docs.google.com/document/d/1wrla8mF_mmq-NW9sdJHYVgMyZsgCmHumJJ5f5WUzTiM/edit">spec&lt;/a>
for Streaming SQL, with engineers from over four organizations contributing to
it. If, like us, you are excited by the prospect of improving the state of
streaming SQL, please join us!&lt;/p>
&lt;p>In addition to SQL, new XML and JSON based declarative DSLs are also in PoC.&lt;/p>
&lt;h2 id="continued-innovation">Continued innovation&lt;/h2>
&lt;p>Innovation is important to the success on any open source project, and Beam has
a rich history of bringing innovative new ideas to the open source community.
Apache Beam was the first to introduce some seminal concepts in the world of
big-data processing:&lt;/p>
&lt;ul>
&lt;li>Unified batch and streaming SDK that enables users to author big-data jobs
without having to learn multiple disparate SDKs/APIs.&lt;/li>
&lt;li>Cross-Engine Portability: Giving enterprises the confidence that workloads
authored today will not have to be re-written when open source engines become
outdated and are supplanted by newer ones.&lt;/li>
&lt;li>&lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Semantics&lt;/a>
essential for reasoning about unbounded unordered data, and achieving
consistent and correct output from a streaming job.&lt;/li>
&lt;/ul>
&lt;p>In 2017, the pace of innovation continued. The following capabilities were
introduced:&lt;/p>
&lt;ul>
&lt;li>Cross-Language Portability framework, and a &lt;a href="https://golang.org/">Go&lt;/a> SDK
developed with it.&lt;/li>
&lt;li>Dynamically Shardable IO (SplittableDoFn)&lt;/li>
&lt;li>Support for schemas in PCollection, allowing us to extend the runner
capabilities.&lt;/li>
&lt;li>Extensions addressing new use cases such as machine learning, and new data
formats.&lt;/li>
&lt;/ul>
&lt;h2 id="areas-of-improvement">Areas of improvement&lt;/h2>
&lt;p>Any retrospective view of a project is incomplete without an honest assessment
of areas of improvement. Two aspects stand out:&lt;/p>
&lt;ul>
&lt;li>Helping runners showcase their individual strengths. After all, portability
does not imply homogeneity. Different runners have different areas in which
they excel, and we need to do a better job of helping them highlight their
strengths.&lt;/li>
&lt;li>Based on the previous point, helping customers make a more informed decision
when they select a runner or migrate from one to another.&lt;/li>
&lt;/ul>
&lt;p>In 2018, we aim to take proactive steps to improve the above aspects.&lt;/p>
&lt;h2 id="ethos-of-the-project-and-its-community">Ethos of the project and its community&lt;/h2>
&lt;p>The world of batch and stream big-data processing today is reminiscent of the
&lt;a href="https://en.wikipedia.org/wiki/Tower_of_Babel">Tower of Babel&lt;/a> parable: a
slowdown of progress because different communities spoke different languages.
Similarly, today there are multiple disparate big-data SDKs/APIs, each with
their own distinct terminology to describe similar concepts. The side effect is
user confusion and slower adoption.&lt;/p>
&lt;p>The Apache Beam project aims to provide an industry standard portable SDK that
will:&lt;/p>
&lt;ul>
&lt;li>Benefit users by providing &lt;em>&lt;strong>innovation with stability&lt;/strong>&lt;/em>: The separation of
SDK and engine enables healthy competition between runners, without requiring
users to constantly learn new SDKs/APIs and rewrite their workloads to
benefit from new innovation.&lt;/li>
&lt;li>Benefit big-data engines by &lt;em>&lt;strong>growing the pie for everyone&lt;/strong>&lt;/em>: Making it
easier for users to author, maintain, upgrade and migrate their big-data
workloads will lead to significant growth in the number of production
big-data deployments.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Timely (and Stateful) Processing with Apache Beam</title><link>/blog/timely-processing/</link><pubDate>Mon, 28 Aug 2017 00:00:01 -0800</pubDate><guid>/blog/timely-processing/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In a &lt;a href="/blog/2017/02/13/stateful-processing.html">prior blog
post&lt;/a>, I
introduced the basics of stateful processing in Apache Beam, focusing on the
addition of state to per-element processing. So-called &lt;em>timely&lt;/em> processing
complements stateful processing in Beam by letting you set timers to request a
(stateful) callback at some point in the future.&lt;/p>
&lt;p>What can you do with timers in Beam? Here are some examples:&lt;/p>
&lt;ul>
&lt;li>You can output data buffered in state after some amount of processing time.&lt;/li>
&lt;li>You can take special action when the watermark estimates that you have
received all data up to a specified point in event time.&lt;/li>
&lt;li>You can author workflows with timeouts that alter state and emit output in
response to the absence of additional input for some period of time.&lt;/li>
&lt;/ul>
&lt;p>These are just a few possibilities. State and timers together form a powerful
programming paradigm for fine-grained control to express a huge variety of
workflows. Stateful and timely processing in Beam is portable across data
processing engines and integrated with Beam&amp;rsquo;s unified model of event time
windowing in both streaming and batch processing.&lt;/p>
&lt;h2 id="what-is-stateful-and-timely-processing">What is stateful and timely processing?&lt;/h2>
&lt;p>In my prior post, I developed an understanding of stateful processing largely
by contrast with associative, commutative combiners. In this post, I&amp;rsquo;ll
emphasize a perspective that I had mentioned only briefly: that elementwise
processing with access to per-key-and-window state and timers represents a
fundamental pattern for &amp;ldquo;embarrassingly parallel&amp;rdquo; computation, distinct from
the others in Beam.&lt;/p>
&lt;p>In fact, stateful and timely computation is the low-level computational pattern
that underlies the others. Precisely because it is lower level, it allows you
to really micromanage your computations to unlock new use cases and new
efficiencies. This incurs the complexity of manually managing your state and
timers - it isn&amp;rsquo;t magic! Let&amp;rsquo;s first look again at the two primary
computational patterns in Beam.&lt;/p>
&lt;h3 id="element-wise-processing-pardo-map-etc">Element-wise processing (ParDo, Map, etc)&lt;/h3>
&lt;p>The most elementary embarrassingly parallel pattern is just using a bunch of
computers to apply the same function to every input element of a massive
collection. In Beam, per-element processing like this is expressed as a basic
&lt;code>ParDo&lt;/code> - analogous to &amp;ldquo;Map&amp;rdquo; from MapReduce - which is like an enhanced &amp;ldquo;map&amp;rdquo;,
&amp;ldquo;flatMap&amp;rdquo;, etc, from functional programming.&lt;/p>
&lt;p>The following diagram illustrates per-element processing. Input elements are
squares, output elements are triangles. The colors of the elements represent
their key, which will matter later. Each input element maps to the
corresponding output element(s) completely independently. Processing may be
distributed across computers in any way, yielding essentially limitless
parallelism.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/ParDo.png"
alt="ParDo offers limitless parallelism"
width="600">&lt;/p>
&lt;p>This pattern is obvious, exists in all data-parallel paradigms, and has
a simple stateless implementation. Every input element can be processed
independently or in arbitrary bundles. Balancing the work between computers is
actually the hard part, and can be addressed by splitting, progress estimation,
work-stealing, etc.&lt;/p>
&lt;h3 id="per-key-and-window-aggregation-combine-reduce-groupbykey-etc">Per-key (and window) aggregation (Combine, Reduce, GroupByKey, etc.)&lt;/h3>
&lt;p>The other embarassingly parallel design pattern at the heart of Beam is per-key
(and window) aggregation. Elements sharing a key are colocated and then
combined using some associative and commutative operator. In Beam this is
expressed as a &lt;code>GroupByKey&lt;/code> or &lt;code>Combine.perKey&lt;/code>, and corresponds to the shuffle
and &amp;ldquo;Reduce&amp;rdquo; from MapReduce. It is sometimes helpful to think of per-key
&lt;code>Combine&lt;/code> as the fundamental operation, and raw &lt;code>GroupByKey&lt;/code> as a combiner that
just concatenates input elements. The communication pattern for the input
elements is the same, modulo some optimizations possible for &lt;code>Combine&lt;/code>.&lt;/p>
&lt;p>In the illustration here, recall that the color of each element represents the
key. So all of the red squares are routed to the same location where they are
aggregated and the red triangle is the output. Likewise for the yellow and
green squares, etc. In a real application, you may have millions of keys, so
the parallelism is still massive.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/CombinePerKey.png"
alt="Gathering elements per key then combining them"
width="600">&lt;/p>
&lt;p>The underlying data processing engine will, at some level of abstraction, use
state to perform this aggregation across all the elements arriving for a key.
In particular, in a streaming execution, the aggregation process may need to
wait for more data to arrive or for the watermark to estimate that all input
for an event time window is complete. This requires some way to store the
intermediate aggregation between input elements as well a way to a receive a
callback when it is time to emit the result. As a result, the &lt;em>execution&lt;/em> of
per key aggregation by a stream processing engine fundamentally involves state
and timers.&lt;/p>
&lt;p>However, &lt;em>your&lt;/em> code is just a declarative expression of the aggregation
operator. The runner can choose a variety of ways to execute your operator.
I went over this in detail in &lt;a href="/blog/2017/02/13/stateful-processing.html">my prior post focused on state alone&lt;/a>. Since you do not
observe elements in any defined order, nor manipulate mutable state or timers
directly, I call this neither stateful nor timely processing.&lt;/p>
&lt;h3 id="per-key-and-window-stateful-timely-processing">Per-key-and-window stateful, timely processing&lt;/h3>
&lt;p>Both &lt;code>ParDo&lt;/code> and &lt;code>Combine.perKey&lt;/code> are standard patterns for parallelism that go
back decades. When implementing these in a massive-scale distributed data
processing engine, we can highlight a few characteristics that are particularly
important.&lt;/p>
&lt;p>Let us consider these characteristics of &lt;code>ParDo&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>You write single-threaded code to process one element.&lt;/li>
&lt;li>Elements are processed in arbitrary order with no dependencies
or interaction between processing of elements.&lt;/li>
&lt;/ul>
&lt;p>And these characteristics for &lt;code>Combine.perKey&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Elements for a common key and window are gathered together.&lt;/li>
&lt;li>A user-defined operator is applied to those elements.&lt;/li>
&lt;/ul>
&lt;p>Combining some of the characteristics of unrestricted parallel mapping and
per-key-and-window combination, we can discern a megaprimitive from which we
build stateful and timely processing:&lt;/p>
&lt;ul>
&lt;li>Elements for a common key and window are gathered together.&lt;/li>
&lt;li>Elements are processed in arbitrary order.&lt;/li>
&lt;li>You write single-threaded code to process one element or timer, possibly
accessing state or setting timers.&lt;/li>
&lt;/ul>
&lt;p>In the illustration below, the red squares are gathered and fed one by one to
the stateful, timely, &lt;code>DoFn&lt;/code>. As each element is processed, the &lt;code>DoFn&lt;/code> has
access to state (the color-partitioned cylinder on the right) and can set
timers to receive callbacks (the colorful clocks on the left).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/StateAndTimers.png"
alt="Gathering elements per key then timely, stateful processing"
width="600">&lt;/p>
&lt;p>So that is the abstract notion of per-key-and-window stateful, timely
processing in Apache Beam. Now let&amp;rsquo;s see what it looks like to write code that
accesses state, sets timers, and receives callbacks.&lt;/p>
&lt;h2 id="example-batched-rpc">Example: Batched RPC&lt;/h2>
&lt;p>To demonstrate stateful and timely processing, let&amp;rsquo;s work through a concrete
example, with code.&lt;/p>
&lt;p>Suppose you are writing a system to analyze events. You have a ton of data
coming in and you need to enrich each event by RPC to an external system. You
can&amp;rsquo;t just issue an RPC per event. Not only would this be terrible for
performance, but it would also likely blow your quota with the external system.
So you&amp;rsquo;d like to gather a number of events, make one RPC for them all, and then
output all the enriched events.&lt;/p>
&lt;h3 id="state">State&lt;/h3>
&lt;p>Let&amp;rsquo;s set up the state we need to track batches of elements. As each element
comes in, we will write the element to a buffer while tracking the number of
elements we have buffered. Here are the state cells in code:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bufferedEvents&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">bag&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">TBD&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">BUFFER_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BagStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;buffer&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">EventCoder&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">COUNT_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">CombiningValueStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;count&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">VarIntCoder&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;span class="n">combiners&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SumCombineFn&lt;/span>&lt;span class="p">())&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Walking through the code, we have:&lt;/p>
&lt;ul>
&lt;li>The state cell &lt;code>&amp;quot;buffer&amp;quot;&lt;/code> is an unordered bag of buffered events.&lt;/li>
&lt;li>The state cell &lt;code>&amp;quot;count&amp;quot;&lt;/code> tracks how many events have been buffered.&lt;/li>
&lt;/ul>
&lt;p>Next, as a recap of reading and writing state, let&amp;rsquo;s write our &lt;code>@ProcessElement&lt;/code>
method. We will choose a limit on the size of the buffer, &lt;code>MAX_BUFFER_SIZE&lt;/code>. If
our buffer reaches this size, we will perform a single RPC to enrich all the
events, and output.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">MAX_BUFFER_SIZE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">500&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bufferedEvents&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">bag&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">firstNonNull&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">1&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">MAX_BUFFER_SIZE&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">EnrichedEvent&lt;/span> &lt;span class="n">enrichedEvent&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">enrichEvents&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">enrichedEvent&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">TBD&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">MAX_BUFFER_SIZE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">500&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">BUFFER_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BagStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;buffer&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">EventCoder&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">COUNT_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">CombiningValueStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;count&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">VarIntCoder&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;span class="n">combiners&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SumCombineFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">MAX_BUFFER_SIZE&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">event&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Here is an illustration to accompany the code:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/BatchedRpcState.png"
alt="Batching elements in state, then performing RPCs"
width="600">&lt;/p>
&lt;ul>
&lt;li>The blue box is the &lt;code>DoFn&lt;/code>.&lt;/li>
&lt;li>The yellow box within it is the &lt;code>@ProcessElement&lt;/code> method.&lt;/li>
&lt;li>Each input event is a red square - this diagram just shows the activity for
a single key, represented by the color red. Your &lt;code>DoFn&lt;/code> will run the same
workflow in parallel for all keys which are perhaps user IDs.&lt;/li>
&lt;li>Each input event is written to the buffer as a red triangle, representing
the fact that you might actually buffer more than just the raw input, even
though this code doesn&amp;rsquo;t.&lt;/li>
&lt;li>The external service is drawn as a cloud. When there are enough buffered
events, the &lt;code>@ProcessElement&lt;/code> method reads the events from state and issues
a single RPC.&lt;/li>
&lt;li>Each output enriched event is drawn as a red circle. To consumers of this
output, it looks just like an element-wise operation.&lt;/li>
&lt;/ul>
&lt;p>So far, we have only used state, but not timers. You may have noticed that
there is a problem - there will usually be data left in the buffer. If no more
input arrives, that data will never be processed. In Beam, every window has
some point in event time when any further input for the window is considered
too late and is discarded. At this point, we say that the window has &amp;ldquo;expired&amp;rdquo;.
Since no further input can arrive to access the state for that window, the
state is also discarded. For our example, we need to ensure that all leftover
events are output when the window expires.&lt;/p>
&lt;h3 id="event-time-timers">Event Time Timers&lt;/h3>
&lt;p>An event time timer requests a call back when the watermark for an input
&lt;code>PCollection&lt;/code> reaches some threshold. In other words, you can use an event time
timer to take action at a specific moment in event time - a particular point of
completeness for a &lt;code>PCollection&lt;/code> - such as when a window expires.&lt;/p>
&lt;p>For our example, let us add an event time timer so that when the window expires,
any events remaining in the buffer are processed.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">TimerSpec&lt;/span> &lt;span class="n">expirySpec&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">EVENT_TIME&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">BoundedWindow&lt;/span> &lt;span class="n">window&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">expiryTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">expiryTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">maxTimestamp&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">allowedLateness&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@OnTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">onExpiry&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">OnTimerContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(!&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">isEmpty&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">EnrichedEvent&lt;/span> &lt;span class="n">enrichedEvent&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">enrichEvents&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">enrichedEvent&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="n">EXPIRY_TIMER&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;expiry&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WATERMARK&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">w&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowParam&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">expiry_timer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TimerParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">EXPIRY_TIMER&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">expiry_timer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">end&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="nd">@on_timer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">EXPIRY_TIMER&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">expiry&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">events&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">event&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Let&amp;rsquo;s unpack the pieces of this snippet:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>We declare an event time timer with &lt;code>@TimerId(&amp;quot;expiry&amp;quot;)&lt;/code>. We will use the
identifier &lt;code>&amp;quot;expiry&amp;quot;&lt;/code> to identify the timer for setting the callback time as
well as receiving the callback.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The variable &lt;code>expiryTimer&lt;/code>, annotated with &lt;code>@TimerId&lt;/code>, is set to the value
&lt;code>TimerSpecs.timer(TimeDomain.EVENT_TIME)&lt;/code>, indicating that we want a
callback according to the event time watermark of the input elements.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the &lt;code>@ProcessElement&lt;/code> element we annotate a parameter &lt;code>@TimerId(&amp;quot;expiry&amp;quot;) Timer&lt;/code>. The Beam runner automatically provides this &lt;code>Timer&lt;/code> parameter by which
we can set (and reset) the timer. It is inexpensive to reset a timer
repeatedly, so we simply set it on every element.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We define the &lt;code>onExpiry&lt;/code> method, annotated with &lt;code>@OnTimer(&amp;quot;expiry&amp;quot;)&lt;/code>, that
performs a final event enrichment RPC and outputs the result. The Beam runner
delivers the callback to this method by matching its identifier.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Illustrating this logic, we have the diagram below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/BatchedRpcExpiry.png"
alt="Batched RPCs with window expiration"
width="600">&lt;/p>
&lt;p>Both the &lt;code>@ProcessElement&lt;/code> and &lt;code>@OnTimer(&amp;quot;expiry&amp;quot;)&lt;/code> methods perform the same
access to buffered state, perform the same batched RPC, and output enriched
elements.&lt;/p>
&lt;p>Now, if we are executing this in a streaming real-time manner, we might still
have unbounded latency for particular buffered data. If the watermark is advancing
very slowly, or event time windows are chosen to be quite large, then a lot of
time might pass before output is emitted based either on enough elements or
window expiration. We can also use timers to limit the amount of wall-clock
time, aka processing time, before we process buffered elements. We can choose
some reasonable amount of time so that even though we are issuing RPCs that are
not as large as they might be, it is still few enough RPCs to avoid blowing our
quota with the external service.&lt;/p>
&lt;h3 id="processing-time-timers">Processing Time Timers&lt;/h3>
&lt;p>A timer in processing time (time as it passes while your pipeline is executing)
is intuitively simple: you want to wait a certain amount of time and then
receive a call back.&lt;/p>
&lt;p>To put the finishing touches on our example, we will set a processing time
timer as soon as any data is buffered. We track whether or not the timer has
been set so we don&amp;rsquo;t continually reset it. When an element arrives, if the
timer has not been set, then we set it for the current moment plus
&lt;code>MAX_BUFFER_DURATION&lt;/code>. After the allotted processing time has passed, a
callback will fire and enrich and emit any buffered elements.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">Duration&lt;/span> &lt;span class="n">MAX_BUFFER_DURATION&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardSeconds&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;stale&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">TimerSpec&lt;/span> &lt;span class="n">staleSpec&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">PROCESSING_TIME&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">BoundedWindow&lt;/span> &lt;span class="n">window&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;stale&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">staleTimer&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">expiryTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="n">staleTimerSet&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">firstNonNull&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">staleSetState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">firstNonNull&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">staleTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">offset&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">MAX_BUFFER_DURATION&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">setRelative&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">processing&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@OnTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;stale&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">onStale&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">OnTimerContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(!&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">isEmpty&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">EnrichedEvent&lt;/span> &lt;span class="n">enrichedEvent&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">enrichEvents&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">enrichedEvent&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">expiry&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="n">STALE_TIMER&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;stale&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">REAL_TIME&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">MAX_BUFFER_DURATION&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">w&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowParam&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">expiry_timer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TimerParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">EXPIRY_TIMER&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">stale_timer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TimerParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">STALE_TIMER&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># We set an absolute timestamp here (not an offset like in the Java SDK)&lt;/span>
&lt;span class="n">stale_timer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">StatefulBufferingFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MAX_BUFFER_DURATION&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="nd">@on_timer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">STALE_TIMER&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">stale&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">events&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">event&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Here is an illustration of the final code:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/BatchedRpcStale.png"
alt="Batching elements in state, then performing RPCs"
width="600">&lt;/p>
&lt;p>Recapping the entirety of the logic:&lt;/p>
&lt;ul>
&lt;li>As events arrive at &lt;code>@ProcessElement&lt;/code> they are buffered in state.&lt;/li>
&lt;li>If the size of the buffer exceeds a maximum, the events are enriched and output.&lt;/li>
&lt;li>If the buffer fills too slowly and the events get stale before the maximum is reached,
a timer causes a callback which enriches the buffered events and outputs.&lt;/li>
&lt;li>Finally, as any window is expiring, any events buffered in that window are
processed and output prior to the state for that window being discarded.&lt;/li>
&lt;/ul>
&lt;p>In the end, we have a full example that uses state and timers to explicitly
manage the low-level details of a performance-sensitive transform in Beam. As
we added more and more features, our &lt;code>DoFn&lt;/code> actually became pretty large. That
is a normal characteristic of stateful, timely processing. You are really
digging in and managing a lot of details that are handled automatically when
you express your logic using Beam&amp;rsquo;s higher-level APIs. What you gain from this
extra effort is an ability to tackle use cases and achieve efficiencies that
may not have been possible otherwise.&lt;/p>
&lt;h2 id="state-and-timers-in-beams-unified-model">State and Timers in Beam&amp;rsquo;s Unified Model&lt;/h2>
&lt;p>Beam&amp;rsquo;s unified model for event time across streaming and batch processing has
novel implications for state and timers. Usually, you don&amp;rsquo;t need to do anything
for your stateful and timely &lt;code>DoFn&lt;/code> to work well in the Beam model. But it will
help to be aware of the considerations below, especially if you have used
similar features before outside of Beam.&lt;/p>
&lt;h3 id="event-time-windowing-just-works">Event Time Windowing &amp;ldquo;Just Works&amp;rdquo;&lt;/h3>
&lt;p>One of the raisons d&amp;rsquo;etre for Beam is correct processing of out-of-order event
data, which is almost all event data. Beam&amp;rsquo;s solution to out-of-order data is
event time windowing, where windows in event time yield correct results no
matter what windowing a user chooses or what order the events come in.&lt;/p>
&lt;p>If you write a stateful, timely transform, it should work no matter how the
surrounding pipeline chooses to window event time. If the pipeline chooses
fixed windows of one hour (sometimes called tumbling windows) or windows of 30
minutes sliding by 10 minutes, the stateful, timely transform should
transparently work correctly.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/WindowingChoices.png"
alt="Two windowing strategies for the same stateful and timely transform"
width="600">&lt;/p>
&lt;p>This works in Beam automatically, because state and timers are partitioned per
key and window. Within each key and window, the stateful, timely processing is
essentially independent. As an added benefit, the passing of event time (aka
advancement of the watermark) allows automatic release of unreachable state
when a window expires, so you often don&amp;rsquo;t have to worry about evicting old
state.&lt;/p>
&lt;h3 id="unified-real-time-and-historical-processing">Unified real-time and historical processing&lt;/h3>
&lt;p>A second tenet of Beam&amp;rsquo;s semantic model is that processing must be unified
between batch and streaming. One important use case for this unification
is the ability to apply the same logic to a stream of events in real time and
to archived storage of the same events.&lt;/p>
&lt;p>A common characteristic of archived data is that it may arrive radically out of
order. The sharding of archived files often results in a totally different
ordering for processing than events coming in near-real-time. The data will
also all be all available and hence delivered instantaneously from the point of
view of your pipeline. Whether running experiments on past data or reprocessing
past results to fix a data processing bug, it is critically important that your
processing logic be applicable to archived events just as easily as incoming
near-real-time data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/UnifiedModel.png"
alt="Unified stateful processing over streams and file archives"
width="600">&lt;/p>
&lt;p>It is (deliberately) possible to write a stateful and timely DoFn that delivers
results that depend on ordering or delivery timing, so in this sense there is
additional burden on you, the &lt;code>DoFn&lt;/code> author, to ensure that this nondeterminism
falls within documented allowances.&lt;/p>
&lt;h2 id="go-use-it">Go use it!&lt;/h2>
&lt;p>I&amp;rsquo;ll end this post in the same way I ended the last. I hope you will go try out
Beam with stateful, timely processing. If it opens up new possibilities for
you, then great! If not, we want to hear about it. Since this is a new feature,
please check the &lt;a href="/documentation/runners/capability-matrix/">capability matrix&lt;/a> to see the level of support for
your preferred Beam backend(s).&lt;/p>
&lt;p>And please do join the Beam community at
&lt;a href="/get-started/support">user@beam.apache.org&lt;/a> and follow
&lt;a href="https://twitter.com/ApacheBeam">@ApacheBeam&lt;/a> on Twitter.&lt;/p></description></item><item><title>Blog: Powerful and modular IO connectors with Splittable DoFn in Apache Beam</title><link>/blog/splittable-do-fn/</link><pubDate>Wed, 16 Aug 2017 00:00:01 -0800</pubDate><guid>/blog/splittable-do-fn/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>One of the most important parts of the Apache Beam ecosystem is its quickly
growing set of connectors that allow Beam pipelines to read and write data to
various data storage systems (&amp;ldquo;IOs&amp;rdquo;). Currently, Beam ships &lt;a href="/documentation/io/built-in/">over 20 IO
connectors&lt;/a> with many more in
active development. As user demands for IO connectors grew, our work on
improving the related Beam APIs (in particular, the Source API) produced an
unexpected result: a generalization of Beam&amp;rsquo;s most basic primitive, &lt;code>DoFn&lt;/code>.&lt;/p>
&lt;h2 id="connectors-as-mini-pipelines">Connectors as mini-pipelines&lt;/h2>
&lt;p>One of the main reasons for this vibrant IO connector ecosystem is that
developing a basic IO is relatively straightforward: many connector
implementations are simply mini-pipelines (composite &lt;code>PTransform&lt;/code>s) made of the
basic Beam &lt;code>ParDo&lt;/code> and &lt;code>GroupByKey&lt;/code> primitives. For example,
&lt;code>ElasticsearchIO.write()&lt;/code>
&lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/io/elasticsearch/src/main/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIO.java#L783">expands&lt;/a>
into a single &lt;code>ParDo&lt;/code> with some batching for performance; &lt;code>JdbcIO.read()&lt;/code>
&lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/JdbcIO.java#L329">expands&lt;/a>
into &lt;code>Create.of(query)&lt;/code>, a reshuffle to &lt;a href="https://cloud.google.com/dataflow/service/dataflow-service-desc#preventing-fusion">prevent
fusion&lt;/a>,
and &lt;code>ParDo(execute sub-query)&lt;/code>. Some IOs
&lt;a href="https://github.com/apache/beam/blob/8503adbbc3a590cd0dc2939f6a45d335682a9442/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java#L1139">construct&lt;/a>
considerably more complicated pipelines.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/splittable-do-fn/jdbcio-expansion.png"
alt="Expansion of the JdbcIO.read() composite transform"
width="600">&lt;/p>
&lt;p>This &amp;ldquo;mini-pipeline&amp;rdquo; approach is flexible, modular, and generalizes to data
sources that read from a dynamically computed &lt;code>PCollection&lt;/code> of locations, such
as
&lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java#L222">&lt;code>SpannerIO.readAll()&lt;/code>&lt;/a>
which reads the results of a &lt;code>PCollection&lt;/code> of queries from Cloud Spanner,
compared to
&lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java#L318">&lt;code>SpannerIO.read()&lt;/code>&lt;/a>
which executes a single query. We believe such dynamic data sources are a very
useful capability, often overlooked by other data processing frameworks.&lt;/p>
&lt;h2 id="when-pardo-and-groupbykey-are-not-enough">When ParDo and GroupByKey are not enough&lt;/h2>
&lt;p>Despite the flexibility of &lt;code>ParDo&lt;/code>, &lt;code>GroupByKey&lt;/code> and their derivatives, in some
cases building an efficient IO connector requires extra capabilities.&lt;/p>
&lt;p>For example, imagine reading files using the sequence &lt;code>ParDo(filepattern → expand into files)&lt;/code>, &lt;code>ParDo(filename → read records)&lt;/code>, or reading a Kafka topic
using &lt;code>ParDo(topic → list partitions)&lt;/code>, &lt;code>ParDo(topic, partition → read records)&lt;/code>. This approach has two big issues:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>In the file example, some files might be much larger than others, so the
second &lt;code>ParDo&lt;/code> may have very long individual &lt;code>@ProcessElement&lt;/code> calls. As a
result, the pipeline can suffer from poor performance due to stragglers.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the Kafka example, implementing the second &lt;code>ParDo&lt;/code> is &lt;em>simply impossible&lt;/em>
with a regular &lt;code>DoFn&lt;/code>, because it would need to output an infinite number of
records per each input element &lt;code>topic, partition&lt;/code> &lt;em>(&lt;a href="/blog/2017/02/13/stateful-processing.html">stateful processing&lt;/a> comes close, but it
has other limitations that make it insufficient for this task&lt;/em>).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="beam-source-api">Beam Source API&lt;/h2>
&lt;p>Apache Beam historically provides a Source API
(&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/BoundedSource.html">BoundedSource&lt;/a>
and
&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/UnboundedSource.html">UnboundedSource&lt;/a>) which does
not have these limitations and allows development of efficient data sources for
batch and streaming systems. Pipelines use this API via the
&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/Read.html">&lt;code>Read.from(Source)&lt;/code>&lt;/a> built-in &lt;code>PTransform&lt;/code>.&lt;/p>
&lt;p>The Source API is largely similar to that of most other data processing
frameworks, and allows the system to read data in parallel using multiple
workers, as well as checkpoint and resume reading from an unbounded data source.
Additionally, the Beam
&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/BoundedSource.html">&lt;code>BoundedSource&lt;/code>&lt;/a>
API provides advanced features such as progress reporting and &lt;a href="/blog/2016/05/18/splitAtFraction-method.html">dynamic
rebalancing&lt;/a>
(which together enable autoscaling), and
&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/UnboundedSource.html">&lt;code>UnboundedSource&lt;/code>&lt;/a> supports
reporting the source&amp;rsquo;s watermark and backlog &lt;em>(until SDF, we believed that
&amp;ldquo;batch&amp;rdquo; and &amp;ldquo;streaming&amp;rdquo; data sources are fundamentally different and thus
require fundamentally different APIs)&lt;/em>.&lt;/p>
&lt;p>Unfortunately, these features come at a price. Coding against the Source API
involves a lot of boilerplate and is error-prone, and it does not compose well
with the rest of the Beam model because a &lt;code>Source&lt;/code> can appear only at the root
of a pipeline. For example:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Using the Source API, it is not possible to read a &lt;code>PCollection&lt;/code> of
filepatterns.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A &lt;code>Source&lt;/code> can not read a side input, or wait on another pipeline step to
produce the data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A &lt;code>Source&lt;/code> can not emit an additional output (for example, records that failed to
parse) and so on.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The Source API is not composable even with itself. For example, suppose Alice
implements an unbounded &lt;code>Source&lt;/code> that watches a directory for new matching
files, and Bob implements an unbounded &lt;code>Source&lt;/code> that tails a file. The Source
API does not let them simply chain the sources together and obtain a &lt;code>Source&lt;/code>
that returns new records in new log files in a directory (a very common user
request). Instead, such a source would have to be developed mostly from
scratch, and our experience shows that a full-featured monolithic
implementation of such a &lt;code>Source&lt;/code> is incredibly difficult and error-prone.&lt;/p>
&lt;p>Another class of issues with the &lt;code>Source&lt;/code> API comes from its strict
bounded/unbounded dichotomy:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It is difficult or impossible to reuse code between seemingly very similar
bounded and unbounded sources, for example, the &lt;code>BoundedSource&lt;/code> that generates
a sequence &lt;code>[a, b)&lt;/code> and the &lt;code>UnboundedSource&lt;/code> that generates a sequence &lt;code>[a, inf)&lt;/code> &lt;a href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingSource.java">don&amp;rsquo;t share any
code&lt;/a>
in the Beam Java SDK.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It is not clear how to classify the ingestion of a very large and
continuously growing dataset. Ingesting its &amp;ldquo;already available&amp;rdquo; part seems to
require a &lt;code>BoundedSource&lt;/code>: the runner could benefit from knowing its size, and
could perform dynamic rebalancing. However, ingesting the continuously arriving
new data seems to require an &lt;code>UnboundedSource&lt;/code> for providing watermarks. From
this angle, the &lt;code>Source&lt;/code> API has &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">the same issues as Lambda
Architecture&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>About two years ago we began thinking about how to address the limitations of
the Source API, and ended up, surprisingly, addressing the limitations of
&lt;code>DoFn&lt;/code> instead.&lt;/p>
&lt;h2 id="enter-splittable-dofn">Enter Splittable DoFn&lt;/h2>
&lt;p>&lt;a href="https://s.apache.org/splittable-do-fn">Splittable DoFn&lt;/a> (SDF) is a
generalization of &lt;code>DoFn&lt;/code> that gives it the core capabilities of &lt;code>Source&lt;/code> while
retaining &lt;code>DoFn&lt;/code>'s syntax, flexibility, modularity, and ease of coding. As a
result, it becomes possible to develop more powerful IO connectors than before,
with shorter, simpler, more reusable code.&lt;/p>
&lt;p>Note that, unlike &lt;code>Source&lt;/code>, SDF &lt;em>does not&lt;/em> have distinct bounded/unbounded APIs,
just as regular &lt;code>DoFn&lt;/code>s don&amp;rsquo;t: there is only one API, which covers both of these
use cases and anything in between. Thus, SDF closes the final gap in the unified
batch/streaming programming model of Apache Beam.&lt;/p>
&lt;p>When reading the explanation of SDF below, keep in mind the running example of a
&lt;code>DoFn&lt;/code> that takes a filename as input and outputs the records in that file.
People familiar with the &lt;code>Source&lt;/code> API may find it useful to think of SDF as a
way to read a &lt;code>PCollection&lt;/code> of sources, treating the source itself as just
another piece of data in the pipeline &lt;em>(this, in fact, was one of the early
design iterations among the work that led to creation of SDF)&lt;/em>.&lt;/p>
&lt;p>The two aspects where &lt;code>Source&lt;/code> has an advantage over a regular &lt;code>DoFn&lt;/code> are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Splittability:&lt;/strong> applying a &lt;code>DoFn&lt;/code> to a single element is &lt;em>monolithic&lt;/em>, but
reading from a &lt;code>Source&lt;/code> is &lt;em>non-monolithic&lt;/em>. The whole &lt;code>Source&lt;/code> doesn&amp;rsquo;t have to
be read at once; rather, it is read in parts, called &lt;em>bundles&lt;/em>. For example, a
large file is usually read in several bundles, each reading some sub-range of
offsets within the file. Likewise, a Kafka topic (which, of course, can never
be read &amp;ldquo;fully&amp;rdquo;) is read over an infinite number of bundles, each reading some
finite number of elements.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Interaction with the runner:&lt;/strong> runners apply a &lt;code>DoFn&lt;/code> to a single element as
a &amp;ldquo;black box&amp;rdquo;, but interact quite richly with &lt;code>Source&lt;/code>. &lt;code>Source&lt;/code> provides the
runner with information such as its estimated size (or its generalization,
&amp;ldquo;backlog&amp;rdquo;), progress through reading the bundle, watermarks etc. The runner
uses this information to tune the execution and control the breakdown of the
&lt;code>Source&lt;/code> into bundles. For example, a slowly progressing large bundle of a file
may be &lt;a href="https://cloud.google.com/blog/big-data/2016/05/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow">dynamically
split&lt;/a>
by a batch-focused runner before it becomes a straggler, and a latency-focused
streaming runner may control how many elements it reads from a source in each
bundle to optimize for latency vs. per-bundle overhead.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="non-monolithic-element-processing-with-restrictions">Non-monolithic element processing with restrictions&lt;/h3>
&lt;p>Splittable &lt;code>DoFn&lt;/code> supports &lt;code>Source&lt;/code>-like features by allowing the processing of
a single element to be non-monolithic.&lt;/p>
&lt;p>The processing of one element by an SDF is decomposed into a (potentially
infinite) number of &lt;em>restrictions&lt;/em>, each describing some part of the work to be
done for the whole element. The input to an SDF&amp;rsquo;s &lt;code>@ProcessElement&lt;/code> call is a
pair of an element and a restriction (compared to a regular &lt;code>DoFn&lt;/code>, which takes
just the element).&lt;/p>
&lt;p>Processing of every element starts by creating an &lt;em>initial restriction&lt;/em> that
describes the entire work, and the initial restriction is then split further
into sub-restrictions which must logically add up to the original. For example,
for a splittable &lt;code>DoFn&lt;/code> called &lt;code>ReadFn&lt;/code> that takes a filename and outputs
records in the file, the restriction may be a pair of starting and ending byte
offset, and &lt;code>ReadFn&lt;/code> may interpret it as &lt;em>read records whose starting offsets
are in the given range&lt;/em>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/splittable-do-fn/restrictions.png"
alt="Specifying parts of work for an element using restrictions"
width="600">&lt;/p>
&lt;p>The idea of restrictions provides non-monolithic execution - the first
ingredient for parity with &lt;code>Source&lt;/code>. The other ingredient is &lt;em>interaction with
the runner&lt;/em>: the runner has access to the restriction of each active
&lt;code>@ProcessElement&lt;/code> call of an SDF, can inquire about the progress of the call,
and most importantly, can &lt;em>split&lt;/em> the restriction while it is being processed
(hence the name &lt;em>Splittable DoFn&lt;/em>).&lt;/p>
&lt;p>Splitting produces a &lt;em>primary&lt;/em> and &lt;em>residual&lt;/em> restriction that add up to the
original restriction being split: the current &lt;code>@ProcessElement&lt;/code> call keeps
processing the primary, and the residual will be processed by another
&lt;code>@ProcessElement&lt;/code> call. For example, a runner may schedule the residual to be
processed in parallel on another worker.&lt;/p>
&lt;p>Splitting of a running &lt;code>@ProcessElement&lt;/code> call has two critically important uses:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Supporting infinite work per element.&lt;/strong> A restriction is, in general, not
required to describe a finite amount of work. For example, reading from a Kafka
topic starting from offset &lt;em>100&lt;/em> can be represented by the
restriction &lt;em>[100, inf)&lt;/em>. A &lt;code>@ProcessElement&lt;/code> call processing this
entire restriction would, of course, never complete. However, while such a call
runs, a runner can split the restriction into a &lt;em>finite&lt;/em> primary &lt;em>[100, 150)&lt;/em>
(letting the current call complete this part) and an &lt;em>infinite&lt;/em> residual &lt;em>[150,
inf)&lt;/em> to be processed later, effectively checkpointing and resuming the call;
this can be repeated forever.&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/splittable-do-fn/kafka-splitting.png"
alt="Splitting an infinite restriction into a finite primary and infinite residual"
width="400">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Dynamic rebalancing.&lt;/strong> When a (typically batch-focused) runner detects that
a &lt;code>@ProcessElement&lt;/code> call is going to take too long and become a straggler, it
can split the restriction in some proportion so that the primary is short enough
to not be a straggler, and can schedule the residual in parallel on another
worker. For details, see &lt;a href="https://cloud.google.com/blog/big-data/2016/05/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow">No Shard Left
Behind&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Logically, the execution of an SDF on an element works according to the
following diagram, where &amp;ldquo;magic&amp;rdquo; stands for the runner-specific ability to split
the restrictions and schedule processing of residuals.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/splittable-do-fn/transform-expansion.png"
alt="Execution of an SDF - pairing with a restriction, splitting
restrictions, processing element/restriction pairs"
width="600">&lt;/p>
&lt;p>This diagram emphasizes that splittability is an implementation detail of the
particular &lt;code>DoFn&lt;/code>: a splittable &lt;code>DoFn&lt;/code> still looks like a &lt;code>DoFn&amp;lt;A, B&amp;gt;&lt;/code> to its
user, and can be applied via a &lt;code>ParDo&lt;/code> to a &lt;code>PCollection&amp;lt;A&amp;gt;&lt;/code> producing a
&lt;code>PCollection&amp;lt;B&amp;gt;&lt;/code>.&lt;/p>
&lt;h3 id="which-dofns-need-to-be-splittable">Which DoFns need to be splittable&lt;/h3>
&lt;p>Note that decomposition of an element into element/restriction pairs is not
automatic or &amp;ldquo;magical&amp;rdquo;: SDF is a new API for &lt;em>authoring&lt;/em> a &lt;code>DoFn&lt;/code>, rather than a
new way to &lt;em>execute&lt;/em> an existing &lt;code>DoFn&lt;/code>. When making a &lt;code>DoFn&lt;/code> splittable, the
author needs to:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Consider the structure of the work it does for every element.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Come up with a scheme for describing parts of this work using restrictions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Write code for creating the initial restriction, splitting it, and executing
an element/restriction pair.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>An overwhelming majority of &lt;code>DoFn&lt;/code>s found in user pipelines do not need to be
made splittable: SDF is an advanced, powerful API, primarily targeting authors
of new IO connectors &lt;em>(though it has interesting non-IO applications as well:
see &lt;a href="https://s.apache.org/splittable-do-fn#heading=h.5cep9s8k4fxv">Non-IO examples&lt;/a>)&lt;/em>.&lt;/p>
&lt;h3 id="execution-of-a-restriction-and-data-consistency">Execution of a restriction and data consistency&lt;/h3>
&lt;p>One of the most important parts of the Splittable &lt;code>DoFn&lt;/code> design is related to
how it achieves data consistency while splitting. For example, while the runner
is preparing to split the restriction of an active &lt;code>@ProcessElement&lt;/code> call, how
can it be sure that the call has not concurrently progressed past the point of
splitting?&lt;/p>
&lt;p>This is achieved by requiring the processing of a restriction to follow a
certain pattern. We think of a restriction as a sequence of &lt;em>blocks&lt;/em> -
elementary indivisible units of work, identified by a &lt;em>position&lt;/em>. A
&lt;code>@ProcessElement&lt;/code> call processes the blocks one by one, first &lt;em>claiming&lt;/em> the
block&amp;rsquo;s position to atomically check if it&amp;rsquo;s still within the range of the
restriction, until the whole restriction is processed.&lt;/p>
&lt;p>The diagram below illustrates this for &lt;code>ReadFn&lt;/code> (a splittable &lt;code>DoFn&lt;/code> that reads
Avro files) processing the element &lt;code>foo.avro&lt;/code> with restriction &lt;code>[30, 70)&lt;/code>. This
&lt;code>@ProcessElement&lt;/code> call scans the Avro file for &lt;a href="https://avro.apache.org/docs/current/spec.html#Object+Container+Files">data
blocks&lt;/a>
starting from offset &lt;code>30&lt;/code> and claims the position of each block in this range.
If a block is claimed successfully, then the call outputs all records in this
data block, otherwise, it terminates.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/splittable-do-fn/blocks.png"
alt="Processing a restriction by claiming blocks inside it"
width="400">&lt;/p>
&lt;p>For more details, see &lt;a href="https://s.apache.org/splittable-do-fn#heading=h.vjs7pzbb7kw">Restrictions, blocks and positions&lt;/a> in the
design proposal document.&lt;/p>
&lt;h3 id="code-example">Code example&lt;/h3>
&lt;p>Let us look at some examples of SDF code. The examples use the Beam Java SDK,
which &lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L527">represents splittable
&lt;code>DoFn&lt;/code>s&lt;/a>
as part of the flexible &lt;a href="https://s.apache.org/a-new-dofn">annotation-based
&lt;code>DoFn&lt;/code>&lt;/a> machinery, and the &lt;a href="https://s.apache.org/splittable-do-fn-python">proposed SDF syntax
for Python&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>A splittable &lt;code>DoFn&lt;/code> is a &lt;code>DoFn&lt;/code> - no new base class needed. Any SDF derives
from the &lt;code>DoFn&lt;/code> class and has a &lt;code>@ProcessElement&lt;/code> method.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;code>@ProcessElement&lt;/code> method takes an additional
&lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/splittabledofn/RestrictionTracker.java">&lt;code>RestrictionTracker&lt;/code>&lt;/a>
parameter that gives access to the current restriction in addition to the
current element.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>An SDF needs to define a &lt;code>@GetInitialRestriction&lt;/code> method that can create a
restriction describing the complete work for a given element.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are several less important optional methods, such as
&lt;code>@SplitRestriction&lt;/code> for pre-splitting the initial restriction into several
smaller restrictions, and a few others.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The &amp;ldquo;Hello World&amp;rdquo; of SDF is a counter, which takes pairs &lt;em>(x, N)&lt;/em> as input and
produces pairs &lt;em>(x, 0), (x, 1), …, (x, N-1)&lt;/em> as output.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">class&lt;/span> &lt;span class="nc">CountFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">OffsetRangeTracker&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="kt">long&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentRestriction&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getFrom&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">tryClaim&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@GetInitialRestriction&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">OffsetRange&lt;/span> &lt;span class="nf">getInitialRange&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">OffsetRange&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getValue&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">input&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="err">…&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ParDo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CountFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;());&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">CountFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RestrictionTrackerParam&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">xrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">current_restriction&lt;/span>&lt;span class="p">()):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">try_claim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">return&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">i&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">get_initial_restriction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>This short &lt;code>DoFn&lt;/code> subsumes the functionality of
&lt;a href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingSource.java">CountingSource&lt;/a>,
but is more flexible: &lt;code>CountingSource&lt;/code> generates only one sequence specified at
pipeline construction time, while this &lt;code>DoFn&lt;/code> can generate a dynamic family of
sequences, one per element in the input collection (it does not matter whether
the input collection is bounded or unbounded).&lt;/p>
&lt;p>However, the &lt;code>Source&lt;/code>-specific capabilities of &lt;code>CountingSource&lt;/code> are still
available in &lt;code>CountFn&lt;/code>. For example, if a sequence has a lot of elements, a
batch-focused runner can still apply dynamic rebalancing to it and generate
different subranges of the sequence in parallel by splitting the &lt;code>OffsetRange&lt;/code>.
Likewise, a streaming-focused runner can use the same splitting logic to
checkpoint and resume the generation of the sequence even if it is, for
practical purposes, infinite (for example, when applied to a &lt;code>KV(..., Long.MAX_VALUE)&lt;/code>).&lt;/p>
&lt;p>A slightly more complex example is the &lt;code>ReadFn&lt;/code> considered above, which reads
data from Avro files and illustrates the idea of &lt;em>blocks&lt;/em>: we provide pseudocode
to illustrate the approach.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">class&lt;/span> &lt;span class="nc">ReadFn&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">AvroRecord&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">OffsetRangeTracker&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">try&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">AvroReader&lt;/span> &lt;span class="n">reader&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Avro&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">open&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// Seek to the first block starting at or after the start offset.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">reader&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">seek&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentRestriction&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getFrom&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">reader&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">readNextBlock&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// Claim the position of the current Avro block
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="o">(!&lt;/span>&lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">tryClaim&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">reader&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentBlockOffset&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// Out of range of the current restriction - we&amp;#39;re done.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="c1">// Emit all records in this block
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">AvroRecord&lt;/span> &lt;span class="n">record&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">reader&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentBlock&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">record&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@GetInitialRestriction&lt;/span>
&lt;span class="n">OffsetRange&lt;/span> &lt;span class="nf">getInitialRestriction&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">String&lt;/span> &lt;span class="n">filename&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">OffsetRange&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">File&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">getSize&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">AvroReader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RestrictionTrackerParam&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">with&lt;/span> &lt;span class="n">fileio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ChannelFactory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nb">file&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stop&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">current_restriction&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># Seek to the first block starting at or after the start offset.&lt;/span>
&lt;span class="nb">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">seek&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">block&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AvroUtils&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_next_block&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">block&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># Claim the position of the current Avro block&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">tracker&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">try_claim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">block&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">()):&lt;/span>
&lt;span class="c1"># Out of range of the current restriction - we&amp;#39;re done.&lt;/span>
&lt;span class="k">return&lt;/span>
&lt;span class="c1"># Emit all records in this block&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">record&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">block&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">records&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">record&lt;/span>
&lt;span class="n">block&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AvroUtils&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_next_block&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">get_initial_restriction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">filename&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fileio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ChannelFactory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size_in_bytes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">))&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>This hypothetical &lt;code>DoFn&lt;/code> reads records from a single Avro file. Notably missing
is the code for expanding a filepattern: it no longer needs to be part of this
&lt;code>DoFn&lt;/code>! Instead, the SDK includes a
&lt;a href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileIO.java">FileIO.matchAll()&lt;/a>
transform for expanding a filepattern into a &lt;code>PCollection&lt;/code> of filenames, and
different file format IOs can reuse the same transform, reading the files with
different &lt;code>DoFn&lt;/code>s.&lt;/p>
&lt;p>This example demonstrates the benefits of increased modularity allowed by SDF:
&lt;code>FileIO.matchAll()&lt;/code> supports continuous ingestion of new files in streaming
pipelines using &lt;code>.continuously()&lt;/code>, and this functionality becomes automatically
available to various file format IOs. For example,
&lt;code>TextIO.read().watchForNewFiles()&lt;/code> &lt;a href="https://github.com/apache/beam/blob/3bd68ecfd7d576d78e02deb0476e549f11e1b5ef/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java#L486">uses &lt;code>FileIO.matchAll()&lt;/code> under the
hood)&lt;/a>.&lt;/p>
&lt;h2 id="current-status">Current status&lt;/h2>
&lt;p>Splittable &lt;code>DoFn&lt;/code> is a major new API, and its delivery and widespread adoption
involves a lot of work in different parts of the Apache Beam ecosystem. Some
of that work is already complete and provides direct benefit to users via new
IO connectors. However, a large amount of work is in progress or planned.&lt;/p>
&lt;p>As of August 2017, SDF is available for use in the Beam Java Direct runner and
Dataflow Streaming runner, and implementation is in progress in the Flink and
Apex runners; see &lt;a href="/documentation/runners/capability-matrix/">capability matrix&lt;/a> for the current status. Support
for SDF in the Python SDK is &lt;a href="https://s.apache.org/splittable-do-fn-python">in active
development&lt;/a>.&lt;/p>
&lt;p>Several SDF-based transforms and IO connectors are available for Beam users at
HEAD and will be included in Beam 2.2.0. &lt;code>TextIO&lt;/code> and &lt;code>AvroIO&lt;/code> finally provide
continuous ingestion of files (one of the most frequently requested features)
via &lt;code>.watchForNewFiles()&lt;/code> which is backed by the utility transforms
&lt;code>FileIO.matchAll().continuously()&lt;/code> and the more general
&lt;a href="https://github.com/apache/beam/blob/f7e8f886c91ea9d0b51e00331eeb4484e2f6e000/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Watch.java">&lt;code>Watch.growthOf()&lt;/code>&lt;/a>.
These utility transforms are also independently useful for &amp;ldquo;power user&amp;rdquo; use
cases.&lt;/p>
&lt;p>To enable more flexible use cases for IOs currently based on the Source API, we
will change them to use SDF. This transition is &lt;a href="https://s.apache.org/textio-sdf">pioneered by
TextIO&lt;/a> and involves temporarily &lt;a href="https://s.apache.org/sdf-via-source">executing SDF
via the Source API&lt;/a> to support runners
lacking the ability to run SDF directly.&lt;/p>
&lt;p>In addition to enabling new IOs, work on SDF has influenced our thinking about
other parts of the Beam programming model:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>SDF unified the final remaining part of the Beam programming model that was
not batch/streaming agnostic (the &lt;code>Source&lt;/code> API). This led us to consider use
cases that cannot be described as purely batch or streaming (for example,
ingesting a large amount of historical data and carrying on with more data
arriving in real time) and to develop a &lt;a href="https://s.apache.org/beam-fn-api-progress-reporting">unified notion of &amp;ldquo;progress&amp;rdquo; and
&amp;ldquo;backlog&amp;rdquo;&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;a href="https://s.apache.org/beam-fn-api">Fn API&lt;/a> - the foundation of Beam&amp;rsquo;s
future support for cross-language pipelines - uses SDF as &lt;em>the only&lt;/em> concept
representing data ingestion.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Implementation of SDF has lead to &lt;a href="https://lists.apache.org/thread.html/86831496a08fe148e3b982cdb904f828f262c0b571543a9fed7b915d@%3Cdev.beam.apache.org%3E">formalizing pipeline termination
semantics&lt;/a>
and making it consistent between runners.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>SDF set a new standard for how modular IO connectors can be, inspiring
creation of similar APIs for some non-SDF-based connectors (for example,
&lt;code>SpannerIO.readAll()&lt;/code> and the
&lt;a href="https://issues.apache.org/jira/browse/BEAM-2706">planned&lt;/a> &lt;code>JdbcIO.readAll()&lt;/code>).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="call-to-action">Call to action&lt;/h2>
&lt;p>Apache Beam thrives on having a large community of contributors. Here are some
ways you can get involved in the SDF effort and help make the Beam IO connector
ecosystem more modular:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Use the currently available SDF-based IO connectors, provide feedback, file
bugs, and suggest or implement improvements.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Propose or develop a new IO connector based on SDF.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Implement or improve support for SDF in your favorite runner.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Subscribe and contribute to the occasional SDF-related discussions on
&lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a> (mailing list for Beam
users) and &lt;a href="mailto:dev@beam.apache.org">dev@beam.apache.org&lt;/a> (mailing list for
Beam developers)!&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam publishes the first stable release</title><link>/blog/beam-first-stable-release/</link><pubDate>Wed, 17 May 2017 00:00:01 -0800</pubDate><guid>/blog/beam-first-stable-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The Apache Beam community is pleased to &lt;a href="https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces12">announce the availability of version 2.0.0&lt;/a>. This is the first stable release of Apache Beam, signifying a statement from the community that it intends to maintain API stability with all releases for the foreseeable future, and making Beam suitable for enterprise deployment.&lt;/p>
&lt;p>This first stable release is the third important milestone for the Apache Beam community. Beam joined the Apache Incubator in February 2016 and graduated as a top-level project of The Apache Software Foundation in December. Through these fifteen months of concentrated effort, a slightly chaotic codebase, merged from multiple organizations, has been developed into a generalized framework for data processing that is truly engine- and environment-independent. Apache Beam has evolved and improved through three incubating and three post-incubation releases, culminating in the stable release announced today as version 2.0.0.&lt;/p>
&lt;p>In the five months since graduation, Apache Beam has seen a significant growth, both in terms of adoption and community contribution. Apache Beam is &lt;a href="https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces12">in use&lt;/a> at Google Cloud, PayPal, and Talend, among others.&lt;/p>
&lt;p>Apache Beam, version 2.0.0 improves user experience across the project, focusing on seamless portability across execution environments, including engines, operating systems, on-premise clusters, cloud providers, and data storage systems. Other highlights include:&lt;/p>
&lt;ul>
&lt;li>API stability and future compatibility within this major version.&lt;/li>
&lt;li>Stateful data processing paradigms that unlock efficient, data-dependent computations.&lt;/li>
&lt;li>Support for user-extensible file systems, with built-in support for Hadoop Distributed File System, among others.&lt;/li>
&lt;li>A metrics subsystem for deeper insight into pipeline execution.&lt;/li>
&lt;/ul>
&lt;p>Many contributors made this release possible, by participating in different roles: contributing code, writing documentation, testing release candidates, supporting users, or helping in some other way. The following is a partial list of contributors – 76 individuals contributed code to the project since the previous release, assembled from source history:&lt;/p>
&lt;ul>
&lt;li>Ahmet Altay&lt;/li>
&lt;li>Eric Anderson&lt;/li>
&lt;li>Raghu Angadi&lt;/li>
&lt;li>Sourabh Bajaj&lt;/li>
&lt;li>Péter Gergő Barna&lt;/li>
&lt;li>Chen Bin&lt;/li>
&lt;li>Davor Bonaci&lt;/li>
&lt;li>Robert Bradshaw&lt;/li>
&lt;li>Ben Chambers&lt;/li>
&lt;li>Etienne Chauchot&lt;/li>
&lt;li>Chang Chen&lt;/li>
&lt;li>Charles Chen&lt;/li>
&lt;li>Craig Citro&lt;/li>
&lt;li>Lukasz Cwik&lt;/li>
&lt;li>Márton Elek&lt;/li>
&lt;li>Pablo Estrada&lt;/li>
&lt;li>Josh Forman-Gornall&lt;/li>
&lt;li>Maria García Herrero&lt;/li>
&lt;li>Jins George&lt;/li>
&lt;li>Damien Gouyette&lt;/li>
&lt;li>Thomas Groh&lt;/li>
&lt;li>Dan Halperin&lt;/li>
&lt;li>Pei He&lt;/li>
&lt;li>Hadar Hod&lt;/li>
&lt;li>Chamikara Jayalath&lt;/li>
&lt;li>Rekha Joshi&lt;/li>
&lt;li>Uwe Jugel&lt;/li>
&lt;li>Sung Junyoung&lt;/li>
&lt;li>Holden Karau&lt;/li>
&lt;li>Vikas Kedigehalli&lt;/li>
&lt;li>Eugene Kirpichov&lt;/li>
&lt;li>Tibor Kiss&lt;/li>
&lt;li>Kenneth Knowles&lt;/li>
&lt;li>Vassil Kolarov&lt;/li>
&lt;li>Chinmay Kolhatkar&lt;/li>
&lt;li>Aljoscha Krettek&lt;/li>
&lt;li>Dipti Kulkarni&lt;/li>
&lt;li>Radhika Kulkarni&lt;/li>
&lt;li>Jason Kuster&lt;/li>
&lt;li>Reuven Lax&lt;/li>
&lt;li>Stas Levin&lt;/li>
&lt;li>Julien Lhermitte&lt;/li>
&lt;li>Jingsong Li&lt;/li>
&lt;li>Neville Li&lt;/li>
&lt;li>Mark Liu&lt;/li>
&lt;li>Michael Luckey&lt;/li>
&lt;li>Andrew Martin&lt;/li>
&lt;li>Ismaël Mejía&lt;/li>
&lt;li>Devon Meunier&lt;/li>
&lt;li>Neda Mirian&lt;/li>
&lt;li>Anil Muppalla&lt;/li>
&lt;li>Gergely Novak&lt;/li>
&lt;li>Jean-Baptiste Onofré&lt;/li>
&lt;li>Melissa Pashniak&lt;/li>
&lt;li>peay&lt;/li>
&lt;li>David Rieber&lt;/li>
&lt;li>Rahul Sabbineni&lt;/li>
&lt;li>Kobi Salant&lt;/li>
&lt;li>Amit Sela&lt;/li>
&lt;li>Mark Shalda&lt;/li>
&lt;li>Stephen Sisk&lt;/li>
&lt;li>Yuya Tajima&lt;/li>
&lt;li>Wesley Tanaka&lt;/li>
&lt;li>JiJun Tang&lt;/li>
&lt;li>Valentyn Tymofieiev&lt;/li>
&lt;li>David Volquartz&lt;/li>
&lt;li>Huafeng Wang&lt;/li>
&lt;li>Thomas Weise&lt;/li>
&lt;li>Rafal Wojdyla&lt;/li>
&lt;li>Yangping Wu&lt;/li>
&lt;li>wyp&lt;/li>
&lt;li>James Xu&lt;/li>
&lt;li>Mingmin Xu&lt;/li>
&lt;li>Ted Yu&lt;/li>
&lt;li>Borisa Zivkovic&lt;/li>
&lt;li>Aviem Zur&lt;/li>
&lt;/ul>
&lt;p>Apache Beam, version 2.0.0, is making its debut at Apache: Big Data, taking place this week in Miami, FL, with four sessions featuring Apache Beam. Apache Beam will also be highlighted at numerous face-to-face meetups and conferences, including the Future of Data San Jose meetup, Strata Data Conference London, Berlin Buzzwords, and DataWorks Summit San Jose.&lt;/p>
&lt;p>We’d like to invite everyone to try out Apache Beam today and consider joining our vibrant community. We welcome feedback, contribution and participation through our mailing lists, issue tracker, pull requests, and events.&lt;/p></description></item><item><title>Blog: Python SDK released in Apache Beam 0.6.0</title><link>/blog/python-sdk-release/</link><pubDate>Thu, 16 Mar 2017 00:00:01 -0800</pubDate><guid>/blog/python-sdk-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam’s latest release, version &lt;a href="/get-started/downloads/">0.6.0&lt;/a>, introduces a new SDK &amp;ndash; this time, for the Python programming language. The Python SDK joins the Java SDK as the second implementation of the Beam programming model.&lt;/p>
&lt;p>The Python SDK incorporates all of the main concepts of the Beam model, including ParDo, GroupByKey, Windowing, and others. It features extensible IO APIs for writing bounded sources and sinks, and provides built-in implementation for reading and writing Text, Avro, and TensorFlow record files, as well as connectors to Google BigQuery and Google Cloud Datastore.&lt;/p>
&lt;p>There are two runners capable of executing pipelines written with the Python SDK today: &lt;a href="/documentation/runners/direct/">Direct Runner&lt;/a> and &lt;a href="/documentation/runners/dataflow/">Dataflow Runner&lt;/a>, both of which are currently limited to batch execution only. Upcoming features will shortly bring the benefits of the Python SDK to additional runners.&lt;/p>
&lt;h4 id="try-the-apache-beam-python-sdk">Try the Apache Beam Python SDK&lt;/h4>
&lt;p>If you would like to try out the Python SDK, a good place to start is the &lt;a href="/get-started/quickstart-py/">Quickstart&lt;/a>. After that, you can take a look at additional &lt;a href="https://github.com/apache/beam/tree/v0.6.0/sdks/python/apache_beam/examples">examples&lt;/a>, and deep dive into the &lt;a href="https://beam.apache.org/releases/pydoc/">API reference&lt;/a>.&lt;/p>
&lt;p>Let’s take a look at a quick example together. First, install the &lt;code>apache-beam&lt;/code> package from PyPI and start your Python interpreter.&lt;/p>
&lt;pre>&lt;code>$ pip install apache-beam
$ python
&lt;/code>&lt;/pre>&lt;p>We will harness the power of Apache Beam to estimate Pi in honor of the recently passed Pi Day.&lt;/p>
&lt;pre>&lt;code>import random
import apache_beam as beam
def run_trials(count):
&amp;quot;&amp;quot;&amp;quot;Throw darts into unit square and count how many fall into unit circle.&amp;quot;&amp;quot;&amp;quot;
inside = 0
for _ in xrange(count):
x, y = random.uniform(0, 1), random.uniform(0, 1)
inside += 1 if x*x + y*y &amp;lt;= 1.0 else 0
return count, inside
def combine_results(results):
&amp;quot;&amp;quot;&amp;quot;Given all the trial results, estimate pi.&amp;quot;&amp;quot;&amp;quot;
total, inside = sum(r[0] for r in results), sum(r[1] for r in results)
return total, inside, 4 * float(inside) / total if total &amp;gt; 0 else 0
p = beam.Pipeline()
(p | beam.Create([500] * 10) # Create 10 experiments with 500 samples each.
| beam.Map(run_trials) # Run experiments in parallel.
| beam.CombineGlobally(combine_results) # Combine the results.
| beam.io.WriteToText('./pi_estimate.txt')) # Write PI estimate to a file.
p.run()
&lt;/code>&lt;/pre>&lt;p>This example estimates Pi by throwing random darts into the unit square and keeping track of the fraction of those darts that fell into the unit circle (see the full &lt;a href="https://github.com/apache/beam/blob/v0.6.0/sdks/python/apache_beam/examples/complete/estimate_pi.py">example&lt;/a> for details). If you are curious, you can check the result of our estimation by looking at the output file.&lt;/p>
&lt;pre>&lt;code>$ cat pi_estimate.txt*
&lt;/code>&lt;/pre>&lt;h4 id="roadmap">Roadmap&lt;/h4>
&lt;p>The first thing on the Python SDK’s roadmap is to address two of its limitations. First, the existing runners are currently limited to bounded PCollections, and we are looking forward to extending the SDK to support unbounded PCollections (“streaming”). Additionally, we are working on extending support to more Apache Beam runners, and the upcoming Fn API will do the heavy lifting.&lt;/p>
&lt;p>Both of these improvements will enable the Python SDK to fulfill the mission of Apache Beam: a unified programming model for batch and streaming data processing that can run on any execution engine.&lt;/p>
&lt;h4 id="join-us">Join us!&lt;/h4>
&lt;p>Please consider joining us, whether as a user or a contributor, as we work towards our first release with API stability. If you’d like to try out Apache Beam today, check out the latest &lt;a href="/get-started/downloads/">0.6.0&lt;/a> release. We welcome contributions and participation from anyone through our mailing lists, issue tracker, pull requests, and events.&lt;/p></description></item><item><title>Blog: Stateful processing with Apache Beam</title><link>/blog/stateful-processing/</link><pubDate>Mon, 13 Feb 2017 00:00:01 -0800</pubDate><guid>/blog/stateful-processing/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Beam lets you process unbounded, out-of-order, global-scale data with portable
high-level pipelines. Stateful processing is a new feature of the Beam model
that expands the capabilities of Beam, unlocking new use cases and new
efficiencies. In this post, I will guide you through stateful processing in
Beam: how it works, how it fits in with the other features of the Beam model,
what you might use it for, and what it looks like in code.&lt;/p>
&lt;p>&lt;strong>Note: This post has been updated in May of 2019, to include Python
snippets!&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Warning: new features ahead!&lt;/strong>: This is a very new aspect of the Beam
model. Runners are still adding support. You can try it out today on multiple
runners, but do check the &lt;a href="/documentation/runners/capability-matrix/">runner capability
matrix&lt;/a> for
the current status in each runner.&lt;/p>
&lt;/blockquote>
&lt;p>First, a quick recap: In Beam, a big data processing &lt;em>pipeline&lt;/em> is a directed,
acyclic graph of parallel operations called &lt;em>&lt;code>PTransforms&lt;/code>&lt;/em> processing data
from &lt;em>&lt;code>PCollections&lt;/code>&lt;/em>. I&amp;rsquo;ll expand on that by walking through this illustration:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/pipeline.png"
alt="A Beam Pipeline - PTransforms are boxes - PCollections are arrows"
width="300">&lt;/p>
&lt;p>The boxes are &lt;code>PTransforms&lt;/code> and the edges represent the data in &lt;code>PCollections&lt;/code>
flowing from one &lt;code>PTransform&lt;/code> to the next. A &lt;code>PCollection&lt;/code> may be &lt;em>bounded&lt;/em> (which
means it is finite and you know it) or &lt;em>unbounded&lt;/em> (which means you don&amp;rsquo;t know if
it is finite or not - basically, it is like an incoming stream of data that may
or may not ever terminate). The cylinders are the data sources and sinks at the
edges of your pipeline, such as bounded collections of log files or unbounded
data streaming over a Kafka topic. This blog post isn&amp;rsquo;t about sources or sinks,
but about what happens in between - your data processing.&lt;/p>
&lt;p>There are two main building blocks for processing your data in Beam: &lt;em>&lt;code>ParDo&lt;/code>&lt;/em>,
for performing an operation in parallel across all elements, and &lt;em>&lt;code>GroupByKey&lt;/code>&lt;/em>
(and the closely related &lt;code>CombinePerKey&lt;/code> that I will talk about quite soon)
for aggregating elements to which you have assigned the same key. In the
picture below (featured in many of our presentations) the color indicates the
key of the element. Thus the &lt;code>GroupByKey&lt;/code>/&lt;code>CombinePerKey&lt;/code> transform gathers all the
green squares to produce a single output element.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/pardo-and-gbk.png"
alt="ParDo and GroupByKey/CombinePerKey:
Elementwise versus aggregating computations"
width="400">&lt;/p>
&lt;p>But not all use cases are easily expressed as pipelines of simple &lt;code>ParDo&lt;/code>/&lt;code>Map&lt;/code> and
&lt;code>GroupByKey&lt;/code>/&lt;code>CombinePerKey&lt;/code> transforms. The topic of this blog post is a new
extension to the Beam programming model: &lt;strong>per-element operation augmented with
mutable state&lt;/strong>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/stateful-pardo.png"
alt="Stateful ParDo - sequential per-key processing with persistent state"
width="300">&lt;/p>
&lt;p>In the illustration above, ParDo now has a bit of durable, consistent state on
the side, which can be read and written during the processing of each element.
The state is partitioned by key, so it is drawn as having disjoint sections for
each color. It is also partitioned per window, but I thought plaid
&lt;img src="/images/blog/stateful-processing/plaid.png"
alt="A plaid storage cylinder" width="20">
would be a bit much :-). I&amp;rsquo;ll talk about
why state is partitioned this way a bit later, via my first example.&lt;/p>
&lt;p>For the rest of this post, I will describe this new feature of Beam in detail -
how it works at a high level, how it differs from existing features, how to
make sure it is still massively scalable. After that introduction at the model
level, I&amp;rsquo;ll walk through a simple example of how you use it in the Beam Java
SDK.&lt;/p>
&lt;h2 id="how-does-stateful-processing-in-beam-work">How does stateful processing in Beam work?&lt;/h2>
&lt;p>The processing logic of your &lt;code>ParDo&lt;/code> transform is expressed through the &lt;code>DoFn&lt;/code>
that it applies to each element. Without stateful augmentations, a &lt;code>DoFn&lt;/code> is a
mostly-pure function from inputs to one or more outputs, corresponding to the
Mapper in a MapReduce. With state, a &lt;code>DoFn&lt;/code> has the ability to access
persistent mutable state while processing each input element. Consider this
illustration:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/stateful-dofn.png"
alt="Stateful DoFn -
the runner controls input but the DoFn controls storage and output"
width="300">&lt;/p>
&lt;p>The first thing to note is that all the data - the little squares, circles, and
triangles - are red. This is to illustrate that stateful processing occurs in
the context of a single key - all of the elements are key-value pairs with the
same key. Calls from your chosen Beam runner to the &lt;code>DoFn&lt;/code> are colored in
yellow, while calls from the &lt;code>DoFn&lt;/code> to the runner are in purple:&lt;/p>
&lt;ul>
&lt;li>The runner invokes the &lt;code>DoFn&lt;/code>'s &lt;code>@ProcessElement&lt;/code> method on each element for a
key+window.&lt;/li>
&lt;li>The &lt;code>DoFn&lt;/code> reads and writes state - the curved arrows to/from the storage on
the side.&lt;/li>
&lt;li>The &lt;code>DoFn&lt;/code> emits output (or side output) to the runner as usual via
&lt;code>ProcessContext.output&lt;/code> (resp. &lt;code>ProcessContext.sideOutput&lt;/code>).&lt;/li>
&lt;/ul>
&lt;p>At this very high level, it is pretty intuitive: In your programming
experience, you have probably at some point written a loop over elements that
updates some mutable variables while performing other actions. The interesting
question is how does this fit into the Beam model: how does it relate with
other features? How does it scale, since state implies some synchronization?
When should it be used versus other features?&lt;/p>
&lt;h2 id="how-does-stateful-processing-fit-into-the-beam-model">How does stateful processing fit into the Beam model?&lt;/h2>
&lt;p>To see where stateful processing fits in the Beam model, consider another
way that you can keep some &amp;ldquo;state&amp;rdquo; while processing many elements: CombineFn. In
Beam, you can write &lt;code>Combine.perKey(CombineFn)&lt;/code> in Java or Python to apply an
associative, commutative accumulating operation across all the elements with a
common key (and window).&lt;/p>
&lt;p>Here is a diagram illustrating the basics of a &lt;code>CombineFn&lt;/code>, the simplest way
that a runner might invoke it on a per-key basis to build an accumulator and
extract an output from the final accumulator:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/combinefn.png"
alt="CombineFn - the runner controls input, storage, and output"
width="300">&lt;/p>
&lt;p>As with the illustration of stateful &lt;code>DoFn&lt;/code>, all the data is colored red, since
this is the processing of Combine for a single key. The illustrated method
calls are colored yellow, since they are all controlled by the runner: The
runner invokes &lt;code>addInput&lt;/code> on each method to add it to the current accumulator.&lt;/p>
&lt;ul>
&lt;li>The runner persists the accumulator when it chooses.&lt;/li>
&lt;li>The runner calls &lt;code>extractOutput&lt;/code> when ready to emit an output element.&lt;/li>
&lt;/ul>
&lt;p>At this point, the diagram for &lt;code>CombineFn&lt;/code> looks a whole lot like the diagram
for stateful &lt;code>DoFn&lt;/code>. In practice, the flow of data is, indeed, quite similar.
But there are important differences, even so:&lt;/p>
&lt;ul>
&lt;li>The runner controls all invocations and storage here. You do not decide when
or how state is persisted, when an accumulator is discarded (based on
triggering) or when output is extracted from an accumulator.&lt;/li>
&lt;li>You can only have one piece of state - the accumulator. In a stateful DoFn
you can read only what you need to know and write only what has changed.&lt;/li>
&lt;li>You don&amp;rsquo;t have the extended features of &lt;code>DoFn&lt;/code>, such as multiple outputs per
input or side outputs. (These could be simulated by a sufficient complex
accumulator, but it would not be natural or efficient. Some other features of
&lt;code>DoFn&lt;/code> such as side inputs and access to the window make perfect sense for
&lt;code>CombineFn&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>But the main thing that &lt;code>CombineFn&lt;/code> allows a runner to do is to
&lt;code>mergeAccumulators&lt;/code>, the concrete expression of the &lt;code>CombineFn&lt;/code>'s associativity.
This unlocks some huge optimizations: the runner can invoke multiple instances
of a &lt;code>CombineFn&lt;/code> on a number of inputs and later combine them in a classic
divide-and-conquer architecture, as in this picture:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/combiner-lifting.png"
alt="Divide-and-conquer aggregation with a CombineFn"
width="600">&lt;/p>
&lt;p>The contract of a &lt;code>CombineFn&lt;/code> is that the result should be exactly the same,
whether or not the runner decides to actually do such a thing, or even more
complex trees with hot-key fanout, etc.&lt;/p>
&lt;p>This merge operation is not (necessarily) provided by a stateful &lt;code>DoFn&lt;/code>: the
runner cannot freely branch its execution and recombine the states. Note that
the input elements are still received in an arbitrary order, so the &lt;code>DoFn&lt;/code> should
be insensitive to ordering and bundling but it doesn&amp;rsquo;t mean the output must be
exactly equal. (fun and easy fact: if the outputs are actually always equal,
then the &lt;code>DoFn&lt;/code> is an associative and commutative operator)&lt;/p>
&lt;p>So now you can see how a stateful &lt;code>DoFn&lt;/code> differs from &lt;code>CombineFn&lt;/code>, but I want to
step back and extrapolate this to a high level picture of how state in Beam
relates to using other features to achieve the same or similar goals: In a lot
of cases, what stateful processing represents is a chance to &amp;ldquo;get under the
hood&amp;rdquo; of the highly abstract mostly-deterministic functional paradigm of Beam
and do potentially-nondeterministic imperative-style programming that is hard
to express any other way.&lt;/p>
&lt;h2 id="example-arbitrary-but-consistent-index-assignment">Example: arbitrary-but-consistent index assignment&lt;/h2>
&lt;p>Suppose that you want to give an index to every incoming element for a
key-and-window. You don&amp;rsquo;t care what the indices are, just as long as they are
unique and consistent. Before diving into the code for how to do this in a Beam
SDK, I&amp;rsquo;ll go over this example from the level of the model. In pictures, you
want to write a transform that maps input to output like this:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/stateful-processing/assign-indices.png"
alt="Assigning arbitrary but unique indices to each element"
width="180">&lt;/p>
&lt;p>The order of the elements A, B, C, D, E is arbitrary, hence their assigned
indices are arbitrary, but downstream transforms just need to be OK with this.
There is no associativity or commutativity as far as the actual values are
concerned. The order-insensitivity of this transform only extends to the point
of ensuring the necessary properties of the output: no duplicated indices, no
gaps, and every element gets an index.&lt;/p>
&lt;p>Conceptually expressing this as a stateful loop is as trivial as you can
imagine: The state you should store is the next index.&lt;/p>
&lt;ul>
&lt;li>As an element comes in, output it along with the next index.&lt;/li>
&lt;li>Increment the index.&lt;/li>
&lt;/ul>
&lt;p>This presents a good opportunity to talk about big data and parallelism,
because the algorithm in those bullet points is not parallelizable at all! If
you wanted to apply this logic over an entire &lt;code>PCollection&lt;/code>, you would have to
process each element of the &lt;code>PCollection&lt;/code> one-at-a-time&amp;hellip; this is obviously a
bad idea. State in Beam is tightly scoped so that most of the time a stateful
&lt;code>ParDo&lt;/code> transform should still be possible for a runner to execute in parallel,
though you still have to be thoughtful about it.&lt;/p>
&lt;p>A state cell in Beam is scoped to a key+window pair. When your DoFn reads or
writes state by the name of &lt;code>&amp;quot;index&amp;quot;&lt;/code>, it is actually accessing a mutable cell
specified by &lt;code>&amp;quot;index&amp;quot;&lt;/code> &lt;em>along with&lt;/em> the key and window currently being
processed. So, when thinking about a state cell, it may be helpful to consider
the full state of your transform as a table, where the rows are named according
to names you use in your program, like &lt;code>&amp;quot;index&amp;quot;&lt;/code>, and the columns are
key+window pairs, like this:&lt;/p>
&lt;div class="table-wrapper">&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>(key, window)&lt;sub>1&lt;/sub>&lt;/th>
&lt;th>(key, window)&lt;sub>2&lt;/sub>&lt;/th>
&lt;th>(key, window)&lt;sub>3&lt;/sub>&lt;/th>
&lt;th>&amp;hellip;&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>&amp;quot;index&amp;quot;&lt;/code>&lt;/td>
&lt;td>&lt;code>3&lt;/code>&lt;/td>
&lt;td>&lt;code>7&lt;/code>&lt;/td>
&lt;td>&lt;code>15&lt;/code>&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>&amp;quot;fizzOrBuzz?&amp;quot;&lt;/code>&lt;/td>
&lt;td>&lt;code>&amp;quot;fizz&amp;quot;&lt;/code>&lt;/td>
&lt;td>&lt;code>&amp;quot;7&amp;quot;&lt;/code>&lt;/td>
&lt;td>&lt;code>&amp;quot;fizzbuzz&amp;quot;&lt;/code>&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&amp;hellip;&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>&lt;/div>
&lt;p>(if you have a superb spatial sense, feel free to imagine this as a cube where
keys and windows are independent dimensions)&lt;/p>
&lt;p>You can provide the opportunity for parallelism by making sure that table has
enough columns. You might have many keys and many windows, or you might have
many of just one or the other:&lt;/p>
&lt;ul>
&lt;li>Many keys in few windows, for example a globally windowed stateful computation
keyed by user ID.&lt;/li>
&lt;li>Many windows over few keys, for example a fixed windowed stateful computation
over a global key.&lt;/li>
&lt;/ul>
&lt;p>Caveat: all Beam runners today parallelize only over the key.&lt;/p>
&lt;p>Most often your mental model of state can be focused on only a single column of
the table, a single key+window pair. Cross-column interactions do not occur
directly, by design.&lt;/p>
&lt;h2 id="state-in-beams-java-sdk">State in Beam&amp;rsquo;s Java SDK&lt;/h2>
&lt;p>Now that I have talked a bit about stateful processing in the Beam model and
worked through an abstract example, I&amp;rsquo;d like to show you what it looks like to
write stateful processing code using Beam&amp;rsquo;s Java SDK. Here is the code for a
stateful &lt;code>DoFn&lt;/code> that assigns an arbitrary-but-consistent index to each element
on a per key-and-window basis:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">MyKey&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">MyValue&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">MyKey&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">MyValue&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// A state cell holding a single Integer per key+window
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;index&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">indexSpec&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">VarIntCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">processElement&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;index&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">current&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">firstNonNull&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">()));&lt;/span>
&lt;span class="n">index&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">IndexAssigningStatefulDoFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">INDEX_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">CombiningStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;index&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">INDEX_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">unused_key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">element&lt;/span>
&lt;span class="n">current_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_index&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">index&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Let&amp;rsquo;s dissect this:&lt;/p>
&lt;ul>
&lt;li>The first thing to look at is the presence of a couple of &lt;code>@StateId(&amp;quot;index&amp;quot;)&lt;/code>
annotations. This calls out that you are using a mutable state cell named
&amp;ldquo;index&amp;rdquo; in this &lt;code>DoFn&lt;/code>. The Beam Java SDK, and from there your chosen runner,
will also note these annotations and use them to wire up your DoFn correctly.&lt;/li>
&lt;li>The first &lt;code>@StateId(&amp;quot;index&amp;quot;)&lt;/code> is annotated on a field of type &lt;code>StateSpec&lt;/code> (for
&amp;ldquo;state specification&amp;rdquo;). This declares and configures the state cell. The
type parameter &lt;code>ValueState&lt;/code> describes the kind of state you can get out of this
cell - &lt;code>ValueState&lt;/code> stores just a single value. Note that the spec itself is not
a usable state cell - you need the runner to provide that during pipeline
execution.&lt;/li>
&lt;li>To fully specify a &lt;code>ValueState&lt;/code> cell, you need to provide the coder
that the runner will use (as necessary) to serialize the value
you will be storing. This is the invocation &lt;code>StateSpecs.value(VarIntCoder.of())&lt;/code>.&lt;/li>
&lt;li>The second &lt;code>@StateId(&amp;quot;index&amp;quot;)&lt;/code> annotation is on a parameter to your
&lt;code>@ProcessElement&lt;/code> method. This indicates access to the ValueState cell that
was specified earlier.&lt;/li>
&lt;li>The state is accessed in the simplest way: &lt;code>read()&lt;/code> to read it, and
&lt;code>write(newvalue)&lt;/code> to write it.&lt;/li>
&lt;li>The other features of &lt;code>DoFn&lt;/code> are available in the usual way - such as
&lt;code>context.output(...)&lt;/code>. You can also use side inputs, side outputs, gain access
to the window, etc.&lt;/li>
&lt;/ul>
&lt;p>A few notes on how the SDK and runners see this DoFn:&lt;/p>
&lt;ul>
&lt;li>Your state cells are all explicitly declared so a Beam SDK or runner can
reason about them, for example to clear them out when a window expires.&lt;/li>
&lt;li>If you declare a state cell and then use it with the wrong type, the Beam
Java SDK will catch that error for you.&lt;/li>
&lt;li>If you declare two state cells with the same ID, the SDK will catch that,
too.&lt;/li>
&lt;li>The runner knows that this is a stateful &lt;code>DoFn&lt;/code> and may run it quite
differently, for example by additional data shuffling and synchronization in
order to avoid concurrent access to state cells.&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s look at one more example of how to use this API, this time a bit more real-world.&lt;/p>
&lt;h2 id="example-anomaly-detection">Example: anomaly detection&lt;/h2>
&lt;p>Suppose you are feeding a stream of actions by your user into some complex
model to predict some quantitative expression of the sorts of actions they
take, for example to detect fraudulent activity. You will build up the model
from events, and also compare incoming events against the latest model to
determine if something has changed.&lt;/p>
&lt;p>If you try to express the building of your model as a &lt;code>CombineFn&lt;/code>, you may have
trouble with &lt;code>mergeAccumulators&lt;/code>. Assuming you could express that, it might
look something like this:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">class&lt;/span> &lt;span class="nc">ModelFromEventsFn&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">CombineFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Model&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Model&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">abstract&lt;/span> &lt;span class="n">Model&lt;/span> &lt;span class="nf">createAccumulator&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">empty&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">abstract&lt;/span> &lt;span class="n">Model&lt;/span> &lt;span class="nf">addInput&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Model&lt;/span> &lt;span class="n">accumulator&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Event&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">accumulator&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">update&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// this is encouraged to mutate, for efficiency
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">abstract&lt;/span> &lt;span class="n">Model&lt;/span> &lt;span class="nf">mergeAccumulators&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Iterable&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Model&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">accumulators&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// ?? can you write this ??
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">abstract&lt;/span> &lt;span class="n">Model&lt;/span> &lt;span class="nf">extractOutput&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Model&lt;/span> &lt;span class="n">accumulator&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">accumulator&lt;/span>&lt;span class="o">;&lt;/span> &lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">ModelFromEventsFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">apache_beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">core&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombineFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">create_accumulator&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="c1"># Create a new empty model&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Model&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">add_input&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">merge_accumulators&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">accumulators&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="c1"># Custom merging logic&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">extract_output&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">model&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Now you have a way to compute the model of a particular user for a window as
&lt;code>Combine.perKey(new ModelFromEventsFn())&lt;/code>. How would you apply this model to
the same stream of events from which it is calculated? A standard way to do
take the result of a &lt;code>Combine&lt;/code> transform and use it while processing the
elements of a &lt;code>PCollection&lt;/code> is to read it as a side input to a &lt;code>ParDo&lt;/code>
transform. So you could side input the model and check the stream of events
against it, outputting the prediction, like so:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">...&lt;/span>
&lt;span class="kd">final&lt;/span> &lt;span class="n">PCollectionView&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Model&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">userModels&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">events&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Combine&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">perKey&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">ModelFromEventsFn&lt;/span>&lt;span class="o">()))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">View&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">asMap&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Prediction&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">predictions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">events&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">processElement&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">ctx&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">UserId&lt;/span> &lt;span class="n">userId&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ctx&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Event&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ctx&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getValue&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Model&lt;/span> &lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ctx&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">sideinput&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">userModels&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">userId&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="c1">// Perhaps some logic around when to output a new prediction
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="err">…&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">userId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">prediction&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="o">)))&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="c1"># Events is a collection of (user, event) pairs.&lt;/span>
&lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromEventSource&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowInto&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">....&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">user_models&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">AsDict&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">events&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">core&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombinePerKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ModelFromEventsFn&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">event_prediction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">user_event&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">models&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">user&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">user_event&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">event&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">user_event&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="c1"># Retrieve the model calculated for this user&lt;/span>
&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">models&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">user&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">user&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">prediction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="c1"># Predictions is a collection of (user, prediction) pairs.&lt;/span>
&lt;span class="n">predictions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">events&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">event_prediction&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">user_models&lt;/span>&lt;span class="p">)&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>In this pipeline, there is just one model emitted by the &lt;code>Combine.perKey(...)&lt;/code>
per user, per window, which is then prepared for side input by the &lt;code>View.asMap()&lt;/code>
transform. The processing of the &lt;code>ParDo&lt;/code> over events will block until that side
input is ready, buffering events, and will then check each event against the
model. This is a high latency, high completeness solution: The model takes into
account all user behavior in the window, but there can be no output until the
window is complete.&lt;/p>
&lt;p>Suppose you want to get some results earlier, or don&amp;rsquo;t even have any
natural windowing, but just want continuous analysis with the &amp;ldquo;model so far&amp;rdquo;,
even though your model may not be as complete. How can you control the updates
to the model against which you are checking your events? Triggers are the
generic Beam feature for managing completeness versus latency tradeoffs. So here
is the same pipeline with an added trigger that outputs a new model one second
after input arrives:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">...&lt;/span>
&lt;span class="n">PCollectionView&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Model&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">userModels&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">events&lt;/span>
&lt;span class="c1">// A tradeoff between latency and cost
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">triggering&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">AfterProcessingTime&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">pastFirstElementInPane&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardSeconds&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Combine&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">perKey&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">ModelFromEventsFn&lt;/span>&lt;span class="o">()))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">View&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">asMap&lt;/span>&lt;span class="o">());&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">...&lt;/span>
&lt;span class="n">user_models&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">AsDict&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">events&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowInto&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">GlobalWindows&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;span class="n">trigger&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">trigger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">AfterAll&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">trigger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">AfterCount&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">trigger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">AfterProcessingTime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombinePerKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ModelFromEventsFn&lt;/span>&lt;span class="p">()))&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>This is often a pretty nice tradeoff between latency and cost: If a huge flood
of events comes in a second, then you will only emit one new model, so you
won&amp;rsquo;t be flooded with model outputs that you cannot even use before they are
obsolete. In practice, the new model may not be present on the side input
channel until many more seconds have passed, due to caches and processing
delays preparing the side input. Many events (maybe an entire batch of
activity) will have passed through the &lt;code>ParDo&lt;/code> and had their predictions
calculated according to the prior model. If the runner gave a tight enough
bound on cache expirations and you used a more aggressive trigger, you might be
able to improve latency at additional cost.&lt;/p>
&lt;p>But there is another cost to consider: you are outputting many uninteresting
outputs from the &lt;code>ParDo&lt;/code> that will be processed downstream. If the
&amp;ldquo;interestingness&amp;rdquo; of the output is only well-defined relative to the prior
output, then you cannot use a &lt;code>Filter&lt;/code> transform to reduce data volume downstream.&lt;/p>
&lt;p>Stateful processing lets you address both the latency problem of side inputs
and the cost problem of excessive uninteresting output. Here is the code, using
only features I have already introduced:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">UserId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Prediction&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;model&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Model&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">modelSpec&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">coder&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;previousPrediction&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Prediction&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">previousPredictionSpec&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Prediction&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">coder&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">processElement&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;previousPrediction&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Prediction&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">previousPredictionState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;model&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Model&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">modelState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">UserId&lt;/span> &lt;span class="n">userId&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Event&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getValue&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="n">Model&lt;/span> &lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">modelState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Prediction&lt;/span> &lt;span class="n">previousPrediction&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">previousPredictionState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Prediction&lt;/span> &lt;span class="n">newPrediction&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">prediction&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">modelState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">previousPrediction&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">null&lt;/span>
&lt;span class="o">||&lt;/span> &lt;span class="n">shouldOutputNewPrediction&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">previousPrediction&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">newPrediction&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">userId&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">newPrediction&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">previousPredictionState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">newPrediction&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">};&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;div class=language-py>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">ModelStatefulFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">PREVIOUS_PREDICTION&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BagStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;previous_pred_state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">PredictionCoder&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">MODEL_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">CombiningValueStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;model_state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">ModelCoder&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;span class="n">ModelFromEventsFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">user_event&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">previous_pred_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">PREVIOUS_PREDICTION&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">model_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MODEL_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">user&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">user_event&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">event&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">user_event&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">previous_prediction&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">previous_pred_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">new_prediction&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">prediction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">model_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">previous_prediction&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="ow">or&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">should_output_prediction&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">previous_prediction&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_prediction&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">previous_pred_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">previous_pred_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_prediction&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">user&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_prediction&lt;/span>&lt;span class="p">)&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>Let&amp;rsquo;s walk through it,&lt;/p>
&lt;ul>
&lt;li>You have two state cells declared, &lt;code>@StateId(&amp;quot;model&amp;quot;)&lt;/code> to hold the current
state of the model for a user and &lt;code>@StateId(&amp;quot;previousPrediction&amp;quot;)&lt;/code> to hold
the prediction output previously.&lt;/li>
&lt;li>Access to the two state cells by annotation in the &lt;code>@ProcessElement&lt;/code> method
is as before.&lt;/li>
&lt;li>You read the current model via &lt;code>modelState.read()&lt;/code>.
per-key-and-window, this is a model just for the UserId of the Event
currently being processed.&lt;/li>
&lt;li>You derive a new prediction &lt;code>model.prediction(event)&lt;/code> and compare it against
the last one you output, accessed via
&lt;code>previousPredicationState.read()&lt;/code>.&lt;/li>
&lt;li>You then update the model &lt;code>model.update()&lt;/code> and write it via
&lt;code>modelState.write(...)&lt;/code>. It is perfectly fine to mutate the value
you pulled out of state as long as you also remember to write the mutated
value, in the same way you are encouraged to mutate &lt;code>CombineFn&lt;/code> accumulators.&lt;/li>
&lt;li>If the prediction has changed a significant amount since the last time you
output, you emit it via &lt;code>context.output(...)&lt;/code> and
save the prediction using &lt;code>previousPredictionState.write(...)&lt;/code>.
Here the decision is relative to the prior prediction output, not the last
one computed - realistically you might have some complex conditions here.&lt;/li>
&lt;/ul>
&lt;p>Most of the above is just talking through Java! But before you go out and
convert all of your pipelines to use stateful processing, I want to go over
some considerations as to whether it is a good fit for your use case.&lt;/p>
&lt;h2 id="performance-considerations">Performance considerations&lt;/h2>
&lt;p>To decide whether to use per-key-and-window state, you need to consider how it
executes. You can dig into how a particular runner manages state, but there are
some general things to keep in mind:&lt;/p>
&lt;ul>
&lt;li>Partitioning per-key-and-window: perhaps the most important thing to
consider is that the runner may have to shuffle your data to colocate all
the data for a particular key+window. If the data is already shuffled
correctly, the runner may take advantage of this.&lt;/li>
&lt;li>Synchronization overhead: the API is designed so the runner takes care of
concurrency control, but this means that the runner cannot parallelize
processing of elements for a particular key+window even when it would otherwise
be advantageous.&lt;/li>
&lt;li>Storage and fault tolerance of state: since state is per-key-and-window, the
more keys and windows you expect to process simultaneously, the more storage
you will incur. Because state benefits from all the fault tolerance /
consistency properties of your other data in Beam, it also adds to the cost of
committing the results of processing.&lt;/li>
&lt;li>Expiration of state: also since state is per-window, the runner can reclaim
the resources when a window expires (when the watermark exceeds its allowed
lateness) but this could mean that the runner is tracking an additional timer
per key and window to cause reclamation code to execute.&lt;/li>
&lt;/ul>
&lt;h2 id="go-use-it">Go use it!&lt;/h2>
&lt;p>If you are new to Beam, I hope you are now interested in seeing if Beam with
stateful processing addresses your use case. If you are already using Beam, I
hope this new addition to the model unlocks new use cases for you. Do check
the &lt;a href="/documentation/runners/capability-matrix/">capability
matrix&lt;/a> to
see the level of support for this new model feature on your favorite
backend(s).&lt;/p>
&lt;p>And please do join the community at
&lt;a href="/get-started/support">user@beam.apache.org&lt;/a>. We&amp;rsquo;d love to
hear from you.&lt;/p></description></item><item><title>Blog: Media recap of the Apache Beam graduation</title><link>/blog/graduation-media-recap/</link><pubDate>Wed, 01 Feb 2017 00:00:01 -0800</pubDate><guid>/blog/graduation-media-recap/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>One year ago today Apache Beam was accepted into incubation at the Apache
Software Foundation. The community&amp;rsquo;s work over the past year culminated, just
over three weeks ago, with an &lt;a href="/blog/2017/01/10/beam-graduates.html">announcement&lt;/a>
that Apache Beam has successfully graduated as a new Top-Level Project at the
foundation. Graduation sparked an additional interest in the project, from
corporate endorsements, news articles, interviews, to the volume of traffic to
our website and mailing lists.&lt;/p>
&lt;p>Corporate endorsements include Google, PayPal, Talend, data Artisans, and
others. You can read more in the following blog posts:&lt;/p>
&lt;ul>
&lt;li>Google: &amp;ldquo;&lt;a href="https://opensource.googleblog.com/2017/01/apache-beam-graduates.html">Apache Beam graduates to a top-level project&lt;/a>&amp;rdquo; by Tyler Akidau.&lt;/li>
&lt;li>Talend: &amp;ldquo;&lt;a href="https://www.talend.com/blog/2017/01/13/future-apache-beam-now-top-level-apache-software-foundation-project/">The Future of Apache Beam, Now a Top-Level Apache Software Foundation Project&lt;/a>&amp;rdquo; by Jean-Baptiste Onofré.&lt;/li>
&lt;li>Talend: &amp;ldquo;&lt;a href="https://www.talend.com/blog/2017/01/23/apache-beam-way-greater-data-agility/?utm_medium=socialpost&amp;amp;utm_source=twitter&amp;amp;utm_campaign=blog">Apache Beam Your Way to Greater Data Agility&lt;/a>&amp;rdquo; by Shane Kent.&lt;/li>
&lt;li>Google: &amp;ldquo;&lt;a href="https://cloud.google.com/blog/big-data/2017/01/apache-beam-graduates-from-incubation-try-it-today-on-google-cloud-dataflow">Apache Beam graduates from incubation: Try it today on Google Cloud Dataflow&lt;/a>&amp;rdquo; by Frances Perry.&lt;/li>
&lt;/ul>
&lt;p>News coverage started with the Apache Software Foundation’s press release in
&lt;a href="https://globenewswire.com/news-release/2017/01/10/904692/0/en/The-Apache-Software-Foundation-Announces-Apache-Beam-as-a-Top-Level-Project.html">Nasdaq GlobeNewswire&lt;/a>,
and followed by coverage in many independent outlets. Some of those in English
include:&lt;/p>
&lt;ul>
&lt;li>ZDNet: &amp;ldquo;&lt;a href="https://www.zdnet.com/article/apache-beam-and-spark-new-coopetition-for-squashing-the-lambda-architecture/">Apache Beam and Spark: New coopetition for squashing the Lambda Architecture?&lt;/a>&amp;rdquo; by Tony Baer.&lt;/li>
&lt;li>Datanami: &amp;ldquo;&lt;a href="https://www.datanami.com/2017/01/10/google-lauds-outside-influence-apache-beam/">Google Lauds Outside Influence on Apache Beam&lt;/a>&amp;rdquo; by Alex Woodie.&lt;/li>
&lt;li>InfoWorld / JavaWorld: &amp;ldquo;&lt;a href="https://www.infoworld.com/article/3156598/big-data/apache-beam-unifies-batch-and-streaming-for-big-data.html">Apache Beam unifies batch and streaming for big data&lt;/a>&amp;rdquo; by Serdar Yegulalp, and republished in &lt;a href="https://www.javaworld.com/article/3156598/big-data/apache-beam-unifies-batch-and-streaming-for-big-data.html">JavaWorld&lt;/a>.&lt;/li>
&lt;li>JAXenter: &amp;ldquo;&lt;a href="https://jaxenter.com/apache-beam-interview-131314.html">In a way, Apache Beam is the glue that connects many big data systems together&lt;/a>&amp;rdquo; by Kypriani Sinaris.&lt;/li>
&lt;li>OStatic: &amp;ldquo;Apache Beam Unifies Batch and Streaming Data Processing&amp;rdquo; by Sam Dean. &lt;!-- http://ostatic.com/blog/apache-beam-unifies-batch-and-streaming-data-processing -->&lt;/li>
&lt;li>Enterprise Apps Today: &amp;ldquo;&lt;a href="http://www.enterpriseappstoday.com/business-intelligence/data-analytics/apache-beam-graduates-to-help-define-streaming-data-processing.html">Apache Beam Graduates to Help Define Streaming Data Processing&lt;/a>&amp;rdquo; by Sean Michael Kerner.&lt;/li>
&lt;li>The Register: &amp;ldquo;&lt;a href="https://www.theregister.co.uk/2017/01/10/google_must_be_ibeamiing_as_apache_announces_its_new_top_level_projects/">Google must be Beaming as Apache announces its new top-level projects&lt;/a>&amp;rdquo; by Alexander J. Martin.&lt;/li>
&lt;li>SiliconANGLE: &amp;ldquo;&lt;a href="https://siliconangle.com/blog/2017/01/11/apache-software-foundation-announces-2-top-level-projects/">Apache Software Foundation announces two more top-level open source projects&lt;/a>&amp;rdquo; by Mike Wheatley.&lt;/li>
&lt;li>SD Times: &amp;ldquo;&lt;a href="https://sdtimes.com/apache-beam-goes-top-level/">Apache Beam goes top level&lt;/a>&amp;rdquo; by Alex Handy.&lt;/li>
&lt;/ul>
&lt;p>Graduation and media coverage helped push Beam website traffic to record levels.
The website traffic, measured in unique sessions per hour, peaked at more than
15 times above the previous week’s numbers. In a steady state, the traffic is
several times larger than before graduation.&lt;/p>
&lt;p>Hopefully these perspectives entice you to join us on this exciting ride, either
as a user or a contributor, as we work towards our first release with API
stability. If you’d like to try out Apache Beam today, check out the latest
&lt;a href="/get-started/downloads/">0.4.0 release&lt;/a>. We welcome
contribution and participation from anyone through our mailing lists, issue
tracker, pull requests, and events.&lt;/p></description></item><item><title>Blog: Apache Beam established as a new top-level project</title><link>/blog/beam-graduates/</link><pubDate>Tue, 10 Jan 2017 00:00:01 -0800</pubDate><guid>/blog/beam-graduates/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Today, the Apache Software Foundation &lt;a href="https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces">announced&lt;/a>
that Apache Beam has successfully graduated from incubation, becoming a new
Top-Level Project at the foundation and signifying that its &amp;ldquo;community and
products have been well-governed under the foundation’s meritocratic process
and principles&amp;rdquo;.&lt;/p>
&lt;p>Graduation is an exciting milestone for Apache Beam. Becoming a top-level
project is a recognition of the amazing growth of the Apache Beam community,
both in terms of size and diversity. It is fantastic to see ever-increasing
participation on our development and user mailing lists, in pull requests,
and at conferences and meetups. Contributions of additional runners and IO
connectors further show the significant interest from other projects and
organizations in being part of our growing community.&lt;/p>
&lt;p>Although graduation is a statement about community, our technical progress has
also been impressive. We started with code donations from several companies
that were tailored with a specific vendor and scenario in mind and had fragile
component boundaries. The whole community worked extremely hard over the past
year to refactor the codebase and documentation into a truly vendor-neutral and
extensible framework. We have established an impressive engineering system and
processes that promote test-driven development. This concerted focus on quality
and execution excellence enabled us to publish four releases, each significantly
better than the previous.&lt;/p>
&lt;p>Going forward, we will continue to extend the core abstractions to distill
additional complex data processing patterns into intuitive APIs, and, at the
same time, enhance the ability to interconnect additional storage/messaging
systems and execution engines. Together, we are excited to push forward the
state of the art in distributed data processing.&lt;/p>
&lt;p>This is also an opportunity to thank those who helped and supported us reaching
this milestone. None of this would be possible without the Apache Software
Foundation and its amazing volunteers who have wholeheartedly welcomed us into
the family. In particular, we’d like to thank the Apache Incubator community
for their guidance producing releases and following processes. We are especially
thankful to the Apache Infra team for their assistance establishing Beam’s
engineering system, particularly in the areas of testing infrastructure and
development productivity.&lt;/p>
&lt;p>On behalf of the whole Apache Beam community, thank you to our incubation
mentors Ted Dunning, Venkatesh Seetharam, and, previously, Bertrand Delacretaz
and Jim Jagielski, for teaching us the Apache Way and guiding us through
incubation. In particular, special thanks goes to our incubation champion and
mentor Jean-Baptiste Onofré, whose extraordinary help, day after day, was
instrumental in getting us here, and who continues as an active code contributor
and a member of the project management committee.&lt;/p>
&lt;p>Please consider joining us, whether as a user or a contributor, as we work
towards our first release with API stability. If you’d like to try out Apache
Beam today, check out the latest
&lt;a href="/get-started/downloads/">0.4.0 release&lt;/a>. We welcome
contribution and participation from anyone through our mailing lists, issue
tracker, pull requests, and events.&lt;/p></description></item><item><title>Blog: Release 0.4.0 adds a runner for Apache Apex</title><link>/blog/added-apex-runner/</link><pubDate>Mon, 09 Jan 2017 10:00:01 -0700</pubDate><guid>/blog/added-apex-runner/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The latest release 0.4.0 of &lt;a href="/">Apache Beam&lt;/a> adds a new runner for &lt;a href="https://apex.apache.org/">Apache Apex&lt;/a>. We are excited to reach this initial milestone and are looking forward to continued collaboration between the Beam and Apex communities to advance the runner.&lt;/p>
&lt;p>Beam evolved from the Google Dataflow SDK and as incubator project has quickly adapted the Apache way, grown the community and attracts increasing interest from users that hope to benefit from a conceptual strong unified programming model that is portable between different big data processing frameworks (see &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Streaming-101&lt;/a> and &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102">Streaming-102&lt;/a>). Multiple Apache projects already provide runners for Beam (see &lt;a href="/documentation/runners/capability-matrix/">runners and capabilities matrix&lt;/a>).&lt;/p>
&lt;p>Apex is a stream processing framework for low-latency, high-throughput, stateful and reliable processing of complex analytics pipelines on clusters. Apex was developed since 2012 and is used in production by large companies for real-time and batch processing at scale.&lt;/p>
&lt;p>The initial revision of the runner was focussed on broad coverage of the Beam model on a functional level. That means, there will be follow up work in several areas to take the runner from functional to scalable and high performance to match the capabilities of Apex and its native API. The runner capabilities matrix shows that the Apex capabilities are well aligned with the Beam model. Specifically, the ability to track computational state in a fault tolerant and efficient manner is needed to broadly support the windowing concepts, including event time based processing.&lt;/p>
&lt;h2 id="stateful-stream-processor">Stateful Stream Processor&lt;/h2>
&lt;p>Apex was built as stateful stream processor from the ground up. Operators checkpoint state in a distributed and asynchronous manner that produces a consistent snapshot for the entire processing graph, which can be used for recovery. Apex also supports such recovery in an incremental, or fine grained, manner. This means only the portion of the DAG that is actually affected by a failure will be recovered while the remaining pipeline continues processing (this can be leveraged to implement use cases with special needs, such as speculative execution to achieve SLA on the processing latency). The state checkpointing along with idempotent processing guarantee is the basis for exactly-once results support in Apex.&lt;/p>
&lt;h2 id="translation-to-apex-dag">Translation to Apex DAG&lt;/h2>
&lt;p>A Beam runner needs to implement the translation from the Beam model to the underlying frameworks execution model. In the case of Apex, the runner will translate the pipeline into the native (compositional, low level) DAG API (which is also the base for a number of other API that are available to specify applications that run on Apex). The DAG consists of operators (functional building blocks that are connected with streams. The runner provides the execution layer. In the case of Apex it is distributed stream processing, operators process data event by event. The minimum set of operators covers Beam’s primitive transforms: &lt;code>ParDo.Bound&lt;/code>, &lt;code>ParDo.BoundMulti&lt;/code>, &lt;code>Read.Unbounded&lt;/code>, &lt;code>Read.Bounded&lt;/code>, &lt;code>GroupByKey&lt;/code>, &lt;code>Flatten.FlattenPCollectionList&lt;/code> etc.&lt;/p>
&lt;h2 id="execution-and-testing">Execution and Testing&lt;/h2>
&lt;p>In this release, the Apex runner executes the pipelines in embedded mode, where, similar to the direct runner, everything is executed in a single JVM. See &lt;a href="/get-started/quickstart/">quickstart&lt;/a> on how to run the Beam examples with the Apex runner.&lt;/p>
&lt;p>Embedded mode is useful for development and debugging. Apex in production runs distributed on Apache Hadoop YARN clusters. An example how a Beam pipeline can be embedded into an Apex application package to run on YARN can be found &lt;a href="https://github.com/tweise/apex-samples/tree/master/beam-apex-wordcount">here&lt;/a> and support for direct launch in the runner is currently being worked on.&lt;/p>
&lt;p>The Beam project has a strong focus on development process and tooling, including testing. For the runners, there is a comprehensive test suite with more than 200 integration tests that are executed against each runner to ensure they don’t break as changes are made. The tests cover the capabilities of the matrix and thus are a measure of completeness and correctness of the runner implementations. The suite was very helpful when developing the Apex runner.&lt;/p>
&lt;h2 id="outlook">Outlook&lt;/h2>
&lt;p>The next step is to take the Apex runner from functional to ready for real applications that run distributed, leveraging the scalability and performance features of Apex, similar to its native API. This includes chaining of ParDos, partitioning, optimizing combine operations etc. To get involved, please see &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20and%20component%20%3D%20runner-apex%20and%20resolution%20%3D%20unresolved">JIRA&lt;/a> and join the Beam community.&lt;/p></description></item><item><title>Blog: Testing Unbounded Pipelines in Apache Beam</title><link>/blog/test-stream/</link><pubDate>Thu, 20 Oct 2016 10:00:00 -0800</pubDate><guid>/blog/test-stream/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The Beam Programming Model unifies writing pipelines for Batch and Streaming
pipelines. We’ve recently introduced a new PTransform to write tests for
pipelines that will be run over unbounded datasets and must handle out-of-order
and delayed data.&lt;/p>
&lt;p>Watermarks, Windows and Triggers form a core part of the Beam programming model
&amp;ndash; they respectively determine how your data are grouped, when your input is
complete, and when to produce results. This is true for all pipelines,
regardless of if they are processing bounded or unbounded inputs. If you’re not
familiar with watermarks, windowing, and triggering in the Beam model,
&lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Streaming 101&lt;/a>
and &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102">Streaming 102&lt;/a>
are an excellent place to get started. A key takeaway from
these articles: in realistic streaming scenarios with intermittent failures and
disconnected users, data can arrive out of order or be delayed. Beam’s
primitives provide a way for users to perform useful, powerful, and correct
computations in spite of these challenges.&lt;/p>
&lt;p>As Beam pipeline authors, we need comprehensive tests that cover crucial
failure scenarios and corner cases to gain real confidence that a pipeline is
ready for production. The existing testing infrastructure within the Beam SDKs
permits tests to be written which examine the contents of a Pipeline at
execution time. However, writing unit tests for pipelines that may receive
late data or trigger multiple times has historically ranged from complex to
not possible, as pipelines that read from unbounded sources do not shut down
without external intervention, while pipelines that read from bounded sources
exclusively cannot test behavior with late data nor most speculative triggers.
Without additional tools, pipelines that use custom triggers and handle
out-of-order data could not be easily tested.&lt;/p>
&lt;p>This blog post introduces our new framework for writing tests for pipelines that
handle delayed and out-of-order data in the context of the LeaderBoard pipeline
from the Mobile Gaming example series.&lt;/p>
&lt;h2 id="leaderboard-and-the-mobile-gaming-example">LeaderBoard and the Mobile Gaming Example&lt;/h2>
&lt;p>&lt;a href="https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java#L177">LeaderBoard&lt;/a>
is part of the &lt;a href="https://github.com/apache/beam/tree/master/examples/java/src/main/java/org/apache/beam/examples/complete/game">Beam mobile gaming examples&lt;/a>
(and &lt;a href="/get-started/mobile-gaming-example/">walkthroughs&lt;/a>)
which produces a continuous accounting of user and team scores. User scores are
calculated over the lifetime of the program, while team scores are calculated
within fixed windows with a default duration of one hour. The LeaderBoard
pipeline produces speculative and late panes as appropriate, based on the
configured triggering and allowed lateness of the pipeline. The expected outputs
of the LeaderBoard pipeline vary depending on when elements arrive in relation
to the watermark and the progress of processing time, which could not previously
be controlled within a test.&lt;/p>
&lt;h2 id="writing-deterministic-tests-to-emulate-nondeterminism">Writing Deterministic Tests to Emulate Nondeterminism&lt;/h2>
&lt;p>The Beam testing infrastructure provides the
&lt;a href="https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/testing/PAssert.html">PAssert&lt;/a>
methods, which assert properties about the contents of a PCollection from within
a pipeline. We have expanded this infrastructure to include
&lt;a href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestStream.java">TestStream&lt;/a>,
which is a PTransform that performs a series of events, consisting of adding
additional elements to a pipeline, advancing the watermark of the TestStream,
and advancing the pipeline processing time clock. TestStream permits tests which
observe the effects of triggers on the output a pipeline produces.&lt;/p>
&lt;p>While executing a pipeline that reads from a TestStream, the read waits for all
of the consequences of each event to complete before continuing on to the next
event, ensuring that when processing time advances, triggers that are based on
processing time fire as appropriate. With this transform, the effect of
triggering and allowed lateness can be observed on a pipeline, including
reactions to speculative and late panes and dropped data.&lt;/p>
&lt;h2 id="element-timings">Element Timings&lt;/h2>
&lt;p>Elements arrive either before, with, or after the watermark, which categorizes
them into the &amp;ldquo;early&amp;rdquo;, &amp;ldquo;on-time&amp;rdquo;, and &amp;ldquo;late&amp;rdquo; divisions. &amp;ldquo;Late&amp;rdquo; elements can be
further subdivided into &amp;ldquo;unobservably&amp;rdquo;, &amp;ldquo;observably&amp;rdquo;, and &amp;ldquo;droppably&amp;rdquo; late,
depending on the window to which they are assigned and the maximum allowed
lateness, as specified by the windowing strategy. Elements that arrive with
these timings are emitted into panes, which can be &amp;ldquo;EARLY&amp;rdquo;, &amp;ldquo;ON-TIME&amp;rdquo;, or
&amp;ldquo;LATE&amp;rdquo;, depending on the position of the watermark when the pane was emitted.&lt;/p>
&lt;p>Using TestStream, we can write tests that demonstrate that speculative panes are
output after their trigger condition is met, that the advancing of the watermark
causes the on-time pane to be produced, and that late-arriving data produces
refinements when it arrives before the maximum allowed lateness, and is dropped
after.&lt;/p>
&lt;p>The following examples demonstrate how you can use TestStream to provide a
sequence of events to the Pipeline, where the arrival of elements is interspersed
with updates to the watermark and the advance of processing time. Each of these
events runs to completion before additional events occur.&lt;/p>
&lt;p>In the diagrams, the time at which events occurred in &amp;ldquo;real&amp;rdquo; (event) time
progresses as the graph moves to the right. The time at which the pipeline
receives them progresses as the graph goes upwards. The watermark is represented
by the squiggly red line, and each starburst is the firing of a trigger and the
associated pane.&lt;/p>
&lt;img class="center-block" src="/images/blog/test-stream/elements-all-on-time.png" alt="Elements on the Event and Processing time axes, with the Watermark and produced panes" width="442">
&lt;h3 id="everything-arrives-on-time">Everything arrives on-time&lt;/h3>
&lt;p>For example, if we create a TestStream where all the data arrives before the
watermark and provide the result PCollection as input to the CalculateTeamScores
PTransform:&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">TestStream&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">infos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TestStream&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">AvroCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">12&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">)),&lt;/span>
                &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;navy&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">)),&lt;/span>
                &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;navy&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">3&lt;/span>&lt;span class="o">))))&lt;/span>
   &lt;span class="c1">// Move the watermark past the end the end of the window
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkTo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">)&lt;/span>
                                &lt;span class="o">.&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkToInfinity&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">teamScores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">createEvents&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CalculateTeamScores&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="o">));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>we can then assert that the result PCollection contains elements that arrived:&lt;/p>
&lt;img class="center-block" src="/images/blog/test-stream/elements-all-on-time.png" alt="Elements all arrive before the watermark, and are produced in the on-time pane" width="442">
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="c1">// Only one value is emitted for the blue team
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">PAssert&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">that&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">teamScores&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">inWindow&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">containsInAnyOrder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">18&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">run&lt;/span>&lt;span class="o">();&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="some-elements-are-late-but-arrive-before-the-end-of-the-window">Some elements are late, but arrive before the end of the window&lt;/h3>
&lt;p>We can also add data to the TestStream after the watermark, but before the end
of the window (shown below to the left of the red watermark), which demonstrates
&amp;ldquo;unobservably late&amp;rdquo; data - that is, data that arrives late, but is promoted by
the system to be on time, as it arrives before the watermark passes the end of
the window&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">TestStream&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">infos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TestStream&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">AvroCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">)),&lt;/span>
        &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;navy&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">3&lt;/span>&lt;span class="o">))))&lt;/span>
   &lt;span class="c1">// Move the watermark up to &amp;#34;near&amp;#34; the end of the window
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkTo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">)&lt;/span>
                                &lt;span class="o">.&lt;/span>&lt;span class="na">minus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">12&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">ZERO&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkToInfinity&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">teamScores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">createEvents&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CalculateTeamScores&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="o">));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;img class="center-block" src="/images/blog/test-stream/elements-unobservably-late.png" alt="An element arrives late, but before the watermark passes the end of the window, and is produced in the on-time pane" width="442">
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="c1">// Only one value is emitted for the blue team
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">PAssert&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">that&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">teamScores&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">inWindow&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">containsInAnyOrder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">18&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">run&lt;/span>&lt;span class="o">();&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="elements-are-late-and-arrive-after-the-end-of-the-window">Elements are late, and arrive after the end of the window&lt;/h3>
&lt;p>By advancing the watermark farther in time before adding the late data, we can
demonstrate the triggering behavior that causes the system to emit an on-time
pane, and then after the late data arrives, a pane that refines the result.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">TestStream&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">infos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TestStream&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">AvroCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">)),&lt;/span>
          &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;navy&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">3&lt;/span>&lt;span class="o">))))&lt;/span>
    &lt;span class="c1">// Move the watermark up to &amp;#34;near&amp;#34; the end of the window
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkTo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">)&lt;/span>
                                 &lt;span class="o">.&lt;/span>&lt;span class="na">minus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">12&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">ZERO&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkToInfinity&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">teamScores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">createEvents&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CalculateTeamScores&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="o">));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;img class="center-block" src="/images/blog/test-stream/elements-observably-late.png" alt="Elements all arrive before the watermark, and are produced in the on-time pane" width="442">
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="c1">// An on-time pane is emitted with the events that arrived before the window closed
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">PAssert&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">that&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">teamScores&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">inOnTimePane&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">containsInAnyOrder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">6&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="c1">// The final pane contains the late refinement
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">PAssert&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">that&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">teamScores&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">inFinalPane&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">containsInAnyOrder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">18&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">run&lt;/span>&lt;span class="o">();&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="elements-are-late-and-after-the-end-of-the-window-plus-the-allowed-lateness">Elements are late, and after the end of the window plus the allowed lateness&lt;/h3>
&lt;p>If we push the watermark even further into the future, beyond the maximum
configured allowed lateness, we can demonstrate that the late element is dropped
by the system.&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">TestStream&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">infos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TestStream&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">AvroCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">ZERO&lt;/span>&lt;span class="o">),&lt;/span>
         &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;navy&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">3&lt;/span>&lt;span class="o">)))&lt;/span>
    &lt;span class="c1">// Move the watermark up to &amp;#34;near&amp;#34; the end of the window
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkTo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">)&lt;/span>
                                        &lt;span class="o">.&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="o">)&lt;/span>
                                        &lt;span class="o">.&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>
                     &lt;span class="s">&amp;#34;sky&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span>
                     &lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span>
                     &lt;span class="n">12&lt;/span>&lt;span class="o">,&lt;/span>
                     &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">minus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkToInfinity&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">teamScores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">createEvents&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CalculateTeamScores&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TEAM_WINDOW_DURATION&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="o">));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;img class="center-block" src="/images/blog/test-stream/elements-droppably-late.png" alt="Elements all arrive before the watermark, and are produced in the on-time pane" width="442">
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="c1">// An on-time pane is emitted with the events that arrived before the window closed
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">PAssert&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">that&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">teamScores&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">inWindow&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">containsInAnyOrder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">6&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">run&lt;/span>&lt;span class="o">();&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h3 id="elements-arrive-before-the-end-of-the-window-and-some-processing-time-passes">Elements arrive before the end of the window, and some processing time passes&lt;/h3>
&lt;p>Using additional methods, we can demonstrate the behavior of speculative
triggers by advancing the processing time of the TestStream. If we add elements
to an input PCollection, occasionally advancing the processing time clock, and
apply &lt;code>CalculateUserScores&lt;/code>&lt;/p>
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">TestStream&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">AvroCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
   &lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;scarlet&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">)),&lt;/span>
               &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;scarlet&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceProcessingTime&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">12&lt;/span>&lt;span class="o">))&lt;/span>
   &lt;span class="o">.&lt;/span>&lt;span class="na">addElements&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;oxblood&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">)).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardSeconds&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">22&lt;/span>&lt;span class="o">)),&lt;/span>
               &lt;span class="k">new&lt;/span> &lt;span class="n">GameActionInfo&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;scarlet&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">4&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0L&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">2&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceProcessingTime&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">15&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">advanceWatermarkToInfinity&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">userScores&lt;/span> &lt;span class="o">=&lt;/span>
   &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">infos&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">CalculateUserScores&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="o">));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;img class="center-block" src="/images/blog/test-stream/elements-processing-speculative.png" alt="Elements all arrive before the watermark, and are produced in the on-time pane" width="442">
&lt;div class=language-java>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">PAssert&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">that&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">userScores&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">inEarlyGlobalWindowPanes&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">containsInAnyOrder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;scarlet&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">5&lt;/span>&lt;span class="o">),&lt;/span>
                        &lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;scarlet&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">9&lt;/span>&lt;span class="o">),&lt;/span>
&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;oxblood&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">2&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">run&lt;/span>&lt;span class="o">();&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;h2 id="teststream---under-the-hood">TestStream - Under the Hood&lt;/h2>
&lt;p>TestStream relies on a pipeline concept we’ve introduced, called quiescence, to
utilize the existing runner infrastructure while providing guarantees about when
a root transform will called by the runner. This consists of properties about
pending elements and triggers, namely:&lt;/p>
&lt;ul>
&lt;li>No trigger is permitted to fire but has not fired&lt;/li>
&lt;li>All elements are either buffered in state or cannot progress until a side input becomes available&lt;/li>
&lt;/ul>
&lt;p>Simplified, this means that, in the absence of an advancement in input
watermarks or processing time, or additional elements being added to the
pipeline, the pipeline will not make progress. Whenever the TestStream PTransform
performs an action, the runner must not reinvoke the same instance until the
pipeline has quiesced. This ensures that the events specified by TestStream
happen &amp;ldquo;in-order&amp;rdquo;, which ensures that input watermarks and the system clock do
not advance ahead of the elements they hoped to hold up.&lt;/p>
&lt;p>The DirectRunner has been modified to use quiescence as the signal that it
should add more work to the Pipeline, and the implementation of TestStream in
that runner uses this fact to perform a single output per event. The DirectRunner
implementation also directly controls the runner’s system clock, ensuring that
tests will complete promptly even if there is a multi-minute processing time
trigger located within the pipeline.&lt;/p>
&lt;p>The TestStream transform is supported in the DirectRunner. For most users, tests
written using TestPipeline and PAsserts will automatically function while using
TestStream.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>The addition of TestStream alongside window and pane-specific matchers in PAssert
has enabled the testing of Pipelines which produce speculative and late panes.
This permits tests for all styles of pipeline to be expressed directly within the
Java SDK. If you have questions or comments, we’d love to hear them on the
&lt;a href="/get-started/support/">mailing lists&lt;/a>.&lt;/p></description></item><item><title>Blog: Strata+Hadoop World and Beam</title><link>/blog/strata-hadoop-world-and-beam/</link><pubDate>Tue, 11 Oct 2016 09:00:00 -0800</pubDate><guid>/blog/strata-hadoop-world-and-beam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Tyler Akidau and I gave a &lt;a href="https://conferences.oreilly.com/strata/hadoop-big-data-ny/public/schedule/detail/52129">three-hour tutorial&lt;/a> on Apache Beam at Strata+Hadoop World 2016. We had a plethora of help from our TAs: Kenn Knowles, Reuven Lax, Felipe Hoffa, Slava Chernyak, and Jamie Grier. There were a total of 66 people that attended the session.&lt;/p>
&lt;img src="/images/blog/IMG_20160927_170956.jpg" alt="Exercise time">
&lt;p>If you want to take a look at the tutorial materials, we’ve put them up &lt;a href="https://github.com/eljefe6a/beamexample">on GitHub&lt;/a>. This includes the &lt;a href="https://github.com/eljefe6a/beamexample/blob/master/BeamTutorial/slides.pdf">actual slides&lt;/a> as well as the &lt;a href="https://github.com/eljefe6a/beamexample/tree/master/BeamTutorial/src/main/java/org/apache/beam/examples/tutorial/game">exercises&lt;/a> that we covered. If you’re looking to learn a little about Beam, this is a good way to start. The exercises are based on an imaginary mobile game where data needs processing and are based on code in the &lt;a href="https://github.com/apache/beam/tree/master/examples/java/src/main/java/org/apache/beam/examples/complete/game">Beam examples directory&lt;/a>. The code has TODOs for where you need to fill in code or there are full sample solutions to look over our code. You can run these examples on your own machine or on a cluster using a runner that Beam supports.&lt;/p>
&lt;p>I want to share some of takeaways I had about Beam during the conference.&lt;/p>
&lt;p>The Data Engineers are looking to Beam as a way to &lt;a href="https://www.oreilly.com/ideas/future-proof-and-scale-proof-your-code">future-proof&lt;/a>, meaning that code is portable between the various Big Data frameworks. In fact, many of the attendees were still on Hadoop MapReduce and looking to transition to a new framework. They’re realizing that continually rewriting code isn’t the most productive approach.&lt;/p>
&lt;p>Data Scientists are really interested in using Beam. They interested in having a single API for doing analysis instead of several different APIs. We talked about Beam’s progress on the Python API. If you want to take a peek, it’s being actively developed on a &lt;a href="https://github.com/apache/beam/tree/master/sdks/python">feature branch&lt;/a>. As Beam matures, we’re looking to add other supported languages.&lt;/p>
&lt;p>We heard &lt;a href="https://twitter.com/jessetanderson/status/781124173108305920">loud and clear&lt;/a> from Beam users that great runner support is crucial to adoption. We have great Apache Flink support. During the conference we had some more volunteers offer their help on the Spark runner.&lt;/p>
&lt;p>On management and thought leader side, Beam went from “what’s Beam?” at previous conferences to “I’m interested in Beam.” or “I’ve formed an informed opinion on Beam.” at this conference. This is one of the metrics I look for in early technology adoption.&lt;/p>
&lt;img src="/images/blog/IMG_20160927_170455.jpg" alt="So much brainpower answering questions">
&lt;p>We rounded out the tutorial with live demonstrations of Beam running on Apache Spark, Apache Flink, the local runner, and DataFlow runner. Then, we brought in the big brainpower and had a Q and A session.&lt;/p>
&lt;p>If you’re attending a conference, we encourage you to look for a Beam session. If you want to use these materials to give your own Beam talk or tutorial, we’re happy to help you. In addition to this tutorial, we have &lt;a href="/contribute/presentation-materials/">other presentation materials&lt;/a>. You can reach out to us on the &lt;a href="/get-started/support/">user mailing list&lt;/a>.&lt;/p></description></item><item><title>Blog: Apache Beam: Six Months in Incubation</title><link>/blog/six-months/</link><pubDate>Wed, 03 Aug 2016 00:00:01 -0700</pubDate><guid>/blog/six-months/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>It’s been just over six months since Apache Beam was formally accepted into incubation with the &lt;a href="http://www.apache.org">Apache Software Foundation&lt;/a>. As a community, we’ve been hard at work getting Beam off the ground.&lt;/p>
&lt;p>Looking just at raw numbers for those first six months, that’s:&lt;/p>
&lt;ul>
&lt;li>48,238 lines of preexisting code donated by Cloudera, dataArtisans, and Google.&lt;/li>
&lt;li>761 pull requests from 45 contributors.&lt;/li>
&lt;li>498 Jira issues opened and 245 resolved.&lt;/li>
&lt;li>1 incubating release (and another 1 in progress).&lt;/li>
&lt;li>4,200 hours of automated tests.&lt;/li>
&lt;li>161 subscribers / 606 messages on user@.&lt;/li>
&lt;li>217 subscribers / 1205 messages on dev@.&lt;/li>
&lt;li>277 stars and 174 forks on GitHub.&lt;/li>
&lt;/ul>
&lt;p>And behind those numbers, there’s been a ton of technical progress, including:&lt;/p>
&lt;ul>
&lt;li>Refactoring of the entire codebase, examples, and tests to be truly runner-independent.&lt;/li>
&lt;li>New functionality in the Apache Flink runner for timestamps/windows in batch and bounded sources and side inputs in streaming mode.&lt;/li>
&lt;li>Work in progress to upgrade the Apache Spark runner to use Spark 2.0.&lt;/li>
&lt;li>Several new runners from the wider Apache community &amp;ndash; Apache Gearpump has its own feature branch, Apache Apex has a PR, and conversations are starting on Apache Storm and others.&lt;/li>
&lt;li>New SDKs/DSLs for exposing the Beam model &amp;ndash; the Python SDK from Google is in on a feature branch, and there are plans to add the Scio DSL from Spotify.&lt;/li>
&lt;li>Support for additional data sources and sinks &amp;ndash; Apache Kafka and JMS are in, there are PRs for Amazon Kinesis, Apache Cassandra, and MongoDB, and more connectors are being planned.&lt;/li>
&lt;/ul>
&lt;p>But perhaps most importantly, we’re committed to building an involved, welcoming community. So far, we’ve:&lt;/p>
&lt;ul>
&lt;li>Started building a vibrant developer community, with detailed design discussions on features like DoFn reuse semantics, serialization technology, and an API for accessing state.&lt;/li>
&lt;li>Started building a user community with an active mailing list and improvements to the website and documentation.&lt;/li>
&lt;li>Had multiple talks on Beam at venues including ApacheCon, Hadoop Summit, Kafka Summit, JBCN Barcelona, and Strata.&lt;/li>
&lt;li>Presented at multiple existing meetups and are starting to organize some of our own.&lt;/li>
&lt;/ul>
&lt;p>While it’s nice to reflect back on all we’ve done, we’re working full &lt;em>stream&lt;/em> ahead towards a stable release and graduation from incubator. And we’d love your help &amp;ndash; join the &lt;a href="/get-started/support/">mailing lists&lt;/a>, check out the &lt;a href="/contribute/contribution-guide/">contribution guide&lt;/a>, and grab a &lt;a href="https://issues.apache.org/jira/browse/BEAM-520?jql=project%20%3D%20BEAM%20AND%20resolution%20%3D%20Unresolved%20AND%20labels%20in%20(newbie%2C%20starter)">starter task&lt;/a> from Jira!&lt;/p></description></item><item><title>Blog: The first release of Apache Beam!</title><link>/blog/first-release/</link><pubDate>Wed, 15 Jun 2016 00:00:01 -0700</pubDate><guid>/blog/first-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>I’m happy to announce that Apache Beam has officially released its first
version &amp;ndash; 0.1.0-incubating. This is an exciting milestone for the project,
which joined the Apache Software Foundation and the Apache Incubator earlier
this year.&lt;/p>
&lt;p>This release publishes the first set of Apache Beam binaries and source code,
making them readily available for our users. The initial release includes the
SDK for Java, along with three runners: Apache Flink, Apache Spark and Google
Cloud Dataflow, a fully-managed cloud service. The release is available both
in the &lt;a href="https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.beam%22">Maven Central Repository&lt;/a>,
as well as a download from the &lt;a href="/get-started/downloads/">project’s website&lt;/a>.&lt;/p>
&lt;p>The goal of this release was process-oriented. In particular, the Beam
community wanted to release existing functionality to our users, build and
validate the release processes, and obtain validation from the Apache Software
Foundation and the Apache Incubator.&lt;/p>
&lt;p>I’d like to encourage everyone to try out this release. Please keep in mind
that this is the first incubating release &amp;ndash; significant changes are to be
expected. As we march toward stability, a rapid cadence of future releases is
anticipated, perhaps one every 1-2 months.&lt;/p>
&lt;p>As always, the Beam community welcomes feedback. Stabilization, usability and
the developer experience will be our focus for the next several months. If you
have any comments or discover any issues, I’d like to invite you to reach out
to us via &lt;a href="/get-started/support/">user’s mailing list&lt;/a> or the
&lt;a href="https://issues.apache.org/jira/browse/BEAM/">Apache JIRA issue tracker&lt;/a>.&lt;/p></description></item><item><title>Blog: How We Added Windowing to the Apache Flink Batch Runner</title><link>/blog/flink-batch-runner-milestone/</link><pubDate>Mon, 13 Jun 2016 09:00:00 -0700</pubDate><guid>/blog/flink-batch-runner-milestone/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We recently achieved a major milestone by adding support for windowing to the &lt;a href="https://flink.apache.org">Apache Flink&lt;/a> Batch runner. In this post we would like to explain what this means for users of Apache Beam and highlight some of the implementation details.&lt;/p>
&lt;p>Before we start, though, let’s quickly talk about the execution of Beam programs and how this is relevant to today’s post. A Beam pipeline can contain bounded and unbounded sources. If the pipeline only contains bounded sources it can be executed in a batch fashion, if it contains some unbounded sources it must be executed in a streaming fashion. When executing a Beam pipeline on Flink, you don’t have to choose the execution mode. Internally, the Flink runner either translates the pipeline to a Flink &lt;code>DataSet&lt;/code> program or a &lt;code>DataStream&lt;/code> program, depending on whether unbounded sources are used in the pipeline. In the following, when we say “Batch runner” what we are really talking about is the Flink runner being in batch execution mode.&lt;/p>
&lt;h2 id="what-does-this-mean-for-users">What does this mean for users?&lt;/h2>
&lt;p>Support for windowing was the last missing puzzle piece for making the Flink Batch runner compatible with the Beam model. With the latest change to the Batch runner users can now run any pipeline that only contains bounded sources and be certain that the results match those of the original reference-implementation runners that were provided by Google as part of the initial code drop coming from the Google Dataflow SDK.&lt;/p>
&lt;p>The most obvious part of the change is that windows can now be assigned to elements and that the runner respects these windows for the &lt;code>GroupByKey&lt;/code> and &lt;code>Combine&lt;/code> operations. A not-so-obvious change concerns side-inputs. In the Beam model, side inputs respect windows; when a value of the main input is being processed only the side input that corresponds to the correct window is available to the processing function, the &lt;code>DoFn&lt;/code>.&lt;/p>
&lt;p>Getting side-input semantics right is an important milestone in it’s own because it allows to use a big suite of unit tests for verifying the correctness of a runner implementation. These tests exercise every obscure detail of the Beam programming model and verify that the results produced by a runner match what you would expect from a correct implementation. In the suite, side inputs are used to compare the expected result to the actual result. With these tests being executed regularly we can now be more confident that the implementation produces correct results for user-specified pipelines.&lt;/p>
&lt;h2 id="under-the-hood">Under the Hood&lt;/h2>
&lt;p>The basis for the changes is the introduction of &lt;code>WindowedValue&lt;/code> in the generated Flink transformations. Before, a Beam &lt;code>PCollection&amp;lt;T&amp;gt;&lt;/code> would be transformed to a &lt;code>DataSet&amp;lt;T&amp;gt;&lt;/code>. Now, we instead create a &lt;code>DataSet&amp;lt;WindowedValue&amp;lt;T&amp;gt;&amp;gt;&lt;/code>. The &lt;code>WindowedValue&amp;lt;T&amp;gt;&lt;/code> stores meta data about the value, such as the timestamp and the windows to which it was assigned.&lt;/p>
&lt;p>With this basic change out of the way we just had to make sure that windows were respected for side inputs and that &lt;code>Combine&lt;/code> and &lt;code>GroupByKey&lt;/code> correctly handled windows. The tricky part there is the handling of merging windows such as session windows. For these we essentially emulate the behavior of a merging &lt;code>WindowFn&lt;/code> in our own code.&lt;/p>
&lt;p>After we got side inputs working we could enable the aforementioned suite of tests to check how well the runner behaves with respect to the Beam model. As can be expected there were quite some discrepancies but we managed to resolve them all. In the process, we also slimmed down the runner implementation. For example, we removed all custom translations for sources and sinks and are now relying only on Beam code for these, thereby greatly reducing the maintenance overhead.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>We reached a major milestone in adding windowing support to the Flink Batch runner, thereby making it compatible with the Beam model. Because of the large suite of tests that can now be executed on the runner we are also confident about the correctness of the implementation and about it staying that way in the future.&lt;/p></description></item><item><title>Blog: Where's my PCollection.map()?</title><link>/blog/where-is-my-pcollection-dot-map/</link><pubDate>Fri, 27 May 2016 09:00:00 -0700</pubDate><guid>/blog/where-is-my-pcollection-dot-map/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Have you ever wondered why Beam has PTransforms for everything instead of having methods on PCollection? Take a look at the history that led to this (and other) design decisions.&lt;/p>
&lt;p>Though Beam is relatively new, its design draws heavily on many years of
experience with real-world pipelines. One of the primary inspirations is
&lt;a href="https://ai.google/research/pubs/pub35650">FlumeJava&lt;/a>, which is Google&amp;rsquo;s
internal successor to MapReduce first introduced in 2009.&lt;/p>
&lt;p>The original FlumeJava API has methods like &lt;code>count&lt;/code> and &lt;code>parallelDo&lt;/code> on the PCollections. Though slightly more succinct, this approach has many disadvantages to extensibility. Every new user to FlumeJava wanted to add transforms, and adding them as methods to PCollection simply doesn&amp;rsquo;t scale well. In contrast, a PCollection in Beam has a single &lt;code>apply&lt;/code> method which takes any PTransform as an argument.&lt;/p>
&lt;table class="table">
&lt;tr>
&lt;th>FlumeJava&lt;/th>
&lt;th>Beam&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>&lt;pre>
PCollection&amp;lt;T&amp;gt; input = …
PCollection&amp;lt;O&amp;gt; output = input.count()
.parallelDo(...);
&lt;/pre>&lt;/td>
&lt;td>&lt;pre>
PCollection&amp;lt;T&amp;gt; input = …
PCollection&amp;lt;O&amp;gt; output = input.apply(Count.perElement())
.apply(ParDo.of(...));
&lt;/pre>&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>This is a more scalable approach for several reasons.&lt;/p>
&lt;h2 id="where-to-draw-the-line">Where to draw the line?&lt;/h2>
&lt;p>Adding methods to PCollection forces a line to be drawn between operations that are &amp;ldquo;useful&amp;rdquo; enough to merit this special treatment and those that are not. It is easy to make the case for flat map, group by key, and combine per key. But what about filter? Count? Approximate count? Approximate quantiles? Most frequent? WriteToMyFavoriteSource? Going too far down this path leads to a single enormous class that contains nearly everything one could want to do. (FlumeJava&amp;rsquo;s PCollection class is over 5000 lines long with around 70 distinct operations, and it could have been &lt;em>much&lt;/em> larger had we accepted every proposal.) Furthermore, since Java doesn’t allow adding methods to a class, there is a sharp syntactic divide between those operations that are added to PCollection and those that aren’t. A traditional way to share code is with a library of functions, but functions (in traditional languages like Java at least) are written prefix-style, which doesn&amp;rsquo;t mix well with the fluent builder style (e.g. &lt;code>input.operation1().operation2().operation3()&lt;/code> vs. &lt;code>operation3(operation1(input).operation2())&lt;/code>).&lt;/p>
&lt;p>Instead in Beam we&amp;rsquo;ve chosen a style that places all transforms&amp;ndash;whether they be primitive operations, composite operations bundled in the SDK, or part of an external library&amp;ndash;on equal footing. This also facilitates alternative implementations (which may even take different options) that are easily interchangeable.&lt;/p>
&lt;table class="table">
&lt;tr>
&lt;th>FlumeJava&lt;/th>
&lt;th>Beam&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>&lt;pre>
PCollection&amp;lt;O&amp;gt; output =
ExternalLibrary.doStuff(
MyLibrary.transform(input, myArgs)
.parallelDo(...),
externalLibArgs);
&lt;/pre>&lt;/td>
&lt;td>&lt;pre>
PCollection&amp;lt;O&amp;gt; output = input
.apply(MyLibrary.transform(myArgs))
.apply(ParDo.of(...))
.apply(ExternalLibrary.doStuff(externalLibArgs));
&amp;nbsp;
&lt;/pre>&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h2 id="configurability">Configurability&lt;/h2>
&lt;p>It makes for a fluent style to let values (PCollections) be the objects passed around and manipulated (i.e. the handles to the deferred execution graph), but it is the operations themselves that need to be composable, configurable, and extendable. Using PCollection methods for the operations doesn&amp;rsquo;t scale well here, especially in a language without default or keyword arguments. For example, a ParDo operation can have any number of side inputs and side outputs, or a write operation may have configurations dealing with encoding and compression. One option is to separate these out into multiple overloads or even methods, but that exacerbates the problems above. (FlumeJava evolved over a dozen overloads of the &lt;code>parallelDo&lt;/code> method!) Another option is to pass each method a configuration object that can be built up using more fluent idioms like the builder pattern, but at that point one might as well make the configuration object the operation itself, which is what Beam does.&lt;/p>
&lt;h2 id="type-safety">Type Safety&lt;/h2>
&lt;p>Many operations can only be applied to collections whose elements are of a specific type. For example, the GroupByKey operation should only be applied to &lt;code>PCollection&amp;lt;KV&amp;lt;K, V&amp;gt;&amp;gt;&lt;/code>s. In Java at least, it&amp;rsquo;s not possible to restrict methods based on the element type parameter alone. In FlumeJava, this led us to add a &lt;code>PTable&amp;lt;K, V&amp;gt;&lt;/code> subclassing &lt;code>PCollection&amp;lt;KV&amp;lt;K, V&amp;gt;&amp;gt;&lt;/code> to contain all the operations specific to PCollections of key-value pairs. This leads to the same question of which element types are special enough to merit being captured by PCollection subclasses. It is not very extensible for third parties and often requires manual downcasts/conversions (which can&amp;rsquo;t be safely chained in Java) and special operations that produce these PCollection specializations.&lt;/p>
&lt;p>This is particularly inconvenient for transforms that produce outputs whose element types are the same as (or related to) their input&amp;rsquo;s element types, requiring extra support to generate the right subclasses (e.g. a filter on a PTable should produce another PTable rather than just a raw PCollection of key-value pairs).&lt;/p>
&lt;p>Using PTransforms allows us to sidestep this entire issue. We can place arbitrary constraints on the context in which a transform may be used based on the type of its inputs; for instance GroupByKey is statically typed to only apply to a &lt;code>PCollection&amp;lt;KV&amp;lt;K, V&amp;gt;&amp;gt;&lt;/code>. The way this happens is generalizable to arbitrary shapes, without needing to introduce specialized types like PTable.&lt;/p>
&lt;h2 id="reusability-and-structure">Reusability and Structure&lt;/h2>
&lt;p>Though PTransforms are generally constructed at the site at which they&amp;rsquo;re used, by pulling them out as separate objects one is able to store them and pass them around.&lt;/p>
&lt;p>As pipelines grow and evolve, it is useful to structure your pipeline into modular, often reusable components, and PTransforms allow one to do this nicely in a data-processing pipeline. In addition, modular PTransforms also expose the logical structure of your code to the system (e.g. for monitoring). Of the three different representations of the WordCount pipeline below, only the structured view captures the high-level intent of the pipeline. Letting even the simple operations be PTransforms means there&amp;rsquo;s less of an abrupt edge to packaging things up into composite operations.&lt;/p>
&lt;img class="center-block" src="/images/blog/simple-wordcount-pipeline.png" alt="Three different visualizations of a simple WordCount pipeline" width="500">
&lt;div class="text-center">
&lt;i>Three different visualizations of a simple WordCount pipeline which computes the number of occurrences of every word in a set of text files. The flat view gives the full DAG of all operations performed. The execution view groups operations according to how they're executed, e.g. after performing runner-specific optimizations like function composition. The structured view nests operations according to their grouping in PTransforms.&lt;/i>
&lt;/div>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Although it&amp;rsquo;s tempting to add methods to PCollections, such an approach is not scalable, extensible, or sufficiently expressive. Putting a single apply method on PCollection and all the logic into the operation itself lets us have the best of both worlds, and avoids hard cliffs of complexity by having a single consistent style across simple and complex pipelines, and between predefined and user-defined operations.&lt;/p></description></item><item><title>Blog: Dynamic work rebalancing for Beam</title><link>/blog/splitatfraction-method/</link><pubDate>Wed, 18 May 2016 11:00:00 -0700</pubDate><guid>/blog/splitatfraction-method/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>This morning, Eugene and Malo from the Google Cloud Dataflow team posted &lt;a href="https://cloud.google.com/blog/big-data/2016/05/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow">&lt;em>No shard left behind: dynamic work rebalancing in Google Cloud Dataflow&lt;/em>&lt;/a>. This article discusses Cloud Dataflow’s solution to the well-known straggler problem.&lt;/p>
&lt;p>In a large batch processing job with many tasks executing in parallel, some of the tasks &amp;ndash; the stragglers &amp;ndash; can take a much longer time to complete than others, perhaps due to imperfect splitting of the work into parallel chunks when issuing the job. Typically, waiting for stragglers means that the overall job completes later than it should, and may also reserve too many machines that may be underutilized at the end. Cloud Dataflow’s dynamic work rebalancing can mitigate stragglers in most cases.&lt;/p>
&lt;p>What I’d like to highlight for the Apache Beam (incubating) community is that Cloud Dataflow’s dynamic work rebalancing is implemented using &lt;em>runner-specific&lt;/em> control logic on top of Beam’s &lt;em>runner-independent&lt;/em> &lt;a href="https://github.com/apache/beam/blob/9fa97fb2491bc784df53fb0f044409dbbc2af3d7/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BoundedSource.java">&lt;code>BoundedSource API&lt;/code>&lt;/a>. Specifically, to steal work from a straggler, a runner need only call the reader’s &lt;a href="https://github.com/apache/beam/blob/3edae9b8b4d7afefb5c803c19bb0a1c21ebba89d/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BoundedSource.java#L266">&lt;code>splitAtFraction method&lt;/code>&lt;/a>. This will generate a new source containing leftover work, and then the runner can pass that source off to another idle worker. As Beam matures, I hope that other runners are interested in figuring out whether these APIs can help them improve performance, implementing dynamic work rebalancing, and collaborating on API changes that will help solve other pain points.&lt;/p></description></item></channel></rss>