<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Running Beam SQL in notebooks</title><meta name=description content="Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes."><link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700" rel=stylesheet><link rel=preload href=/scss/main.min.408fddfe3e8a45f87a5a8c9a839d77db667c1c534e5e5cd0d957ffc3dd6c14cf.css as=style><link href=/scss/main.min.408fddfe3e8a45f87a5a8c9a839d77db667c1c534e5e5cd0d957ffc3dd6c14cf.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-2.2.4.min.js></script><style>.body__contained img{max-width:100%}</style><script type=text/javascript src=/js/bootstrap.min.2979f9a6e32fc42c3e7406339ee9fe76b31d1b52059776a02b4a7fa6a4fd280a.js defer></script>
<script type=text/javascript src=/js/language-switch-v2.min.121952b7980b920320ab229551857669209945e39b05ba2b433a565385ca44c6.js defer></script>
<script type=text/javascript src=/js/fix-menu.min.039174b67107465f2090a493f91e126f7aa797f29420f9edab8a54d9dd4b3d2d.js defer></script>
<script type=text/javascript src=/js/section-nav.min.1405fd5e70fab5f6c54037c269b1d137487d8f3d1b3009032525f6db3fbce991.js defer></script>
<script type=text/javascript src=/js/page-nav.min.af231204c9c52c5089d53a4c02739eacbb7f939e3be1c6ffcc212e0ac4dbf879.js defer></script>
<script type=text/javascript src=/js/expandable-list.min.75a4526624a3b8898fe7fb9e3428c205b581f8b38c7926922467aef17eac69f2.js defer></script>
<script type=text/javascript src=/js/copy-to-clipboard.min.364c06423d7e8993fc42bb4abc38c03195bc8386db26d18774ce775d08d5b18d.js defer></script>
<script type=text/javascript src=/js/calendar.min.336664054fa0f52b08bbd4e3c59b5cb6d63dcfb2b4d602839746516b0817446b.js defer></script>
<script type=text/javascript src=/js/fix-playground-nested-scroll.min.0283f1037cb1b9d5074c6eaf041292b524a8148a7cdb803d5ccd6d1fc4eb3253.js defer></script>
<script type=text/javascript src=/js/anchor-content-jump-fix.min.22d3240f81632e4c11179b9d2aaf37a40da9414333c43aa97344e8b21a7df0e4.js defer></script>
<link rel=alternate type=application/rss+xml title="Apache Beam" href=/feed.xml><link rel=canonical href=/blog/beam-sql-with-notebooks/ data-proofer-ignore><link rel="shortcut icon" type=image/x-icon href=/images/favicon.ico><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.4.1/css/all.css integrity=sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz crossorigin=anonymous><link rel=stylesheet href=https://unpkg.com/swiper@8/swiper-bundle.min.css><script async src=https://platform.twitter.com/widgets.js></script>
<script>(function(e,t,n,s,o,i){e.hj=e.hj||function(){(e.hj.q=e.hj.q||[]).push(arguments)},e._hjSettings={hjid:2182187,hjsv:6},o=t.getElementsByTagName("head")[0],i=t.createElement("script"),i.async=1,i.src=n+e._hjSettings.hjid+s+e._hjSettings.hjsv,o.appendChild(i)})(window,document,"https://static.hotjar.com/c/hotjar-",".js?sv=")</script></head><body class=body><nav class="navigation-bar-mobile header navbar navbar-fixed-top"><div class=navbar-header><a href=/ class=navbar-brand><img alt=Brand style=height:46px;width:43px src=/images/beam_logo_navbar_mobile.png></a>
<a class=navbar-link href=/get-started/>Get Started</a>
<a class=navbar-link href=/documentation/>Documentation</a>
<button type=button class="navbar-toggle menu-open" aria-expanded=false aria-controls=navbar onclick=openMenu()>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="navbar-mask closed"></div><div id=navbar class="navbar-container closed"><button type=button class=navbar-toggle aria-expanded=false aria-controls=navbar id=closeMenu>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button><ul class="nav navbar-nav"><li><div class=searchBar-mobile><script>(function(){var t,n="012923275103528129024:4emlchv9wzi",e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cse.google.com/cse.js?cx="+n,t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script><gcse:search></gcse:search></div></li><li><a class=navbar-link href=/about>About</a></li><li><a class=navbar-link href=/get-started/>Get Started</a></li><li><span class=navbar-link>Documentation</span><ul><li><a href=/documentation/>General</a></li><li><a href=/documentation/sdks/java/>Languages</a></li><li><a href=/documentation/runners/capability-matrix/>Runners</a></li><li><a href=/documentation/io/connectors/>I/O Connectors</a></li></ul></li><li><a class=navbar-link href=/roadmap/>Roadmap</a></li><li><a class=navbar-link href=/community/>Community</a></li><li><a class=navbar-link href=/contribute/>Contribute</a></li><li><a class=navbar-link href=/blog/>Blog</a></li><li><a class=navbar-link href=/case-studies/>Case Studies</a></li></ul><ul class="nav navbar-nav navbar-right"><li><a href=https://github.com/apache/beam/edit/master/website/www/site/content/en/blog/beam-sql-with-notebooks.md data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" width="25" height="24" fill="none" viewBox="0 0 25 24"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M4.543 20h4l10.5-10.5c.53-.53.828-1.25.828-2s-.298-1.47-.828-2-1.25-.828-2-.828-1.47.298-2 .828L4.543 16v4zm9.5-13.5 4 4"/></svg></a></li><li class=dropdown><a href=# class=dropdown-toggle id=apache-dropdown data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false><img src=https://www.apache.org/foundation/press/kit/feather_small.png alt="Apache Logo" style=height:20px>
&nbsp;Apache
<span class=arrow-icon><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="none" viewBox="0 0 20 20"><circle cx="10" cy="10" r="10" fill="#ff6d00"/><path stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8.535 5.28l4.573 4.818-4.573 4.403"/></svg></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a target=_blank href=https://www.apache.org/>ASF Homepage</a></li><li><a target=_blank href=https://www.apache.org/licenses/>License</a></li><li><a target=_blank href=https://www.apache.org/security/>Security</a></li><li><a target=_blank href=https://www.apache.org/foundation/thanks.html>Thanks</a></li><li><a target=_blank href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li><li><a target=_blank href=https://www.apache.org/foundation/policies/conduct>Code of Conduct</a></li></ul></li></ul></div></nav><nav class=navigation-bar-desktop><a href=/ class=navbar-logo><img src=/images/beam_logo_navbar.png alt="Beam Logo"></a><div class=navbar-bar-left><div class=navbar-links><a class=navbar-link href=/about>About</a>
<a class=navbar-link href=/get-started/>Get Started</a><li class="dropdown navbar-dropdown navbar-dropdown-documentation"><a href=# class="dropdown-toggle navbar-link" role=button aria-haspopup=true aria-expanded=false>Documentation
<span><svg xmlns="http://www.w3.org/2000/svg" width="12" height="11" fill="none" viewBox="0 0 12 11"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.666 4.535 5.847 9.108 1.444 4.535"/></svg></span></a><ul class=dropdown-menu><li><a class=navbar-dropdown-menu-link href=/documentation/>General</a></li><li><a class=navbar-dropdown-menu-link href=/documentation/sdks/java/>Languages</a></li><li><a class=navbar-dropdown-menu-link href=/documentation/runners/capability-matrix/>Runners</a></li><li><a class=navbar-dropdown-menu-link href=/documentation/io/connectors/>I/O Connectors</a></li></ul></li><a class=navbar-link href=/roadmap/>Roadmap</a>
<a class=navbar-link href=/community/>Community</a>
<a class=navbar-link href=/contribute/>Contribute</a>
<a class=navbar-link href=/blog/>Blog</a>
<a class=navbar-link href=/case-studies/>Case Studies</a></div><div id=iconsBar><a type=button onclick=showSearch()><svg xmlns="http://www.w3.org/2000/svg" width="25" height="24" fill="none" viewBox="0 0 25 24"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M10.191 17c3.866.0 7-3.134 7-7s-3.134-7-7-7-7 3.134-7 7 3.134 7 7 7zm11 4-6-6"/></svg></a><a target=_blank href=https://github.com/apache/beam/edit/master/website/www/site/content/en/blog/beam-sql-with-notebooks.md data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" width="25" height="24" fill="none" viewBox="0 0 25 24"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M4.543 20h4l10.5-10.5c.53-.53.828-1.25.828-2s-.298-1.47-.828-2-1.25-.828-2-.828-1.47.298-2 .828L4.543 16v4zm9.5-13.5 4 4"/></svg></a><li class="dropdown navbar-dropdown navbar-dropdown-apache"><a href=# class=dropdown-toggle role=button aria-haspopup=true aria-expanded=false><img src=https://www.apache.org/foundation/press/kit/feather_small.png alt="Apache Logo" style=height:20px>
&nbsp;Apache
<span class=arrow-icon><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="none" viewBox="0 0 20 20"><circle cx="10" cy="10" r="10" fill="#ff6d00"/><path stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8.535 5.28l4.573 4.818-4.573 4.403"/></svg></span></a><ul class=dropdown-menu><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/>ASF Homepage</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/licenses/>License</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/security/>Security</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/foundation/thanks.html>Thanks</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/foundation/policies/conduct>Code of Conduct</a></li></ul></li></div><div class="searchBar disappear"><script>(function(){var t,n="012923275103528129024:4emlchv9wzi",e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="https://cse.google.com/cse.js?cx="+n,t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script><gcse:search></gcse:search>
<a type=button onclick=endSearch()><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M21.122 20.827 4.727 4.432M21.122 4.43 4.727 20.827"/></svg></a></div></div></nav><div class=header-push></div><div class="top-banners swiper"><div class=swiper-wrapper><div class=swiper-slide><a href=https://tour.beam.apache.org><img class=banner-img-desktop src=/images/banners/tour-of-beam/tour-of-beam-desktop.png alt="Start Tour of Beam">
<img class=banner-img-mobile src=/images/banners/tour-of-beam/tour-of-beam-mobile.png alt="Start Tour of Beam"></a></div><div class=swiper-slide><a href=https://beam.apache.org/documentation/ml/overview/><img class=banner-img-desktop src=/images/banners/machine-learning/machine-learning-desktop.jpg alt="Machine Learning">
<img class=banner-img-mobile src=/images/banners/machine-learning/machine-learning-mobile.jpg alt="Machine Learning"></a></div></div><div class=swiper-pagination></div><div class=swiper-button-prev></div><div class=swiper-button-next></div></div><script src=/js/swiper-bundle.min.min.e0e8f81b0b15728d35ff73c07f42ddbb17a108d6f23df4953cb3e60df7ade675.js></script>
<script src=/js/sliders/top-banners.min.afa7d0a19acf7a3b28ca369490b3d401a619562a2a4c9612577be2f66a4b9855.js></script>
<script>function showSearch(){addPlaceholder();var e,t=document.querySelector(".searchBar");t.classList.remove("disappear"),e=document.querySelector("#iconsBar"),e.classList.add("disappear")}function addPlaceholder(){$("input:text").attr("placeholder","What are you looking for?")}function endSearch(){var e,t=document.querySelector(".searchBar");t.classList.add("disappear"),e=document.querySelector("#iconsBar"),e.classList.remove("disappear")}function blockScroll(){$("body").toggleClass("fixedPosition")}function openMenu(){addPlaceholder(),blockScroll()}</script><div class="body__contained center no__padding content-up"><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class=post-content><div class=post-info><p>blog</p><p>2022/04/28</p></div><header class=post-header><h2 itemprop="name headline">Running Beam SQL in notebooks</h1><div class=post-info><span>Ning Kang [<a href=https://twitter.com/ningkang0957>@ningkang0957</a>]</span></div></header><div class="arrow-list header-top-margin" itemprop=articleBody><h2 id=intro>Intro</h2><p><a href=/documentation/dsls/sql/overview/>Beam SQL</a> allows a
Beam user to query PCollections with SQL statements.
<a href=https://github.com/apache/beam/tree/master/sdks/python/apache_beam/runners/interactive#interactive-beam>Interactive Beam</a>
provides an integration between Apache Beam and
<a href=https://docs.jupyter.org/en/latest/>Jupyter Notebooks</a> (formerly known as
IPython Notebooks) to make pipeline prototyping and data exploration much faster
and easier.
You can set up your own notebook user interface (for example,
<a href=https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html>JupyterLab</a>
or classic <a href=https://docs.jupyter.org/en/latest/install.html>Jupyter Notebooks</a>)
on your own device following their documentations. Alternatively, you can
choose a hosted solution that does everything for you. You are free to select
whichever notebook user interface you prefer. For simplicity, this
post does not go through the notebook environment setup and uses
<a href=https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development>Apache Beam Notebooks</a>
that provides a cloud-hosted
<a href=https://jupyterlab.readthedocs.io/en/stable/>JupyterLab</a> environment and lets
a Beam user iteratively develop pipelines, inspect pipeline graphs, and parse
individual PCollections in a read-eval-print-loop (REPL) workflow.</p><p>In this post, you will see how to use <code>beam_sql</code>, a notebook
<a href=https://ipython.readthedocs.io/en/stable/interactive/magics.html>magic</a>, to
execute Beam SQL in notebooks and inspect the results.</p><p>By the end of the post, it also demonstrates how to use the <code>beam_sql</code> magic
with a production environment, such as running it as a one-shot job on
Dataflow. It&rsquo;s optional. To follow those steps, you should have a project in
Google Cloud Platform with
<a href=https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#before_you_begin>necessary APIs enabled</a>
, and you should have enough permissions to create a Google Cloud Storage bucket
(or to use an existing one), query a public Google Cloud BigQuery dataset, and
run Dataflow jobs.</p><p>If you choose to use the cloud hosted notebook solution, once you have your
Google Cloud project ready, you will need to create an Apache Beam Notebooks
instance and open the JupyterLab web interface. Please follow the instructions
given at:
<a href=https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance>https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance</a></p><h2 id=getting-familiar-with-the-environment>Getting familiar with the environment</h2><h3 id=landing-page>Landing page</h3><p>After starting your own notebook user interface: for example, if using Apche
Beam Notebooks, after clicking the <code>OPEN JUPYTERLAB</code> link, you will land on
the default launcher page of the notebook environment.</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image1.png alt="Beam SQL in Notebooks: landing page"></p><p>On the left side, there is a file explorer to view examples, tutorials and
assets on the notebook instance. To easily navigate the files, you may
double-click the <code>00-Start_Here.md</code> (#1 in the screenshot) file to view detailed
information about the files.</p><p>On the right side, it displays the default launcher page of JupyterLab. To
create and open a completely new notebook file and code with a selected version
of Apache Beam, click one of (#2) the items with Apache Beam >=2.34.0 (because
<code>beam_sql</code> was introduced in 2.34.0) installed.</p><h3 id=createopen-a-notebook>Create/open a notebook</h3><p>For example, if you clicked the image button with Apache Beam 2.36.0, you would
see an <code>Untitled.ipynb</code> file created and opened.</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image2.png alt="Beam SQL in Notebooks: create/open a notebook "></p><p>In the file explorer, your new notebook file has been created as
<code>Untitled.ipynb</code>.</p><p>On the right side, in the opened notebook, there are 4 buttons on top that you
may interact most frequently with:</p><ul><li>#1: insert an empty code block after the selected / highlighted code block</li><li>#2: execute the code in the block that is selected / highlighted</li><li>#3: interrupt code execution if your code execution is stuck</li><li>#4: “Restart the kernel”: clear all states from code executions and start
from fresh</li></ul><p>There is a button on the top-right (#5) for you to choose a different Apache
Beam version if needed, so it’s not set in stone.</p><p>You can always double-click a file from the file explorer to open it without
creating a new one.</p><h2 id=beam-sql>Beam SQL</h2><h3 id=beam_sql-magic><code>beam_sql</code> magic</h3><p><code>beam_sql</code> is an IPython
<a href=https://ipython.readthedocs.io/en/stable/config/custommagics.html>custom magic</a>.
If you&rsquo;re not familiar with magics, here are some
<a href=https://ipython.readthedocs.io/en/stable/interactive/magics.html>built-in examples</a>.
It&rsquo;s a convenient way to validate your queries locally against known/test data
sources when prototyping a Beam pipeline with SQL, before productionizing it on
remote cluster/services.</p><p>The Apache Beam Notebooks environment has preloaded the <code>beam_sql</code> magic and
basic <code>apache-beam</code> modules so you can directly use them without additional
imports. You can also explicitly load the magic via
<code>%load_ext apache_beam.runners.interactive.sql.beam_sql_magics</code> and
<code>apache-beam</code> modules if you set up your own notebook elsewhere.</p><p>You can type:</p><pre tabindex=0><code>%beam_sql -h
</code></pre><p>and then execute the code to learn how to use the magic:</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image3.png alt="Beam SQL in Notebooks: beam_sql magic help message "></p><p>The selected/highlighted block is called a notebook cell. It mainly has 3
components:</p><ul><li>#1: The execution count. <code>[1]</code> indicates this block is the first executed
code. It increases by 1 for each piece of code you execute even if you
re-execute the same piece of code. <code>[ ]</code> indicates this block is not
executed.</li><li>#2: The cell input: the code gets executed.</li><li>#3: The cell output: the output of the code execution. Here it contains the
help documentation of the <code>beam_sql</code> magic.</li></ul><h3 id=create-a-pcollection>Create a PCollection</h3><p>There are 3 scenarios for Beam SQL when creating a PCollection:</p><ol><li>Use Beam SQL to create a PCollection from constant values</li></ol><pre tabindex=0><code>%%beam_sql -o pcoll
SELECT CAST(1 AS INT) AS id, CAST(&#39;foo&#39; AS VARCHAR) AS str, CAST(3.14 AS DOUBLE) AS flt
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image4.png alt="Beam SQL in Notebooks: beam_sql creates a PCollection from raw values."></p><p>The <code>beam_sql</code> magic creates and outputs a PCollection named <code>pcoll</code> with
element_type like <code>BeamSchema_...(id: int32, str: str, flt: float64)</code>.</p><p><strong>Note</strong> that you have <strong>not</strong> explicitly created a Beam pipeline. You get a
PCollection because the <code>beam_sql</code> magic always implicitly creates a pipeline to
execute your SQL query. To hold the elements with each field&rsquo;s type info, Beam
automatically creates a
<a href=/documentation/programming-guide/#what-is-a-schema>schema</a>
as the <code>element_type</code> for the created PCollection. You will learn more about
schema-aware PCollections later.</p><ol start=2><li>Use Beam SQL to query a PCollection</li></ol><p>You can chain another SQL using the output from a previous SQL (or any
schema-aware PCollection produced by any normal Beam PTransforms) as the input
to produce a new PCollection.</p><p><strong>Note</strong>: if you name the output PCollection, make sure that it’s unique in your
notebook to avoid overwriting a different PCollection.</p><pre tabindex=0><code>%%beam_sql -o id_pcoll
SELECT id FROM pcoll
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image5.png alt="Beam SQL in Notebooks: beam_sql creates a PCollection from another."></p><ol start=3><li>Use Beam SQL to join multiple PCollections</li></ol><p>You can query multiple PCollections from a single query.</p><pre tabindex=0><code>%%beam_sql -o str_with_same_id
SELECT id, str FROM pcoll JOIN id_pcoll USING (id)
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image6.png alt="Beam SQL in Notebooks: beam_sql creates a PCollection from multiple PCollections."></p><p>Now you have learned how to use the <code>beam_sql</code> magic to create PCollections and
inspect their results.</p><p><strong>Tip</strong>: if you accidentally delete some of the notebook cell outputs, you can
always check the content of a PCollection by invoking <code>ib.show(pcoll_name)</code> or
<code>ib.collect(pcoll_name)</code> where <code>ib</code> stands for “Interactive Beam”
(<a href=https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#reading_and_visualizing_the_data>learn more</a>).</p><h3 id=schema-aware-pcollections>Schema-aware PCollections</h3><p>The <code>beam_sql</code> magic provides the flexibility to seamlessly mix SQL and non-SQL
Beam statements to build pipelines and even run them on Dataflow. However, each
PCollection queried by Beam SQL needs to have a
<a href=/documentation/programming-guide/#what-is-a-schema>schema</a>.
For the <code>beam_sql</code> magic, it’s recommended to use <code>typing.NamedTuple</code> when a
schema is desired. You can go through the below example to learn more details
about schema-aware PCollections.</p><h4 id=setup>Setup</h4><p>In the setup of this example, you will:</p><ul><li>Install PyPI package <code>names</code> using the built-in <code>%pip</code> magic: you will use
the module to generate some random English names as the raw data input.</li><li>Define a schema with <code>NamedTuple</code> that has 2 attributes: <code>id</code> - an unique
numeric identifier of a person; <code>name</code> - a string name of a person.</li><li>Define a pipeline with an <code>InteractiveRunner</code> to utilize notebook related
features of Apache Beam.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>%</span><span class=n>pip</span> <span class=n>install</span> <span class=n>names</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>names</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>NamedTuple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Person</span><span class=p>(</span><span class=n>NamedTuple</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>id</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>=</span> <span class=n>beam</span><span class=o>.</span><span class=n>Pipeline</span><span class=p>(</span><span class=n>InteractiveRunner</span><span class=p>())</span>
</span></span></code></pre></div><p>There is no visible output for the code execution.</p><h4 id=create-schema-aware-pcollections-without-using-sql>Create schema-aware PCollections without using SQL</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>persons</span> <span class=o>=</span> <span class=p>(</span><span class=n>p</span>
</span></span><span class=line><span class=cl>           <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>Create</span><span class=p>([</span><span class=n>Person</span><span class=p>(</span><span class=nb>id</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>names</span><span class=o>.</span><span class=n>get_full_name</span><span class=p>())</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>)]))</span>
</span></span><span class=line><span class=cl><span class=n>ib</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>persons</span><span class=p>)</span>
</span></span></code></pre></div><p><img class=center-block src=/images/blog/beam-sql-notebooks/image7.png alt="Beam SQL in Notebooks: create a schema-aware PCollection without SQL."></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>persons_2</span> <span class=o>=</span> <span class=p>(</span><span class=n>p</span>
</span></span><span class=line><span class=cl>             <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>Create</span><span class=p>([</span><span class=n>Person</span><span class=p>(</span><span class=nb>id</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>names</span><span class=o>.</span><span class=n>get_full_name</span><span class=p>())</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>15</span><span class=p>)]))</span>
</span></span><span class=line><span class=cl><span class=n>ib</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>persons_2</span><span class=p>)</span>
</span></span></code></pre></div><p><img class=center-block src=/images/blog/beam-sql-notebooks/image8.png alt="Beam SQL in Notebooks: create another schema-aware PCollection without SQL."></p><p>Now you have 2 PCollections both with the same schema defined by the <code>Person</code>
class:</p><ul><li><code>persons</code> contains 10 records for 10 persons with ids ranging from 0 to 9,</li><li><code>persons_2</code> contains another 10 records for 10 persons with ids ranging from
5 to 14.</li></ul><h4 id=encode-and-decode-of-schema-aware-pcollections>Encode and Decode of schema-aware PCollections</h4><p>For this example, you still need one more piece of data from the first <code>pcoll</code>
that you have created with instructions in this post.</p><p>You can use the original <code>pcoll</code>. Optionally, if you want to exercise using
coders explicitly with schema-aware PCollections, you can add a Text I/O into
the mix: write the content of <code>pcoll</code> into a text file retaining its schema
information, then read the file back into a new schema-aware PCollection called
<code>pcoll_in_file</code>, and use the new PCollection to join <code>persons</code> and <code>persons_2</code>
to find names with the common id in all three of them.</p><p>To encode <code>pcoll</code> into a file, execute:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>coder</span><span class=o>=</span><span class=n>beam</span><span class=o>.</span><span class=n>coders</span><span class=o>.</span><span class=n>registry</span><span class=o>.</span><span class=n>get_coder</span><span class=p>(</span><span class=n>pcoll</span><span class=o>.</span><span class=n>element_type</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pcoll</span> <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>textio</span><span class=o>.</span><span class=n>WriteToText</span><span class=p>(</span><span class=s1>&#39;/tmp/pcoll&#39;</span><span class=p>,</span> <span class=n>coder</span><span class=o>=</span><span class=n>coder</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pcoll</span><span class=o>.</span><span class=n>pipeline</span><span class=o>.</span><span class=n>run</span><span class=p>()</span><span class=o>.</span><span class=n>wait_until_finish</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>cat</span> <span class=o>/</span><span class=n>tmp</span><span class=o>/</span><span class=n>pcoll</span><span class=o>*</span>
</span></span></code></pre></div><p><img class=center-block src=/images/blog/beam-sql-notebooks/image9.png alt="Beam SQL in Notebooks: write a schema-aware PCollection into a text file."></p><p>The above code execution writes the PCollection <code>pcoll</code> (basically
<code>{id: 1, str: foo, flt: 3.14}</code>) into a text file using the coder assigned by
Beam. As you can see, the file content is recorded in a binary non
human-readable format, and that’s normal.</p><p>To decode the file content into a new PCollection, execute:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pcoll_in_file</span> <span class=o>=</span> <span class=n>p</span> <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>ReadFromText</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;/tmp/pcoll*&#39;</span><span class=p>,</span> <span class=n>coder</span><span class=o>=</span><span class=n>coder</span><span class=p>)</span><span class=o>.</span><span class=n>with_output_types</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>pcoll</span><span class=o>.</span><span class=n>element_type</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ib</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>pcoll_in_file</span><span class=p>)</span>
</span></span></code></pre></div><p><img class=center-block src=/images/blog/beam-sql-notebooks/image10.png alt="Beam SQL in Notebooks: read a schema-aware PCollection from a text file."></p><p><strong>Note</strong> you have to use the same coder during encoding and decoding, and
furthermore you may assign the schema explicitly to the new PCollection through
<code>with_output_types()</code>.</p><p>Reading out the encoded binary content from the text file and decoding it with
the correct coder, the content of <code>pcoll</code> is recovered into <code>pcoll_in_file</code>. You
can use this technique to save and share your data through any Beam I/O (not
necessarily a text file) with collaborators who work on their own pipelines (not
just in your notebook session or pipelines).</p><h4 id=schema-in-beam_sql-magic>Schema in <code>beam_sql</code> magic</h4><p>The <code>beam_sql</code> magic automatically registers a <code>RowCoder</code> for your <code>NamedTuple</code>
schema so that you only need to focus on preparing your data for query without
worrying about coders. To see more verbose details of what the <code>beam_sql</code> magic
does behind the scenes, you can use the <code>-v</code> option.</p><p>For example, you can look for all elements with <code>id &lt; 5</code> in <code>persons</code> with the
below query and assign the output to <code>persons_id_lt_5</code>.</p><pre tabindex=0><code>%%beam_sql -o persons_id_lt_5 -v
SELECT * FROM persons WHERE id &lt; 5
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image11.png alt="Beam SQL in Notebooks: beam_sql registers a schema for a PCollection."></p><p>Since this is the first time running this query, you might see a warning message
about:</p><blockquote><p>Schema Person has not been registered to use a RowCoder. Automatically
registering it by running:
beam.coders.registry.register_coder(Person, beam.coders.RowCoder)</p></blockquote><p>The <code>beam_sql</code> magic helps registering a <code>RowCoder</code> for each schema you define
and use whenever it finds one. You can also explicitly run the same code to do
so.</p><p><strong>Note</strong> the output element type is <code>Person(id: int, name: str)</code> instead of
<code>BeamSchema_…</code> because you have selected all the fields from a single
PCollection of the known type <code>Person(id: int, name: str)</code>.</p><p>Another example, you can query for all names from <code>persons</code> and <code>persons_2</code> with
the same ids and assign the output to <code>persons_with_common_id</code>:</p><pre tabindex=0><code>%%beam_sql -o persons_with_common_id -v
SELECT * FROM persons JOIN persons_2 USING (id)
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image12.png alt="Beam SQL in Notebooks: beam_sql creates a schema for a query."></p><p>Note the output element type is now some
<code>BeamSchema_...(id: int64, name: str, name0: str)</code>. Because you have selected
columns from both PCollections, there is no known schema to hold the result.
Beam automatically creates a schema and differentiates the conflicted field
<code>name</code> by suffixing 0 to one of them.</p><p>And since <code>Person</code> is already previously registered with a <code>RowCoder</code>, there is
no more warning about registering it even with the <code>-v</code> option.</p><p>Additionally, you can do a join with <code>pcoll_in_file</code>, <code>persons</code> and <code>persons_2</code>:</p><pre tabindex=0><code>%%beam_sql -o entry_with_common_id

SELECT pcoll_in_file.id, persons.name AS name_1, persons_2.name AS name_2
FROM pcoll_in_file JOIN persons ON pcoll_in_file.id = persons.id
JOIN persons_2 ON pcoll_in_file.id = persons_2.id
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image13.png alt="Beam SQL in Notebooks: rename fields in a query."></p><p>The schema generated reflects the column renaming you have done in the SQL.</p><h2 id=an-example>An Example</h2><p>You will go through an example to find out the US state with the most COVID
positive cases on a specific day with data provided by the
<a href=https://covidtracking.com/>covid tracking project</a>.</p><h3 id=get-the-data>Get the data</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The covidtracking project has stopped collecting new data, current data ends on 2021-03-07</span>
</span></span><span class=line><span class=cl><span class=n>json_current</span><span class=o>=</span><span class=s1>&#39;https://api.covidtracking.com/v1/states/current.json&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_json_data</span><span class=p>(</span><span class=n>url</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>requests</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span> <span class=k>as</span> <span class=n>session</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>session</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>)</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>current_data</span> <span class=o>=</span> <span class=n>get_json_data</span><span class=p>(</span><span class=n>json_current</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>current_data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p><img class=center-block src=/images/blog/beam-sql-notebooks/image14.png alt="Beam SQL in Notebooks: preview example data."></p><p>The data is dated as 2021-03-07. It contains many details about COVID cases for
different states in the US. <code>current_data[0]</code> is just one of the data points.</p><p>You can get rid of most of the columns of the data. For example, just focus on
“date”, “state”, “positive” and “negative”, and then define a schema
<code>UsCovidData</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Optional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>UsCovidData</span><span class=p>(</span><span class=n>NamedTuple</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>partition_date</span><span class=p>:</span> <span class=nb>str</span>  <span class=c1># Remember to str(e[&#39;date&#39;]).</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>positive</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>negative</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span>
</span></span></code></pre></div><p><strong>Note</strong>:</p><ul><li><code>date</code> is a keyword in (Calcite)SQL, use a different field name such as
<code>partition_date</code>;</li><li><code>date</code> from the data is an <code>int</code> type, not <code>str</code>. Make sure you convert the
data using <code>str()</code> or use <code>date: int</code>.</li><li><code>negative</code> has missing values and the default is <code>None</code>. So instead of
<code>negative: int</code>, it should be <code>negative: Optional[int]</code>. Or you can convert
<code>None</code> into 0 when using the schema.</li></ul><p>Then parse the json data into a PCollection with the schema:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>p_sql</span> <span class=o>=</span> <span class=n>beam</span><span class=o>.</span><span class=n>Pipeline</span><span class=p>(</span><span class=n>runner</span><span class=o>=</span><span class=n>InteractiveRunner</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>covid_data</span> <span class=o>=</span> <span class=p>(</span><span class=n>p_sql</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=s1>&#39;Create PCollection from json&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>Create</span><span class=p>(</span><span class=n>current_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=s1>&#39;Parse&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>Map</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=k>lambda</span> <span class=n>e</span><span class=p>:</span> <span class=n>UsCovidData</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>partition_date</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=o>=</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;state&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>positive</span><span class=o>=</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;positive&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>negative</span><span class=o>=</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;negative&#39;</span><span class=p>]))</span><span class=o>.</span><span class=n>with_output_types</span><span class=p>(</span><span class=n>UsCovidData</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ib</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>covid_data</span><span class=p>)</span>
</span></span></code></pre></div><p><img class=center-block src=/images/blog/beam-sql-notebooks/image15.png alt="Beam SQL in Notebooks: parse example data with a schema."></p><h3 id=query>Query</h3><p>You can now find the biggest positive on the “current day” (2021-03-07).</p><pre tabindex=0><code>%%beam_sql -o max_positive
SELECT partition_date, MAX(positive) AS positive
FROM covid_data
GROUP BY partition_date
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image16.png alt="Beam SQL in Notebooks: find the biggest positive from the data."></p><p>However, this is just the positive number. You cannot observe the state that has
this maximum number nor the negative case number for the state.</p><p>To enrich your result, you have to join this data back to the original data set
you have parsed.</p><pre tabindex=0><code>%%beam_sql -o entry_with_max_positive
SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS negative
FROM covid_data JOIN max_positive
USING (partition_date, positive)
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image17.png alt="Beam SQL in Notebooks: enriched data with biggest positive."></p><p>Now you can see all columns of the data with the maximum positive case on
2021-03-07.
<strong>Note</strong>: to handle missing values of the negative column in the original data,
you can use <code>{fn IFNULL(covid_data.negative, 0)}</code> to set null values to 0.</p><p>When you&rsquo;re ready to scale up, you can translate the SQLs into a pipeline with
<code>SqlTransform</code>s and run your pipeline on a distributed runner like Flink or
Spark. This post demonstrates it by launching a one-shot job on Dataflow from
the notebook with the help of <code>beam_sql</code> magic.</p><h3 id=run-on-dataflow>Run on Dataflow</h3><p>Now that you have a pipeline that parses US COVID data from json to find
positive/negative/state information for the state with the most positive cases
on each day, you can try applying it to all historical daily data and running it
on Dataflow.</p><p>The new data source you will use is a public dataset from USAFacts US
Coronavirus Database that contains all historical daily summary of COVID cases
in the US.</p><p>The schema of data is very similar to what the covid tracking project website
provides. The fields you will query are: <code>date</code>, <code>state</code>, <code>confirmed_cases</code>, and
<code>deaths</code>.</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image18.png alt="Beam SQL in Notebooks: schema of cloud data."></p><p>A preview of the data looks like below (you may skip the inspection in BigQuery
and just take a look at the screenshot):</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image19.png alt="Beam SQL in Notebooks: preview of cloud data."></p><p>The format of the data is <strong>slightly different</strong> from the json data you parsed
in the previous pipeline because the numbers are grouped by counties instead of
states, thus some additional aggregations need to be done in the SQLs.</p><p>If you need a fresh execution, you may click the “Restart the kernel” button on
the top menu.</p><p>Full code is as below, on-top of the original pipeline and queries:</p><ul><li>It changes the source from a single-day data to a more complete historical
data;</li><li>It changes the I/O and schema to accommodate the new dataset;</li><li>It changes the SQLs to include more aggregations to accommodate the new
format of the dataset.</li></ul><p><strong>Prepare the data with schema</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>NamedTuple</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Optional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Public BQ dataset.</span>
</span></span><span class=line><span class=cl><span class=n>table</span> <span class=o>=</span> <span class=s1>&#39;bigquery-public-data:covid19_usafacts.summary&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># Replace with your project.</span>
</span></span><span class=line><span class=cl><span class=n>project</span> <span class=o>=</span> <span class=s1>&#39;YOUR-PROJECT-NAME-HERE&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># Replace with your GCS bucket.</span>
</span></span><span class=line><span class=cl><span class=n>gcs_location</span> <span class=o>=</span> <span class=s1>&#39;gs://YOUR_GCS_BUCKET_HERE&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>UsCovidData</span><span class=p>(</span><span class=n>NamedTuple</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>partition_date</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>confirmed_cases</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>deaths</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_on_dataflow</span> <span class=o>=</span> <span class=n>beam</span><span class=o>.</span><span class=n>Pipeline</span><span class=p>(</span><span class=n>runner</span><span class=o>=</span><span class=n>InteractiveRunner</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>covid_data</span> <span class=o>=</span> <span class=p>(</span><span class=n>p_on_dataflow</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=s1>&#39;Read dataset&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>ReadFromBigQuery</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>project</span><span class=o>=</span><span class=n>project</span><span class=p>,</span> <span class=n>table</span><span class=o>=</span><span class=n>table</span><span class=p>,</span> <span class=n>gcs_location</span><span class=o>=</span><span class=n>gcs_location</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=s1>&#39;Parse&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>Map</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=k>lambda</span> <span class=n>e</span><span class=p>:</span> <span class=n>UsCovidData</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>partition_date</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=o>=</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;state&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>confirmed_cases</span><span class=o>=</span><span class=nb>int</span><span class=p>(</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;confirmed_cases&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                <span class=n>deaths</span><span class=o>=</span><span class=nb>int</span><span class=p>(</span><span class=n>e</span><span class=p>[</span><span class=s1>&#39;deaths&#39;</span><span class=p>])))</span><span class=o>.</span><span class=n>with_output_types</span><span class=p>(</span><span class=n>UsCovidData</span><span class=p>))</span>
</span></span></code></pre></div><p><strong>Run on Dataflow</strong></p><p>To run SQL on Dataflow is very simple, you just need to add the option
<code>-r DataflowRunner</code>.</p><pre tabindex=0><code>%%beam_sql -o data_by_state -r DataflowRunner
SELECT partition_date, state, SUM(confirmed_cases) as confirmed_cases, SUM(deaths) as deaths
FROM covid_data
GROUP BY partition_date, state
</code></pre><p>Different from previous <code>beam_sql</code> magic executions, you won’t see the result
immediately. Instead, a form like below is printed in the notebook cell output:</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image20.png alt="Beam SQL in Notebooks: empty run-on-dataflow form."></p><p>The <code>beam_sql</code> magic tries its best to guess your project id and preferred cloud
region. You still have to input additional information necessary to submit a
Dataflow job, such as a GCS bucket to stage the Dataflow job and any additional
Python dependencies the job needs.</p><p>For now, ignore the form in the cell output, because you still need 2 more SQLs
to: 1) find the maximum confirmed cases on each day; 2) join the maximum case
data with the full data_by_state. The <code>beam_sql</code> magic allows you to chain SQLs,
so chain 2 more by executing:</p><pre tabindex=0><code>%%beam_sql -o max_cases -r DataflowRunner
SELECT partition_date, MAX(confirmed_cases) as confirmed_cases
FROM data_by_state
GROUP BY partition_date
</code></pre><p>And</p><pre tabindex=0><code>%%beam_sql -o data_with_max_cases -r DataflowRunner
SELECT data_by_state.partition_date, data_by_state.state, data_by_state.confirmed_cases, data_by_state.deaths
FROM data_by_state JOIN max_cases
USING (partition_date, confirmed_cases)
</code></pre><p>By default, when running <code>beam_sql</code> on Dataflow, the output PCollection will be
written to a text file on GCS. The “write” is automatically provided by
<code>beam_sql</code> and mainly for your inspection of the output data for this one-shot
Dataflow job. It’s lightweight and does not encode elements for further
development. To save the output and share it with others, you can add more Beam
I/Os into the mix.</p><p>For example, you can appropriately encode elements into text files using the
technique described in the above schema-aware PCollections example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>apache_beam.options.pipeline_options</span> <span class=kn>import</span> <span class=n>GoogleCloudOptions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>coder</span> <span class=o>=</span> <span class=n>beam</span><span class=o>.</span><span class=n>coders</span><span class=o>.</span><span class=n>registry</span><span class=o>.</span><span class=n>get_coder</span><span class=p>(</span><span class=n>data_with_max_cases</span><span class=o>.</span><span class=n>element_type</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>max_data_file</span> <span class=o>=</span> <span class=n>gcs_location</span> <span class=o>+</span> <span class=s1>&#39;/encoded_max_data&#39;</span>
</span></span><span class=line><span class=cl><span class=n>data_with_max_cases</span> <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>textio</span><span class=o>.</span><span class=n>WriteToText</span><span class=p>(</span><span class=n>max_data_file</span><span class=p>,</span> <span class=n>coder</span><span class=o>=</span><span class=n>coder</span><span class=p>)</span>
</span></span></code></pre></div><p>Furthermore, you can create a new BQ dataset in your own project to store the
processed data.</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image21.png alt="Beam SQL in Notebooks: create a new BQ dataset."></p><p>You have to select the same data location as the public BigQuery data you are
reading. In this case, “us (multiple regions in United States)”.</p><p>Once you finish creating an empty dataset, you can execute below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>output_table</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>project</span><span class=si>}</span><span class=s1>:covid_data.max_analysis&#39;</span>
</span></span><span class=line><span class=cl><span class=n>bq_schema</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;fields&#39;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;partition_date&#39;</span><span class=p>,</span> <span class=s1>&#39;type&#39;</span><span class=p>:</span> <span class=s1>&#39;STRING&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;state&#39;</span><span class=p>,</span> <span class=s1>&#39;type&#39;</span><span class=p>:</span> <span class=s1>&#39;STRING&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;confirmed_cases&#39;</span><span class=p>,</span> <span class=s1>&#39;type&#39;</span><span class=p>:</span> <span class=s1>&#39;INTEGER&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;deaths&#39;</span><span class=p>,</span> <span class=s1>&#39;type&#39;</span><span class=p>:</span> <span class=s1>&#39;INTEGER&#39;</span><span class=p>}]}</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>data_with_max_cases</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span> <span class=s1>&#39;To json-like&#39;</span> <span class=o>&gt;&gt;</span> <span class=n>beam</span><span class=o>.</span><span class=n>Map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;partition_date&#39;</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>partition_date</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;state&#39;</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>state</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;confirmed_cases&#39;</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>confirmed_cases</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;deaths&#39;</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>deaths</span><span class=p>})</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span> <span class=n>beam</span><span class=o>.</span><span class=n>io</span><span class=o>.</span><span class=n>WriteToBigQuery</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>table</span><span class=o>=</span><span class=n>output_table</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>schema</span><span class=o>=</span><span class=n>bq_schema</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>method</span><span class=o>=</span><span class=s1>&#39;STREAMING_INSERTS&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>custom_gcs_temp_location</span><span class=o>=</span><span class=n>gcs_location</span><span class=p>))</span>
</span></span></code></pre></div><p>Now back in the form of the last SQL cell output, you may fill in necessary
information to run the pipeline on Dataflow. An example input looks like below:</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image22.png alt="Beam SQL in Notebooks: fill in the run-on-Dataflow form."></p><p>Because this pipeline doesn’t use any additional Python dependency, “Additional
Packages” is left empty. In the previous example where you have installed a
package called <code>names</code>, to run that pipeline on Dataflow, you have to put
<code>names</code> in this field.</p><p>Once you finish updating your inputs, you can click the <code>Show Options</code> button to
view what pipeline options have been configured based on your inputs. A variable
<code>options_[YOUR_OUTPUT_PCOLL_NAME]</code> is generated, and you can supply more
pipeline options to it if the form is not enough for your execution.</p><p>Once you are ready to submit the Dataflow job, click the <code>Run on Dataflow</code>
button. It tells you where the default output would be written, and after a
while, a line with:</p><blockquote><p>Click here for the details of your Dataflow job.</p></blockquote><p>would be displayed. You can click on the hyperlink to go to your Dataflow job
page. (Optionally, you can ignore the form and continue development to extend
your pipeline. Once you are satisfied with the state of your pipeline, you can
come back to the form and submit the job to Dataflow.)</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image23.png alt="Beam SQL in Notebooks: a Dataflow job graph."></p><p>As you can see, each transform name of the generated Dataflow job is prefixed
with a string <code>[number]: </code>. This is to distinguish re-executed codes in
notebooks because Beam requires each transform to have a distinct name. Under
the hood, the <code>beam_sql</code> magic also stages your schema information to Dataflow,
so you might see transforms named as <code>schema_loaded_beam_sql_…</code>. This is because
the <code>NamedTuple</code> defined in the notebook is likely in the <code>__main__</code> scope and
Dataflow is not aware of them at all. To minimize user intervention and avoid
pickling the whole main session (and it’s infeasible to pickle the main session
when it contains unpickle-able attributes), the <code>beam_sql</code> magic optimizes the
staging process by serializing your schemas, staging them to Dataflow, and then
deserialize/load them for job execution.</p><p>Once the job succeeds, the result of the output PCollection would be written to
places instructed by your I/O transforms. <strong>Note</strong>: running <code>beam_sql</code> on
Dataflow generates a one-shot job and it’s not interactive.</p><p>A simple inspection of the data from the default output location:</p><pre tabindex=0><code>!gsutil cat &#39;gs://ningk-so-test/bq/staging/data_with_max_cases*&#39;
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image24.png alt="Beam SQL in Notebooks: inspect the default output file."></p><p>The text file with encoded binary data written by your <code>WriteToText</code>:</p><pre tabindex=0><code>!gsutil cat &#39;gs://ningk-so-test/bq/encoded_max_data*&#39;
</code></pre><p><img class=center-block src=/images/blog/beam-sql-notebooks/image25.png alt="Beam SQL in Notebooks: inspect the user-defined output file."></p><p>The table <code>YOUR-PROJECT:covid_data.max_analysis</code> created by your
<code>WriteToBigQuery</code>:</p><p><img class=center-block src=/images/blog/beam-sql-notebooks/image26.png alt="Beam SQL in Notebooks: inspect the output BQ dataset."></p><h3 id=run-on-other-oss-runners-directly-with-the-beam_sql-magic>Run on other OSS runners directly with the <code>beam_sql</code> magic</h3><p>On the day this blog is posted, the <code>beam_sql</code> magic only supports DirectRunner
(interactive) and DataflowRunner (one-shot). It&rsquo;s a simple wrapper on top of
the <code>SqlTransform</code> with interactive input widgets implemented by
<a href=https://ipywidgets.readthedocs.io/en/stable/>ipywidgets</a>. You can implement
your own runner support or utilities by following the
<a href=https://lists.apache.org/thread/psrx1xhbyjcqbhxx6trf5nvh66c6pk3y>instructions</a>.</p><p>Additionally, support for other OSS runners are WIP, for example,
<a href=https://issues.apache.org/jira/browse/BEAM-14373>support using FlinkRunner with the <code>beam_sql</code> magic</a>.</p><h2 id=conclusions>Conclusions</h2><p>The <code>beam_sql</code> magic and Apache Beam Notebooks combined is a convenient tool for
you to learn Beam SQL and mix Beam SQL into prototyping and productionizing (
e.g., to Dataflow) your Beam pipelines with minimum setups.</p><p>For more details about the Beam SQL syntax, check out the Beam Calcite SQL
<a href=/documentation/dsls/sql/calcite/overview/>compatibility</a>
and the Apache Calcite SQL
<a href=https://calcite.apache.org/docs/reference.html>syntax</a>.</p></div></div><div class=blog-content><h2>Latest from the blog</h2></div><div class=posts-list><a class=post-card href=/blog/gsoc-25-infra/ data-categories="blog gsoc "><div class="post-info post-category"><p>blog & gsoc
                    
      </p><p>2025/09/15</p></div><div class=post><p class=post-title>Google Summer of Code 25 - Improving Apache Beam's Infrastructure</p><p class=post-info>Enrique Calderon</p></div></a><a class=post-card href=/blog/beam-2.67.0/ data-categories="blog release "><div class="post-info post-category"><p>blog & release
                    
   </p><p>2025/08/12</p></div><div class=post><p class=post-title>Apache Beam 2.67.0</p><p class=post-info>Vitalii Terentev</p></div></a><a class=post-card href=/blog/beam-summit-2025-hackathon-pcollectors-blog/ data-categories="blog "><div class="post-info post-category"><p>blog</p><p>2025/07/08</p></div><div class=post><p class=post-title>Our Experience at Beam College 2025: 1st Place Hackathon Winners</p><p class=post-info>Aditya Shukla &
Darshan Kanade</p></div></a></div></article></div><footer class=footer><div class=footer__contained><div class=footer__cols><div class="footer__cols__col footer__cols__col__logos"><div class=footer__cols__col__logo><img src=/images/beam_logo_circle.svg class=footer__logo alt="Beam logo"></div><div class=footer__cols__col__logo><img src=/images/apache_logo_circle.svg class=footer__logo alt="Apache logo"></div></div><div class=footer-wrapper><div class=wrapper-grid><div class=footer__cols__col><div class=footer__cols__col__title>Start</div><div class=footer__cols__col__link><a href=/get-started/beam-overview/>Overview</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-java/>Quickstart (Java)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-py/>Quickstart (Python)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-go/>Quickstart (Go)</a></div><div class=footer__cols__col__link><a href=/get-started/downloads/>Downloads</a></div></div><div class=footer__cols__col><div class=footer__cols__col__title>Docs</div><div class=footer__cols__col__link><a href=/documentation/programming-guide/>Concepts</a></div><div class=footer__cols__col__link><a href=/documentation/pipelines/design-your-pipeline/>Pipelines</a></div><div class=footer__cols__col__link><a href=/documentation/runners/capability-matrix/>Runners</a></div></div><div class=footer__cols__col><div class=footer__cols__col__title>Community</div><div class=footer__cols__col__link><a href=/contribute/>Contribute</a></div><div class=footer__cols__col__link><a href=https://projects.apache.org/committee.html?beam target=_blank>Team<img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></div><div class=footer__cols__col__link><a href=/community/presentation-materials/>Media</a></div><div class=footer__cols__col__link><a href=/community/in-person/>Events/Meetups</a></div><div class=footer__cols__col__link><a href=/community/contact-us/>Contact Us</a></div></div><div class=footer__cols__col><div class=footer__cols__col__title>Resources</div><div class=footer__cols__col__link><a href=/blog/>Blog</a></div><div class=footer__cols__col__link><a href=https://github.com/apache/beam>GitHub</a></div></div></div><div class=footer__bottom>&copy;
<a href=https://www.apache.org>The Apache Software Foundation</a>
| <a href=/privacy_policy>Privacy Policy</a>
| <a href=/feed.xml>RSS Feed</a><br><br>Apache Beam, Apache, Beam, the Beam logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation. All other products or name brands are trademarks of their respective holders, including The Apache Software Foundation.</div></div><div class="footer__cols__col footer__cols__col__logos"><div class=footer__cols__col--group><div class=footer__cols__col__logo><a href=https://github.com/apache/beam><img src=/images/logos/social-icons/github-logo-150.png class=footer__logo alt="Github logo"></a></div><div class=footer__cols__col__logo><a href=https://www.linkedin.com/company/apache-beam/><img src=/images/logos/social-icons/linkedin-logo-150.png class=footer__logo alt="Linkedin logo"></a></div></div><div class=footer__cols__col--group><div class=footer__cols__col__logo><a href=https://twitter.com/apachebeam><img src=/images/logos/social-icons/twitter-logo-150.png class=footer__logo alt="Twitter logo"></a></div><div class=footer__cols__col__logo><a href=https://www.youtube.com/channel/UChNnb_YO_7B0HlW6FhAXZZQ><img src=/images/logos/social-icons/youtube-logo-150.png class=footer__logo alt="Youtube logo"></a></div></div></div></div></div></footer></body></html>