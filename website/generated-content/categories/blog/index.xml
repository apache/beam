<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – blog</title><link>/categories/blog/</link><description>Recent content in blog on Apache Beam</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 14 Oct 2025 00:00:00 +0800</lastBuildDate><atom:link href="/categories/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Google Summer of Code 2025 - Enhanced Interactive Pipeline Development Environment for JupyterLab</title><link>/blog/gsoc-25-jupyterlab-extensions/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0800</pubDate><guid>/blog/gsoc-25-jupyterlab-extensions/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="gsoc-2025-basic-information">GSoC 2025 Basic Information&lt;/h1>
&lt;p>&lt;strong>Student:&lt;/strong> [Canyu Chen] (&lt;a href="https://github.com/Chenzo1001">@Chenzo1001&lt;/a>)
&lt;strong>Mentors:&lt;/strong> [XQ Hu] (&lt;a href="https://github.com/liferoad">@liferoad&lt;/a>)
&lt;strong>Organization:&lt;/strong> [Apache Beam]
&lt;strong>Proposal Link:&lt;/strong> &lt;a href="https://drive.google.com/file/d/1gmrSUGpXMXujVnFffuj0UWQjbghWI8Oy/view?usp=sharing">Here&lt;/a>&lt;/p>
&lt;h1 id="project-overview">Project Overview&lt;/h1>
&lt;p>BeamVision significantly enhances the Apache Beam development experience within JupyterLab by providing a unified, visual interface for pipeline inspection and analysis. This project successfully delivered a production-ready JupyterLab extension that replaces fragmented workflows with an integrated workspace, featuring a dynamic side panel for pipeline visualization and a multi-tab interface for comparative workflow analysis.&lt;/p>
&lt;p>Core Achievements:&lt;/p>
&lt;p>Modernized Extension: Upgraded the JupyterLab Sidepanel to v4.x, ensuring compatibility with the latest ecosystem and releasing the package on both &lt;a href="https://www.npmjs.com/package/apache-beam-jupyterlab-sidepanel">NPM&lt;/a> and &lt;a href="https://pypi.org/project/apache-beam-jupyterlab-sidepanel/">PyPI&lt;/a>.&lt;/p>
&lt;p>YAML Visualization Suite: Implemented a powerful visual editor for Beam YAML, combining a code editor, an interactive flow chart (built with @xyflow/react-flow), and a collapsible key-value panel for intuitive pipeline design.&lt;/p>
&lt;p>Enhanced Accessibility &amp;amp; Stability: Added pip installation support and fixed critical bugs in Interactive Beam, improving stability and user onboarding.&lt;/p>
&lt;p>Community Engagement: Active participation in the Beam community, including contributing to a hackathon project and successfully integrating all work into the Apache Beam codebase via merged Pull Requests.&lt;/p>
&lt;h1 id="development-workflow">Development Workflow&lt;/h1>
&lt;p>As early as the beginning of March, I saw Apache&amp;rsquo;s project information on the official GSoC website and came across Beam among the projects released by Apache. Since I have some interest in front-end development and wanted to truly integrate into the open-source community for development work, I contacted mentor XQ Hu via email and received positive feedback from him. In April, XQ Hu posted notes for all GSoC students on the Beam Mailing List. It was essential to keep an eye on the Mailing List promptly. Between March and May, besides completing the project proposal and preparation work, I also used my spare time to partially migrate the Beam JupyterLab Extension to version 4.0. This helped me get into the development state more quickly.&lt;/p>
&lt;p>I also participated in the Beam Hackathon held in May. There were several topics to choose from, and I opted for the free topic. This allowed me to implement any innovative work on Beam. I combined Beam and GCP to create an &lt;a href="https://github.com/Chenzo1001/Beam_auto_emotion_analysis">Automatic Emotion Analysis Tool for comments&lt;/a>. This tool integrates Beam Pipeline, Flink, Docker, and GCP to collect and perform sentiment analysis on real-time comment stream data, storing the results in GCP&amp;rsquo;s BigQuery. This is a highly meaningful task because sentiment analysis of comments can help businesses better understand users&amp;rsquo; opinions about their products, thereby improving the products more effectively. However, the time during the Hackathon was too tight, so I haven&amp;rsquo;t fully completed this project yet, and it can be further improved later. This Hackathon gave me a deeper understanding of Beam and GCP, and also enhanced my knowledge of the development of the Beam JupyterLab Extension.&lt;/p>
&lt;p>In June, I officially started the project development and maintained close communication with my mentor to ensure the project progressed smoothly. XQ Hu and I held a half-hour weekly meeting every Monday on Google Meet, primarily to address issues encountered during the previous week&amp;rsquo;s development and to discuss the tasks for the upcoming week. XQ Hu is an excellent mentor, and I had no communication barriers with him whatsoever. He is also very understanding; sometimes, when I needed to postpone some development tasks due to personal reasons, he was always supportive and gave me ample freedom. During this month, I improved the plugin to make it fully compatible with JupyterLab 4.0.&lt;/p>
&lt;p>In July and August, I made some modifications to the plugin&amp;rsquo;s source code structure and published it on PyPI to facilitate user installation and promote the plugin. During this period, I also fixed several bugs. Afterwards, I began developing a new feature: the YAML visual editor (design doc &lt;a href="https://s.apache.org/beam-yaml-jupyterlab">HERE&lt;/a>). This feature is particularly meaningful because Beam&amp;rsquo;s Pipeline is described through YAML files, and a visual editor for YAML files can significantly improve developers&amp;rsquo; efficiency. In July, I published the proposal for the YAML visual editor and, after gathering feedback from the community for some time, started working on its development. Initially, I planned to use native Cytoscape to build the plugin from scratch, but the workload was too heavy, and there were many mature flow chart plugins in the open-source community that could be referenced. Therefore, I chose XYFlow as the component for flow visualization and integrated it into the plugin. In August, I further optimized the YAML visual editor and fixed some bugs.&lt;/p>
&lt;img src="/images/blog/gsoc-25-jupyterlab-extensions/Yaml_main.png" alt="Main page of the YAML visual editor" width="100%">
&lt;p>In September, I completed the project submission, passed Google&amp;rsquo;s review, and successfully concluded the project.&lt;/p>
&lt;h1 id="development-conclusion">Development Conclusion&lt;/h1>
&lt;p>Overall, collaborating with Apache Beam&amp;rsquo;s developers was a very enjoyable process. I learned a lot about Beam, and since I am a student engaged in high-performance geographic computing research, Beam may play a significant role in my future studies and work.&lt;/p>
&lt;p>I am excited to remain an active member of the Beam community. I hope to continue contributing to its development, applying what I have learned to both my academic pursuits and future collaborative projects. The experience has strengthened my commitment to open-source innovation and has set a strong foundation for ongoing participation in Apache Beam and related technologies.&lt;/p>
&lt;h1 id="special-thanks">Special Thanks&lt;/h1>
&lt;p>I would like to express my sincere gratitude to my mentor XQ Hu for his guidance and support throughout the project. Without his help, I would not have been able to complete this project successfully. His professionalism, patience, and passion have been truly inspiring. As a Google employee, he consistently dedicated time each week to the open-source community and willingly assisted students like me. His selfless dedication to open source is something I deeply admire and strive to emulate. He is also an exceptionally devoted teacher who not only imparted technical knowledge but also taught me how to communicate more effectively, handle interpersonal relationships, and collaborate better in a team setting. He always patiently addressed my questions and provided invaluable advice. I am immensely grateful to him and hope to have the opportunity to work with him again in the future.&lt;/p>
&lt;p>I also want to thank the Apache Beam community for their valuable feedback and suggestions, which have greatly contributed to the improvement of the plugin. I feel incredibly fortunate that we, as a society, have open-source communities where individuals contribute their intellect and time to drive collective technological progress and innovation. These communities provide students like me with invaluable opportunities to grow and develop rapidly.&lt;/p>
&lt;p>Finally, I would like to thank the Google Summer of Code program for providing me with this opportunity to contribute to open-source projects and gain valuable experience. Without Google Summer of Code, I might never have had the chance to engage with so many open-source projects, take that first step into the open-source community, or experience such substantial personal and professional growth.&lt;/p></description></item><item><title>Blog: Google Summer of Code 2025 - Beam ML Vector DB/Feature Store integrations</title><link>/blog/gsoc-25-ml-connectors/</link><pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate><guid>/blog/gsoc-25-ml-connectors/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="what-will-i-cover-in-this-blog-post">What Will I Cover In This Blog Post?&lt;/h2>
&lt;p>I have three objectives in mind when writing this blog post:&lt;/p>
&lt;ul>
&lt;li>Documenting the work I&amp;rsquo;ve been doing during this GSoC period in collaboration
with the Apache Beam community&lt;/li>
&lt;li>A thoughtful and cumulative thank you to my mentor and the Beam Community&lt;/li>
&lt;li>Writing to an older version of myself before making my first ever contribution
to Beam. This can be helpful for future contributors&lt;/li>
&lt;/ul>
&lt;h2 id="what-was-this-gsoc-project-about">What Was This GSoC Project About?&lt;/h2>
&lt;p>The goal of this project is to enhance Beam&amp;rsquo;s Python SDK by developing
connectors for vector databases like Milvus and feature stores like Tecton. These
integrations will improve support for ML use cases such as Retrieval-Augmented
Generation (RAG) and feature engineering. By bridging Beam with these systems,
this project will attract more users, particularly in the ML community.&lt;/p>
&lt;h2 id="why-was-this-project-important">Why Was This Project Important?&lt;/h2>
&lt;p>While Beam&amp;rsquo;s Python SDK supports some vector databases, feature stores and
embedding generators, the current integrations are limited to a few systems as
mentioned in the tables down below. Expanding this ecosystem will provide more
flexibility and richness for ML workflows particularly in feature engineering
and RAG applications, potentially attracting more users, particularly in the ML
community.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Vector Database&lt;/th>
&lt;th>Feature Store&lt;/th>
&lt;th>Embedding Generator&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BigQuery&lt;/td>
&lt;td>Vertex AI&lt;/td>
&lt;td>Vertex AI&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AlloyDB&lt;/td>
&lt;td>Feast&lt;/td>
&lt;td>Hugging Face&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="why-did-i-choose-beam-as-part-of-gsoc-among-180-orgs">Why Did I Choose Beam As Part of GSoC Among 180+ Orgs?&lt;/h2>
&lt;p>I chose to apply to Beam from among 180+ GSoC organizations because it aligns
well with my passion for data processing systems that serve information
retrieval systems and my core career values:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Freedom:&lt;/strong> Working on Beam supports open-source development, liberating
developers from vendor lock-in through its unified programming model while
enabling services like &lt;a href="https://projectshield.withgoogle.com/landing">Project Shield&lt;/a> to protect free
speech globally&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Innovation:&lt;/strong> Working on Beam allows engagement with cutting-edge data
processing techniques and distributed computing paradigms&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Accessibility:&lt;/strong> Working on Beam helps build open-source technology that
makes powerful data processing capabilities available to all organizations
regardless of size or resources. This accessibility enables projects like
Project Shield to provide free protection to media, elections, and human rights
websites worldwide&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-did-i-work-on-during-the-gsoc-program">What Did I Work On During the GSoC Program?&lt;/h2>
&lt;p>During my GSoC program, I focused on developing connectors for vector databases,
feature stores, and embedding generators to enhance Beam&amp;rsquo;s ML capabilities.
Here are the artifacts I worked on and what remains to be done:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>System&lt;/th>
&lt;th>Artifact&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Enrichment Handler&lt;/td>
&lt;td>Milvus&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35216">PR #35216&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35577">PR #35577&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35467">PR #35467&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sink I/O&lt;/td>
&lt;td>Milvus&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35708">PR #35708&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35944">PR #35944&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Enrichment Handler&lt;/td>
&lt;td>Tecton&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/36062">PR #36062&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sink I/O&lt;/td>
&lt;td>Tecton&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/36078">PR #36078&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Gen&lt;/td>
&lt;td>OpenAI&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/36081">PR #36081&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Gen&lt;/td>
&lt;td>Anthropic&lt;/td>
&lt;td>To Be Added&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Here are side-artifacts that are not directly linked to my project:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>System&lt;/th>
&lt;th>Artifact&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AI Code Review&lt;/td>
&lt;td>Gemini Code Assist&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35532">PR #35532&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Enrichment Handler&lt;/td>
&lt;td>CloudSQL&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/34398">PR #34398&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35473">PR #35473&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pytest Markers&lt;/td>
&lt;td>GitHub CI&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35655">PR #35655&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35740">PR #35740&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35816">PR #35816&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For more granular contributions, checking out my
&lt;a href="https://github.com/apache/beam/pulls?q=is%3Apr+author%3Amohamedawnallah">ongoing Beam contributions&lt;/a>.&lt;/p>
&lt;h2 id="how-did-i-approach-this-project">How Did I Approach This Project?&lt;/h2>
&lt;p>My approach centered on community-driven design and iterative implementation,
Originally inspired by my mentor&amp;rsquo;s work. Here&amp;rsquo;s how it looked:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Design Document&lt;/strong>: Created a comprehensive design document outlining the
proposed ML connector architecture&lt;/li>
&lt;li>&lt;strong>Community Feedback&lt;/strong>: Shared the design with the Beam developer community
mailing list for review&lt;/li>
&lt;li>&lt;strong>Iterative Implementation&lt;/strong>: Incorporated community feedback and applied
learnings in subsequent pull requests&lt;/li>
&lt;li>&lt;strong>Continuous Improvement&lt;/strong>: Refined the approach based on real-world usage
patterns and maintainer guidance&lt;/li>
&lt;/ol>
&lt;p>Here are some samples of those design docs:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Design Document&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Milvus&lt;/td>
&lt;td>Vector Enrichment Handler&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/4c6l20tjopd94cqg6vsgj20xl2qgywtx">[Proposal][GSoC 2025] Milvus Vector Enrichment Handler for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Milvus&lt;/td>
&lt;td>Vector Sink I/O Connector&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/cwlbwnhnf1kl7m0dn40jrqfsf4ho98tf">[Proposal][GSoC 2025] Milvus Vector Sink I/O Connector for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tecton&lt;/td>
&lt;td>Feature Store Enrichment Handler&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/7ynn4r8b8b1c47ojxlk39fhsn3t0jrd1">[Proposal][GSoC 2025] Tecton Feature Store Enrichment Handler for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tecton&lt;/td>
&lt;td>Feature Store Sink I/O Connector&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/dthd3t6md9881ksvbf4v05rxnlj1fgvn">[Proposal][GSoC 2025] Tecton Feature Store Sink I/O Connector for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="where-did-challenges-arise-during-the-project">Where Did Challenges Arise During The Project?&lt;/h2>
&lt;p>There were 2 places where challenges arose:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Running Docker TestContainers in Beam Self-Hosted CI Environment:&lt;/strong> The main
challenge was that Beam runs in CI on Ubuntu 20.04, which caused compatibility
and connectivity issues with Milvus TestContainers due to the Docker-in-Docker
environment. After several experiments with trial and error, I eventually tested
with Ubuntu latest (which at the time of writing this blog post is Ubuntu 25.04),
and no issues arose. This version compatibility problem led to the container
startup failures and network connectivity issues&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Triggering and Modifying the PostCommit Python Workflows:&lt;/strong> This challenge
magnified the above issue since for every experiment update to the given
workflow, I had to do a round trip to my mentor to include those changes in the
relevant workflow files and evaluate the results. I also wasn&amp;rsquo;t aware that
someone can trigger post-commit Python workflows by updating the trigger files
in &lt;code>.github/trigger_files&lt;/code> until near the middle of GSoC. I discovered there is
actually a workflows README document in &lt;code>.github/workflows/README.md&lt;/code> that was
not referenced in the &lt;code>CONTRIBUTING.md&lt;/code> file at the time of writing this post&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-did-this-project-start-to-attract-users-in-the-ml-community">How Did This Project Start To Attract Users in the ML Community?&lt;/h2>
&lt;p>It is observed that after we had a Milvus Enrichment Handler PR before even
merging, we started to see community-driven contributions like
&lt;a href="https://github.com/apache/beam/pull/35686">this one that adds Qdrant&lt;/a>. Qdrant
is a competitor to Milvus in the vector space. This demonstrates how
the project&amp;rsquo;s momentum and visibility in the ML community space attracted
contributors who wanted to expand the Beam ML ecosystem with additional vector
database integrations.&lt;/p>
&lt;h2 id="how-did-this-gsoc-experience-working-with-beam-community-shape-me">How Did This GSoC Experience Working With Beam Community Shape Me?&lt;/h2>
&lt;p>If I have to boil it down across three dimensions, they would be:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mindset:&lt;/strong> Before I was probably working in solitude making PRs about new
integrations with mental chatter in the form of fingers crossed, hoping that
there will be no divergence on the design. Now I can engage people I am working
with through design docs, making sure my work aligns with their vision, which
potentially leads to faster PR merges&lt;/li>
&lt;li>&lt;strong>Skillset:&lt;/strong> It was one year before contributing to Beam where I wrote
professionally in Python, so it was a great opprtunity to brush up on my Python
skills and seeing how some design patterns are used in practice, like the query
builder pattern seen in CloudSQL Vector Ingestion in the RAG package. I also
learned about vector databases and feature stores, and also some AI
integrations. I also think I got a bit better than before in root cause analysis
and filtering signals from noise in long log files like PostCommit Python
workflows&lt;/li>
&lt;li>&lt;strong>Toolset:&lt;/strong> Learning about Beam Python SDK, Milvus, Tecton, Google CloudSQL,
OpenAI and Anthropic text embedding generators, and lnav for effective log file
navigation, including their capabilities and limitations&lt;/li>
&lt;/ul>
&lt;h2 id="tips-for-future-contributors">Tips for Future Contributors&lt;/h2>
&lt;p>If I have to boil them down to three, they would be:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Observing:&lt;/strong> Observing how experienced developers in the Beam dev team
work—how their PRs look, how they write design docs, what kind of feedback they
get on their design docs and PRs, and how you can apply it (if feasible) to
avoid getting the same feedback again. What kind of follow-up PRs do they create
after their initial ones? How do they document and illustrate their work? What
kind of comments do they post when reviewing other people&amp;rsquo;s related work? Over
time, you build your own mental model and knowledge base on how the ideal
contribution looks in this area. There is a lot to learn and explore in an
exciting, not intimidating way&lt;/li>
&lt;li>&lt;strong>Orienting:&lt;/strong> Understanding your place in the ecosystem and aligning your
work with the project&amp;rsquo;s context. This means grasping how your contribution fits
into Beam&amp;rsquo;s architecture and roadmap, identifying your role in addressing
current gaps, and mapping stakeholders who will review, use, and maintain your
work. Most importantly, align with both your mentor&amp;rsquo;s vision and the community&amp;rsquo;s
vision to ensure your work serves the broader goals&lt;/li>
&lt;li>&lt;strong>Acting:&lt;/strong> Acting on feedback from code reviews, design document discussions,
and community input. This means thoughtfully addressing suggested changes in a
way that moves the discussion forward, addressing concerns raised by
maintainers, and iterating on your work based on community guidance. Being
responsive to feedback, asking clarifying questions when needed, and
demonstrating that you&amp;rsquo;re incorporating the community&amp;rsquo;s input into your
contributions given that it is aligned with the project direction&lt;/li>
&lt;/ul>
&lt;h2 id="who-do-i-want-to-thank-for-making-this-journey-possible">Who Do I Want To Thank for Making This Journey Possible?&lt;/h2>
&lt;p>If I have to boil them down to three, they would be:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>My Mentor, Danny McCormick:&lt;/strong> I wouldn&amp;rsquo;t hesitate to say that Danny is the
best mentor I have worked with so far, given that I have worked with several
mentors. What makes me say that:
&lt;ul>
&lt;li>&lt;strong>Generosity:&lt;/strong> Danny is very generous with his time, feedback, and
genuinely committed to reviewing my work on a regular basis. We have weekly
30-minute sync calls over almost 21 weeks (5 months) since the official
community bonding period, where he shares with me his contextual expertise and
addresses any questions I may have with openness to extend time if needed and
flexible about skipping calls when there was no agenda&lt;/li>
&lt;li>&lt;strong>Flexibility:&lt;/strong> When I got accepted to GSoC, after a few days I also got
accepted to a part-time internship that I had applied to before GSoC, while
also managing my last semester in my Bachelor of Computer Science, which was
probably the hardest semester. During our discussion about working capacity,
Danny was very flexible regarding that, with more emphasis on making progress,
which encouraged me to make even more progress. I have also never felt there
are very hard boundaries around my project scope—I felt there was an area to
explore that motivated me to think of and add some side-artifacts to Beam,
e.g., adding Gemini Code Assist for AI code review&lt;/li>
&lt;li>&lt;strong>Proactivity&lt;/strong>: Danny was very proactive in offering support and help
without originally asking, e.g., making Beam Infra tickets that add API keys
to unblock my work&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Beam Community:&lt;/strong> From my first ever contribution to Beam &lt;a href="https://github.com/apache/beam/issues/32840#issuecomment-2424055627">adding FlattenWith and Tee examples to the playground&lt;/a>,
I was welcomed with open arms and felt encouraged to make more contributions.
Also, for their valuable comments on my design documents on the dev mailing list
as well as the PRs&lt;/li>
&lt;li>&lt;strong>Google:&lt;/strong> I would like to genuinely thank Google for introducing me to open
source in &lt;a href="https://summerofcode.withgoogle.com/archive/2023/projects/u7Y9S6sc">GSoC 2023&lt;/a>
and giving me a second chance to interact with Apache Beam through GSoC 2025.
Without it, I probably wouldn&amp;rsquo;t be here writing this blog post, nor would I have
this fruitful experience&lt;/li>
&lt;/ul>
&lt;h2 id="whats-next">What&amp;rsquo;s Next?&lt;/h2>
&lt;p>I am now focusing on helping move the remaining artifacts in this project scope
from the in-progress state to the merging state. After this, I would love to
keep my contributions alive in Beam Python and Go SDK, to name a few. I would
also love to connect with you all on my
&lt;a href="https://www.linkedin.com/in/mohamedawnallah/">LinkedIn&lt;/a> and
&lt;a href="https://github.com/mohamedawnallah">GitHub&lt;/a>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://summerofcode.withgoogle.com/programs/2025/projects/X32yGjqz">Google Summer of Code Project Listing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1YOeK3jb94kSOUxucfqeZL0pkRI08dYljV_4v5SH5i5U/edit?usp=sharing">Original GSoC Proposal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/issues/35046">GSoC 2025 Tracking Issue&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Google Summer of Code 2025 - Beam YAML, Kafka and Iceberg User Accessibility</title><link>/blog/gsoc-25-yaml-user-accessibility/</link><pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate><guid>/blog/gsoc-25-yaml-user-accessibility/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The relatively new Beam YAML SDK was introduced in the spirit of making data processing easy,
but it has gained little adoption for complex ML tasks and hasn’t been widely used with
&lt;a href="https://beam.apache.org/documentation/io/managed-io/">Managed I/O&lt;/a> such as Kafka and Iceberg.
As part of Google Summer of Code 2025, new illustrative, production-ready pipeline examples
of ML use cases with Kafka and Iceberg data sources using the YAML SDK have been developed
to address this adoption gap.&lt;/p>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>The YAML SDK was introduced in Spring 2024 as Beam’s first no-code SDK. It follows a declarative approach
of defining a data processing pipeline using a YAML DSL, as opposed to other programming language specific SDKs.
At the time, it had few meaningful examples and documentation to go along with it. Key missing examples
were ML workflows and integration with the Kafka and Iceberg Managed I/O. Foundational work had already been done
to add support for ML capabilities as well as Kafka and Iceberg IO connectors in the YAML SDK, but there were no
end-to-end examples demonstrating their usage.&lt;/p>
&lt;p>Beam, as well as Kafka and Iceberg, are mainstream big data technologies but they also have a learning curve.
The overall theme of the project is to help democratize data processing for scientists and analysts who traditionally
don’t have a strong background in software engineering. They can now refer to these meaningful examples as the starting point,
helping them onboard faster and be more productive when authoring ML/data pipelines to their use cases with Beam and its YAML DSL.&lt;/p>
&lt;h2 id="contributions">Contributions&lt;/h2>
&lt;p>The data pipelines/workflows developed are production-ready: Kafka and Iceberg data sources are set up on GCP,
and the data used are raw public datasets. The pipelines are tested end-to-end on Google Cloud Dataflow and
are also unit tested to ensure correct transformation logic.&lt;/p>
&lt;p>Delivered pipelines/workflows, each with documentation as README.md, address 4 main ML use cases below:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Streaming Classification Inference&lt;/strong>: A streaming ML pipeline that demonstrates Beam YAML capability to perform
classification inference on a stream of incoming data from Kafka. The overall workflow also includes
DistilBERT model deployment and serving on Google Cloud Vertex AI where the pipeline can access for remote inferences.
The pipeline is applied to a sentiment analysis task on a stream of YouTube comments, preprocessing data and classifying
whether a comment is positive or negative. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/sentiment_analysis/streaming_sentiment_analysis.yaml">pipeline&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/sentiment_analysis">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Streaming Regression Inference&lt;/strong>: A streaming ML pipeline that demonstrates Beam YAML capability to perform
regression inference on a stream of incoming data from Kafka. The overall workflow also includes
custom model training, deployment and serving on Google Cloud Vertex AI where the pipeline can access for remote inferences.
The pipeline is applied to a regression task on a stream of taxi rides, preprocessing data and predicting the fare amount
for every ride. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/taxi_fare/streaming_taxifare_prediction.yaml">pipeline&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/taxi_fare">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Batch Anomaly Detection&lt;/strong>: A ML workflow that demonstrates ML-specific transformations
and reading from/writing to Iceberg IO. The workflow contains unsupervised model training and several pipelines that leverage
Iceberg for storing results, BigQuery for storing vector embeddings and MLTransform for computing embeddings to demonstrate
an end-to-end anomaly detection workflow on a dataset of system logs. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/log_analysis/batch_log_analysis.sh">workflow&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/log_analysis">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Feature Engineering &amp;amp; Model Evaluation&lt;/strong>: A ML workflow that demonstrates Beam YAML capability to do feature engineering
which is subsequently used for model evaluation, and its integration with Iceberg IO. The workflow contains model training
and several pipelines, showcasing an end-to-end Fraud Detection MLOps solution that generates features and evaluates models
to detect credit card transaction frauds. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/fraud_detection/fraud_detection_mlops_beam_yaml_sdk.ipynb">workflow&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/fraud_detection">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>The main challenge of the project was a lack of previous YAML pipeline examples and good documentation to rely on.
Unlike the Python or Java SDKs where there are already many notebooks and end-to-end examples demonstrating various use cases,
the examples for YAML SDK only involved simple transformations such as filter, group by, etc. More complex transforms like
&lt;code>MLTransform&lt;/code> and &lt;code>ReadFromIceberg&lt;/code> had no examples and requires configurations that didn&amp;rsquo;t have clear API reference at the time.
As a result, there were a lot of deep dives into the actual implementation of the PTransforms across YAML, Python and Java SDKs to
understand the error messages and how to correctly use the transforms.&lt;/p>
&lt;p>Another challenge was writing unit tests for the pipeline to ensure that the pipeline’s logic is correct.
It was a learning curve to understand how the existing test suite is set up and how it can be used to write unit tests for
the data pipelines. A lot of time was spent on properly writing mocks for the pipeline&amp;rsquo;s sources and sinks, as well as for the
transforms that require external services such as Vertex AI.&lt;/p>
&lt;h2 id="conclusion--personal-thoughts">Conclusion &amp;amp; Personal Thoughts&lt;/h2>
&lt;p>These production-ready pipelines demonstrate the potential of Beam YAML SDK to author complex ML workflows
that interact with Iceberg and Kafka. The examples are a nice addition to Beam, especially with Beam 3.0.0 milestones
coming up where low-code/no-code, ML capabilities and Managed I/O are focused on.&lt;/p>
&lt;p>I had an amazing time working with the big data technologies Beam, Iceberg, and Kafka as well as many Google Cloud services
(Dataflow, Vertex AI and Google Kubernetes Engine, to name a few). I’ve always wanted to work more in the ML space, and this
experience has been a great growth opportunity for me. Google Summer of Code this year has been selective, and the project&amp;rsquo;s success
would not have been possible without the support of my mentor, Chamikara Jayalath. It&amp;rsquo;s been a pleasure working closely
with him and the broader Beam community to contribute to this open-source project that has a meaningful impact on the
data engineering community.&lt;/p>
&lt;p>My advice for future Google Summer of Code participants is to first and foremost research and choose a project that aligns closely
with your interest. Most importantly, spend a lot of time making yourself visible and writing a good proposal when the program
is opened for applications. Being visible (e.g. by sharing your proposal, or generally any ideas and questions on the project&amp;rsquo;s
communication channel early on) makes it more likely for you to be selected; and a good proposal not only will make you even
more likely to be in the program, but also give you a lot of confidence when contributing to and completing the project.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://summerofcode.withgoogle.com/programs/2025/projects/f4kiDdus">Google Summer of Code Project Listing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1MSAVF6X9ggtVZbqz8YJGmMgkolR_dve0Lr930cByyac/edit?usp=sharing">Google Summer of Code Final Report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam 2.68.0</title><link>/blog/beam-2.68.0/</link><pubDate>Mon, 22 Sep 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.68.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.68.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2680-2025-09-??">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.68.0, check out the &lt;a href="https://github.com/apache/beam/milestone/36?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>[Python] Prism runner now enabled by default for most Python pipelines using the direct runner (&lt;a href="https://github.com/apache/beam/pull/34612">#34612&lt;/a>). This may break some tests, see &lt;a href="https://github.com/apache/beam/pull/34612">https://github.com/apache/beam/pull/34612&lt;/a> for details on how to handle issues.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Upgraded Iceberg dependency to 1.9.2 (&lt;a href="https://github.com/apache/beam/pull/35981">#35981&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>BigtableRead Connector for BeamYaml added with new Config Param (&lt;a href="https://github.com/apache/beam/pull/35696">#35696&lt;/a>)&lt;/li>
&lt;li>MongoDB Java driver upgraded from 3.12.11 to 5.5.0 with API refactoring and GridFS implementation updates (Java) (&lt;a href="https://github.com/apache/beam/pull/35946">#35946&lt;/a>).&lt;/li>
&lt;li>Introduced a dedicated module for JUnit-based testing support: &lt;code>sdks/java/testing/junit&lt;/code>, which provides &lt;code>TestPipelineExtension&lt;/code> for JUnit 5 while maintaining backward compatibility with existing JUnit 4 &lt;code>TestRule&lt;/code>-based tests (Java) (&lt;a href="https://github.com/apache/beam/issues/18733">#18733&lt;/a>, &lt;a href="https://github.com/apache/beam/pull/35688">#35688&lt;/a>).
&lt;ul>
&lt;li>To use JUnit 5 with Beam tests, add a test-scoped dependency on &lt;code>org.apache.beam:beam-sdks-java-testing-junit&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Google CloudSQL enrichment handler added (Python) (&lt;a href="https://github.com/apache/beam/pull/34398">#34398&lt;/a>).
Beam now supports data enrichment capabilities using SQL databases, with built-in support for:
&lt;ul>
&lt;li>Managed PostgreSQL, MySQL, and Microsoft SQL Server instances on CloudSQL&lt;/li>
&lt;li>Unmanaged SQL database instances not hosted on CloudSQL (e.g., self-hosted or on-premises databases)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Python] Added the &lt;code>ReactiveThrottler&lt;/code> and &lt;code>ThrottlingSignaler&lt;/code> classes to streamline throttling behavior in DoFns, expose throttling mechanisms for users (&lt;a href="https://github.com/apache/beam/pull/35984">#35984&lt;/a>)&lt;/li>
&lt;li>Added a pipeline option to specify the processing timeout for a single element by any PTransform (Java/Python/Go) (&lt;a href="https://github.com/apache/beam/issues/35174">#35174&lt;/a>).
&lt;ul>
&lt;li>When specified, the SDK harness automatically restarts if an element takes too long to process. Beam runner may then retry processing of the same work item.&lt;/li>
&lt;li>Use the &lt;code>--element_processing_timeout_minutes&lt;/code> option to reduce the chance of having stalled pipelines due to unexpected cases of slow processing, where slowness might not happen again if processing of the same element is retried.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>(Python) Adding GCP Spanner Change Stream support for Python (apache_beam.io.gcp.spanner) (&lt;a href="https://github.com/apache/beam/issues/24103">#24103&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Previously deprecated Beam ZetaSQL component has been removed (&lt;a href="https://github.com/apache/beam/issues/34423">#34423&lt;/a>).
ZetaSQL users could migrate to Calcite SQL with BigQuery dialect enabled.&lt;/li>
&lt;li>Upgraded Beam vendored Calcite to 1.40.0 for Beam SQL (&lt;a href="https://github.com/apache/beam/issues/35483">#35483&lt;/a>), which
improves support for BigQuery and other SQL dialects. Note: Minor behavior changes are observed such as output
significant digits related to casting.&lt;/li>
&lt;li>(Python) The deterministic fallback coder for complex types like NamedTuple, Enum, and dataclasses now uses cloudpickle instead of dill. If your pipeline is affected, you may see a warning like: &amp;ldquo;Using fallback deterministic coder for type X&amp;hellip;&amp;rdquo;. You can revert to the previous behavior by using the pipeline option &lt;code>--update_compatibility_version=2.67.0&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/35725">35725&lt;/a>). Report any pickling related issues to &lt;a href="https://github.com/apache/beam/issues/34903">#34903&lt;/a>&lt;/li>
&lt;li>(Python) Prism runner now enabled by default for most Python pipelines using the direct runner (&lt;a href="https://github.com/apache/beam/pull/34612">#34612&lt;/a>). This may break some tests, see &lt;a href="https://github.com/apache/beam/pull/34612">https://github.com/apache/beam/pull/34612&lt;/a> for details on how to handle issues.&lt;/li>
&lt;li>Dropped Java 8 support for &lt;a href="https://central.sonatype.com/artifact/org.apache.beam/beam-sdks-java-io-expansion-service">IO expansion-service&lt;/a>. Cross-language pipelines using this expansion service will need a Java11+ runtime (&lt;a href="https://github.com/apache/beam/pull/35981">#35981&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Python SDK native SpannerIO (apache_beam/io/gcp/experimental/spannerio) is deprecated. Use cross-language wrapper
(apache_beam/io/gcp/spanner) instead (Python) (&lt;a href="https://github.com/apache/beam/issues/35860">#35860&lt;/a>).&lt;/li>
&lt;li>Samza runner is deprecated and scheduled for removal in Beam 3.0 (&lt;a href="https://github.com/apache/beam/issues/35448">#35448&lt;/a>).&lt;/li>
&lt;li>Twister2 runner is deprecated and scheduled for removal in Beam 3.0 (&lt;a href="https://github.com/apache/beam/issues/35905">#35905&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>(Python) Fixed Java YAML provider fails on Windows (&lt;a href="https://github.com/apache/beam/issues/35617">#35617&lt;/a>).&lt;/li>
&lt;li>Fixed BigQueryIO creating temporary datasets in wrong project when temp_dataset is specified with a different project than the pipeline project. For some jobs, temporary datasets will now be created in the correct project (Python) (&lt;a href="https://github.com/apache/beam/issues/35813">#35813&lt;/a>).&lt;/li>
&lt;li>(Go) Fix duplicates due to reads after blind writes to Bag State (&lt;a href="https://github.com/apache/beam/issues/35869">#35869&lt;/a>).
&lt;ul>
&lt;li>Earlier Go SDK versions can avoid the issue by not reading in the same call after a blind write.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.68.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud, Andrew Crites, Ashok Devireddy, Chamikara Jayalath, Charles Nguyen, Danny McCormick, Davda James, Derrick Williams, Diego Hernandez, Dip Patel, Dustin Rhodes, Enrique Calderon, Hai Joey Tran, Jack McCluskey, Kenneth Knowles, Keshav, Khorbaladze A., LEEKYE, Lanny Boarts, Mattie Fu, Minbo Bae, Mohamed Awnallah, Naireen Hussain, Nathaniel Young, Radosław Stankiewicz, Razvan Culea, Robert Bradshaw, Robert Burke, Sam Whittle, Shehab, Shingo Furuyama, Shunping Huang, Steven van Rossum, Suvrat Acharya, Svetak Sundhar, Tarun Annapareddy, Tom Stepp, Valentyn Tymofieiev, Vitaly Terentyev, XQ Hu, Yi Hu, apanich, arnavarora2004, claudevdm, flpablo, kristynsmith, shreyakhajanchi&lt;/p></description></item><item><title>Blog: Google Summer of Code 25 - Improving Apache Beam's Infrastructure</title><link>/blog/gsoc-25-infra/</link><pubDate>Mon, 15 Sep 2025 00:00:00 -0600</pubDate><guid>/blog/gsoc-25-infra/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>I loved contributing to Apache Beam during Google Summer of Code 2025. I worked on improving the infrastructure of Apache Beam, which included enhancing the CI/CD pipelines, automating various tasks, and improving the overall developer experience.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Since I was in high school, I have been fascinated by computers, but when I discovered Open Source, I was amazed by the idea of people from all around the world collaborating to build software that anyone can use, just for the love of it. I started participating in open source communities, and I found it to be a great way to learn and grow as a developer.&lt;/p>
&lt;p>When I heard about Google Summer of Code, I saw it as an opportunity to take my open source contributions to the next level. The idea of working on a real-world project while being mentored by experienced developers sounded like an amazing opportunity. I heard about Apache Beam from another contributor and ex-GSoC participant, and I was immediately drawn to the project, specifically on the infrastructure side of things, as I have a strong interest in DevOps and automation.&lt;/p>
&lt;h2 id="the-challenge">The Challenge&lt;/h2>
&lt;p>When searching for a project, I was told that Apache Beam&amp;rsquo;s infrastructure had several areas that could be improved. I was excited because the ideas were focused on improving the developer experience, and creating tools that could benefit not only Beam&amp;rsquo;s developers but also the wider open source community.&lt;/p>
&lt;p>There were four main challenges:&lt;/p>
&lt;ol>
&lt;li>Automating the cleanup of unused cloud resources to reduce costs and improve resource management.&lt;/li>
&lt;li>Implementing a system for managing permissions through Git, allowing for better tracking and auditing of changes.&lt;/li>
&lt;li>Creating a tool for rotating service account keys to enhance security.&lt;/li>
&lt;li>Developing a security monitoring system to detect and respond to potential threats.&lt;/li>
&lt;/ol>
&lt;h2 id="the-solution">The Solution&lt;/h2>
&lt;p>I worked closely with my mentor to break down and define each challenge into manageable tasks, creating a plan for the summer. I started by taking a look at the current state of the infrastructure, after which I began working on each challenge one by one.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Automating the cleanup of unused cloud resources:&lt;/strong> We noticed that some resources in the GCP project, especially Pub/Sub topics created for testing, were often forgotten, leading to unnecessary costs. Since the infrastructure is primarily for testing and development, there&amp;rsquo;s no need to keep unused resources. I developed a Python script that identifies and removes stale Pub/Sub topics that have existed for too long. This tool is now scheduled to run periodically via a GitHub Actions workflow to keep the project tidy and cost-effective.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Implementing a system for managing permissions through Git:&lt;/strong> This was more challenging, as it required a good understanding of both GCP IAM and the existing workflow. After some investigation, I learned that the current process was mostly manual and error-prone. The task involved creating a more automated and reliable system. This was achieved by using Terraform to define the desired state of IAM roles and permissions in code, which allows for better tracking and auditing of changes. This also included some custom roles, but that is still a work in progress.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Creating a tool for rotating service account keys:&lt;/strong> Key rotation is a security practice that we don&amp;rsquo;t always follow, but it is essential to ensure that service account keys are not compromised. I noticed that GCP had some APIs that could help with this, but the rotation process itself was not automated. So I wrote a Python script that automates the rotation of GCP service account keys, enhancing the security of service account credentials.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Developing a security monitoring system:&lt;/strong> To keep track of incorrect usage and potential threats, I built a log analysis tool that monitors GCP audit logs for suspicious activity, collecting and parsing logs to identify potential security threats, delivering email alerts when something unusual is detected.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>As an extra, and after noticing that some of these tools and policies could be ignored by developers, we also came up with the idea of an enforcement module to ensure the usage of these new tools and policies. This module would be integrated into the CI/CD pipeline, checking for compliance with the new infrastructure policies and notifying developers of any violations.&lt;/p>
&lt;h2 id="the-impact">The Impact&lt;/h2>
&lt;p>The tools developed during this project will have an impact on the Apache Beam community and the wider open source community. The automation of resource cleanup will help reduce costs and improve resource management, while the permission management system will provide better tracking and auditing of changes. The service account key rotation tool will enhance security, and the security monitoring system will help detect and respond to potential threats.&lt;/p>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>This project has been an incredible learning experience for me. I have gained a better understanding of how GCP works, as well as how to use Terraform and GitHub Actions. I have also learned a lot about security best practices and how to implement them in a real-world project.&lt;/p>
&lt;p>I also learned a lot about working in an open source community, having direct communication with such experienced developers, and the importance of collaboration and communication in a distributed team. I am grateful for the opportunity to work on such an important project and to contribute to the Apache Beam community.&lt;/p>
&lt;p>Finally, a special thanks to my mentor, Pablo Estrada, for his guidance and support throughout the summer. I am grateful not only for his amazing technical skills but especially for his patience and encouragement on my journey contributing to open source.&lt;/p>
&lt;p>You can find my final report &lt;a href="https://gist.github.com/ksobrenat32/b028b8303393afbe73a8fc5e17daff90">here&lt;/a> if you want to take a look at the details of my work.&lt;/p>
&lt;h2 id="advice-for-future-participants">Advice for Future Participants&lt;/h2>
&lt;p>If you are considering participating in Google Summer of Code, my advice would be to choose an area you are passionate about; this will make any coding challenge easier to overcome. Also, don&amp;rsquo;t be afraid to ask questions and seek help from your mentors and the community. At the start, I made that mistake, and I learned that asking for help is a sign of strength, not weakness.&lt;/p>
&lt;p>Finally, make sure to manage your time effectively and stay organized (keeping a progress journal is a great idea). GSoC is a great opportunity to learn and grow as a developer, but it can also be time-consuming, so it&amp;rsquo;s important to stay focused and on track.&lt;/p></description></item><item><title>Blog: Apache Beam 2.67.0</title><link>/blog/beam-2.67.0/</link><pubDate>Tue, 12 Aug 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.67.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.67.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2670-2025-08-12">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.67.0, check out the &lt;a href="https://github.com/apache/beam/milestone/35?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Debezium IO upgraded to 3.1.1 requires Java 17 (Java) (&lt;a href="https://github.com/apache/beam/issues/34747">#34747&lt;/a>).&lt;/li>
&lt;li>Add support for streaming writes in IOBase (Python)&lt;/li>
&lt;li>Implement support for streaming writes in FileBasedSink (Python)&lt;/li>
&lt;li>Expose support for streaming writes in TextIO (Python)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Added support for Processing time Timer in the Spark Classic runner (&lt;a href="https://github.com/apache/beam/issues/33633">#33633&lt;/a>).&lt;/li>
&lt;li>Add pip-based install support for JupyterLab Sidepanel extension (&lt;a href="https://github.com/apache/beam/issues/35397">#35397&lt;/a>).&lt;/li>
&lt;li>[IcebergIO] Create tables with a specified table properties (&lt;a href="https://github.com/apache/beam/pull/35496">#35496&lt;/a>)&lt;/li>
&lt;li>Add support for comma-separated options in Python SDK (Python) (&lt;a href="https://github.com/apache/beam/pull/35580">#35580&lt;/a>).
Python SDK now supports comma-separated values for experiments and dataflow_service_options,
matching Java SDK behavior while maintaining backward compatibility.&lt;/li>
&lt;li>Milvus enrichment handler added (Python) (&lt;a href="https://github.com/apache/beam/pull/35216">#35216&lt;/a>).
Beam now supports Milvus enrichment handler capabilities for vector, keyword,
and hybrid search operations.&lt;/li>
&lt;li>[Beam SQL] Add support for DATABASEs, with an implementation for Iceberg (&lt;a href="https://github.com/apache/beam/issues/35637">#35637&lt;/a>)&lt;/li>
&lt;li>Respect BatchSize and MaxBufferingDuration when using &lt;code>JdbcIO.WriteWithResults&lt;/code>. Previously, these settings were ignored (&lt;a href="https://github.com/apache/beam/pull/35669">#35669&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Go: The pubsubio.Read transform now accepts ReadOptions as a value type instead of a pointer, and requires exactly one of Topic or Subscription to be set (they are mutually exclusive). Additionally, the ReadOptions struct now includes a Topic field for specifying the topic directly, replacing the previous topic parameter in the Read function signature (&lt;a href="https://github.com/apache/beam/pull/35369">#35369&lt;/a>).&lt;/li>
&lt;li>SQL: The &lt;code>ParquetTable&lt;/code> external table provider has changed its handling of the &lt;code>LOCATION&lt;/code> property. To read from a directory, the path must now end with a trailing slash (e.g., &lt;code>LOCATION '/path/to/data/'&lt;/code>). Previously, a trailing slash was not required. This change was made to enable support for glob patterns and single-file paths (&lt;a href="https://github.com/apache/beam/pull/35582">#35582&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>[YAML] Fixed handling of missing optional fields in JSON parsing (&lt;a href="https://github.com/apache/beam/issues/35179">#35179&lt;/a>).&lt;/li>
&lt;li>[Python] Fix WriteToBigQuery transform using CopyJob does not work with WRITE_TRUNCATE write disposition (&lt;a href="https://github.com/apache/beam/issues/34247">#34247&lt;/a>)&lt;/li>
&lt;li>[Python] Fixed dicomio tags mismatch in integration tests (&lt;a href="https://github.com/apache/beam/issues/30760">#30760&lt;/a>).&lt;/li>
&lt;li>[Java] Fixed spammy logging issues that affected versions 2.64.0 to 2.66.0.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>(&lt;a href="https://github.com/apache/beam/issues/35666">#35666&lt;/a>). YAML Flatten incorrectly drops fields when input PCollections&amp;rsquo; schema are different. This issue exists for all versions since 2.52.0.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.67.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aditya Shukla, Ahmed Abualsaud, Arun Pandian, Boris Li, Chamikara Jayalath, Charles Nguyen, Chenzo, Danny McCormick, David Adeniji, Derrick Williams, Dmytro Tsyliuryk, Dustin Rhodes, Enrique Calderon, Gottipati Gautam, Hai Joey Tran, Hunor Portik, Jack McCluskey, Kenneth Knowles, Khorbaladze A., Marcio Sugar, Minh Son Nguyen, Mohamed Awnallah, Nathaniel Young, Nhon Dinh, Quentin Sommer, Rafael Raposo, Rakesh Kumar, Razvan Culea, Reuven Lax, Robert Bradshaw, Sam Whittle, Shunping Huang, Steven van Rossum, Talat UYARER, Tanu Sharma, Tarun Annapareddy, Tobi Kaymak, Tobias Kaymak, Valentyn Tymofieiev, Veronica Wasson, Vitaly Terentyev, XQ Hu, Yi Hu, akashorabek, arnavarora2004, changliiu, claudevdm, fozzie15, mvhensbergen, twosom&lt;/p></description></item><item><title>Blog: Our Experience at Beam College 2025: 1st Place Hackathon Winners</title><link>/blog/beam-summit-2025-hackathon-pcollectors-blog/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>/blog/beam-summit-2025-hackathon-pcollectors-blog/</guid><description>
&lt;!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements. See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="introduction-the-beam-of-an-idea">Introduction: The Beam of an Idea&lt;/h2>
&lt;p>In the world of machine learning for healthcare, preprocessing large pathology image datasets at scale remains a bottleneck. Whole Slide Images (WSIs) in medical imaging can reach massive sizes. Traditional Python tools (PIL, etc.) fail under memory pressure, especially when handling thousands of such high-resolution images. This becomes a bottleneck for ML modeling tasks using standard tools.&lt;/p>
&lt;p>Having previously worked on image processing for object detection in machine learning, we also understood how crucial it is to preprocess and structure image data correctly for downstream tasks. These challenges are non-trivial and even more critical in healthcare, making it a natural and high-impact use case for scalable data processing frameworks like Apache Beam.&lt;/p>
&lt;p>So, in the &lt;a href="https://beamcollege.dev/hackathon/">Beam Summit 2025 Hackathon&lt;/a>, we joined as team &amp;ldquo;PCollectors&amp;rdquo; with the goal to leverage Beam to process large image data and convert it to a format suitable for downstream ML tasks. We were amazed to know that we secured 1st place with the implemented solution!&lt;/p>
&lt;h2 id="the-project-scalable-wsi-preprocessing-beam-pipeline">The Project: Scalable WSI Preprocessing Beam Pipeline&lt;/h2>
&lt;p>&lt;a href="https://github.com/adityashukla8/medical_image_processing_beam">GitHub Repo&lt;/a>&lt;/p>
&lt;h3 id="the-goal">The Goal&lt;/h3>
&lt;p>The primary objective of the pipeline was to process patient data (CSV) &amp;amp; WSIs, extract embeddings, combine the metadata, and output the final dataset in TFRecord format, ready for large-scale ML training.&lt;/p>
&lt;h3 id="solution-overview">Solution Overview&lt;/h3>
&lt;p>Our pipeline processes:&lt;/p>
&lt;ul>
&lt;li>Patient metadata (CSV)&lt;/li>
&lt;li>WSI files (.tif)&lt;/li>
&lt;li>Split the images into “tiles”&lt;/li>
&lt;li>Extract filtered image tiles based on the background threshold&lt;/li>
&lt;li>Generate max &amp;amp; avg embeddings per patient using EfficientNet&lt;/li>
&lt;li>Merge metadata + embeddings into TFRecords&lt;/li>
&lt;/ul>
&lt;p>All in a scalable, memory-efficient, cloud-native pipeline using Apache Beam and Dataflow.&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>Source: Mayo Clinic STRIP AI Dataset (Kaggle)
Metadata: Each row = { image_id, center_id, patient_id, image_num, label }
Multiple images per patient
Labels exist only at the patient level
Images:
High-res .tif pathology slides&lt;/p>
&lt;h3 id="tech-stack">Tech Stack&lt;/h3>
&lt;ul>
&lt;li>Apache Beam: Orchestration engine&lt;/li>
&lt;li>Google Cloud Dataflow: Scalable runner&lt;/li>
&lt;li>Google Cloud Storage: Input TIFFs + output TFRecords&lt;/li>
&lt;li>TensorFlow: For embedding generation (EfficientNet) and TFRecord serialization&lt;/li>
&lt;/ul>
&lt;h2 id="the-hackathon-journey">The Hackathon Journey&lt;/h2>
&lt;p>Participating in the hackathon introduced us to multiple new things and allowed us to learn and implement simultaneously. Through the hackathon weekend, we:&lt;/p>
&lt;ul>
&lt;li>Designed the end-to-end pipeline&lt;/li>
&lt;li>Integrated pyvips + openslide for efficient image loading&lt;/li>
&lt;li>Used Beam&amp;rsquo;s RunInference API with TensorFlow&lt;/li>
&lt;li>Tiled and filtered images&lt;/li>
&lt;li>Wrote patient-level embeddings to TFRecords&lt;/li>
&lt;/ul>
&lt;h2 id="what-we-learnt">What we Learnt&lt;/h2>
&lt;p>Apache Beam is really powerful for parallel and cloud-native ML preprocessing.
Dataflow is the go-to tool when processing large data, like medical images&lt;/p>
&lt;h2 id="whats-next-for-the-project">What’s Next for The Project&lt;/h2>
&lt;p>Looking ahead, the pipeline can be extended beyond fixed-size tiling by incorporating image segmentation techniques to generate more meaningful patches based on tissue regions. This approach can improve ML model performance by focusing only on relevant areas. Moreover, the same preprocessing framework can be adapted for video data, where frames can be treated as time-indexed image slices, effectively enabling temporal modeling for time-series tasks such as motion analysis or progression tracking. Finally, we plan to adapt this pipeline to multiple downstream use cases for AI in healthcare by combining histology images with genomic data, clinical notes, or radiology scans, paving the way for more comprehensive and context-aware models in biomedical machine learning.&lt;/p>
&lt;p>&lt;strong>Project Submission Demo&lt;/strong>: &lt;a href="https://drive.google.com/file/d/1Os5SvgqHiqfMkoCWOuaVvEPXsnhqXlLx/view?usp=sharing">Beam Demo - PCollectors.mp4&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We are ML Engineers, working at &lt;a href="www.intuitive.cloud">Intuitive.Cloud&lt;/a>, where we play around with large-scale data to build scalable, efficient, dynamic data processing pipelines that prepare it for downstream ML tasks, with Apache Beam and Google Cloud DataFlow being the central pieces.&lt;/p>
&lt;p>Participating in the hackathon was a great learning opportunity, huge thanks to the organizers, mentors, and the Apache Beam community!&lt;/p>
&lt;p>- &lt;a href="https://www.linkedin.com/in/adityashukla8/">Aditya Shukla&lt;/a> &amp;amp; &lt;a href="https://in.linkedin.com/in/darshan-kanade-0797851b3">Darshan Kanade&lt;/a>&lt;/p></description></item><item><title>Blog: Apache Beam 2.66.0</title><link>/blog/beam-2.66.0/</link><pubDate>Tue, 01 Jul 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.66.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.66.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2660-2025-07-01">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.66.0, check out the &lt;a href="https://github.com/apache/beam/milestone/30?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="beam-300-development-highlights">Beam 3.0.0 Development Highlights&lt;/h2>
&lt;ul>
&lt;li>[Java] Java 8 support is now deprecated. It is still supported until Beam 3.
From now, pipeline submitted by Java 8 client uses Java 11 SDK container for
remote pipeline execution (&lt;a href="https://github.com/apache/beam/pull/35064">35064&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>[Python] Several quality-of-life improvements to the vLLM model handler. If you use Beam RunInference with vLLM model handlers, we strongly recommend updating past this release.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>[IcebergIO] Now available with Beam SQL! (&lt;a href="https://github.com/apache/beam/pull/34799">#34799&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Support reading with column pruning (&lt;a href="https://github.com/apache/beam/pull/34856">#34856&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Support reading with pushdown filtering (&lt;a href="https://github.com/apache/beam/pull/34827">#34827&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Create tables with a specified partition spec (&lt;a href="https://github.com/apache/beam/pull/34966">#34966&lt;/a>, &lt;a href="https://github.com/apache/beam/pull/35268">#35268&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Dynamically create namespaces if needed (&lt;a href="https://github.com/apache/beam/pull/35228">#35228&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>[Beam SQL] Introducing Beam Catalogs (&lt;a href="https://github.com/apache/beam/pull/35223">#35223&lt;/a>)&lt;/li>
&lt;li>Adding Google Storage Requests Pays feature (Golang)(&lt;a href="https://github.com/apache/beam/issues/30747">#30747&lt;/a>).&lt;/li>
&lt;li>[Python] Prism runner now auto-enabled for some Python pipelines using the direct runner (&lt;a href="https://github.com/apache/beam/pull/34921">#34921&lt;/a>).&lt;/li>
&lt;li>[YAML] WriteToTFRecord and ReadFromTFRecord Beam YAML support&lt;/li>
&lt;li>Python: Added JupyterLab 4.x extension compatibility for enhanced notebook integration (&lt;a href="https://github.com/apache/beam/pull/34495">#34495&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Yapf version upgraded to 0.43.0 for formatting (Python) (&lt;a href="https://github.com/apache/beam/pull/34801/">#34801&lt;/a>).&lt;/li>
&lt;li>Python: Added JupyterLab 4.x extension compatibility for enhanced notebook integration (&lt;a href="https://github.com/apache/beam/pull/34495">#34495&lt;/a>).&lt;/li>
&lt;li>Python: Argument abbreviation is no longer enabled within Beam. If you previously abbreviated arguments (e.g. &lt;code>--r&lt;/code> for &lt;code>--runner&lt;/code>), you will now need to specify the whole argument (&lt;a href="https://github.com/apache/beam/pull/34934">#34934&lt;/a>).&lt;/li>
&lt;li>Java: Users of ReadFromKafkaViaSDF transform might encounter pipeline graph compatibility issues when updating the pipeline. To mitigate, set the &lt;code>updateCompatibilityVersion&lt;/code> option to the SDK version used for the original pipeline, example &lt;code>--updateCompatabilityVersion=2.64.0&lt;/code>&lt;/li>
&lt;li>Python: Updated &lt;code>AlloyDBVectorWriterConfig&lt;/code> API to align with new &lt;code>PostgresVectorWriter&lt;/code> transform. Heres a quick guide to update your code: (&lt;a href="https://github.com/apache/beam/issues/35225">#35225&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>(Java) Fixed CassandraIO ReadAll does not let a pipeline handle or retry exceptions (&lt;a href="https://github.com/apache/beam/pull/34191">#34191&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed vLLM model handlers breaking Beam logging. (&lt;a href="https://github.com/apache/beam/pull/35053">#35053&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed vLLM connection leaks that caused a throughput bottleneck and underutilization of GPU (&lt;a href="https://github.com/apache/beam/pull/35053">#35053&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed vLLM server recovery mechanism in the event of a process termination (&lt;a href="https://github.com/apache/beam/pull/35234">#35234&lt;/a>).&lt;/li>
&lt;li>(Python) Fixed cloudpickle overwriting class states every time loading a same object of dynamic class (&lt;a href="https://github.com/apache/beam/issues/35062">#35062&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed pip install apache-beam[interactive] causes crash on google colab (&lt;a href="https://github.com/apache/beam/pull/35148">#35148&lt;/a>).&lt;/li>
&lt;li>[IcebergIO] Fixed Beam &amp;lt;-&amp;gt; Iceberg conversion logic for arrays of structs and maps of structs (&lt;a href="https://github.com/apache/beam/pull/35230">#35230&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;p>N/A&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.66.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aditya Yadav, Adrian Stoll, Ahmed Abualsaud, Bhargavkonidena, Chamikara Jayalath, Charles Nguyen, Chenzo, Damon, Danny McCormick, Derrick Williams, Enrique Calderon, Hai Joey Tran, Jack McCluskey, Kenneth Knowles, Leonardo Cesar Borges, Michael Gruschke, Minbo Bae, Minh Son Nguyen, Niel Markwick, Radosław Stankiewicz, Rakesh Kumar, Robert Bradshaw, S. Veyrié, Sam Whittle, Shubham Jaiswal, Shunping Huang, Steven van Rossum, Tanu Sharma, Vardhan Thigle, Vitaly Terentyev, XQ Hu, Yi Hu, akashorabek, atask-g, atognolag, bullet03, changliiu, claudevdm, fozzie15, ikarapanca, kristynsmith, Pablo Rodriguez Defino, tvalentyn, twosom, wollowizard&lt;/p></description></item><item><title>Blog: My Experience at Beam College 2025: 3rd Place Hackathon Winner</title><link>/blog/beam-college-2025-anomaflow/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>/blog/beam-college-2025-anomaflow/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="introduction-the-spark-of-an-idea">Introduction: The Spark of an Idea&lt;/h2>
&lt;p>In 2025, I had the opportunity to participate in the &lt;a href="https://beamcollege.dev/hackathon/">Beam College Hackathon&lt;/a>, a fantastic event that brings together students and professionals to explore the power of Apache Beam.&lt;/p>
&lt;p>For my project, I built &lt;strong>&lt;a href="https://github.com/msugar/anomaflow">Anomaflow&lt;/a>&lt;/strong>, an anomaly detection pipeline using &lt;strong>Apache Beam&lt;/strong> and &lt;strong>Google Cloud Dataflow&lt;/strong>. It was my first public hackathon, and the experience was both rewarding and creatively energizing. I’m proud to share that Anomaflow earned &lt;strong>3rd place&lt;/strong> in the competition.&lt;/p>
&lt;h2 id="the-project-building-anomaflow">The Project: Building Anomaflow&lt;/h2>
&lt;h3 id="goal">Goal&lt;/h3>
&lt;p>The primary objective of Anomaflow was to process &lt;strong>host telemetry data&lt;/strong> from systems monitored by OpenTelemetry agents. The long-term goal is to detect anomalies in near real time, which has important applications in cybersecurity.&lt;/p>
&lt;h3 id="tech-stack">Tech Stack&lt;/h3>
&lt;p>Here’s a breakdown of the technologies used:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Apache Beam (Python SDK)&lt;/strong> for pipeline development&lt;/li>
&lt;li>&lt;strong>Bindplane Collector &amp;amp; Server&lt;/strong> for OpenTelemetry data collection and configuration&lt;/li>
&lt;li>&lt;strong>Google Compute Engine (GCE)&lt;/strong> instancess for hosting the Bindplane Server and Collector&lt;/li>
&lt;li>&lt;strong>Google Cloud Dataflow&lt;/strong> for running the Beam pipeline at scale&lt;/li>
&lt;li>&lt;strong>Google Cloud Storage (GCS)&lt;/strong> as the pipeline’s source and sink during the hackathon&lt;/li>
&lt;li>&lt;strong>Terraform&lt;/strong> to provision GCP infrastructure&lt;/li>
&lt;li>&lt;strong>Docker&lt;/strong> for packaging and deployment&lt;/li>
&lt;/ul>
&lt;h2 id="the-hackathon-journey-from-streaming-vision-to-batch-reality">The Hackathon Journey: From Streaming Vision to Batch Reality&lt;/h2>
&lt;h3 id="the-initial-vision">The Initial Vision&lt;/h3>
&lt;p>The original plan was to create a fully streaming pipeline: the &lt;strong>Bindplane Collector&lt;/strong> running on &lt;strong>GCE&lt;/strong> would upload telemetry files to &lt;strong>GCS&lt;/strong>, which would trigger &lt;strong>notifications via Pub/Sub&lt;/strong>. These notifications would then initiate processing in a &lt;strong>Beam pipeline&lt;/strong>, with enriched results written to &lt;strong>BigQuery&lt;/strong> for analysis and visualization.&lt;/p>
&lt;h3 id="the-pivot">The Pivot&lt;/h3>
&lt;p>However, working solo during a time-limited hackathon meant I had to be pragmatic. I decided to implement a &lt;strong>batch pipeline&lt;/strong> instead, reading from and writing to &lt;strong>GCS buckets&lt;/strong>. This allowed me to deliver a functional MVP while preserving a foundation that can evolve toward the original streaming vision.&lt;/p>
&lt;h3 id="key-learnings">Key Learnings&lt;/h3>
&lt;p>Although I already had some real-world experience with Apache Beam, the hackathon gave me the freedom to explore new patterns and tools in a low-risk environment. It was refreshing to iterate rapidly, test ideas, and push beyond my daily work scope.&lt;/p>
&lt;h2 id="whats-next-for-anomaflow">What’s Next for Anomaflow?&lt;/h2>
&lt;p>Anomaflow is just getting started.&lt;/p>
&lt;p>I plan to evolve the pipeline into a true &lt;strong>streaming system&lt;/strong> using &lt;strong>Pub/Sub&lt;/strong> and &lt;strong>BigQuery&lt;/strong>. I also want to explore &lt;strong>sliding windows&lt;/strong>, &lt;strong>custom anomaly detection models&lt;/strong>, and &lt;strong>alerting mechanisms&lt;/strong>. With its modular design and strong foundation in Beam, Anomaflow will serve as a base for several future cybersecurity analytics tools I have in mind.&lt;/p>
&lt;p>&lt;strong>Watch the demo&lt;/strong>: &lt;a href="https://www.youtube.com/watch?v=dpbOm5ekOTc">https://www.youtube.com/watch?v=dpbOm5ekOTc&lt;/a>&lt;/p>
&lt;h2 id="tips-for-future-participants">Tips for Future Participants&lt;/h2>
&lt;p>If you’re considering joining Beam College next year:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Use the mentors!&lt;/strong> The Beam community professionals volunteering their time are a valuable resource. Ask questions, get feedback.&lt;/li>
&lt;li>&lt;strong>Check out the &lt;a href="https://beam.apache.org/get-started/resources/learning-resources/">Beam learning resources&lt;/a>&lt;/strong>. They’re super helpful, especially for getting started with the Beam model and runners.&lt;/li>
&lt;/ul>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>I’m currently a &lt;strong>Software Architect and Data Engineer at TELUS Security&lt;/strong>, where I work on the &lt;strong>Cybersecurity Analytics and Software Engineering&lt;/strong> team. We design data pipelines to detect and respond to threats at scale.&lt;/p>
&lt;p>Participating in Beam College was a great way to stretch my skills, meet passionate Beam users, and contribute to a vibrant open source community. I’m excited to see what others will build in future editions!&lt;/p>
&lt;p>– &lt;a href="https://www.linkedin.com/in/marcio-sugar/">Marcio Sugar&lt;/a>&lt;/p></description></item><item><title>Blog: Apache Beam 2.65.0</title><link>/blog/beam-2.65.0/</link><pubDate>Mon, 12 May 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.65.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.65.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2650-2025-05-12">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.65.0, check out the &lt;a href="https://github.com/apache/beam/milestone/29?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Upgraded GoogleAdsAPI to v19 for GoogleAdsIO (Java) (&lt;a href="https://github.com/apache/beam/pull/34497">#34497&lt;/a>). Changed PTransform method from version-specified (&lt;code>v17()&lt;/code>) to &lt;code>current()&lt;/code> for better backward compatibility in the future.&lt;/li>
&lt;li>Added support for writing to Pubsub with ordering keys (Java) (&lt;a href="https://github.com/apache/beam/issues/21162">#21162&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Added support for streaming side-inputs in the Spark Classic runner (&lt;a href="https://github.com/apache/beam/issues/18136">#18136&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>[Python] Cloudpickle is set as the default &lt;code>pickle_library&lt;/code>, where previously
dill was the default in &lt;a href="https://github.com/apache/beam/pull/34695">#34695&lt;/a>.
For known issues, reporting new issues, and understanding cloudpickle
behavior refer to &lt;a href="https://github.com/apache/beam/issues/34903">#34903&lt;/a>.&lt;/li>
&lt;li>[Python] Reshuffle now preserves PaneInfo, where previously PaneInfo was lost
after reshuffle. To opt out of this change, set the
update_compatibility_version to a previous Beam version e.g. &amp;ldquo;2.64.0&amp;rdquo;.
(&lt;a href="https://github.com/apache/beam/pull/34348">#34348&lt;/a>).&lt;/li>
&lt;li>[Python] PaneInfo is encoded by PaneInfoCoder, where previously PaneInfo was
encoded with FastPrimitivesCoder falling back to PickleCoder. This only
affects cases where PaneInfo is directly stored as an element.
(&lt;a href="https://github.com/apache/beam/pull/34824">#34824&lt;/a>).&lt;/li>
&lt;li>[Python] BigQueryFileLoads now adds a Reshuffle before triggering load jobs.
This fixes a bug where there can be data loss in a streaming pipeline if there
is a pending load job during autoscaling. To opt out of this change, set the
update_compatibility_version to a previous Beam version e.g. &amp;ldquo;2.64.0&amp;rdquo;.
(&lt;a href="https://github.com/apache/beam/pull/34657">#34657&lt;/a>)&lt;/li>
&lt;li>[YAML] Kafka source and sink will be automatically replaced with compatible managed transforms.
For older Beam versions, streaming update compatiblity can be maintained by specifying the pipeline
option &lt;code>update_compatibility_version&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/34767">#34767&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Beam ZetaSQL is deprecated and will be removed no earlier than Beam 2.68.0 (&lt;a href="https://github.com/apache/beam/issues/34423">#34423&lt;/a>).
Users are recommended to switch to &lt;a href="https://beam.apache.org/documentation/dsls/sql/calcite/overview/">Calcite SQL&lt;/a> dialect.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Fixed read Beam rows from cross-lang transform (for example, ReadFromJdbc) involving negative 32-bit integers incorrectly decoded to large integers (&lt;a href="https://github.com/apache/beam/issues/34089">#34089&lt;/a>)&lt;/li>
&lt;li>(Java) Fixed SDF-based KafkaIO (ReadFromKafkaViaSDF) to properly handle custom deserializers that extend Deserializer&lt;Row> interface(&lt;a href="https://github.com/apache/beam/pull/34505">#34505&lt;/a>)&lt;/li>
&lt;li>[Python] &lt;code>TypedDict&lt;/code> typehints are now compatible with &lt;code>Mapping&lt;/code> and &lt;code>Dict&lt;/code> type annotations.&lt;/li>
&lt;/ul>
&lt;h3 id="security-fixes">Security Fixes&lt;/h3>
&lt;ul>
&lt;li>Fixed &lt;a href="https://www.cve.org/CVERecord?id=CVE-2025-30065">CVE-2025-30065&lt;/a> (Java) (&lt;a href="https://github.com/apache/beam/pull/34573">#34573&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;p>N/A&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.65.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Trelstad, Adrian Stoll, Ahmed Abualsaud, akashorabek, Arun Pandian, Bentsi Leviav, Bryan Dang, Celeste Zeng, Chamikara Jayalath, claudevdm, Danny McCormick, Derrick Williams, Ozzie Fernandez, Gabija Balvociute, Gayatri Kate, illoise, Jack McCluskey, Jan Lukavský, Jinho Lee, Justin Bandoro, Kenneth Knowles, XQ Hu, Luke Tsekouras, Martin Trieu, Matthew Suozzo, Naireen Hussain, Niel Markwick, Radosław Stankiewicz, Razvan Culea, Robert Bradshaw, Robert Burke, RuiLong J., Sam Whittle, Sarthak, Shubham Jaiswal, Shunping Huang, Steven van Rossum, Suvrat Acharya, &lt;a href="mailto:sveyrie@luminatedata.com">sveyrie@luminatedata.com&lt;/a>, Talat Uyarer, TanuSharma2511, Tobias Kaymak, Tom Stepp, Valentyn Tymofieiev, twosom, Vitaly Terentyev, wollowizard, Yi Hu, Yifan Ye, Zilin Du&lt;/p></description></item><item><title>Blog: Apache Beam 2.64.0</title><link>/blog/beam-2.64.0/</link><pubDate>Mon, 31 Mar 2025 10:30:00 -0500</pubDate><guid>/blog/beam-2.64.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.64.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/%7B$DOWNLOAD_ANCHOR%7D">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.64.0, check out the &lt;a href="https://github.com/apache/beam/milestone/28">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Managed API for &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/managed/Managed.html">Java&lt;/a> and &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.managed.html#module-apache_beam.transforms.managed">Python&lt;/a> supports &lt;a href="https://beam.apache.org/documentation/io/connectors/">key I/O connectors&lt;/a> Iceberg, Kafka, and BigQuery.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>[Java] Use API compatible with both com.google.cloud.bigdataoss:util 2.x and 3.x in BatchLoads (&lt;a href="https://github.com/apache/beam/pull/34105">#34105&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Added new CDC source for batch and streaming, available as &lt;code>Managed.ICEBERG_CDC&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/33504">#33504&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Address edge case where bundle retry following a successful data commit results in data duplication (&lt;a href="https://github.com/apache/beam/pull/34264">#34264&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>[Python] Support custom coders in Reshuffle (&lt;a href="https://github.com/apache/beam/issues/29908">#29908&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/33356">#33356&lt;/a>).&lt;/li>
&lt;li>[Java] Upgrade SLF4J to 2.0.16. Update default Spark version to 3.5.0. (&lt;a href="https://github.com/apache/beam/pull/33574">#33574&lt;/a>)&lt;/li>
&lt;li>[Java] Support for &lt;code>--add-modules&lt;/code> JVM option is added through a new pipeline option &lt;code>JdkAddRootModules&lt;/code>. This allows extending the module graph with optional modules such as SDK incubator modules. Sample usage: &lt;code>&amp;lt;pipeline invocation&amp;gt; --jdkAddRootModules=jdk.incubator.vector&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/30281">#30281&lt;/a>).&lt;/li>
&lt;li>Managed API for &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/managed/Managed.html">Java&lt;/a> and &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.managed.html#module-apache_beam.transforms.managed">Python&lt;/a> supports &lt;a href="https://beam.apache.org/documentation/io/connectors/">key I/O connectors&lt;/a> Iceberg, Kafka, and BigQuery.&lt;/li>
&lt;li>Prism now supports event time triggers for most common cases. (&lt;a href="https://github.com/apache/beam/issues/31438">#31438&lt;/a>)
&lt;ul>
&lt;li>Prism does not yet support triggered side inputs, or triggers on merging windows (such as session windows).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>[Python] Reshuffle now correctly respects user-specified type hints, fixing a previous bug where it might use FastPrimitivesCoder wrongly. This change could break pipelines with incorrect type hints in Reshuffle. If you have issues after upgrading, temporarily set update_compatibility_version to a previous Beam version to use the old behavior. The recommended solution is to fix the type hints in your code. (&lt;a href="https://github.com/apache/beam/pull/33932">#33932&lt;/a>)&lt;/li>
&lt;li>[Java] SparkReceiver 2 has been moved to SparkReceiver 3 that supports Spark 3.x. (&lt;a href="https://github.com/apache/beam/pull/33574">#33574&lt;/a>)&lt;/li>
&lt;li>[Python] Correct parsing of &lt;code>collections.abc.Sequence&lt;/code> type hints was added, which can lead to pipelines failing type hint checks that were previously passing erroneously. These issues will be most commonly seen trying to consume a PCollection with a &lt;code>Sequence&lt;/code> type hint after a GroupByKey or a CoGroupByKey. (&lt;a href="https://github.com/apache/beam/pull/33999">#33999&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Python) Fixed occasional pipeline stuckness that was affecting Python 3.11 users (&lt;a href="https://github.com/apache/beam/issues/33966">#33966&lt;/a>).&lt;/li>
&lt;li>(Java) Fixed TIME field encodings for BigQuery Storage API writes on GenericRecords (&lt;a href="https://github.com/apache/beam/pull/34059">#34059&lt;/a>).&lt;/li>
&lt;li>(Java) Fixed a race condition in JdbcIO which could cause hangs trying to acquire a connection (&lt;a href="https://github.com/apache/beam/pull/34058">#34058&lt;/a>).&lt;/li>
&lt;li>(Java) Fix BigQuery Storage Write compatibility with Avro 1.8 (&lt;a href="https://github.com/apache/beam/pull/34281">#34281&lt;/a>).&lt;/li>
&lt;li>Fixed checkpoint recovery and streaming behavior in Spark Classic and Portable runner&amp;rsquo;s Flatten transform by replacing queueStream with SingleEmitInputDStream (&lt;a href="https://github.com/apache/beam/pull/34080">#34080&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/18144">#18144&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/20426">#20426&lt;/a>)&lt;/li>
&lt;li>(Java) Fixed Read caching of UnboundedReader objects to effectively cache across multiple DoFns and avoid checkpointing unstarted reader. &lt;a href="https://github.com/apache/beam/pull/34146">#34146&lt;/a> &lt;a href="https://github.com/apache/beam/pull/33901">#33901&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>(Java) Current version of protobuf has a &lt;a href="https://github.com/protocolbuffers/protobuf/issues/20599">bug&lt;/a> leading to incompatibilities with clients using older versions of Protobuf (&lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/issues/2191">example issue&lt;/a>). This issue has been seen in SpannerIO in particular. Tracked in &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/issues/34452">#34452&lt;/a>.&lt;/li>
&lt;li>(Java) When constructing &lt;code>SpannerConfig&lt;/code> for &lt;code>SpannerIO&lt;/code>, calling &lt;code>withHost&lt;/code> with a null or empty host will now result in a Null Pointer Exception (&lt;code>java.lang.NullPointerException: Cannot invoke &amp;quot;java.lang.CharSequence.length()&amp;quot; because &amp;quot;this.text&amp;quot; is null&lt;/code>). See &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/issues/34489">https://github.com/GoogleCloudPlatform/DataflowTemplates/issues/34489&lt;/a> for context.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.64.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Bentsi Leviav&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Nguyen&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>claudevdm&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>Derrick Williams&lt;/p>
&lt;p>fozzie15&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>jrmccluskey&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>Luv Agarwal&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Matar&lt;/p>
&lt;p>Matthew Suozzo&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Mohamed Awnallah&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Pablo Rodriguez Defino&lt;/p>
&lt;p>Radosław Stankiewicz&lt;/p>
&lt;p>Rakesh Kumar&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rohit&lt;/p>
&lt;p>Rohit Sinha&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Saumil Patel&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>So-shi Nakachi&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Suvrat Acharya&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>synenka&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>twosom&lt;/p>
&lt;p>utkarshparekh&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zilin Du&lt;/p></description></item><item><title>Blog: Apache Beam 2.63.0</title><link>/blog/beam-2.63.0/</link><pubDate>Tue, 18 Feb 2025 10:30:00 -0500</pubDate><guid>/blog/beam-2.63.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.63.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/%7B$DOWNLOAD_ANCHOR%7D">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.63.0, check out the &lt;a href="https://github.com/apache/beam/milestone/27">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support gcs-connector 3.x+ in GcsUtil (&lt;a href="https://github.com/apache/beam/pull/33368">#33368&lt;/a>)&lt;/li>
&lt;li>Support for X source added (Java/Python) (&lt;a href="https://github.com/apache/beam/issues/X">#X&lt;/a>).&lt;/li>
&lt;li>Introduced &lt;code>--groupFilesFileLoad&lt;/code> pipeline option to mitigate side-input related issues in BigQueryIO
batch FILE_LOAD on certain runners (including Dataflow Runner V2) (Java) (&lt;a href="https://github.com/apache/beam/pull/33587">#33587&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add BigQuery vector/embedding ingestion and enrichment components to apache_beam.ml.rag (Python) (&lt;a href="https://github.com/apache/beam/pull/33413">#33413&lt;/a>).&lt;/li>
&lt;li>Upgraded to protobuf 4 (Java) (&lt;a href="https://github.com/apache/beam/issues/33192">#33192&lt;/a>).&lt;/li>
&lt;li>[GCSIO] Added retry logic to each batch method of the GCS IO (Python) (&lt;a href="https://github.com/apache/beam/pull/33539">#33539&lt;/a>)&lt;/li>
&lt;li>[GCSIO] Enable recursive deletion for GCSFileSystem Paths (Python) (&lt;a href="https://github.com/apache/beam/pull/33611">#33611&lt;/a>).&lt;/li>
&lt;li>External, Process based Worker Pool support added to the Go SDK container. (&lt;a href="https://github.com/apache/beam/pull/33572">#33572&lt;/a>)
&lt;ul>
&lt;li>This is used to enable sidecar containers to run SDK workers for some runners.&lt;/li>
&lt;li>See &lt;a href="https://beam.apache.org/documentation/runtime/sdk-harness-config/">https://beam.apache.org/documentation/runtime/sdk-harness-config/&lt;/a> for details.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Support the Process Environment for execution in the Go SDK. (&lt;a href="https://github.com/apache/beam/pull/33651">#33651&lt;/a>)&lt;/li>
&lt;li>Prism
&lt;ul>
&lt;li>Prism now uses the same single port for both pipeline submission and execution on workers. Requests are differentiated by worker-id. (&lt;a href="https://github.com/apache/beam/pull/33438">#33438&lt;/a>)
&lt;ul>
&lt;li>This avoids port starvation and provides clarity on port use when running Prism in non-local environments.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Support for @RequiresTimeSortedInputs added. (&lt;a href="https://github.com/apache/beam/issues/33513">#33513&lt;/a>)&lt;/li>
&lt;li>Initial support for AllowedLateness added. (&lt;a href="https://github.com/apache/beam/pull/33542">#33542&lt;/a>)&lt;/li>
&lt;li>The Go SDK&amp;rsquo;s inprocess Prism runner (AKA the Go SDK default runner) now supports non-loopback mode environment types. (&lt;a href="https://github.com/apache/beam/pull/33572">#33572&lt;/a>)&lt;/li>
&lt;li>Support the Process Environment for execution in Prism (&lt;a href="https://github.com/apache/beam/pull/33651">#33651&lt;/a>)&lt;/li>
&lt;li>Support the AnyOf Environment for execution in Prism (&lt;a href="https://github.com/apache/beam/pull/33705">#33705&lt;/a>)
&lt;ul>
&lt;li>This improves support for developing Xlang pipelines, when using a compatible cross language service.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Partitions are now configurable for the DaskRunner in the Python SDK (&lt;a href="https://github.com/apache/beam/pull/33805">#33805&lt;/a>).&lt;/li>
&lt;li>[Dataflow Streaming] Enable Windmill GetWork Response Batching by default (&lt;a href="https://github.com/apache/beam/pull/33847">#33847&lt;/a>).
&lt;ul>
&lt;li>With this change user workers will request batched GetWork responses from backend and backend will send multiple WorkItems in the same response proto.&lt;/li>
&lt;li>The feature can be disabled by passing &lt;code>--windmillRequestBatchedGetWorkResponse=false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>AWS V1 I/Os have been removed (Java). As part of this, x-lang Python Kinesis I/O has been updated to consume the V2 IO and it also no longer supports setting producer_properties (&lt;a href="https://github.com/apache/beam/issues/33430">#33430&lt;/a>).&lt;/li>
&lt;li>Upgraded to protobuf 4 (Java) (&lt;a href="https://github.com/apache/beam/issues/33192">#33192&lt;/a>), but forced Debezium IO to use protobuf 3 (&lt;a href="https://github.com/apache/beam/issues/33541">#33541&lt;/a> because Debezium clients are not protobuf 4 compatible. This may cause conflicts when using clients which are only compatible with protobuf 4.&lt;/li>
&lt;li>Minimum Go version for Beam Go updated to 1.22.10 (&lt;a href="https://github.com/apache/beam/pull/33609">#33609&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fix data loss issues when reading gzipped files with TextIO (Python) (&lt;a href="https://github.com/apache/beam/issues/18390">#18390&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/31040">#31040&lt;/a>).&lt;/li>
&lt;li>[BigQueryIO] Fixed an issue where Storage Write API sometimes doesn&amp;rsquo;t pick up auto-schema updates (&lt;a href="https://github.com/apache/beam/pull/33231">#33231&lt;/a>)&lt;/li>
&lt;li>Prism
&lt;ul>
&lt;li>Fixed an edge case where Bundle Finalization might not become enabled. (&lt;a href="https://github.com/apache/beam/issues/33493">#33493&lt;/a>).&lt;/li>
&lt;li>Fixed session window aggregation, which wasn&amp;rsquo;t being performed per-key. (&lt;a href="https://github.com/apache/beam/issues/33542">#33542&lt;/a>).)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Dataflow Streaming Appliance] Fixed commits failing with KeyCommitTooLargeException when a key outputs &amp;gt;180MB of results. &lt;a href="https://github.com/apache/beam/issues/33588">#33588&lt;/a>.&lt;/li>
&lt;li>Fixed a Dataflow template creation issue that ignores template file creation errors (Java) (&lt;a href="https://github.com/apache/beam/issues/33636">#33636&lt;/a>)&lt;/li>
&lt;li>Correctly documented Pane Encodings in the portability protocols (&lt;a href="https://github.com/apache/beam/issues/33840">#33840&lt;/a>).&lt;/li>
&lt;li>Fixed the user mailing list address (&lt;a href="https://github.com/apache/beam/issues/26013">#26013&lt;/a>).&lt;/li>
&lt;li>[Dataflow Streaming] Fixed an issue where Dataflow Streaming workers were reporting lineage metrics as cumulative rather than delta. (&lt;a href="https://github.com/apache/beam/pull/33691">#33691&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>(Java) Current version of protobuf has a &lt;a href="https://github.com/protocolbuffers/protobuf/issues/20599">bug&lt;/a> leading to incompatibilities with clients using older versions of Protobuf (&lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/issues/2191">example issue&lt;/a>). This issue has been seen in SpannerIO in particular. Tracked in &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/issues/34452">#34452&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.63.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Alex Merose,
Andrej Galad,
Andrew Crites,
Arun Pandian,
Bartosz Zablocki,
Chamikara Jayalath,
Claire McGinty,
Clay Johnson,
Damon Douglas,
Danish Amjad,
Danny McCormick,
Deep1998,
Derrick Williams,
Dmitry Labutin,
Dmytro Sadovnychyi,
Eduardo Ramírez,
Filipe Regadas,
Hai Joey Tran,
Jack McCluskey,
Jan Lukavský,
Jeff Kinard,
Jozef Vilcek,
Julien Tournay,
Kenneth Knowles,
Michel Davit,
Miguel Trigueira,
Minbo Bae,
Mohamed Awnallah,
Mohit Paddhariya,
Nahian-Al Hasan,
Naireen Hussain,
Niall Pemberton,
Radosław Stankiewicz,
Razvan Culea,
Robert Bradshaw,
Robert Burke,
Rohit Sinha,
S. Veyrié,
Sam Whittle,
Sergei Lilichenko,
Shingo Furuyama,
Shunping Huang,
Thiago Nunes,
Tim Heckman,
Tobias Bredow,
Tom Stepp,
Tony Tang,
VISHESH TRIPATHI,
Vitaly Terentyev,
Yi Hu,
XQ Hu,
akashorabek,
claudevdm&lt;/p></description></item><item><title>Blog: Apache Beam 2.62.0</title><link>/blog/beam-2.62.0/</link><pubDate>Tue, 21 Jan 2025 10:30:00 -0500</pubDate><guid>/blog/beam-2.62.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.62.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/%7B$DOWNLOAD_ANCHOR%7D">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.62.0, check out the &lt;a href="https://github.com/apache/beam/milestone/26">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for stateful processing in Spark Runner for streaming pipelines. Timer functionality is not yet supported and will be implemented in a future release (&lt;a href="https://github.com/apache/beam/issues/33237">#33237&lt;/a>).&lt;/li>
&lt;li>The datetime module is now available for use in jinja templatization for yaml.&lt;/li>
&lt;li>Improved batch performance of SparkRunner&amp;rsquo;s GroupByKey (&lt;a href="https://github.com/apache/beam/pull/20943">#20943&lt;/a>).&lt;/li>
&lt;li>Support OnWindowExpiration in Prism (&lt;a href="https://github.com/apache/beam/issues/32211">#32211&lt;/a>).
&lt;ul>
&lt;li>This enables initial Java GroupIntoBatches support.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Support OrderedListState in Prism (&lt;a href="https://github.com/apache/beam/issues/32929">#32929&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>gcs-connector config options can be set via GcsOptions (Java) (&lt;a href="https://github.com/apache/beam/pull/32769">#32769&lt;/a>).&lt;/li>
&lt;li>[Managed Iceberg] Support partitioning by time (year, month, day, hour) for types &lt;code>date&lt;/code>, &lt;code>time&lt;/code>, &lt;code>timestamp&lt;/code>, and &lt;code>timestamp(tz)&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/32939">#32939&lt;/a>)&lt;/li>
&lt;li>Upgraded the default version of Hadoop dependencies to 3.4.1. Hadoop 2.10.2 is still supported (Java) (&lt;a href="https://github.com/apache/beam/issues/33011">#33011&lt;/a>).&lt;/li>
&lt;li>[BigQueryIO] Create managed BigLake tables dynamically (&lt;a href="https://github.com/apache/beam/pull/33125">#33125&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Upgraded ZetaSQL to 2024.11.1 (&lt;a href="https://github.com/apache/beam/pull/32902">#32902&lt;/a>). Java11+ is now needed if Beam&amp;rsquo;s ZetaSQL component is used.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed EventTimeTimer ordering in Prism. (&lt;a href="https://github.com/apache/beam/issues/32222">#32222&lt;/a>).&lt;/li>
&lt;li>[Managed Iceberg] Fixed a bug where DataFile metadata was assigned incorrect partition values (&lt;a href="https://github.com/apache/beam/pull/33549">#33549&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &lt;a href="https://www.cve.org/CVERecord?id=CVE-2024-47561">CVE-2024-47561&lt;/a> (Java) by upgrading Avro version to 1.11.4&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>[Python] If you are using the official Apache Beam Python containers for version 2.62.0, be aware that they include NumPy version 1.26.4. It is strongly recommended that you explicitly specify numpy==1.26.4 in your project&amp;rsquo;s dependency list. (&lt;a href="https://github.com/apache/beam/issues/33639">#33639&lt;/a>).&lt;/li>
&lt;li>[Dataflow Streaming Appliance] Commits fail with KeyCommitTooLargeException when a key outputs &amp;gt;180MB of results. Bug affects versions 2.60.0 to 2.62.0,
&lt;ul>
&lt;li>fix will be released with 2.63.0. &lt;a href="https://github.com/apache/beam/issues/33588">#33588&lt;/a>.&lt;/li>
&lt;li>To resolve this issue, downgrade to 2.59.0 or upgrade to 2.63.0 or enable &lt;a href="https://cloud.google.com/dataflow/docs/streaming-engine#use">Streaming Engine&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.62.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Ahmet Altay,
Alex Merose,
Andrew Crites,
Arnout Engelen,
Attila Doroszlai,
Bartosz Zablocki,
Chamikara Jayalath,
Claire McGinty,
Claude van der Merwe,
Damon Douglas,
Danny McCormick,
Gabija Balvociute,
Hai Joey Tran,
Hakampreet Singh Pandher,
Ian Sullivan,
Jack McCluskey,
Jan Lukavský,
Jeff Kinard,
Jeffrey Kinard,
Laura Detmer,
Kenneth Knowles,
Martin Trieu,
Mattie Fu,
Michel Davit,
Naireen Hussain,
Nick Anikin,
Radosław Stankiewicz,
Ravi Magham,
Reeba Qureshi,
Robert Bradshaw,
Robert Burke,
Rohit Sinha,
S. Veyrié,
Sam Whittle,
Shingo Furuyama,
Shunping Huang,
Svetak Sundhar,
Valentyn Tymofieiev,
Vlado Djerek,
XQ Hu,
Yi Hu,
twosom&lt;/p></description></item><item><title>Blog: Apache Beam 2.61.0</title><link>/blog/beam-2.61.0/</link><pubDate>Mon, 25 Nov 2024 15:00:00 -0500</pubDate><guid>/blog/beam-2.61.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.61.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2610-2024-11-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.61.0, check out the &lt;a href="https://github.com/apache/beam/milestone/25">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>[Python] Introduce Managed Transforms API (&lt;a href="https://github.com/apache/beam/pull/31495">#31495&lt;/a>)&lt;/li>
&lt;li>Flink 1.19 support added (&lt;a href="https://github.com/apache/beam/pull/32648">#32648&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>[Managed Iceberg] Support creating tables if needed (&lt;a href="https://github.com/apache/beam/pull/32686">#32686&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Now available in Python SDK (&lt;a href="https://github.com/apache/beam/pull/31495">#31495&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Add support for TIMESTAMP, TIME, and DATE types (&lt;a href="https://github.com/apache/beam/pull/32688">#32688&lt;/a>)&lt;/li>
&lt;li>BigQuery CDC writes are now available in Python SDK, only supported when using StorageWrite API at least once mode (&lt;a href="https://github.com/apache/beam/issues/32527">#32527&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Allow updating table partition specs during pipeline runtime (&lt;a href="https://github.com/apache/beam/pull/32879">#32879&lt;/a>)&lt;/li>
&lt;li>Added BigQueryIO as a Managed IO (&lt;a href="https://github.com/apache/beam/pull/31486">#31486&lt;/a>)&lt;/li>
&lt;li>Support for writing to &lt;a href="https://solace.com/">Solace messages queues&lt;/a> (&lt;code>SolaceIO.Write&lt;/code>) added (Java) (&lt;a href="https://github.com/apache/beam/issues/31905">#31905&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for read with metadata in MqttIO (Java) (&lt;a href="https://github.com/apache/beam/issues/32195">#32195&lt;/a>)&lt;/li>
&lt;li>Added support for processing events which use a global sequence to &amp;ldquo;ordered&amp;rdquo; extension (Java) (&lt;a href="https://github.com/apache/beam/pull/32540">#32540&lt;/a>)&lt;/li>
&lt;li>Add new meta-transform FlattenWith and Tee that allow one to introduce branching
without breaking the linear/chaining style of pipeline construction.&lt;/li>
&lt;li>Use Prism as a fallback to the Python Portable runner when running a pipeline with the Python Direct runner (&lt;a href="https://github.com/apache/beam/pull/32876">#32876&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Removed support for Flink 1.15 and 1.16&lt;/li>
&lt;li>Removed support for Python 3.8&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Java) Fixed tearDown not invoked when DoFn throws on Portable Runners (&lt;a href="https://github.com/apache/beam/issues/18592">#18592&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/31381">#31381&lt;/a>).&lt;/li>
&lt;li>(Java) Fixed protobuf error with MapState.remove() in Dataflow Streaming Java Legacy Runner without Streaming Engine (&lt;a href="https://github.com/apache/beam/issues/32892">#32892&lt;/a>).&lt;/li>
&lt;li>Adding flag to support conditionally disabling auto-commit in JdbcIO ReadFn (&lt;a href="https://github.com/apache/beam/issues/31111">#31111&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>[Managed Iceberg] DataFile metadata is assigned incorrect partition values (&lt;a href="https://github.com/apache/beam/issues/33497">#33497&lt;/a>).
&lt;ul>
&lt;li>Fixed in 2.62.0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Python] If you are using the official Apache Beam Python containers for version 2.61.0, be aware that they include NumPy version 1.26.4. It is strongly recommended that you explicitly specify numpy==1.26.4 in your project&amp;rsquo;s dependency list. (&lt;a href="https://github.com/apache/beam/issues/33639">#33639&lt;/a>).&lt;/li>
&lt;li>[Dataflow Streaming Appliance] Commits fail with KeyCommitTooLargeException when a key outputs &amp;gt;180MB of results. Bug affects versions 2.60.0 to 2.62.0,
&lt;ul>
&lt;li>fix will be released with 2.63.0. &lt;a href="https://github.com/apache/beam/issues/33588">#33588&lt;/a>.&lt;/li>
&lt;li>To resolve this issue, downgrade to 2.59.0 or upgrade to 2.63.0 or enable &lt;a href="https://cloud.google.com/dataflow/docs/streaming-engine#use">Streaming Engine&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.60.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud, Ahmet Altay, Arun Pandian, Ayush Pandey, Chamikara Jayalath, Chris Ashcraft, Christoph Grotz, DKPHUONG, Damon, Danny Mccormick, Dmitry Ulyumdzhiev, Ferran Fernández Garrido, Hai Joey Tran, Hyeonho Kim, Idan Attias, Israel Herraiz, Jack McCluskey, Jan Lukavský, Jeff Kinard, Jeremy Edwards, Joey Tran, Kenneth Knowles, Maciej Szwaja, Manit Gupta, Mattie Fu, Michel Davit, Minbo Bae, Mohamed Awnallah, Naireen Hussain, Rebecca Szper, Reeba Qureshi, Reuven Lax, Robert Bradshaw, Robert Burke, S. Veyrié, Sam Whittle, Sergei Lilichenko, Shunping Huang, Steven van Rossum, Tan Le, Thiago Nunes, Vitaly Terentyev, Vlado Djerek, Yi Hu, claudevdm, fozzie15, johnjcasey, kushmiD, liferoad, martin trieu, pablo rodriguez defino, razvanculea, s21lee, tvalentyn, twosom&lt;/p></description></item><item><title>Blog: Apache Beam 2.60.0</title><link>/blog/beam-2.60.0/</link><pubDate>Thu, 17 Oct 2024 15:00:00 -0500</pubDate><guid>/blog/beam-2.60.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.60.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2600-2024-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.60.0, check out the &lt;a href="https://github.com/apache/beam/milestone/24">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for using vLLM in the RunInference transform (Python) (&lt;a href="https://github.com/apache/beam/issues/32528">#32528&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Added support for streaming writes (&lt;a href="https://github.com/apache/beam/pull/32451">#32451&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Added auto-sharding for streaming writes (&lt;a href="https://github.com/apache/beam/pull/32612">#32612&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] Added support for writing to dynamic destinations (&lt;a href="https://github.com/apache/beam/pull/32565">#32565&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Dataflow worker can install packages from Google Artifact Registry Python repositories (Python) (&lt;a href="https://github.com/apache/beam/issues/32123">#32123&lt;/a>).&lt;/li>
&lt;li>Added support for Zstd codec in SerializableAvroCodecFactory (Java) (&lt;a href="https://github.com/apache/beam/issues/32349">#32349&lt;/a>)&lt;/li>
&lt;li>Added support for using vLLM in the RunInference transform (Python) (&lt;a href="https://github.com/apache/beam/issues/32528">#32528&lt;/a>)&lt;/li>
&lt;li>Prism release binaries and container bootloaders are now being built with the latest Go 1.23 patch. (&lt;a href="https://github.com/apache/beam/pull/32575">#32575&lt;/a>)&lt;/li>
&lt;li>Prism
&lt;ul>
&lt;li>Prism now supports Bundle Finalization. (&lt;a href="https://github.com/apache/beam/pull/32425">#32425&lt;/a>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Significantly improved performance of Kafka IO reads that enable &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html#commitOffsetsInFinalize--">commitOffsetsInFinalize&lt;/a> by removing the data reshuffle from SDF implementation. (&lt;a href="https://github.com/apache/beam/pull/31682">#31682&lt;/a>).&lt;/li>
&lt;li>Added support for dynamic writing in MqttIO (Java) (&lt;a href="https://github.com/apache/beam/issues/19376">#19376&lt;/a>)&lt;/li>
&lt;li>Optimized Spark Runner parDo transform evaluator (Java) (&lt;a href="https://github.com/apache/beam/issues/32537">#32537&lt;/a>)&lt;/li>
&lt;li>[Managed Iceberg] More efficient manifest file writes/commits (&lt;a href="https://github.com/apache/beam/issues/32666">#32666&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>In Python, assert_that now throws if it is not in a pipeline context instead of silently succeeding (&lt;a href="https://github.com/apache/beam/pull/30771">#30771&lt;/a>)&lt;/li>
&lt;li>In Python and YAML, ReadFromJson now override the dtype from None to
an explicit False. Most notably, string values like &lt;code>&amp;quot;123&amp;quot;&lt;/code> are preserved
as strings rather than silently coerced (and possibly truncated) to numeric
values. To retain the old behavior, pass &lt;code>dtype=True&lt;/code> (or any other value
accepted by &lt;code>pandas.read_json&lt;/code>).&lt;/li>
&lt;li>Users of KafkaIO Read transform that enable &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html#commitOffsetsInFinalize--">commitOffsetsInFinalize&lt;/a> might encounter pipeline graph compatibility issues when updating the pipeline. To mitigate, set the &lt;code>updateCompatibilityVersion&lt;/code> option to the SDK version used for the original pipeline, example &lt;code>--updateCompatabilityVersion=2.58.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Python 3.8 is reaching EOL and support is being removed in Beam 2.61.0. The 2.60.0 release will warn users
when running on 3.8. (&lt;a href="https://github.com/apache/beam/issues/31192">#31192&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Java) Fixed custom delimiter issues in TextIO (&lt;a href="https://github.com/apache/beam/issues/32249">#32249&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/32251">#32251&lt;/a>).&lt;/li>
&lt;li>(Java, Python, Go) Fixed PeriodicSequence backlog bytes reporting, which was preventing Dataflow Runner autoscaling from functioning properly (&lt;a href="https://github.com/apache/beam/issues/32506">#32506&lt;/a>).&lt;/li>
&lt;li>(Java) Fix improper decoding of rows with schemas containing nullable fields when encoded with a schema with equal encoding positions but modified field order. (&lt;a href="https://github.com/apache/beam/issues/32388">#32388&lt;/a>).&lt;/li>
&lt;li>(Java) Skip close on bundles in BigtableIO.Read (&lt;a href="https://github.com/apache/beam/pull/32661">#32661&lt;/a>, &lt;a href="https://github.com/apache/beam/pull/32759">#32759&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Managed Iceberg] DataFile metadata is assigned incorrect partition values (&lt;a href="https://github.com/apache/beam/issues/33497">#33497&lt;/a>).
&lt;ul>
&lt;li>Fixed in 2.62.0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Dataflow Streaming Appliance] Commits fail with KeyCommitTooLargeException when a key outputs &amp;gt;180MB of results. Bug affects versions 2.60.0 to 2.62.0,
&lt;ul>
&lt;li>fix will be released with 2.63.0. &lt;a href="https://github.com/apache/beam/issues/33588">#33588&lt;/a>.&lt;/li>
&lt;li>To resolve this issue, downgrade to 2.59.0 or upgrade to 2.63.0 or enable &lt;a href="https://cloud.google.com/dataflow/docs/streaming-engine#use">Streaming Engine&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.60.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud, Aiden Grossman, Arun Pandian, Bartosz Zablocki, Chamikara Jayalath, Claire McGinty, DKPHUONG, Damon Douglass, Danny McCormick, Dip Patel, Ferran Fernández Garrido, Hai Joey Tran, Hyeonho Kim, Igor Bernstein, Israel Herraiz, Jack McCluskey, Jaehyeon Kim, Jeff Kinard, Jeffrey Kinard, Joey Tran, Kenneth Knowles, Kirill Berezin, Michel Davit, Minbo Bae, Naireen Hussain, Niel Markwick, Nito Buendia, Reeba Qureshi, Reuven Lax, Robert Bradshaw, Robert Burke, Rohit Sinha, Ryan Fu, Sam Whittle, Shunping Huang, Svetak Sundhar, Udaya Chathuranga, Vitaly Terentyev, Vlado Djerek, Yi Hu, Claude van der Merwe, XQ Hu, Martin Trieu, Valentyn Tymofieiev, twosom&lt;/p></description></item><item><title>Blog: Apache Beam Summit 2024: Unlocking the power of ML for data processing</title><link>/blog/beam-summit-2024-overview/</link><pubDate>Wed, 16 Oct 2024 00:00:01 -0800</pubDate><guid>/blog/beam-summit-2024-overview/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>At the recently concluded &lt;a href="https://beamsummit.org/">Beam Summit 2024&lt;/a>, a two-day event held from September 4 to 5, numerous captivating presentations showcased the potential of Beam to address a wide range of challenges, with an emphasis on machine learning (ML). These challenges included feature engineering, data enrichment, and model inference for large-scale distributed data. In all, the summit included &lt;a href="https://beamsummit.org/sessions/2024/">47 talks&lt;/a>, with 16 focused specifically on ML use cases or features and many more touching on these topics.&lt;/p>
&lt;p>The talks displayed the breadth and diversity of the Beam community. Among the speakers and attendees, &lt;a href="https://docs.google.com/presentation/d/1IJ1sExHzrzIFF5QXKWlcAuPdp7lKOepRQKl9BnfHxJw/edit#slide=id.g3058d3e2f5f_0_10">23 countries&lt;/a> were represented. Attendees included Beam users, committers in the Beam project, Beam Google Summer of Code contributors, and data processing/machine learning experts.&lt;/p>
&lt;h2 id="user-friendly-turnkey-transforms-for-ml">User-friendly turnkey transforms for ML&lt;/h2>
&lt;p>With the features recently added to Beam, Beam now offers a set of rich turn-key transforms for ML users that handle a wide range of ML-Ops tasks. These transforms include:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/documentation/ml/overview/#prediction-and-inference">RunInference&lt;/a>: deploy ML models on CPUs and GPUs&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/ml/overview/#data-processing">Enrichment&lt;/a>: enrich data for ML feature enhancements&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/ml/overview/#data-processing">MLTransform&lt;/a>: transform data into ML features&lt;/li>
&lt;/ul>
&lt;p>The Summit talks covering both how to use these features and how people are already using them. Highlights included:&lt;/p>
&lt;ul>
&lt;li>A talk about &lt;a href="https://beamsummit.org/slides/2024/ScalingAutonomousDrivingwithApacheBeam.pdf">scaling autonomous driving at Cruise&lt;/a>&lt;/li>
&lt;li>Multiple talks about deploying LLMs for batch and streaming inference&lt;/li>
&lt;li>Three different talks about streaming processing for &lt;a href="https://cloud.google.com/use-cases/retrieval-augmented-generation">RAG&lt;/a> (including &lt;a href="https://www.youtube.com/watch?v=X_VzKQOcpC4">a talk&lt;/a> from one of Beam&amp;rsquo;s Google Summer of Code contributors!)&lt;/li>
&lt;/ul>
&lt;h2 id="beam-yaml-simplifying-ml-data-processing">Beam YAML: Simplifying ML data processing&lt;/h2>
&lt;p>Beam pipeline creation can be challenging and often requires learning concepts, managing dependencies, debugging, and maintaining code for ML tasks. To simplify the entry point, &lt;a href="https://beam.apache.org/blog/beam-yaml-release/">Beam YAML&lt;/a> introduces a declarative approach that uses YAML configuration files to create data processing pipelines. No coding is required.&lt;/p>
&lt;p>Beam Summit was the first opportunity that the Beam community had to show off some of the use cases of Beam YAML. It featured several talks about how Beam YAML is already a core part of many users&amp;rsquo; workflows at companies like &lt;a href="https://beamsummit.org/slides/2024/ALowCodeStructuredApproachtoDeployingApacheBeamMLWorkloadsonKubernetesusingBeamStack.pdf">MavenCode&lt;/a> and &lt;a href="https://youtu.be/avSXvbScbW0">ChartBoost&lt;/a>. With Beam YAML, these companies are able to build configuration-based data processing systems, significantly lowering the bar for entry at their companies.&lt;/p>
&lt;h2 id="prism-provide-a-unified-ml-pipeline-development-framework-for-local-and-remote-runner-environments">Prism: Provide a unified ML pipeline development framework for local and remote runner environments&lt;/h2>
&lt;p>Beam provides a variety of support for portable runners, but developing a local pipeline has traditionally been painful. Local runners are often incomplete and incompatible with remote runners, such as DataflowRunner and FlinkRunner.&lt;/p>
&lt;p>At Beam Summit, Beam contributors introduced &lt;a href="https://youtu.be/R4iNwLBa3VQ">the Prism local runner&lt;/a> to the community. Prism greatly improves the local developer experience and reduces the gap between local and remote execution. In particular, when handling complicated ML tasks, Prism guarantees consistent runner behavior across these runners, a task that had previously lacked consistent support.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Beam Summit 2024 showcased the tremendous potential of Apache Beam for addressing a wide range of data processing and machine learning challenges. We look forward to seeing even more innovative use cases and contributions in the future.&lt;/p>
&lt;p>To stay updated on the latest Beam developments and events, visit &lt;a href="https://beam.apache.org/get-started/">the Apache Beam website&lt;/a> and follow us on &lt;a href="https://www.linkedin.com/company/apache-beam/">social media&lt;/a>. We encourage you to join &lt;a href="https://beam.apache.org/community/contact-us/">the Beam community&lt;/a> and &lt;a href="https://beam.apache.org/contribute/">contribute to the project&lt;/a>. Together, let&amp;rsquo;s unlock the full potential of Beam and shape the future of data processing and machine learning.&lt;/p></description></item><item><title>Blog: Efficient Streaming Data Processing with Beam YAML and Protobuf</title><link>/blog/beam-yaml-proto/</link><pubDate>Fri, 20 Sep 2024 11:53:38 +0200</pubDate><guid>/blog/beam-yaml-proto/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="efficient-streaming-data-processing-with-beam-yaml-and-protobuf">Efficient Streaming Data Processing with Beam YAML and Protobuf&lt;/h1>
&lt;p>As streaming data processing grows, so do its maintenance, complexity, and costs.
This post explains how to efficiently scale pipelines by using &lt;a href="https://protobuf.dev/">Protobuf&lt;/a>,
which ensures that pipelines are reusable and quick to deploy. The goal is to keep this process simple
for engineers to implement using &lt;a href="https://beam.apache.org/documentation/sdks/yaml/">Beam YAML&lt;/a>.&lt;/p>
&lt;h2 id="simplify-pipelines-with-beam-yaml">Simplify pipelines with Beam YAML&lt;/h2>
&lt;p>Creating a pipeline in Beam can be somewhat difficult, especially for new Apache Beam users.
Setting up the project, managing dependencies, and so on can be challenging.
Beam YAML eliminates most of the boilerplate code,
which allows you to focus on the most important part of the work: data transformation.&lt;/p>
&lt;p>Some of the key benefits of Beam YAML include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Readability:&lt;/strong> By using a declarative language (&lt;a href="https://yaml.org/">YAML&lt;/a>), the pipeline configuration is more human readable.&lt;/li>
&lt;li>&lt;strong>Reusability:&lt;/strong> Reusing the same components across different pipelines is simplified.&lt;/li>
&lt;li>&lt;strong>Maintainability:&lt;/strong> Pipeline maintenance and updates are easier.&lt;/li>
&lt;/ul>
&lt;p>The following template shows an example of reading events from a &lt;a href="https://kafka.apache.org/intro">Kafka&lt;/a> topic and
writing them into &lt;a href="https://cloud.google.com/bigquery?hl=en">BigQuery&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromKafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">topic&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;TOPIC_NAME&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">format&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">RAW/AVRO/JSON/PROTO&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;BOOTSTRAP_SERVERS&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">schema&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;SCHEMA&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToBigQuery&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">table&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;PROJECT_ID.DATASET.MOVIE_EVENTS_TABLE&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">useAtLeastOnceSemantics&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">streaming&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">dataflow_service_options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="l">streaming_mode_at_least_once]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-complete-workflow">The complete workflow&lt;/h2>
&lt;p>This section demonstrates the complete workflow for this pipeline.&lt;/p>
&lt;h3 id="create-a-simple-proto-event">Create a simple proto event&lt;/h3>
&lt;p>The following code creates a simple movie event.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-protobuf" data-lang="protobuf">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// events/v1/movie_event.proto
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="n">syntax&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;proto3&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="kn">package&lt;/span> &lt;span class="nn">event&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v1&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;bq_field.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;bq_table.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;buf/validate/validate.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">import&lt;/span> &lt;span class="s">&amp;#34;google/protobuf/wrappers.proto&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="kd">message&lt;/span> &lt;span class="nc">MovieEvent&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="k">option&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery_opts&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">table_name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;movie_table&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.StringValue&lt;/span> &lt;span class="n">event_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Unique Event ID&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.StringValue&lt;/span> &lt;span class="n">user_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Unique User ID&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.StringValue&lt;/span> &lt;span class="n">movie_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Unique Movie ID&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">google.protobuf.Int32Value&lt;/span> &lt;span class="n">rating&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="n">buf.validate.field&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="kt">int32&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="c1">// validates the average rating is at least 0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">gte&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="c1">// validates the average rating is at most 100
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">lte&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">100&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">},&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Movie rating&amp;#34;&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="kt">string&lt;/span> &lt;span class="n">event_dt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">type_override&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;DATETIME&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">gen_bq_schema.bigquery&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;UTC Datetime representing when we received this event. Format: YYYY-MM-DDTHH:MM:SS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">buf.validate.field&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="kt">string&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">pattern&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s">&amp;#34;^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}$&amp;#34;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">},&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">ignore_empty&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Because these events are written to BigQuery,
the &lt;a href="https://buf.build/googlecloudplatform/bq-schema-api/file/main:bq_field.proto">&lt;code>bq_field&lt;/code>&lt;/a> proto
and the &lt;a href="https://buf.build/googlecloudplatform/bq-schema-api/file/main:bq_table.proto">&lt;code>bq_table&lt;/code>&lt;/a> proto are imported.
These proto files help generate the BigQuery JSON schema.
This example also demonstrates a shift-left approach, which moves testing, quality,
and performance as early as possible in the development process. For example, to ensure that only valid events are generated from the source, the &lt;code>buf.validate&lt;/code> elements are included.&lt;/p>
&lt;p>After you create the &lt;code>movie_event.proto&lt;/code> proto in the &lt;code>events/v1&lt;/code> folder, you can generate
the necessary &lt;a href="https://buf.build/docs/reference/descriptors">file descriptor&lt;/a>.
A file descriptor is a compiled representation of the schema that allows various tools and systems
to understand and work with protobuf data dynamically. To simplify the process, this example uses Buf,
which requires the following configuration files.&lt;/p>
&lt;p>&lt;b>Buf configuration:&lt;/b>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># buf.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">deps&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">buf.build/googlecloudplatform/bq-schema-api&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">buf.build/bufbuild/protovalidate&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">breaking&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">use&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">FILE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">lint&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">use&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">DEFAULT&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># buf.gen.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">managed&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">enabled&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">plugins&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># Python Plugins&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/protocolbuffers/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/grpc/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># Java Plugins&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/protocolbuffers/java:v25.2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/maven/src/main/java&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/grpc/java&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gen/maven/src/main/java&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># BQ Schemas&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">remote&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">buf.build/googlecloudplatform/bq-schema:v1.1.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">out&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">protoc-gen/bq_schema&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Run the following two commands to generate the necessary Java, Python, BigQuery schema, and Descriptor file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">// Generate the buf.lock file
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">buf deps update
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">// It generates the descriptor in descriptor.binp.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">buf build . -o descriptor.binp --exclude-imports
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">// It generates the Java, Python and BigQuery schema as described in buf.gen.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">buf generate --include-imports
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="make-the-beam-yaml-read-proto">Make the Beam YAML read proto&lt;/h3>
&lt;p>Make the following modifications to the to the YAML file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># movie_events_pipeline.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromKafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">topic&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;movie_proto&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">format&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">PROTO&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;BOOTSTRAP_SERVERS&amp;gt;&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">file_descriptor_path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;gs://my_proto_bucket/movie/v1.0.0/descriptor.binp&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">message_name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;event.v1.MovieEvent&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToBigQuery&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadProtoMovieEvents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">table&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;PROJECT_ID&amp;gt;.raw.movie_table&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">useAtLeastOnceSemantics&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">streaming&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">dataflow_service_options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="l">streaming_mode_at_least_once]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This step changes the format to &lt;code>PROTO&lt;/code> and adds the &lt;code>file_descriptor_path&lt;/code> and the &lt;code>message_name&lt;/code>.&lt;/p>
&lt;h3 id="deploy-the-pipeline-with-terraform">Deploy the pipeline with Terraform&lt;/h3>
&lt;p>You can use &lt;a href="https://www.terraform.io/">Terraform&lt;/a> to deploy the Beam YAML pipeline
with &lt;a href="https://cloud.google.com/products/dataflow?hl=en">Dataflow&lt;/a> as the runner.
The following Terraform code example demonstrates how to achieve this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-hcl" data-lang="hcl">&lt;span class="line">&lt;span class="cl">&lt;span class="err">//&lt;/span> &lt;span class="k">Enable&lt;/span> &lt;span class="k">Dataflow&lt;/span> &lt;span class="k">API&lt;/span>&lt;span class="p">.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">resource&lt;/span> &lt;span class="s2">&amp;#34;google_project_service&amp;#34; &amp;#34;enable_dataflow_api&amp;#34;&lt;/span> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_project_id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;dataflow.googleapis.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">//&lt;/span> &lt;span class="k">DF&lt;/span> &lt;span class="k">Beam&lt;/span> &lt;span class="k">YAML&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">resource&lt;/span> &lt;span class="s2">&amp;#34;google_dataflow_flex_template_job&amp;#34; &amp;#34;data_movie_job&amp;#34;&lt;/span> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> provider&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">google&lt;/span>&lt;span class="err">-&lt;/span>&lt;span class="k">beta&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_project_id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;movie-proto-events&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> container_spec_gcs_path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;gs://dataflow-templates-${var.gcp_region}/latest/flex/Yaml_Template&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> region&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_region&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> on_delete&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;drain&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> machine_type&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;n2d-standard-4&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> enable_streaming_engine&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kt">true&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> subnetwork&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">subnetwork&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> skip_wait_on_job_termination&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kt">true&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> parameters&lt;/span> &lt;span class="o">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> yaml_pipeline_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;gs://${var.bucket_name}/yamls/${var.package_version}/movie_events_pipeline.yml&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> max_num_workers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">40&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> worker_zone&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">var&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">gcp_zone&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n"> depends_on&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="k">google_project_service&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="k">enable_dataflow_api&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Assuming the BigQuery table exists, which you can do by using Terraform and Proto,
this code creates a Dataflow job by using the Beam YAML code that reads Proto events from
Kafka and writes them into BigQuery.&lt;/p>
&lt;h2 id="improvements-and-conclusions">Improvements and conclusions&lt;/h2>
&lt;p>The following community contributions could improve the Beam YAML code in this example:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Support schema registries:&lt;/strong> Integrate with schema registries such as Buf Registry or Apicurio for
better schema management. The current workflow generates the descriptors by using Buf and store them in Google Cloud Storage.
The descriptors could be stored in a schema registry instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Enhanced Monitoring:&lt;/strong> Implement advanced monitoring and alerting mechanisms to quickly identify and address
issues in the data pipeline.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Leveraging Beam YAML and Protobuf lets us streamline the creation and maintenance of
data processing pipelines, significantly reducing complexity. This approach ensures that engineers can more
efficiently implement and scale robust, reusable pipelines without needs to manually write Beam code.&lt;/p>
&lt;h2 id="contribute">Contribute&lt;/h2>
&lt;p>Developers who want to help build out and add functionalities are welcome to start contributing to the effort in the
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml">Beam YAML module&lt;/a>.&lt;/p>
&lt;p>There is also a list of open &lt;a href="https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Ayaml">bugs&lt;/a> found
on the GitHub repo - now marked with the &lt;code>yaml&lt;/code> tag.&lt;/p>
&lt;p>Although Beam YAML is marked stable as of Beam 2.52, it is still under heavy development, with new features being
added with each release. Those who want to be part of the design decisions and give insights to how the framework is
being used are highly encouraged to join the &lt;a href="https://beam.apache.org/community/contact-us/">dev mailing list&lt;/a>, where those discussions are occurring.&lt;/p></description></item><item><title>Blog: Unit Testing in Beam: An opinionated guide</title><link>/blog/unit-testing-in-beam/</link><pubDate>Fri, 13 Sep 2024 00:00:01 -0800</pubDate><guid>/blog/unit-testing-in-beam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Testing remains one of the most fundamental components of software engineering. In this blog post, we shed light on some of the constructs that Apache Beam provides for testing.
We cover an opinionated set of best practices to write unit tests for your data pipeline. This post doesn&amp;rsquo;t include integration tests, and you need to author those separately.
All snippets in this post are included in &lt;a href="https://github.com/apache/beam/blob/master/examples/notebooks/blog/unittests_in_beam.ipynb">this notebook&lt;/a>. Additionally, to see tests that exhibit best practices, look at the &lt;a href="https://beam.apache.org/blog/beam-starter-projects/">Beam starter projects&lt;/a>, which contain tests that exhibit best practices.&lt;/p>
&lt;h2 id="best-practices">Best practices&lt;/h2>
&lt;p>When testing Beam pipelines, we recommend the following best practices:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Don&amp;rsquo;t write unit tests for the already supported connectors in the Beam Library, such as &lt;code>ReadFromBigQuery&lt;/code> and &lt;code>WriteToText&lt;/code>. These connectors are already tested in Beam’s test suite to ensure correct functionality. They add unnecessary cost and dependencies to a unit test.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ensure that your function is well tested when using it with &lt;code>Map&lt;/code>, &lt;code>FlatMap&lt;/code>, or &lt;code>Filter&lt;/code>. You can assume your function will work as intended when using &lt;code>Map(your_function)&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For more complex transforms such as &lt;code>ParDo&lt;/code>’s, side inputs, timestamp inspection, etc., treat the entire transform as a unit, and test it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If needed, use mocking to mock any API calls that might be present in your DoFn. The purpose of mocking is to test your functionality extensively, even if this testing requires a specific response from an API call.&lt;/p>
&lt;ol>
&lt;li>Be sure to modularize your API calls in separate functions, rather than making the API call directly in the &lt;code>DoFn&lt;/code>. This step provides a cleaner experience when mocking the external API calls.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="example-1">Example 1&lt;/h2>
&lt;p>Use the following pipeline as an example. You don&amp;rsquo;t have to write a separate unit test to test this function in the context of this pipeline, assuming the function &lt;code>median_house_value_per_bedroom&lt;/code> is unit tested elsewhere in the code. You can trust that the &lt;code>Map&lt;/code> primitive works as expected (this illustrates point #2 noted previously).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The following code computes the median house value per bedroom.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/california_housing_test.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">skip_header_lines&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">median_house_value_per_bedroom&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example2&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example-2">Example 2&lt;/h2>
&lt;p>Use the following function as the example. The functions &lt;code>median_house_value_per_bedroom&lt;/code> and &lt;code>multiply_by_factor&lt;/code> are tested elsewhere, but the pipeline as a whole, which consists of composite transforms, is not.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/california_housing_test.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">skip_header_lines&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">median_house_value_per_bedroom&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply_by_factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombinePerKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example3&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The best practice for the previous code is to create a transform with all functions between &lt;code>ReadFromText&lt;/code> and &lt;code>WriteToText&lt;/code>. This step separates the transformation logic from the I/Os, allowing you to unit test the transformation logic. The following example is a refactoring of the previous code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">transform_data_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">median_house_value_per_bedroom&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">multiply_by_factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombinePerKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Define a new class that inherits from beam.PTransform.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MapAndCombineTransform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pcoll&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">transform_data_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/california_housing_test.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">skip_header_lines&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">MapAndCombineTransform&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example3&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This code shows the corresponding unit test for the previous example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">unittest&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">beam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.testing.test_pipeline&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">TestPipeline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.testing.util&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">assert_that&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">equal_to&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">TestBeam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unittest&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TestCase&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># This test corresponds to example 3, and is written to confirm the pipeline works as intended.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">test_transform_data_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expected&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">10570.185786231425&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">13.375337533753376&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">13.315649867374006&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;121.05,99.99,23.30,39.5,55.55,41.01,10,34,74.30,91.91&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;122.05,100.99,24.30,40.5,56.55,42.01,11,35,75.30,92.91&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;-120.05,39.37,29.00,4085.00,681.00,1557.00,626.00,6.8085,364700.00&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_elements&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MapAndCombineTransform&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">assert_that&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">equal_to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">expected&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example-3">Example 3&lt;/h2>
&lt;p>Suppose we write a pipeline that reads data from a JSON file, passes it through a custom function that makes external API calls for parsing, and then writes it to a custom destination (for example, if we need to do some custom data formatting to have data prepared for a downstream application).&lt;/p>
&lt;p>The pipeline has the following structure:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The following packages are used to run the example pipelines.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">beam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.io&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">WriteToText&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">PipelineOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MyDoFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">returned_record&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">MyApiCall&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;http://my-api-call.com&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">returned_record&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">!=&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">raise&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Length of record does not match expected length&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">yield&lt;/span> &lt;span class="n">returned_record&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p3&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/sample_data/anscombe.json&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MyDoFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/example1&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This test checks whether the API response is a record of the wrong length and throws the expected error if the test fails.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="err">!&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">mock&lt;/span> &lt;span class="c1"># Install the &amp;#39;mock&amp;#39; module.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Import the mock package for mocking functionality.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">unittest.mock&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Mock&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">patch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># from MyApiCall import get_data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">mock&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># MyApiCall is a function that calls get_data to fetch some data by using an API call.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@patch&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;MyApiCall.get_data&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">test_error_message_wrong_length&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mock_get_data&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">response&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;field1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;field2&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mock_get_data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Mock&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mock_get_data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_value&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_value&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">response&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1">#input length 9&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">assertRaisesRegex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="ne">ValueError&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Length of record does not match expected length&amp;#39;&amp;#34;&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p3&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_elements&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MyDoFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="other-testing-best-practices">Other testing best practices:&lt;/h2>
&lt;ol>
&lt;li>Test all error messages that you raise.&lt;/li>
&lt;li>Cover any edge cases that might exist in your data.&lt;/li>
&lt;li>Example 1 could have written the &lt;code>beam.Map&lt;/code> step with lambda functions instead of with &lt;code>beam.Map(median_house_value_per_bedroom)&lt;/code>:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>beam.Map(lambda x: x.strip().split(&amp;#39;,&amp;#39;)) | beam.Map(lambda x: float(x[8])/float(x[4])
&lt;/code>&lt;/pre>&lt;p>Separating lambdas into a helper function by using &lt;code>beam.Map(median_house_value_per_bedroom)&lt;/code> is the recommended approach for more testable code, because changes to the function would be modularized.&lt;/p>
&lt;ol start="4">
&lt;li>Use the &lt;code>assert_that&lt;/code> statement to ensure that &lt;code>PCollection&lt;/code> values match correctly, as in the previous example.&lt;/li>
&lt;/ol>
&lt;p>For more guidance about testing on Beam and Dataflow, see the &lt;a href="https://cloud.google.com/dataflow/docs/guides/develop-and-test-pipelines">Google Cloud documentation&lt;/a>. For more examples of unit testing in Beam, see the &lt;code>base_test.py&lt;/code> &lt;a href="https://github.com/apache/beam/blob/736cf50430b375d32093e793e1556567557614e9/sdks/python/apache_beam/ml/inference/base_test.py#L262">code&lt;/a>.&lt;/p>
&lt;p>Special thanks to Robert Bradshaw, Danny McCormick, XQ Hu, Surjit Singh, and Rebecca Spzer, who helped refine the ideas in this post.&lt;/p></description></item><item><title>Blog: Apache Beam 2.59.0</title><link>/blog/beam-2.59.0/</link><pubDate>Wed, 11 Sep 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.59.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.59.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2590-2024-09-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.59.0, check out the &lt;a href="https://github.com/apache/beam/milestone/23">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for setting a configureable timeout when loading a model and performing inference in the &lt;a href="https://beam.apache.org/documentation/ml/inference-overview/">RunInference&lt;/a> transform using &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference.with_exception_handling">with_exception_handling&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/32137">#32137&lt;/a>)&lt;/li>
&lt;li>Initial experimental support for using &lt;a href="/documentation/runners/prism/">Prism&lt;/a> with the Java and Python SDKs
&lt;ul>
&lt;li>Prism is presently targeting local testing usage, or other small scale execution.&lt;/li>
&lt;li>For Java, use &amp;lsquo;PrismRunner&amp;rsquo;, or &amp;lsquo;TestPrismRunner&amp;rsquo; as an argument to the &lt;code>--runner&lt;/code> flag.&lt;/li>
&lt;li>For Python, use &amp;lsquo;PrismRunner&amp;rsquo; as an argument to the &lt;code>--runner&lt;/code> flag.&lt;/li>
&lt;li>Go already uses Prism as the default local runner.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Improvements to the performance of BigqueryIO when using withPropagateSuccessfulStorageApiWrites(true) method (Java) (&lt;a href="https://github.com/apache/beam/pull/31840">#31840&lt;/a>).&lt;/li>
&lt;li>[Managed Iceberg] Added support for writing to partitioned tables (&lt;a href="https://github.com/apache/beam/pull/32102">#32102&lt;/a>)&lt;/li>
&lt;li>Update ClickHouseIO to use the latest version of the ClickHouse JDBC driver (&lt;a href="https://github.com/apache/beam/issues/32228">#32228&lt;/a>).&lt;/li>
&lt;li>Add ClickHouseIO dedicated User-Agent (&lt;a href="https://github.com/apache/beam/issues/32252">#32252&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>BigQuery endpoint can be overridden via PipelineOptions, this enables BigQuery emulators (Java) (&lt;a href="https://github.com/apache/beam/issues/28149">#28149&lt;/a>).&lt;/li>
&lt;li>Go SDK Minimum Go Version updated to 1.21 (&lt;a href="https://github.com/apache/beam/pull/32092">#32092&lt;/a>).&lt;/li>
&lt;li>[BigQueryIO] Added support for withFormatRecordOnFailureFunction() for STORAGE_WRITE_API and STORAGE_API_AT_LEAST_ONCE methods (Java) (&lt;a href="https://github.com/apache/beam/issues/31354">#31354&lt;/a>).&lt;/li>
&lt;li>Updated Go protobuf package to new version (Go) (&lt;a href="https://github.com/apache/beam/issues/21515">#21515&lt;/a>).&lt;/li>
&lt;li>Added support for setting a configureable timeout when loading a model and performing inference in the &lt;a href="https://beam.apache.org/documentation/ml/inference-overview/">RunInference&lt;/a> transform using &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference.with_exception_handling">with_exception_handling&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/32137">#32137&lt;/a>)&lt;/li>
&lt;li>Adds OrderedListState support for Java SDK via FnApi.&lt;/li>
&lt;li>Initial support for using Prism from the Python and Java SDKs.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed incorrect service account impersonation flow for Python pipelines using BigQuery IOs (&lt;a href="https://github.com/apache/beam/issues/32030">#32030&lt;/a>).&lt;/li>
&lt;li>Auto-disable broken and meaningless &lt;code>upload_graph&lt;/code> feature when using Dataflow Runner V2 (&lt;a href="https://github.com/apache/beam/issues/32159">#32159&lt;/a>).&lt;/li>
&lt;li>(Python) Upgraded google-cloud-storage to version 2.18.2 to fix a data corruption issue (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>).&lt;/li>
&lt;li>(Go) Fix corruption on State API writes. (&lt;a href="https://github.com/apache/beam/issues/32245">#32245&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Prism is under active development and does not yet support all pipelines. See &lt;a href="https://github.com/apache/beam/issues/29650">#29650&lt;/a> for progress.
&lt;ul>
&lt;li>In the 2.59.0 release, Prism passes most runner validations tests with the exceptions of pipelines using the following features:
OrderedListState, OnWindowExpiry (eg. GroupIntoBatches), CustomWindows, MergingWindowFns, Trigger and WindowingStrategy associated features, Bundle Finalization, Looping Timers, and some Coder related issues such as with Python combiner packing, and Java Schema transforms, and heterogenous flatten coders. Processing Time timers do not yet have real time support.&lt;/li>
&lt;li>If your pipeline is having difficulty with the Python or Java direct runners, but runs well on Prism, please let us know.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Java file-based IOs read or write lots (100k+) files could experience slowness and/or broken metrics visualization on Dataflow UI &lt;a href="https://github.com/apache/beam/issues/32649">#32649&lt;/a>.&lt;/li>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Managed Iceberg] DataFile metadata is assigned incorrect partition values (&lt;a href="https://github.com/apache/beam/issues/33497">#33497&lt;/a>).
&lt;ul>
&lt;li>Fixed in 2.62.0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[FileBasedIO] StringSet metrics can grow unlimitedly large when pipeline involves read/write large number of files, and degrading functionalities such us metrics monitoring and Dataflow job upgrade.
&lt;ul>
&lt;li>Mitigated in 2.60.0 (&lt;a href="https://github.com/apache/beam/issues/32649">#32649&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.59.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,Ahmet Altay,Andrew Crites,atask-g,Axel Magnuson,Ayush Pandey,Bartosz Zablocki,Chamikara Jayalath,cutiepie-10,Damon,Danny McCormick,dependabot[bot],Eddie Phillips,Francis O&amp;rsquo;Hara,Hyeonho Kim,Israel Herraiz,Jack McCluskey,Jaehyeon Kim,Jan Lukavský,Jeff Kinard,Jeffrey Kinard,jonathan-lemos,jrmccluskey,Kirill Berezin,Kiruphasankaran Nataraj,lahariguduru,liferoad,lostluck,Maciej Szwaja,Manit Gupta,Mark Zitnik,martin trieu,Naireen Hussain,Prerit Chandok,Radosław Stankiewicz,Rebecca Szper,Robert Bradshaw,Robert Burke,ron-gal,Sam Whittle,Sergei Lilichenko,Shunping Huang,Svetak Sundhar,Thiago Nunes,Timothy Itodo,tvalentyn,twosom,Vatsal,Vitaly Terentyev,Vlado Djerek,Yifan Ye,Yi Hu&lt;/p></description></item><item><title>Blog: Apache Beam 2.58.1</title><link>/blog/beam-2.58.1/</link><pubDate>Thu, 15 Aug 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.58.1/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.58.1 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2580-2024-08-06">download page&lt;/a> for this release.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Fixed issue where KafkaIO Records read with &lt;code>ReadFromKafkaViaSDF&lt;/code> are redistributed and may contain duplicates regardless of the configuration. This affects Java pipelines with Dataflow v2 runner and xlang pipelines reading from Kafka, (&lt;a href="https://github.com/apache/beam/issues/32196">#32196&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Large Dataflow graphs using runner v2, or pipelines explicitly enabling the &lt;code>upload_graph&lt;/code> experiment, will fail at construction time (&lt;a href="https://github.com/apache/beam/issues/32159">#32159&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.58.1 release. Thank you to all contributors!&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Sam Whittle&lt;/p></description></item><item><title>Blog: Apache Beam 2.58.0</title><link>/blog/beam-2.58.0/</link><pubDate>Tue, 06 Aug 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.58.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.58.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2580-2024-08-06">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information about changes in 2.58.0, check out the &lt;a href="https://github.com/apache/beam/milestone/22">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for &lt;a href="https://solace.com/">Solace&lt;/a> source (&lt;code>SolaceIO.Read&lt;/code>) added (Java) (&lt;a href="https://github.com/apache/beam/issues/31440">#31440&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Multiple RunInference instances can now share the same model instance by setting the model_identifier parameter (Python) (&lt;a href="https://github.com/apache/beam/issues/31665">#31665&lt;/a>).&lt;/li>
&lt;li>Added options to control the number of Storage API multiplexing connections (&lt;a href="https://github.com/apache/beam/pull/31721">#31721&lt;/a>)&lt;/li>
&lt;li>[BigQueryIO] Better handling for batch Storage Write API when it hits AppendRows throughput quota (&lt;a href="https://github.com/apache/beam/pull/31837">#31837&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] All specified catalog properties are passed through to the connector (&lt;a href="https://github.com/apache/beam/pull/31726">#31726&lt;/a>)&lt;/li>
&lt;li>Removed a third-party LGPL dependency from the Go SDK (&lt;a href="https://github.com/apache/beam/issues/31765">#31765&lt;/a>).&lt;/li>
&lt;li>Support for &lt;code>MapState&lt;/code> and &lt;code>SetState&lt;/code> when using Dataflow Runner v1 with Streaming Engine (Java) ([&lt;a href="https://github.com/apache/beam/issues/18200">#18200&lt;/a>])&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>[IcebergIO] &lt;code>IcebergCatalogConfig&lt;/code> was changed to support specifying catalog properties in a key-store fashion (&lt;a href="https://github.com/apache/beam/pull/31726">#31726&lt;/a>)&lt;/li>
&lt;li>[SpannerIO] Added validation that query and table cannot be specified at the same time for &lt;code>SpannerIO.read()&lt;/code>. Previously &lt;code>withQuery&lt;/code> overrides &lt;code>withTable&lt;/code>, if set (&lt;a href="https://github.com/apache/beam/issues/24956">#24956&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bug-fixes">Bug fixes&lt;/h2>
&lt;ul>
&lt;li>[BigQueryIO] Fixed a bug in batch Storage Write API that frequently exhausted concurrent connections quota (&lt;a href="https://github.com/apache/beam/pull/31710">#31710&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;li>[KafkaIO] Records read with &lt;code>ReadFromKafkaViaSDF&lt;/code> are redistributed and may contain duplicates regardless of the configuration. This affects Java pipelines with Dataflow v2 runner and xlang pipelines reading from Kafka, (&lt;a href="https://github.com/apache/beam/issues/32196">#32196&lt;/a>)&lt;/li>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.58.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexandre Moueddene&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Andrew Crites&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon Douglass&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Dilnaz Amanzholova&lt;/p>
&lt;p>Florian Bernard&lt;/p>
&lt;p>Francis O&amp;rsquo;Hara&lt;/p>
&lt;p>George Ma&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jaehyeon Kim&lt;/p>
&lt;p>James Roseman&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Maciej Szwaja&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Minh Son Nguyen&lt;/p>
&lt;p>Naireen&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Oliver Cardoza&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rohit Sinha&lt;/p>
&lt;p>S. Veyrié&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>TongruiLi&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Yi Hu&lt;/p></description></item><item><title>Blog: Apache Beam 2.57.0</title><link>/blog/beam-2.57.0/</link><pubDate>Wed, 26 Jun 2024 13:00:00 -0800</pubDate><guid>/blog/beam-2.57.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.57.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2570-2024-06-26">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.57.0, check out the &lt;a href="https://github.com/apache/beam/milestone/21">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam adds Python 3.12 support (&lt;a href="https://github.com/apache/beam/issues/29149">#29149&lt;/a>).&lt;/li>
&lt;li>Added FlinkRunner for Flink 1.18 (&lt;a href="https://github.com/apache/beam/issues/30789">#30789&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Ensure that BigtableIO closes the reader streams (&lt;a href="https://github.com/apache/beam/issues/31477">#31477&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added Feast feature store handler for enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/issues/30964">#30957&lt;/a>).&lt;/li>
&lt;li>BigQuery per-worker metrics are reported by default for Streaming Dataflow Jobs (Java) (&lt;a href="https://github.com/apache/beam/pull/31015">#31015&lt;/a>)&lt;/li>
&lt;li>Adds &lt;code>inMemory()&lt;/code> variant of Java List and Map side inputs for more efficient lookups when the entire side input fits into memory.&lt;/li>
&lt;li>Beam YAML now supports the jinja templating syntax.
Template variables can be passed with the (json-formatted) &lt;code>--jinja_variables&lt;/code> flag.&lt;/li>
&lt;li>DataFrame API now supports pandas 2.1.x and adds 12 more string functions for Series.(&lt;a href="https://github.com/apache/beam/pull/31185">#31185&lt;/a>).&lt;/li>
&lt;li>Added BigQuery handler for enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/pull/31295">#31295&lt;/a>)&lt;/li>
&lt;li>Disable soft delete policy when creating the default bucket for a project (Java) (&lt;a href="https://github.com/apache/beam/pull/31324">#31324&lt;/a>).&lt;/li>
&lt;li>Added &lt;code>DoFn.SetupContextParam&lt;/code> and &lt;code>DoFn.BundleContextParam&lt;/code> which can be used
as a python &lt;code>DoFn.process&lt;/code>, &lt;code>Map&lt;/code>, or &lt;code>FlatMap&lt;/code> parameter to invoke a context
manager per DoFn setup or bundle (analogous to using &lt;code>setup&lt;/code>/&lt;code>teardown&lt;/code>
or &lt;code>start_bundle&lt;/code>/&lt;code>finish_bundle&lt;/code> respectively.)&lt;/li>
&lt;li>Go SDK Prism Runner
&lt;ul>
&lt;li>Pre-built Prism binaries are now part of the release and are available via the Github release page. (&lt;a href="https://github.com/apache/beam/issues/29697">#29697&lt;/a>).&lt;/li>
&lt;li>ProcessingTime is now handled synthetically with TestStream pipelines and Non-TestStream pipelines, for fast test pipeline execution by default. (&lt;a href="https://github.com/apache/beam/issues/30083">#30083&lt;/a>).
&lt;ul>
&lt;li>Prism does NOT yet support &amp;ldquo;real time&amp;rdquo; execution for this release.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Improve processing for large elements to reduce the chances for exceeding 2GB protobuf limits (Python)([https://github.com/apache/beam/issues/31607]).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Java&amp;rsquo;s View.asList() side inputs are now optimized for iterating rather than
indexing when in the global window.
This new implementation still supports all (immutable) List methods as before,
but some of the random access methods like get() and size() will be slower.
To use the old implementation one can use View.asList().withRandomAccess().&lt;/li>
&lt;li>SchemaTransforms implemented with TypedSchemaTransformProvider now produce a
configuration Schema with snake_case naming convention
(&lt;a href="https://github.com/apache/beam/pull/31374">#31374&lt;/a>). This will make the following
cases problematic:
&lt;ul>
&lt;li>Running a pre-2.57.0 remote SDK pipeline containing a 2.57.0+ Java SchemaTransform,
and vice versa:&lt;/li>
&lt;li>Running a 2.57.0+ remote SDK pipeline containing a pre-2.57.0 Java SchemaTransform&lt;/li>
&lt;li>All direct uses of Python&amp;rsquo;s &lt;a href="https://github.com/apache/beam/blob/a998107a1f5c3050821eef6a5ad5843d8adb8aec/sdks/python/apache_beam/transforms/external.py#L381">SchemaAwareExternalTransform&lt;/a>
should be updated to use new snake_case parameter names.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Upgraded Jackson Databind to 2.15.4 (Java) (&lt;a href="https://github.com/apache/beam/issues/26743">#26743&lt;/a>).
jackson-2.15 has known breaking changes. An important one is it imposed a buffer limit for parser.
If your custom PTransform/DoFn are affected, refer to &lt;a href="https://github.com/apache/beam/pull/31580">#31580&lt;/a> for mitigation.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;li>BigQuery Enrichment (Python): The following issues are present when using the BigQuery enrichment transform (&lt;a href="https://github.com/apache/beam/pull/32780">#32780&lt;/a>):
&lt;ul>
&lt;li>Duplicate Rows: Multiple conditions may be applied incorrectly, leading to the duplication of rows in the output.&lt;/li>
&lt;li>Incorrect Results with Batched Requests: Conditions may not be correctly scoped to individual rows within the batch, potentially causing inaccurate results.&lt;/li>
&lt;li>Fixed in 2.61.0.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.57.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anody Zhang&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Ben Konz&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Colm O hEigeartaigh&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Justin Uang&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kevin Zhou&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Maarten Vercruysse&lt;/p>
&lt;p>Maciej Szwaja&lt;/p>
&lt;p>Maja Kontrec Rönn&lt;/p>
&lt;p>Marc hurabielle&lt;/p>
&lt;p>Martin Trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Min Zhu&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nick Anikin&lt;/p>
&lt;p>Pablo Rodriguez Defino&lt;/p>
&lt;p>Paul King&lt;/p>
&lt;p>Priyans Desai&lt;/p>
&lt;p>Radosław Stankiewicz&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rodrigo Bozzolo&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shahar Epstein&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tomo Suzuki&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vincent Stollenwerk&lt;/p>
&lt;p>Vineet Kumar&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>kberezin&lt;/p></description></item><item><title>Blog: Deploy Python pipelines on Kubernetes using the Flink runner</title><link>/blog/deploy-python-pipeline-on-flink-runner/</link><pubDate>Thu, 20 Jun 2024 13:56:15 +1000</pubDate><guid>/blog/deploy-python-pipeline-on-flink-runner/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="deploy-python-pipelines-on-kubernetes-using-the-flink-runner">Deploy Python pipelines on Kubernetes using the Flink runner&lt;/h1>
&lt;p>The &lt;a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/concepts/overview/">Apache Flink Kubernetes Operator&lt;/a> acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. With the operator, we can simplify the deployment and management of Apache Beam pipelines.&lt;/p>
&lt;p>In this post, we develop an &lt;a href="https://beam.apache.org/">Apache Beam&lt;/a> pipeline using the &lt;a href="https://beam.apache.org/documentation/sdks/python/">Python SDK&lt;/a> and deploy it on an &lt;a href="https://flink.apache.org/">Apache Flink&lt;/a> cluster by using the &lt;a href="https://beam.apache.org/documentation/runners/flink/">Apache Flink runner&lt;/a>. We first deploy an &lt;a href="https://kafka.apache.org/">Apache Kafka&lt;/a> cluster on a &lt;a href="https://minikube.sigs.k8s.io/docs/">minikube&lt;/a> cluster, because the pipeline uses Kafka topics for its data source and sink. Then, we develop the pipeline as a Python package and add the package to a custom Docker image so that Python user code can be executed externally. For deployment, we create a Flink session cluster using the Flink Kubernetes Operator, and deploy the pipeline using a Kubernetes job. Finally, we check the output of the application by sending messages to the input Kafka topic using a Python producer application.&lt;/p>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#resources-to-run-a-python-beam-pipeline-on-flink">Resources to run a Python Beam pipeline on Flink&lt;/a>&lt;/li>
&lt;li>&lt;a href="#set-up-the-kafka-cluster">Set up the Kafka cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#deploy-the-strimzi-operator">Deploy the Strimzi operator&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deploy-the-kafka-cluster">Deploy the Kafka cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deploy-the-kafka-ui">Deploy the Kafka UI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#develop-a-stream-processing-app">Develop a stream processing app&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#beam-pipeline-code">Beam pipeline code&lt;/a>&lt;/li>
&lt;li>&lt;a href="#build-docker-images">Build Docker images&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#deploy-the-stream-processing-app">Deploy the stream processing app&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#deploy-the-flink-kubernetes-operator">Deploy the Flink Kubernetes Operator&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deploy-the-beam-pipeline">Deploy the Beam pipeline&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kafka-producer">Kafka producer&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h2 id="resources-to-run-a-python-beam-pipeline-on-flink">Resources to run a Python Beam pipeline on Flink&lt;/h2>
&lt;p>We develop an Apache Beam pipeline using the Python SDK and deploy it on an Apache Flink cluster using the Apache Flink runner. Although the Flink cluster is created by the Flink Kubernetes Operator, we need two components to run the pipeline on the &lt;em>Flink runner&lt;/em>: the &lt;strong>job service&lt;/strong> and the &lt;a href="https://beam.apache.org/documentation/runtime/sdk-harness-config/">&lt;strong>SDK harness&lt;/strong>&lt;/a>. Roughly speaking, the job service converts details about a Python pipeline into a format that the Flink runner can understand. The SDK harness executes the Python user code. The Python SDK provides convenience wrappers to manage those components, and you can use it by specifying &lt;em>FlinkRunner&lt;/em> in the pipeline option, for example, &lt;code>--runner=FlinkRunner&lt;/code>. The &lt;em>job service&lt;/em> is managed automatically. We rely on our own &lt;em>SDK harness&lt;/em> as a sidecar container for simplicity. Also, we need the &lt;strong>Java IO Expansion Service&lt;/strong>, because the pipeline uses Apache Kafka topics for its data source and sink, and the Kafka Connector I/O is developed in Java. Simply put, the expansion service is used to serialize data for the Java SDK.&lt;/p>
&lt;h2 id="set-up-the-kafka-cluster">Set up the Kafka cluster&lt;/h2>
&lt;p>An Apache Kafka cluster is deployed using the &lt;a href="https://strimzi.io/">Strimzi Operator&lt;/a> on a minikube cluster. We install Strimzi version 0.39.0 and Kubernetes version 1.25.3. After the &lt;a href="https://minikube.sigs.k8s.io/docs/start/">minikube CLI&lt;/a> and &lt;a href="https://www.docker.com/">Docker&lt;/a> are installed, you can create a minikube cluster by specifying the Kubernetes version. You can find the source code for this blog post in the &lt;a href="https://github.com/jaehyeon-kim/beam-demos/tree/master/beam-deploy">GitHub repository&lt;/a>.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">minikube start --cpus&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;max&amp;#39;&lt;/span> --memory&lt;span class="o">=&lt;/span>&lt;span class="m">20480&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --addons&lt;span class="o">=&lt;/span>metrics-server --kubernetes-version&lt;span class="o">=&lt;/span>v1.25.3&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-strimzi-operator">Deploy the Strimzi operator&lt;/h3>
&lt;p>The GitHub repository keeps manifest files that you can use to deploy the Strimzi operator, Kafka cluster, and Kafka management application. To download a different version of the operator, download the relevant manifest file by specifying the version. By default, the manifest file assumes that the resources are deployed in the &lt;em>myproject&lt;/em> namespace. However, because we deploy them in the &lt;em>default&lt;/em> namespace, we need to change the resource namespace. We change the resource namespace using &lt;a href="https://www.gnu.org/software/sed/manual/sed.html">sed&lt;/a>.&lt;/p>
&lt;p>To deploy the operator, use the &lt;code>kubectl create&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Download and deploy the Strimzi operator.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">STRIMZI_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.39.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Optional: If downloading a different version, include this step.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">DOWNLOAD_URL&lt;/span>&lt;span class="o">=&lt;/span>https://github.com/strimzi/strimzi-kafka-operator/releases/download/&lt;span class="nv">$STRIMZI_VERSION&lt;/span>/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">curl -L -o kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> &lt;span class="si">${&lt;/span>&lt;span class="nv">DOWNLOAD_URL&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Update the namespace from myproject to default.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sed -i &lt;span class="s1">&amp;#39;s/namespace: .*/namespace: default/&amp;#39;&lt;/span> kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Deploy the Strimzi cluster operator.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create -f kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Verify that the Strimzi Operator runs as a Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">deployment&lt;/a>.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get deploy,rs,po
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY UP-TO-DATE AVAILABLE AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># deployment.apps/strimzi-cluster-operator 1/1 1 1 2m50s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME DESIRED CURRENT READY AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># replicaset.apps/strimzi-cluster-operator-8d6d4795c 1 1 1 2m50s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/strimzi-cluster-operator-8d6d4795c-94t8c 1/1 Running 0 2m49s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-kafka-cluster">Deploy the Kafka cluster&lt;/h3>
&lt;p>We deploy a Kafka cluster with a single broker and Zookeeper node. It has both internal and external listeners on ports 9092 and 29092, respectively. The external listener is used to access the Kafka cluster outside the minikube cluster. Also, the cluster is configured to allow automatic creation of topics (&lt;code>auto.create.topics.enable: &amp;quot;true&amp;quot;&lt;/code>), and the default number of partitions is set to 3 (&lt;code>num.partitions: 3&lt;/code>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># kafka/manifests/kafka-cluster.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka.strimzi.io/v1beta2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Kafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">kafka&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3.5.2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">256Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">250m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">512Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">500m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">listeners&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">plain&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">9092&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">internal&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">tls&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">external&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">29092&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nodeport&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">tls&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">storage&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">jbod&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">id&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">persistent-claim&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">size&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">20Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">deleteClaim&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">offsets.topic.replication.factor&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transaction.state.log.replication.factor&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transaction.state.log.min.isr&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">default.replication.factor&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">min.insync.replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">inter.broker.protocol.version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;3.5&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">auto.create.topics.enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;true&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">num.partitions&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">zookeeper&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">256Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">250m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">512Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">500m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">storage&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">persistent-claim&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">size&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">10Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">deleteClaim&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy he Kafka cluster using the &lt;code>kubectl create&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f kafka/manifests/kafka-cluster.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>The Kafka and Zookeeper nodes are managed by the &lt;a href="https://strimzi.io/docs/operators/latest/configuring.html#type-StrimziPodSet-reference">&lt;em>StrimziPodSet&lt;/em>&lt;/a> custom resource. It also creates multiple Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">services&lt;/a>. In this series, we use the following services:&lt;/p>
&lt;ul>
&lt;li>communication within the Kubernetes cluster
&lt;ul>
&lt;li>&lt;code>demo-cluster-kafka-bootstrap&lt;/code> - to access Kafka brokers from the client and management apps&lt;/li>
&lt;li>&lt;code>demo-cluster-zookeeper-client&lt;/code> - to access Zookeeper node from the management app&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>communication from the host
&lt;ul>
&lt;li>&lt;code>demo-cluster-kafka-external-bootstrap&lt;/code> - to access Kafka brokers from the producer app&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get po,strimzipodsets.core.strimzi.io,svc -l app.kubernetes.io/instance&lt;span class="o">=&lt;/span>demo-cluster
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/demo-cluster-kafka-0 1/1 Running 0 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/demo-cluster-zookeeper-0 1/1 Running 0 2m20s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME PODS READY PODS CURRENT PODS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># strimzipodset.core.strimzi.io/demo-cluster-kafka 1 1 1 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># strimzipodset.core.strimzi.io/demo-cluster-zookeeper 1 1 1 2m20s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-bootstrap ClusterIP 10.101.175.64 &amp;lt;none&amp;gt; 9091/TCP,9092/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-brokers ClusterIP None &amp;lt;none&amp;gt; 9090/TCP,9091/TCP,8443/TCP,9092/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-external-0 NodePort 10.106.155.20 &amp;lt;none&amp;gt; 29092:32475/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-kafka-external-bootstrap NodePort 10.111.244.128 &amp;lt;none&amp;gt; 29092:32674/TCP 115s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-zookeeper-client ClusterIP 10.100.215.29 &amp;lt;none&amp;gt; 2181/TCP 2m20s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/demo-cluster-zookeeper-nodes ClusterIP None &amp;lt;none&amp;gt; 2181/TCP,2888/TCP,3888/TCP 2m20s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-kafka-ui">Deploy the Kafka UI&lt;/h3>
&lt;p>&lt;a href="https://docs.kafka-ui.provectus.io/overview/readme">UI for Apache Kafka (&lt;code>kafka-ui&lt;/code>)&lt;/a> is a free and open-source Kafka management application. It&amp;rsquo;s deployed as a Kubernetes Deployment. The Deployment is configured to have a single instance, and the Kafka cluster access details are specified as environment variables.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># kafka/manifests/kafka-ui.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Service&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ClusterIP&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">8080&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">targetPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">8080&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nn">---&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">provectuslabs/kafka-ui:v0.7.1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kafka-ui-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">containerPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">8080&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">env&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">KAFKA_CLUSTERS_0_NAME&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster-kafka-bootstrap:9092&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">KAFKA_CLUSTERS_0_ZOOKEEPER&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">demo-cluster-zookeeper-client:2181&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">256Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">250m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">512Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">500m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy the Kafka management app (&lt;code>kafka-ui&lt;/code>) using the &lt;code>kubectl create&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f kafka/manifests/kafka-ui.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl get all -l &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>kafka-ui
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/kafka-ui-65dbbc98dc-zl5gv 1/1 Running 0 35s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/kafka-ui ClusterIP 10.109.14.33 &amp;lt;none&amp;gt; 8080/TCP 36s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY UP-TO-DATE AVAILABLE AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># deployment.apps/kafka-ui 1/1 1 1 35s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME DESIRED CURRENT READY AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># replicaset.apps/kafka-ui-65dbbc98dc 1 1 1 35s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>We use &lt;code>kubectl port-forward&lt;/code> to connect to the &lt;code>kafka-ui&lt;/code> server running in the minikube cluster on port 8080.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl port-forward svc/kafka-ui &lt;span class="m">8080&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/kafka-ui.png"
alt="Kafka UI">&lt;/p>
&lt;h2 id="develop-a-stream-processing-app">Develop a stream processing app&lt;/h2>
&lt;p>We develop an Apache Beam pipeline as a Python package and add it to a custom Docker image, which is used to execute Python user code (&lt;em>SDK harness&lt;/em>). We also build another custom Docker image, which adds the Java SDK of Apache Beam to the official Flink base image. This image is used to deploy a Flink cluster and to execute Java user code of the &lt;em>Kafka Connector I/O&lt;/em>.&lt;/p>
&lt;h3 id="beam-pipeline-code">Beam pipeline code&lt;/h3>
&lt;p>The application first reads text messages from an input Kafka topic. Next, it extracts words by splitting the messages (&lt;code>ReadWordsFromKafka&lt;/code>). Then, the elements (words) are added to a fixed time window of 5 seconds, and their average length is calculated (&lt;code>CalculateAvgWordLen&lt;/code>). Finally, we include the window start and end timestamps, and send the updated element to an output Kafka topic (&lt;code>WriteWordLenToKafka&lt;/code>).&lt;/p>
&lt;p>We create a custom &lt;em>Java IO Expansion Service&lt;/em> (&lt;code>get_expansion_service&lt;/code>) and add it to the &lt;code>ReadFromKafka&lt;/code> and &lt;code>WriteToKafka&lt;/code> transforms of the Kafka Connector I/O. Although the Kafka I/O provides a function to create that service, it did not work for me (or I do not understand how to make use of it yet). Instead, I created a custom service, as illustrated in &lt;a href="https://www.packtpub.com/product/building-big-data-pipelines-with-apache-beam/9781800564930">Building Big Data Pipelines with Apache Beam by Jan Lukavský&lt;/a>. The expansion service Jar file (&lt;code>beam-sdks-java-io-expansion-service.jar&lt;/code>) must exist in the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">&lt;em>job&lt;/em>&lt;/a> that executes the pipeline, while the Java SDK (&lt;code>/opt/apache/beam/boot&lt;/code>) must exist in the runner worker.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># beam/word_len/word_len.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">argparse&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">re&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">logging&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">typing&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">beam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">pvalue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.io&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">kafka&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.transforms.window&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">FixedWindows&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">PipelineOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">SetupOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.transforms.external&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">JavaJarExpansionService&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_expansion_service&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">jar&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/opt/apache/beam/jars/beam-sdks-java-io-expansion-service.jar&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">args&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--defaultEnvironmentType=PROCESS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;--defaultEnvironmentConfig={&amp;#34;command&amp;#34;: &amp;#34;/opt/apache/beam/boot&amp;#34;}&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--experiments=use_deprecated_read&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">JavaJarExpansionService&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">jar&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;{{PORT}}&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">length&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">count&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">WordAccum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RowCoder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">decode_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kafka_kv&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kafka_kv&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">kafka_kv&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">tokenize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">re&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">findall&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">r&lt;/span>&lt;span class="s2">&amp;#34;[A-Za-z\&amp;#39;]+&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">create_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">]):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">msg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dumps&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">dict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">zip&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="s2">&amp;#34;window_start&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;window_end&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;avg_len&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">msg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">msg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">AverageFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombineFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">create_accumulator&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">add_input&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mutable_accumulator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mutable_accumulator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">length&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">merge_accumulators&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">accumulators&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">WordAccum&lt;/span>&lt;span class="p">]):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lengths&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">counts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">zip&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">accumulators&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lengths&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">counts&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">extract_output&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">accumulator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">WordAccum&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">accumulator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">length&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;NaN&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">get_accumulator_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">WordAccum&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">AddWindowTS&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">avg_len&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">win_param&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowParam&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">yield&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">win_param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to_rfc3339&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">win_param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to_rfc3339&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">avg_len&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ReadWordsFromKafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topics&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">group_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Any&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">label&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">label&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topics&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">topics&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">group_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">group_id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">verbose&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">verbose&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expansion_service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PBegin&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;ReadFromKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">kafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">consumer_config&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;bootstrap.servers&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;auto.offset.reset&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;latest&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># &amp;#34;enable.auto.commit&amp;#34;: &amp;#34;true&amp;#34;,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;group.id&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">group_id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topics&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topics&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">timestamp_policy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">kafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromKafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create_time_policy&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">commit_offset_in_finalize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;DecodeMessage&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">decode_message&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;Tokenize&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">FlatMap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokenize&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">CalculateAvgWordLen&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PCollection&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;Windowing&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowInto&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FixedWindows&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;GetAvgWordLength&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CombineGlobally&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">AverageFn&lt;/span>&lt;span class="p">())&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">without_defaults&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">WriteWordLenToKafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PTransform&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topic&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Any&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">label&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">label&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">topic&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expansion_service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">pvalue&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PCollection&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;AddWindowTS&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">AddWindowTS&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;CreateMessages&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">create_message&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">typing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">bytes&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">bytes&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;WriteToKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">kafka&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">producer_config&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;bootstrap.servers&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">boostrap_servers&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topic&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">argv&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">save_main_session&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">argparse&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ArgumentParser&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">description&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Beam pipeline arguments&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--deploy&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;deploy&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;store_true&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Flag to indicate whether to deploy to a cluster&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--bootstrap_servers&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;bootstrap&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;host.docker.internal:29092&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka bootstrap server addresses&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--input_topic&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;input&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;input-topic&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka input topic name&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--output_topic&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;output&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;output-topic-beam&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka output topic name&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_argument&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;--group_id&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dest&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;group&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;beam-word-len&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">help&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Kafka output group ID&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">known_args&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pipeline_args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">parser&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parse_known_args&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">argv&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pipeline_args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># We use the save_main_session option because one or more DoFn elements in this&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># workflow rely on global context. That is, a module imported at the module level.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pipeline_options&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">PipelineOptions&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pipeline_args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pipeline_options&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">SetupOptions&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save_main_session&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">save_main_session&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deploy&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_expansion_service&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">options&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">pipeline_options&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;ReadWordsFromKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">ReadWordsFromKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topics&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">group_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">group&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;CalculateAvgWordLen&amp;#34;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">CalculateAvgWordLen&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s2">&amp;#34;WriteWordLenToKafka&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">WriteWordLenToKafka&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">topic&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">known_args&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expansion_service&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">expansion_service&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logging&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getLogger&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">setLevel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logging&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logging&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">info&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Building pipeline ...&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="vm">__name__&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;__main__&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The pipeline script is added to a Python package under a folder named &lt;code>word_len&lt;/code>. A simple module named &lt;code>run&lt;/code> is created, because it is executed as a module, for example, &lt;code>python -m ...&lt;/code>. When I ran the pipeline as a script, I encountered an error. This packaging method is for demonstration only. For a recommended way of packaging a pipeline, see &lt;a href="https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/">Managing Python Pipeline Dependencies&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># beam/word_len/run.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">.&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="o">*&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Overall, the pipeline package uses the following structure.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">tree beam/word_len
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">beam/word_len
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">├── __init__.py
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">├── run.py
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">└── word_len.py&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="build-docker-images">Build Docker images&lt;/h3>
&lt;p>As discussed previously, we build a custom Docker image (&lt;em>beam-python-example:1.16&lt;/em>) and use it to deploy a Flink cluster and to run the Java user code of the Kafka Connector I/O.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/Dockerfile&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> flink:1.16&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> --from&lt;span class="o">=&lt;/span>apache/beam_java11_sdk:2.56.0 /opt/apache/beam/ /opt/apache/beam/&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We also build a custom Docker image (&lt;em>beam-python-harness:2.56.0&lt;/em>) to run Python user code (&lt;em>SDK harness&lt;/em>). From the Python SDK Docker image, it first installs the Java Development Kit (JDK) and downloads the &lt;em>Java IO Expansion Service&lt;/em> Jar file. Then, the Beam pipeline packages are copied to the &lt;code>/app&lt;/code> folder. The app folder is added to the &lt;code>PYTHONPATH&lt;/code> environment variable, which makes the packages searchable.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/Dockerfile-python-harness&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> apache/beam_python3.10_sdk:2.56.0&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ARG&lt;/span> BEAM_VERSION&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="k">:-&lt;/span>&lt;span class="nv">2&lt;/span>&lt;span class="p">.56.0&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">REPO_BASE_URL&lt;/span>&lt;span class="o">=&lt;/span>https://repo1.maven.org/maven2/org/apache/beam&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> apt-get update &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> apt-get install -y default-jdk&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> mkdir -p /opt/apache/beam/jars &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> wget &lt;span class="si">${&lt;/span>&lt;span class="nv">REPO_BASE_URL&lt;/span>&lt;span class="si">}&lt;/span>/beam-sdks-java-io-expansion-service/&lt;span class="si">${&lt;/span>&lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="si">}&lt;/span>/beam-sdks-java-io-expansion-service-&lt;span class="si">${&lt;/span>&lt;span class="nv">BEAM_VERSION&lt;/span>&lt;span class="si">}&lt;/span>.jar &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --progress&lt;span class="o">=&lt;/span>bar:force:noscroll -O /opt/apache/beam/jars/beam-sdks-java-io-expansion-service.jar&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> word_len /app/word_len&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> word_count /app/word_count&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">PYTHONPATH&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="nv">$PYTHONPATH&lt;/span>&lt;span class="s2">:/app&amp;#34;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Because the custom images need to be accessible in the minikube cluster, we point the terminal&amp;rsquo;s &lt;code>docker-cli&lt;/code> to the minikube&amp;rsquo;s Docker engine. Then, we can build the images using the &lt;code>docker build&lt;/code> command.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">eval&lt;/span> &lt;span class="k">$(&lt;/span>minikube docker-env&lt;span class="k">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker build -t beam-python-example:1.16 beam/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker build -t beam-python-harness:2.56.0 -f beam/Dockerfile-python-harness beam/&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="deploy-the-stream-processing-app">Deploy the stream processing app&lt;/h2>
&lt;p>The Beam pipeline is executed on a &lt;a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/overview/#session-cluster-deployments">Flink session cluster&lt;/a>, which is deployed by the Flink Kubernetes Operator. The &lt;a href="https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/overview/#application-deployments">application deployment mode&lt;/a> where the Beam pipeline is deployed as a Flink job doesn&amp;rsquo;t seem to work (or I don&amp;rsquo;t understand how to do so yet) due to either a job submission timeout error or a failure to upload the job artifact. After the pipeline is deployed, we check the output of the application by sending text messages to the input Kafka topic.&lt;/p>
&lt;h3 id="deploy-the-flink-kubernetes-operator">Deploy the Flink Kubernetes Operator&lt;/h3>
&lt;p>First, to make it possible to add the webhook component, install the &lt;a href="https://github.com/cert-manager/cert-manager">certificate manager&lt;/a> on the minikube cluster. Then, use a Helm chart to install the operator. Version 1.8.0 is installed in the post.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.8.0/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm install flink-kubernetes-operator flink-operator-repo/flink-kubernetes-operator
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME: flink-kubernetes-operator&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># LAST DEPLOYED: Mon Jun 03 21:37:45 2024&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAMESPACE: default&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># STATUS: deployed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># REVISION: 1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># TEST SUITE: None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm list
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># flink-kubernetes-operator default 1 2024-06-03 21:37:45.579302452 +1000 AEST deployed flink-kubernetes-operator-1.8.0 1.8.0&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="deploy-the-beam-pipeline">Deploy the Beam pipeline&lt;/h3>
&lt;p>First, create a Flink session cluster. In the manifest file, configure common properties, such as the Docker image, Flink version, cluster configuration, and pod template. These properties are applied to the Flink job manager and task manager. In addition, specify the replica and resource. We add a sidecar container to the task manager, and this &lt;em>SDK harness&lt;/em> container is configured to execute Python user code - see the following job configuration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/word_len_cluster.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink.apache.org/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FlinkDeployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">word-len-cluster&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-python-example:1.16&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">imagePullPolicy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Never&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1_16&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskmanager.numberOfTaskSlots&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;10&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">serviceAccount&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-main-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeMounts&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/opt/flink/log&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-logs&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-logs&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">emptyDir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>{}&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jobManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048Mi&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048Mi&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">python-harness&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-python-harness:2.56.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">args&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;-worker_pool&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ports&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">containerPort&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">50000&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">harness-port&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The pipeline is deployed using a Kubernetes job, and the custom &lt;em>SDK harness&lt;/em> image is used to execute the pipeline as a module. The first two arguments are application-specific. The rest of the arguments are for pipeline options. For more information about the pipeline arguments, see the &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/options/pipeline_options.py">pipeline options source&lt;/a> and &lt;a href="https://beam.apache.org/documentation/runners/flink/">Flink Runner document&lt;/a>. To execute Python user code in the sidecar container, we set the environment type to &lt;code>EXTERNAL&lt;/code> and the environment config to &lt;code>localhost:50000&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># beam/word_len_job.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">batch/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">word-len-job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">app&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">word-len-job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-word-len-job&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">beam-python-harness:2.56.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">command&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;python&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">args&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;-m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;word_len.run&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--deploy&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--bootstrap_servers=demo-cluster-kafka-bootstrap:9092&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--runner=FlinkRunner&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--flink_master=word-len-cluster-rest:8081&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--job_name=beam-word-len&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--streaming&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--parallelism=3&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--flink_submit_uber_jar&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--environment_type=EXTERNAL&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--environment_config=localhost:50000&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;--checkpointing_interval=10000&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">restartPolicy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Never&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy the session cluster and job using the &lt;code>kubectl create&lt;/code> command. The session cluster is created by the &lt;em>FlinkDeployment&lt;/em> custom resource, and it manages the job manager deployment, task manager pod, and associated services. When we check the log of the job&amp;rsquo;s pod, we see that it does the following tasks:&lt;/p>
&lt;ul>
&lt;li>starts the &lt;em>Job Service&lt;/em> after downloading the Jar file&lt;/li>
&lt;li>uploads the pipeline artifact&lt;/li>
&lt;li>submits the pipeline as a Flink job&lt;/li>
&lt;li>continuously monitors the job status&lt;/li>
&lt;/ul>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create -f beam/word_len_cluster.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># flinkdeployment.flink.apache.org/word-len-cluster created&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create -f beam/word_len_job.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># job.batch/word-len-job created&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl logs word-len-job-p5rph -f
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:root:Building pipeline ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.flink_runner:Adding HTTP protocol scheme to flink_master parameter: http://word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: --checkpointing_interval=10000. Ignore if flags are used for internal purposes.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:apache_beam.runners.portability.abstract_job_service:Got Prepare request.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/config HTTP/1.1&amp;#34; 200 240&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.utils.subprocess_server:Downloading job server jar from https://repo.maven.apache.org/maven2/org/apache/beam/beam-runners-flink-1.16-job-server/2.56.0/beam-runners-flink-1.16-job-server-2.56.0.jar&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.abstract_job_service:Artifact server started on port 43287&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:apache_beam.runners.portability.abstract_job_service:Prepared job &amp;#39;job&amp;#39; as &amp;#39;job-edc1c2f1-80ef-48b7-af14-7e6fc86f338a&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.abstract_job_service:Running job &amp;#39;job-edc1c2f1-80ef-48b7-af14-7e6fc86f338a&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;POST /v1/jars/upload HTTP/1.1&amp;#34; 200 148&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;POST /v1/jars/e1984c45-d8bc-4aa1-9b66-369a23826921_beam.jar/run HTTP/1.1&amp;#34; 200 44&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.flink_uber_jar_job_server:Started Flink job as a403cb2f92fecee65b8fd7cc8ac6e68a&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): word-len-cluster-rest:8081&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># DEBUG:urllib3.connectionpool:http://word-len-cluster-rest:8081 &amp;#34;GET /v1/jobs/a403cb2f92fecee65b8fd7cc8ac6e68a/execution-result HTTP/1.1&amp;#34; 200 31&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ...&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>After the deployment completes, we can see the following Flink session cluster and job related resources.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get all -l &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>word-len-cluster
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/word-len-cluster-7c98f6f868-d4hbx 1/1 Running 0 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/word-len-cluster-taskmanager-1-1 2/2 Running 0 4m3s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/word-len-cluster ClusterIP None &amp;lt;none&amp;gt; 6123/TCP,6124/TCP 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># service/word-len-cluster-rest ClusterIP 10.104.23.28 &amp;lt;none&amp;gt; 8081/TCP 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY UP-TO-DATE AVAILABLE AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># deployment.apps/word-len-cluster 1/1 1 1 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME DESIRED CURRENT READY AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># replicaset.apps/word-len-cluster-7c98f6f868 1 1 1 5m32s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl get all -l &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>word-len-job
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME READY STATUS RESTARTS AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pod/word-len-job-24r6q 1/1 Running 0 5m24s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># NAME COMPLETIONS DURATION AGE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># job.batch/word-len-job 0/1 5m24s 5m24s&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>You can access the Flink web UI using the &lt;code>kubectl port-forward&lt;/code> command on port 8081. The job graph shows two tasks. The first task adds word elements into a fixed time window. The second task sends the average word length records to the output topic.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl port-forward svc/flink-word-len-rest &lt;span class="m">8081&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/flink-ui.png"
alt="Flink UI">&lt;/p>
&lt;p>The Kafka I/O automatically creates a topic if it doesn&amp;rsquo;t exist, and we can see the input topic is created on &lt;code>kafka-ui&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/kafka-topics-1.png"
alt="Kafka Input Topic">&lt;/p>
&lt;h3 id="kafka-producer">Kafka producer&lt;/h3>
&lt;p>A simple Python Kafka producer is created to check the output of the application. By default, the producer app sends random text from the &lt;a href="https://faker.readthedocs.io/en/master/">Faker&lt;/a> package to the input Kafka topic every one second.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># kafka/client/producer.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">os&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">faker&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Faker&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">kafka&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">KafkaProducer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">TextProducer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">topic_name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap_servers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">bootstrap_servers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic_name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">topic_name&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kafka_producer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create_producer&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">create_producer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Returns a KafkaProducer instance
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">KafkaProducer&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bootstrap_servers&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_serializer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">send_to_kafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">timestamp_ms&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Sends text to a Kafka topic.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;topic&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topic_name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;value&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">timestamp_ms&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">args&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;timestamp_ms&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">timestamp_ms&lt;/span>&lt;span class="p">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kafka_producer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kafka_producer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flush&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">except&lt;/span> &lt;span class="ne">Exception&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">raise&lt;/span> &lt;span class="ne">RuntimeError&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;fails to send a message&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">e&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="vm">__name__&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;__main__&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">producer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TextProducer&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">os&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getenv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;BOOTSTRAP_SERVERS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;localhost:29092&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">os&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getenv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;TOPIC_NAME&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;input-topic&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fake&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Faker&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_events&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fake&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">producer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send_to_kafka&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">num_events&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">num_events&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> text sent... current&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">os&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getenv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;DELAY_SECONDS&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;1&amp;#34;&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Expose the Kafka bootstrap server on port 29092 using the &lt;code>kubectl port-forward&lt;/code> command. Execute the Python script to start the producer app.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl port-forward svc/demo-cluster-kafka-external-bootstrap &lt;span class="m">29092&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python kafka/client/producer.py&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>We can see the output topic (&lt;code>output-topic-beam&lt;/code>) is created on &lt;code>kafka-ui&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/kafka-topics-2.png"
alt="Kafka Output Topic">&lt;/p>
&lt;p>Also, we can check that the output messages are created as expected in the &lt;strong>Topics&lt;/strong> tab.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/deploy-python-pipeline-on-flink-runner/output-topic-messages.png"
alt="Kafka Output Topic Messages">&lt;/p>
&lt;h1 id="delete-resources">Delete resources&lt;/h1>
&lt;p>Delete the Kubernetes resources and the minikube cluster using the following steps.&lt;/p>
&lt;div class='language-bash snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Delete the Flink Operator and related resources.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f beam/word_len_cluster.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f beam/word_len_job.yml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm uninstall flink-kubernetes-operator
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo remove flink-operator-repo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Delete the Kafka cluster and related resources.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">STRIMZI_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.39.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f kafka/manifests/kafka-cluster.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f kafka/manifests/kafka-ui.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete -f kafka/manifests/strimzi-cluster-operator-&lt;span class="nv">$STRIMZI_VERSION&lt;/span>.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Delete the minikube.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">minikube delete&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Blog: Apache Beam 2.56.0</title><link>/blog/beam-2.56.0/</link><pubDate>Wed, 01 May 2024 10:00:00 -0400</pubDate><guid>/blog/beam-2.56.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.56.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2550-2023-03-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.56.0, check out the &lt;a href="https://github.com/apache/beam/milestone/20">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added FlinkRunner for Flink 1.17, removed support for Flink 1.12 and 1.13. Previous version of Pipeline running on Flink 1.16 and below can be upgraded to 1.17, if the Pipeline is first updated to Beam 2.56.0 with the same Flink version. After Pipeline runs with Beam 2.56.0, it should be possible to upgrade to FlinkRunner with Flink 1.17. (&lt;a href="https://github.com/apache/beam/issues/29939">#29939&lt;/a>)&lt;/li>
&lt;li>New Managed I/O Java API (&lt;a href="https://github.com/apache/beam/pull/30830">#30830&lt;/a>).&lt;/li>
&lt;li>New Ordered Processing PTransform added for processing order-sensitive stateful data (&lt;a href="https://github.com/apache/beam/pull/30735">#30735&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Upgraded Avro version to 1.11.3, kafka-avro-serializer and kafka-schema-registry-client versions to 7.6.0 (Java) (&lt;a href="https://github.com/apache/beam/pull/30638">#30638&lt;/a>).
The newer Avro package is known to have breaking changes. If you are affected, you can keep pinned to older Avro versions which are also tested with Beam.&lt;/li>
&lt;li>Iceberg read/write support is available through the new Managed I/O Java API (&lt;a href="https://github.com/apache/beam/pull/30830">#30830&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Profiling of Cythonized code has been disabled by default. This might improve performance for some Python pipelines (&lt;a href="https://github.com/apache/beam/pull/30938">#30938&lt;/a>).&lt;/li>
&lt;li>Bigtable enrichment handler now accepts a custom function to build a composite row key. (Python) (&lt;a href="https://github.com/apache/beam/issues/30975">#30974&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Default consumer polling timeout for KafkaIO.Read was increased from 1 second to 2 seconds. Use KafkaIO.read().withConsumerPollingTimeout(Duration duration) to configure this timeout value when necessary (&lt;a href="https://github.com/apache/beam/issues/30870">#30870&lt;/a>).&lt;/li>
&lt;li>Python Dataflow users no longer need to manually specify &amp;ndash;streaming for pipelines using unbounded sources such as ReadFromPubSub.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed locking issue when shutting down inactive bundle processors. Symptoms of this issue include slowness or stuckness in long-running jobs (Python) (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Fixed logging issue that caused silecing the pip output when installing of dependencies provided in &lt;code>--requirements_file&lt;/code> (Python).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.56.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Aravind Pedapudi&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Brachi Packter&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clement DAL PALU&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Daria Bezkorovaina&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Evan Burrell&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Luís Bianchin&lt;/p>
&lt;p>Maciej Szwaja&lt;/p>
&lt;p>Melody Shen&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shahar Epstein&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timothy Itodo&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>dmitryor&lt;/p>
&lt;p>github-actions[bot]&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xianhualiu&lt;/p></description></item><item><title>Blog: Introducing Beam YAML: Apache Beam's First No-code SDK</title><link>/blog/beam-yaml-release/</link><pubDate>Thu, 11 Apr 2024 10:00:00 -0400</pubDate><guid>/blog/beam-yaml-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Writing a Beam pipeline can be a daunting task. Learning the Beam model, downloading dependencies for the SDK language
of choice, debugging the pipeline, and maintaining the pipeline code is a lot of overhead for users who want to write a
simple to intermediate data processing pipeline. There have been strides in making the SDK&amp;rsquo;s entry points easier, but
for many, it is still a long way from being a painless process.&lt;/p>
&lt;p>To address some of these issues and simplify the entry point to Beam, we have introduced a new way to specify Beam
pipelines by using configuration files rather than code. This new SDK, known as
&lt;a href="https://beam.apache.org/documentation/sdks/yaml/">Beam YAML&lt;/a>, employs a declarative approach to creating
data processing pipelines using &lt;a href="https://yaml.org/">YAML&lt;/a>, a widely used data serialization language.&lt;/p>
&lt;h1 id="benefits-of-using-beam-yaml">Benefits of using Beam YAML&lt;/h1>
&lt;p>The primary goal of Beam YAML is to make the entry point to Beam as welcoming as possible. However, this should not
come at the expense of sacrificing the rich features that Beam offers.&lt;/p>
&lt;p>Here are some of the benefits of using Beam YAML:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>No-code development:&lt;/strong> Allows users to develop pipelines without writing any code. This makes it easier to get
started with Beam and to develop pipelines quickly and easily.&lt;/li>
&lt;li>&lt;strong>Maintainability&lt;/strong>: Configuration-based pipelines are easier to maintain than code-based pipelines. YAML format
enables clear separation of concerns, simplifying changes and updates without affecting other code sections.&lt;/li>
&lt;li>&lt;strong>Declarative language:&lt;/strong> Provides a declarative language, which means that it is based on the description of the
desired outcome rather than expressing the intent through code. This makes it easy to understand the structure and
flow of a pipeline. The YAML syntax is also widely used with a rich community of resources for learning and
leveraging the YAML syntax.&lt;/li>
&lt;li>&lt;strong>Powerful features:&lt;/strong> Supports a wide range of features, including a variety of data sources and sinks, turn-key
transforms, and execution parameters. This makes it possible to develop complex data processing pipelines with Beam
YAML.&lt;/li>
&lt;li>&lt;strong>Reusability&lt;/strong>: Beam YAML promotes code reuse by providing a way to define and share common pipeline patterns. You
can create reusable YAML snippets or blocks that can be easily shared and reused in different pipelines. This reduces
the need to write repetitive tasks and helps maintain consistency across pipelines.&lt;/li>
&lt;li>&lt;strong>Extensibility&lt;/strong>: Beam YAML offers a structure for integrating custom transformations into a pipeline, enabling
organizations to contribute or leverage a pre-existing catalog of transformations that can be seamlessly accessed
using the Beam YAML syntax across multiple pipelines. It is also possible to build third-party extensions, including
custom parsers and other tools, that do not need to depend on Beam directly.&lt;/li>
&lt;li>&lt;strong>Backwards Compatibility&lt;/strong>: Beam YAML is still being actively worked on, bringing exciting new features and
capabilities, but as these features are added, backwards compatibility will be preserved. This way, once a pipeline
is written, it will continue to work despite future released versions of the SDK.&lt;/li>
&lt;/ul>
&lt;p>Overall, using Beam YAML provides a number of advantages. It makes pipeline development and management more efficient
and effective, enabling users to focus on the business logic and data processing tasks, rather than spending time on
low-level coding details.&lt;/p>
&lt;h1 id="case-study-a-simple-business-analytics-use-case">Case Study: A simple business analytics use-case&lt;/h1>
&lt;p>Let&amp;rsquo;s take the following sample transaction data for a department store:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">transaction_id&lt;/th>
&lt;th style="text-align:left">product_name&lt;/th>
&lt;th style="text-align:left">category&lt;/th>
&lt;th style="text-align:left">price&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">T0012&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T5034&lt;/td>
&lt;td style="text-align:left">Leather Jacket&lt;/td>
&lt;td style="text-align:left">Apparel&lt;/td>
&lt;td style="text-align:left">109.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0024&lt;/td>
&lt;td style="text-align:left">Aluminum Mug&lt;/td>
&lt;td style="text-align:left">Kitchen&lt;/td>
&lt;td style="text-align:left">29.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0104&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0302&lt;/td>
&lt;td style="text-align:left">Monitor&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">249.99&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now, let&amp;rsquo;s say that the business wants to get a record of transactions for all purchases made in the Electronics
department for audit purposes. Assuming the records are stored as a CSV file, a Beam YAML pipeline may look something
like this:&lt;/p>
&lt;p>Source code for this example can be found
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/simple_filter.yaml">here&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/input.csv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Filter&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">language&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">keep&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">category == &amp;#34;Electronics&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteOutputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/output&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This would leave us with the following data:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">transaction_id&lt;/th>
&lt;th style="text-align:left">product_name&lt;/th>
&lt;th style="text-align:left">category&lt;/th>
&lt;th style="text-align:left">price&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">T0012&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0104&lt;/td>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">59.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">T0302&lt;/td>
&lt;td style="text-align:left">Monitor&lt;/td>
&lt;td style="text-align:left">Electronics&lt;/td>
&lt;td style="text-align:left">249.99&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now, let&amp;rsquo;s say the business wants to determine how much of each Electronics item is being sold to ensure that the
correct number is being ordered from the supplier. Let&amp;rsquo;s also assume that they want to determine the total revenue for
each item. This simple aggregation can follow the Filter from the previous example as such:&lt;/p>
&lt;p>Source code for this example can be found
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/simple_filter_and_combine.yaml">here&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">pipeline&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">transforms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadFromCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/input.csv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Filter&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ReadInputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">language&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">python&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">keep&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">category == &amp;#34;Electronics&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Combine&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">CountNumberSold&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FilterWithCategory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">group_by&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">product_name&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">combine&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">num_sold&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">product_name&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">fn&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">count&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">total_revenue&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">price&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">fn&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">sum&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteToCsv&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">WriteOutputFile&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">input&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">CountNumberSold&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/path/to/output&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This would leave us with the following data:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">product_name&lt;/th>
&lt;th style="text-align:left">num_sold&lt;/th>
&lt;th style="text-align:left">total_revenue&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">Headphones&lt;/td>
&lt;td style="text-align:left">2&lt;/td>
&lt;td style="text-align:left">119.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Monitor&lt;/td>
&lt;td style="text-align:left">1&lt;/td>
&lt;td style="text-align:left">249.99&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>While this was a relatively simple use-case, it shows the power of Beam YAML and how easy it is to go from business
use-case to a prototype data pipeline in just a few lines of YAML.&lt;/p>
&lt;h1 id="getting-started-with-beam-yaml">Getting started with Beam YAML&lt;/h1>
&lt;p>There are several resources that have been compiled to help users get familiar with Beam YAML.&lt;/p>
&lt;h2 id="day-zero-notebook">Day Zero Notebook&lt;/h2>
&lt;a target="_blank" href="https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-yaml.ipynb">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>
&lt;p>To help get started with Apache Beam, there is a Day Zero Notebook available on
&lt;a href="https://colab.sandbox.google.com/">Google Colab&lt;/a>, an online Python notebook environment with a free attachable
runtime, containing some basic YAML pipeline examples.&lt;/p>
&lt;h2 id="documentation">Documentation&lt;/h2>
&lt;p>The Apache Beam website provides a set of &lt;a href="https://beam.apache.org/documentation/sdks/yaml/">docs&lt;/a> that demonstrate the
current capabilities of the Beam YAML SDK. There is also a catalog of currently-supported turnkey transforms found
&lt;a href="https://beam.apache.org/releases/yamldoc/current/">here&lt;/a>.&lt;/p>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>A catalog of examples can be found
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples">here&lt;/a>. These examples showcase
all the turnkey transforms that can be utilized in Beam YAML. There are also a number of Dataflow Cookbook examples
that can be found &lt;a href="https://github.com/GoogleCloudPlatform/dataflow-cookbook/tree/main/Python/yaml">here&lt;/a>.&lt;/p>
&lt;h2 id="contributing">Contributing&lt;/h2>
&lt;p>Developers who wish to help build out and add functionalities are welcome to start contributing to the effort in the
Beam YAML module found &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml">here&lt;/a>.&lt;/p>
&lt;p>There is also a list of open &lt;a href="https://github.com/apache/beam/issues?q=is%3Aopen+is%3Aissue+label%3Ayaml">bugs&lt;/a> found
on the GitHub repo - now marked with the &amp;lsquo;yaml&amp;rsquo; tag.&lt;/p>
&lt;p>While Beam YAML has been marked stable as of Beam 2.52, it is still under heavy development, with new features being
added with each release. Those who wish to be part of the design decisions and give insights to how the framework is
being used are highly encouraged to join the dev mailing list as those discussions will be directed there. A link to
the dev list can be found &lt;a href="https://beam.apache.org/community/contact-us/">here&lt;/a>.&lt;/p></description></item><item><title>Blog: Apache Beam 2.55.0</title><link>/blog/beam-2.55.0/</link><pubDate>Mon, 25 Mar 2024 10:00:00 -0400</pubDate><guid>/blog/beam-2.55.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.55.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2550-2023-03-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.55.0, check out the &lt;a href="https://github.com/apache/beam/milestone/19">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>The Python SDK will now include automatically generated wrappers for external Java transforms! (&lt;a href="https://github.com/apache/beam/pull/29834">#29834&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added support for handling bad records to BigQueryIO (&lt;a href="https://github.com/apache/beam/pull/30081">#30081&lt;/a>).
&lt;ul>
&lt;li>Full Support for Storage Read and Write APIs&lt;/li>
&lt;li>Partial Support for File Loads (Failures writing to files supported, failures loading files to BQ unsupported)&lt;/li>
&lt;li>No Support for Extract or Streaming Inserts&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for handling bad records to PubSubIO (&lt;a href="https://github.com/apache/beam/pull/30372">#30372&lt;/a>).
&lt;ul>
&lt;li>Support is not available for handling schema mismatches, and enabling error handling for writing to Pub/Sub topics with schemas is not recommended&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--enableBundling&lt;/code> pipeline option for BigQueryIO DIRECT_READ is replaced by &lt;code>--enableStorageReadApiV2&lt;/code>. Both were considered experimental and subject to change (Java) (&lt;a href="https://github.com/apache/beam/issues/26354">#26354&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow writing clustered and not time-partitioned BigQuery tables (Java) (&lt;a href="https://github.com/apache/beam/pull/30094">#30094&lt;/a>).&lt;/li>
&lt;li>Redis cache support added to RequestResponseIO and Enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/pull/30307">#30307&lt;/a>)&lt;/li>
&lt;li>Merged &lt;code>sdks/java/fn-execution&lt;/code> and &lt;code>runners/core-construction-java&lt;/code> into the main SDK. These artifacts were never meant for users, but noting
that they no longer exist. These are steps to bring portability into the core SDK alongside all other core functionality.&lt;/li>
&lt;li>Added Vertex AI Feature Store handler for Enrichment transform (Python) (&lt;a href="https://github.com/apache/beam/pull/30388">#30388&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Arrow version was bumped to 15.0.0 from 5.0.0 (&lt;a href="https://github.com/apache/beam/pull/30181">#30181&lt;/a>).&lt;/li>
&lt;li>Go SDK users who build custom worker containers may run into issues with the move to distroless containers as a base (see Security Fixes).
&lt;ul>
&lt;li>The issue stems from distroless containers lacking additional tools, which current custom container processes may rely on.&lt;/li>
&lt;li>See &lt;a href="https://beam.apache.org/documentation/runtime/environments/#from-scratch-go">https://beam.apache.org/documentation/runtime/environments/#from-scratch-go&lt;/a> for instructions on building and using a custom container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Python SDK has changed the default value for the &lt;code>--max_cache_memory_usage_mb&lt;/code> pipeline option from 100 to 0. This option was first introduced in the 2.52.0 SDK version. This change restores the behavior of the 2.51.0 SDK, which does not use the state cache. If your pipeline uses iterable side inputs views, consider increasing the cache size by setting the option manually. (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="bug-fixes">Bug fixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &lt;code>SpannerIO.readChangeStream&lt;/code> to support propagating credentials from pipeline options
to the &lt;code>getDialect&lt;/code> calls for authenticating with Spanner (Java) (&lt;a href="https://github.com/apache/beam/pull/30361">#30361&lt;/a>).&lt;/li>
&lt;li>Reduced the number of HTTP requests in GCSIO function calls (Python) (&lt;a href="https://github.com/apache/beam/pull/30205">#30205&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Go SDK base container image moved to distroless/base-nossl-debian12, reducing vulnerable container surface to kernel and glibc (&lt;a href="https://github.com/apache/beam/pull/30011">#30011&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.55.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Crites&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Chris Gray&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Ellis&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Daria Bezkorovaina&lt;/p>
&lt;p>Dima I&lt;/p>
&lt;p>Edward Cui&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>GStravinsky&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jason Mitchell&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Ritesh Tarway&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Scott Strong&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>Ukjae Jeong (Jay)&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>akashorabek&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>dengwe1&lt;/p>
&lt;p>dhruvdua&lt;/p>
&lt;p>hardshah&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p></description></item><item><title>Blog: Apache Beam 2.54.0</title><link>/blog/beam-2.54.0/</link><pubDate>Wed, 14 Feb 2024 09:00:00 -0400</pubDate><guid>/blog/beam-2.54.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.54.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.54.0, check out the &lt;a href="https://github.com/apache/beam/milestone/18">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://s.apache.org/enrichment-transform">Enrichment Transform&lt;/a> along with GCP BigTable handler added to Python SDK (&lt;a href="https://github.com/apache/beam/pull/30001">#30001&lt;/a>).&lt;/li>
&lt;li>Beam Java Batch pipelines run on Google Cloud Dataflow will default to the Portable Runner (v2) starting with this version. (All other languages are already on Runner V2.) See &lt;a href="https://cloud.google.com/dataflow/docs/runner-v2">Runner V2 documentation&lt;/a> for how to enable or disable it intentionally.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added support for writing to BigQuery dynamic destinations with Python&amp;rsquo;s Storage Write API (&lt;a href="https://github.com/apache/beam/pull/30045">#30045&lt;/a>)&lt;/li>
&lt;li>Adding support for Tuples DataType in ClickHouse (Java) (&lt;a href="https://github.com/apache/beam/pull/29715">#29715&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to FileIO, TextIO, AvroIO (&lt;a href="https://github.com/apache/beam/pull/29670">#29670&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to BigtableIO (&lt;a href="https://github.com/apache/beam/pull/29885">#29885&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://s.apache.org/enrichment-transform">Enrichment Transform&lt;/a> along with GCP BigTable handler added to Python SDK (&lt;a href="https://github.com/apache/beam/pull/30001">#30001&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed a memory leak affecting some Go SDK since 2.46.0. (&lt;a href="https://github.com/apache/beam/pull/28142">#28142&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>N/A&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the &lt;code>--max_cache_memory_usage_mb=0&lt;/code> pipeline option. (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.54.0 SDKs and perform file operations on GCS might be affected by excess HTTP requests. This could lead to a performance regression or a permission issue. (&lt;a href="https://github.com/apache/beam/issues/28398">#28398&lt;/a>)&lt;/li>
&lt;li>In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.54.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Crites&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Changyu Li&lt;/p>
&lt;p>Cheskel Twersky&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>Edward Cheng&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>hugo-syn&lt;/p>
&lt;p>Issac&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Jerry Wang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>Joey Tran&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Knut Olav Løite&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>Marc&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Neeraj Bansal&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>S. Veyrié&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jian&lt;/p></description></item><item><title>Blog: Behind the Scenes: Crafting an Autoscaler for Apache Beam in a High-Volume Streaming Environment</title><link>/blog/apache-beam-flink-and-kubernetes-part3/</link><pubDate>Mon, 05 Feb 2024 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes-part3/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h3 id="introduction-to-the-design-of-our-autoscaler-for-apache-beam-jobs">Introduction to the Design of Our Autoscaler for Apache Beam Jobs&lt;/h3>
&lt;p>Welcome to the third and final part of our blog series on building a scalable, self-managed streaming infrastructure with Beam and Flink. &lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">In our previous post&lt;/a>, we delved into the scale of our streaming platforms, highlighting our capacity to manage over 40,000 streaming jobs and process upwards of 10 million events per second. This impressive scale sets the stage for the challenge we address today: the intricate task of resource allocation in a dynamic streaming environment.&lt;/p>
&lt;p>In this blog post &lt;a href="https://www.linkedin.com/in/talatuyarer/">Talat Uyarer (Architect / Senior Principal Engineer)&lt;/a> and &lt;a href="https://www.linkedin.com/in/rishabhkedia/">Rishabh Kedia (Principal Engineer)&lt;/a> describe more details about our Autoscaler. Imagine a scenario where your streaming system is inundated with fluctuating workloads. Our case presents a unique challenge, as our customers, equipped with firewalls distributed globally, generate logs at various times of the day. This results in workloads that not only vary by time but also escalate over time due to changes in settings or the addition of new cybersecurity solutions from PANW. Furthermore, updates to our codebase necessitate rolling out changes across all streaming jobs, leading to a temporary surge in demand as the system processes unprocessed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/resource-allocation.png"
alt="Resource Allocation">&lt;/p>
&lt;p>Traditionally, managing this ebb and flow of demand involves a manual, often inefficient approach. One might over-provision resources to handle peak loads, inevitably leading to resource wastage during off-peak hours. Conversely, a more cost-conscious strategy might involve accepting delays during peak times, with the expectation of catching up later. However, both methods demand constant monitoring and manual adjustment - a far from ideal situation.&lt;/p>
&lt;p>In this modern era, where automated scaling of web front-ends is a given, we aspire to bring the same level of efficiency and automation to streaming infrastructure. Our goal is to develop a system that can dynamically track and adjust to the workload demands of our streaming operations. In this blog post, we will introduce you to our innovative solution - an autoscaler designed specifically for Apache Beam jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/auto-tuned-worker.png"
alt="Auto Tuned Resource Allocation">&lt;/p>
&lt;p>For clarity, when we refer to &amp;ldquo;resources&amp;rdquo; in this context, we mean the number of Flink Task Managers, or Kubernetes Pods, that process your streaming pipeline. These Task Managers aren&amp;rsquo;t just about CPU; they also involve RAM, Network, Disk IO, and other computational resources.&lt;/p>
&lt;p>However, our solution is predicated on certain assumptions. Primarily, it&amp;rsquo;s geared towards operations processing substantial data volumes. If your workload only requires a couple of Task Managers, this system might not be the best fit. In Our case we have 10K+ workload and each each of them has different workload. Manual tuning was not an option for us. We also assume that the data is evenly distributed, allowing for increased throughput with the addition of more Task Managers. This assumption is crucial for effective horizontal scaling. While there are real-world complexities that might challenge these assumptions, for the scope of this discussion, we will focus on scenarios where these conditions hold true.&lt;/p>
&lt;p>Join us as we delve into the design and functionality of our autoscaler, a solution tailored to bring efficiency, adaptability, and a touch of intelligence to the world of streaming infrastructure.&lt;/p>
&lt;h2 id="identifying-the-right-signals-for-autoscaling">Identifying the Right Signals for Autoscaling&lt;/h2>
&lt;p>When we&amp;rsquo;re overseeing a system like Apache Beam jobs on Flink, it&amp;rsquo;s crucial to identify key signals that help us understand the relationship between our workload and resources. These signals are our guiding lights, showing us when we&amp;rsquo;re lagging behind or wasting resources. By accurately identifying these signals, we can formulate effective scaling policies and implement changes in real-time. Imagine needing to expand from 100 to 200 TaskManagers — how do we smoothly make that transition? That&amp;rsquo;s where these signals come into play.&lt;/p>
&lt;p>Remember, we&amp;rsquo;re aiming for a universal solution applicable to any workload and pipeline. While specific problems might benefit from unique signals, our focus here is on creating a one-size-fits-all approach.&lt;/p>
&lt;p>In Flink, tasks form the basic execution unit and consist of one or more operators, such as map, filter, or reduce. Flink optimizes performance by chaining these operators into single tasks when possible, minimizing overheads like thread context switching and network I/O. Your pipeline, when optimized, turns into a directed acyclic graph of stages, each processing elements based on your code. Don&amp;rsquo;t confuse stages with physical machines — they&amp;rsquo;re separate concepts. In our job we measure backlog information by using Apache Beam&amp;rsquo;s &lt;a href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/SourceMetrics.java#L32">&lt;code>backlog_bytes&lt;/code> and &lt;code>backlog_elements&lt;/code>&lt;/a> metrics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/flink-operator-chaining.png"
alt="Apache Beam Pipeline Optimization by Apache Flink">&lt;/p>
&lt;h5 id="upscaling-signals">&lt;strong>Upscaling Signals&lt;/strong>&lt;/h5>
&lt;h5 id="backlog-growth">&lt;em>Backlog Growth&lt;/em>&lt;/h5>
&lt;p>Let’s take a practical example. Consider a pipeline reading from Kafka, where different operators handle data parsing, formatting, and accumulation. The key metric here is throughput — how much data each operstor processes over time. But throughput alone isn&amp;rsquo;t enough. We need to examine the queue size or backlog at each operator. A growing backlog indicates we&amp;rsquo;re falling behind. We measure this as backlog growth — the first derivative of backlog size over time, highlighting our processing deficit.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/backlog_growth.png"
alt="Backlog Growth Calculation">&lt;/p>
&lt;h5 id="backlog-time">&lt;em>Backlog Time&lt;/em>&lt;/h5>
&lt;p>This leads us to backlog time, a derived metric that compares backlog size with throughput. It’s a measure of how long it would take to clear the current backlog, assuming no new data arrives. This helps us identify if a backlog of a certain size is acceptable or problematic, based on our specific processing needs and thresholds.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/backlog_time.png"
alt="Backlog Time Calculation">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/operator-backlog.png"
alt="Close looks at Operator Backlog">&lt;/p>
&lt;h4 id="downscaling-when-less-is-more">&lt;strong>Downscaling: When Less is More&lt;/strong>&lt;/h4>
&lt;h5 id="cpu-utilization">&lt;em>CPU Utilization&lt;/em>&lt;/h5>
&lt;p>A key signal for downscaling is CPU utilization. Low CPU utilization suggests we&amp;rsquo;re using more resources than necessary. By monitoring this, we can scale down efficiently without compromising performance.&lt;/p>
&lt;h4 id="signals-summary">&lt;strong>Signals Summary&lt;/strong>&lt;/h4>
&lt;p>In summary, the signals we&amp;rsquo;ve identified for effective autoscaling are:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Throughput:&lt;/strong> The baseline of our performance.&lt;/li>
&lt;li>&lt;strong>Backlog Growth:&lt;/strong> Indicates if we’re keeping pace with incoming data.&lt;/li>
&lt;li>&lt;strong>Backlog Time:&lt;/strong> Helps understand the severity of backlog.&lt;/li>
&lt;li>&lt;strong>CPU Utilization:&lt;/strong> Guides us in resource optimization.&lt;/li>
&lt;/ol>
&lt;p>These signals might seem straightforward, but their simplicity is key to a scalable, workload-agnostic autoscaling solution.&lt;/p>
&lt;h2 id="simplifying-autoscaling-policies-for-apache-beam-jobs-on-flink">Simplifying Autoscaling Policies for Apache Beam Jobs on Flink&lt;/h2>
&lt;p>In the world of Apache Beam jobs running on Flink, deciding when to scale up or down is a bit like being a chef in a busy kitchen. You need to keep an eye on several ingredients — your workload, virtual machines (VMs), and how they interact. It&amp;rsquo;s about maintaining a perfect balance. Our main goals? Avoid falling behind in processing (no backlog growth), ensure that any existing backlog is manageable (short backlog time), and use our resources (like CPU) efficiently.&lt;/p>
&lt;h4 id="up-scaling-keeping-up-and-catching-up">&lt;strong>Up-scaling: Keeping Up and Catching Up&lt;/strong>&lt;/h4>
&lt;p>Imagine your system is like a team of chefs working together. Here&amp;rsquo;s how we decide when to bring more chefs into the kitchen (a.k.a. upscaling):&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Keeping Up:&lt;/strong> First, we look at our current team size (number of VMs) and how much they&amp;rsquo;re processing (throughput). We then adjust our team size based on the amount of incoming orders (input rate). It&amp;rsquo;s about ensuring that our team is big enough to handle the current demand.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Catching Up:&lt;/strong> Sometimes, we might have a backlog of orders. In that case, we decide how many extra chefs we need to clear this backlog within a desired time (like 60 seconds). This part of the policy helps us get back on track swiftly.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="scaling-example-a-practical-look">&lt;strong>Scaling Example: A Practical Look&lt;/strong>&lt;/h4>
&lt;p>Let&amp;rsquo;s paint a picture with an example. Initially, we have a steady flow of orders (input rate) matching our processing capacity (throughput), so there&amp;rsquo;s no backlog. But suddenly, orders increase, and our team starts falling behind, creating a backlog. We respond by increasing our team size to match the new rate of orders. Though the backlog doesn&amp;rsquo;t grow further, it still exists. Finally, we add a few more chefs to the team, which allows us to clear the backlog quickly and return to a new, balanced state.&lt;/p>
&lt;h4 id="downscaling-when-to-reduce-resources">&lt;strong>Downscaling: When to Reduce Resources&lt;/strong>&lt;/h4>
&lt;p>Downscaling is like knowing when some chefs can take a break after a rush hour. We consider this when:&lt;/p>
&lt;ul>
&lt;li>Our backlog is low — we&amp;rsquo;ve caught up with the orders.&lt;/li>
&lt;li>The backlog isn&amp;rsquo;t growing — we&amp;rsquo;re keeping up with incoming orders.&lt;/li>
&lt;li>Our kitchen (CPU) isn&amp;rsquo;t working too hard — we&amp;rsquo;re using our resources efficiently.&lt;/li>
&lt;/ul>
&lt;p>Downscaling is all about reducing resources without affecting the quality of service. It&amp;rsquo;s about ensuring that we&amp;rsquo;re not overstaffed when the rush hour is over.&lt;/p>
&lt;h4 id="summary-a-recipe-for-effective-scaling">&lt;strong>Summary: A Recipe for Effective Scaling&lt;/strong>&lt;/h4>
&lt;p>In summary, our scaling policy is for scale up, we first ensure that the time to drain the backlog is beyond the threshold (120s) or the cpu is above the threshold (90%)&lt;/p>
&lt;p>Increasing Backlog aka Backlog Growth &amp;gt; 0 :&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/worker_require.png"
alt="Required Worker Calculation">&lt;/p>
&lt;p>Consistent Backlog aka Backlog Growth = 0:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/worker_extra.png"
alt="Extra Worker Calculation">&lt;/p>
&lt;p>To Sum up:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/worker_scaleup.png"
alt="Scale up Worker Calculation">&lt;/p>
&lt;p>To scale down, we need to ensure the machine utilization is low (&amp;lt; 70%) and there is no backlog growth and current time to drain backlog is less than the limit (10s)&lt;/p>
&lt;p>So the only driving factor to calculate the required resources after a scale down is CPU&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/cpurate_desired.png"
alt="Desired Cpu Rate Calculation">&lt;/p>
&lt;h2 id="executing-autoscaling-decision">Executing Autoscaling Decision&lt;/h2>
&lt;p>In our setup we use Reactive Mode which uses Adaptive Scheduler and Declarative Resources manager. We wanted to align resources with slots. As advised in most of the Flink documentation we set one per vCPU slot. Most of our jobs use 1 vCPU 4GB Memory combination for TaskManager.&lt;/p>
&lt;p>Reactive Mode, a unique feature of the Adaptive Scheduler, operates under the principle of one job per cluster, a rule enforced in Application Mode. In this mode, a job is configured to utilize all available resources within the cluster. Adding a TaskManager will increase the job&amp;rsquo;s scale, while removing resources will decrease it. In this setup, Flink autonomously manages the job&amp;rsquo;s parallelism, always maximizing it.&lt;/p>
&lt;p>During a rescaling event, Reactive Mode restarts the job using the most recent checkpoint. This eliminates the need for creating a savepoint, typically required for manual job rescaling. The volume of data reprocessed after rescaling is influenced by the checkpointing interval(10 seconds for us), and the time it takes to restore depends on the size of the state.&lt;/p>
&lt;p>The scheduler determines the parallelism of each operator within a job. This setting is not user-configurable and any attempts to set it, whether for individual operators or the entire job, will be overlooked.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part3/adaptive_scheduler_rescale.png"
alt="How Reactive Mode Works">&lt;/p>
&lt;p>Parallelism can only be influenced by setting a maximum for pipelines, which the scheduler will honor. Our maxParallelism is limited by the total count of partitions that the pipeline will process, as well as by the job itself. We cap the maximum number of TaskManagers with maxWorker count and control the job&amp;rsquo;s key count in shuffle by setting maxParallelism. Additionally, we set maxParallelism per pipeline to manage pipeline parallelism. The job cannot exceed the job&amp;rsquo;s maxParallelism in terms of workers.&lt;/p>
&lt;p>After autoscaler analysis, we will tag if the job needs to be scaled up, no action or scaled down. To interact with the job, we use a library we have built over Flink Kubernetes Operator. This library allows us to interact with our flink jobs via a simple java method call. Library will convert our method call to a kubernetes command.&lt;/p>
&lt;p>In the kubernetes world, the call will look like this for a scale up:&lt;/p>
&lt;p>&lt;code>kubectl scale flinkdeployment job-name --replicas=100&lt;/code>&lt;/p>
&lt;p>Apache Flink will handle the rest of the work needed to scale up.&lt;/p>
&lt;h2 id="maintaining-state-for-stateful-streaming-application-with-autoscaling">Maintaining State for Stateful Streaming Application with Autoscaling&lt;/h2>
&lt;p>Adapting Apache Flink&amp;rsquo;s state recovery mechanisms for autoscaling involves leveraging its robust features like max parallelism, checkpointing, and the Adaptive Scheduler to ensure efficient and resilient stream processing, even as the system dynamically adjusts to varying loads. Here&amp;rsquo;s how these components work together in an autoscaling context:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Max Parallelism&lt;/strong> sets an upper limit on how much a job can scale out, ensuring that state can be redistributed across a larger or smaller number of nodes without exceeding predefined boundaries. This is crucial for autoscaling because it allows Flink to manage state effectively, even as the number of task slots changes to accommodate varying workloads.&lt;/li>
&lt;li>&lt;strong>Checkpointing&lt;/strong> is at the heart of Flink&amp;rsquo;s fault tolerance mechanism, periodically saving the state of each job to a durable storage (in our case it is GCS bucket). In an autoscaling scenario, checkpointing enables Flink to recover to a consistent state after scaling operations. When the system scales out (adds more resources) or scales in (removes resources), Flink can restore the state from these checkpoints, ensuring data integrity and processing continuity without losing critical information. In scale down or up situations there could be a moment to reprocess data from last checkpoint. To reduce that amount we reduce the checkpointing interval to 10 seconds.&lt;/li>
&lt;li>&lt;strong>Reactive Mode&lt;/strong> is a special mode for Adaptive Scheduler, that assumes a single job per-cluster (enforced by the Application Mode). Reactive Mode configures a job so that it always uses all resources available in the cluster. Adding a TaskManager will scale up your job, removing resources will scale it down. Flink will manage the parallelism of the job, always setting it to the highest possible values. When a job undergoes resizing, Reactive Mode triggers a restart using the most recent successful checkpoint.&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this blog series, we&amp;rsquo;ve taken a deep dive into the creation of an autoscaler for Apache Beam in a high-volume streaming environment, highlighting the journey from conceptualization to implementation. This endeavor not only tackled the complexities of dynamic resource allocation but also set a new standard for efficiency and adaptability in streaming infrastructure. By marrying intelligent scaling policies with the robust capabilities of Apache Beam and Flink, we&amp;rsquo;ve showcased a scalable solution that optimizes resource use and maintains performance under varying loads. This project stands as a testament to the power of teamwork, innovation, and a forward-thinking approach to streaming data processing. As we wrap up this series, we express our gratitude to all contributors and look forward to the continuous evolution of this technology, inviting the community to join us in further discussions and developments.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;p>[1] Streaming Auto-scaling in Google Cloud Dataflow &lt;a href="https://www.infoq.com/presentations/google-cloud-dataflow/">https://www.infoq.com/presentations/google-cloud-dataflow/&lt;/a>&lt;/p>
&lt;p>[2] Pipeline lifecycle &lt;a href="https://cloud.google.com/dataflow/docs/pipeline-lifecycle">https://cloud.google.com/dataflow/docs/pipeline-lifecycle&lt;/a>&lt;/p>
&lt;p>[3] Flink Elastic Scaling &lt;a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/&lt;/a>&lt;/p>
&lt;h1 id="acknowledgements">Acknowledgements&lt;/h1>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed Flink based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitik Dani, and David He.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Explore More:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">Part 1: Introduction to Building and Managing Apache Beam Flink Services on Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes-part2/">Part 2: Build a scalable, self-managed streaming infrastructure with Flink: Tackling Autoscaling Challenges - Part 2&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Join the conversation and share your experiences on our &lt;a href="https://beam.apache.org/community/">Community&lt;/a> or contribute to our ongoing projects on &lt;a href="https://github.com/apache/beam">GitHub&lt;/a>. Your feedback is invaluable. If you have any comments or questions about this series, please feel free to reach out to us via &lt;a href="https://beam.apache.org/community/contact-us/">User Mailist&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>Stay connected with us for more updates and insights into Apache Beam, Flink, and Kubernetes.&lt;/em>&lt;/p></description></item><item><title>Blog: Apache Beam 2.53.0</title><link>/blog/beam-2.53.0/</link><pubDate>Thu, 04 Jan 2024 09:00:00 -0400</pubDate><guid>/blog/beam-2.53.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.53.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.53.0, check out the &lt;a href="https://github.com/apache/beam/milestone/17">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python streaming users that use 2.47.0 and newer versions of Beam should update to version 2.53.0, which fixes a known issue: (&lt;a href="https://github.com/apache/beam/issues/27330">#27330&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>TextIO now supports skipping multiple header lines (Java) (&lt;a href="https://github.com/apache/beam/issues/17990">#17990&lt;/a>).&lt;/li>
&lt;li>Python GCSIO is now implemented with GCP GCS Client instead of apitools (&lt;a href="https://github.com/apache/beam/issues/25676">#25676&lt;/a>)&lt;/li>
&lt;li>Adding support for LowCardinality DataType in ClickHouse (Java) (&lt;a href="https://github.com/apache/beam/pull/29533">#29533&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to KafkaIO (Java) (&lt;a href="https://github.com/apache/beam/pull/29546">#29546&lt;/a>)&lt;/li>
&lt;li>Add support for generating text embeddings in MLTransform for Vertex AI and Hugging Face Hub models.(&lt;a href="https://github.com/apache/beam/pull/29564">#29564&lt;/a>)&lt;/li>
&lt;li>NATS IO connector added (Go) (&lt;a href="https://github.com/apache/beam/issues/29000">#29000&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now type checks &lt;code>collections.abc.Collections&lt;/code> types properly. Some type hints that were erroneously allowed by the SDK may now fail. (&lt;a href="https://github.com/apache/beam/pull/29272">#29272&lt;/a>)&lt;/li>
&lt;li>Running multi-language pipelines locally no longer requires Docker.
Instead, the same (generally auto-started) subprocess used to perform the
expansion can also be used as the cross-language worker.&lt;/li>
&lt;li>Framework for adding Error Handlers to composite transforms added in Java (&lt;a href="https://github.com/apache/beam/pull/29164">#29164&lt;/a>).&lt;/li>
&lt;li>Python 3.11 images now include google-cloud-profiler (&lt;a href="https://github.com/apache/beam/pull/29651">#29561&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Euphoria DSL is deprecated and will be removed in a future release (not before 2.56.0) (&lt;a href="https://github.com/apache/beam/issues/29451">#29451&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Python) Fixed sporadic crashes in streaming pipelines that affected some users of 2.47.0 and newer SDKs (&lt;a href="https://github.com/apache/beam/issues/27330">#27330&lt;/a>).&lt;/li>
&lt;li>(Python) Fixed a bug that caused MLTransform to drop identical elements in the output PCollection (&lt;a href="https://github.com/apache/beam/issues/29600">#29600&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Upgraded to go 1.21.5 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-45285">CVE-2023-45285&lt;/a> and &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39326">CVE-2023-39326&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Potential race condition causing NPE in DataflowExecutionStateSampler in Dataflow Java Streaming pipelines (&lt;a href="https://github.com/apache/beam/issues/29987">#29987&lt;/a>).&lt;/li>
&lt;li>Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the &lt;code>--max_cache_memory_usage_mb=0&lt;/code> pipeline option. (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.54.0 SDKs and perform file operations on GCS might be affected by excess HTTP requests. This could lead to a performance regression or a permission issue. (&lt;a href="https://github.com/apache/beam/issues/28398">#28398&lt;/a>)&lt;/li>
&lt;li>In Python pipelines, when shutting down inactive bundle processors, shutdown logic can overaggressively hold the lock, blocking acceptance of new work. Symptoms of this issue include slowness or stuckness in long-running jobs. Fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/30679">#30679&lt;/a>).&lt;/li>
&lt;li>Python pipelines that run with 2.53.0-2.58.0 SDKs and read data from GCS might be affected by a data corruption issue (&lt;a href="https://github.com/apache/beam/issues/32169">#32169&lt;/a>). The issue will be fixed in 2.59.0 (&lt;a href="https://github.com/apache/beam/pull/32135">#32135&lt;/a>). To work around this, update the google-cloud-storage package to version 2.18.2 or newer.&lt;/li>
&lt;/ul>
&lt;p>For the most up to date list of known issues, see &lt;a href="https://github.com/apache/beam/blob/master/CHANGES.md">https://github.com/apache/beam/blob/master/CHANGES.md&lt;/a>&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.53.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Calvin Swenson Jr&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Georgii Zemlianyi&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jacob Tomlinson&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julian Braha&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Lawrence Qiu&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Mike Williamson&lt;/p>
&lt;p>Naireen&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Radosław Stankiewicz&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>Tom Stepp&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lrakla&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p></description></item><item><title>Blog: Scaling a streaming workload on Apache Beam, 1 million events per second and beyond</title><link>/blog/scaling-streaming-workload/</link><pubDate>Wed, 03 Jan 2024 00:00:01 -0800</pubDate><guid>/blog/scaling-streaming-workload/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/0-intro.png"
alt="Streaming Processing">&lt;/p>
&lt;p>Scaling a streaming workload is critical for ensuring that a pipeline can process large amounts of data while also minimizing latency and executing efficiently. Without proper scaling, a pipeline may experience performance issues or even fail entirely, delaying the time to insights for the business.&lt;/p>
&lt;p>Given the Apache Beam support for the sources and sinks needed by the workload, developing a streaming pipeline can be easy. You can focus on the processing (transformations, enrichments, or aggregations) and on setting the right configurations for each case.&lt;/p>
&lt;p>However, you need to identify the key performance bottlenecks and make sure that the pipeline has the resources it needs to handle the load efficiently. This can involve right-sizing the number of workers, understanding the settings needed for the source and sinks of the pipeline, optimizing the processing logic, and even determining the transport formats.&lt;/p>
&lt;p>This article illustrates how to manage the problem of scaling and optimizing a streaming workload developed in Apache Beam and run on Google Cloud using Dataflow. The goal is to reach one million events per second, while also minimizing latency and resource use during execution. The workload uses Pub/Sub as the streaming source and BigQuery as the sink. We describe the reasoning behind the configuration settings and code changes we used to help the workload achieve the desired scale and beyond.&lt;/p>
&lt;p>The progression described in this article maps to the evolution of a real-life workload, with simplifications. After the initial business requirements for the pipeline were achieved, the focus shifted to optimizing the performance and reducing the resources needed for the pipeline execution.&lt;/p>
&lt;h2 id="execution-setup">Execution setup&lt;/h2>
&lt;p>For this article, we created a test suite that creates the necessary components for the pipelines to execute. You can find the code in &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests">this Github repository&lt;/a>. You can find the subsequent configuration changes that are introduced on every run in this &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/scaling-streaming-workload-blog">folder&lt;/a> as scripts that you can run to achieve similar results.&lt;/p>
&lt;p>All of the execution scripts can also execute a Terraform-based automation to create a Pub/Sub topic and subscription as well as a BigQuery dataset and table to run the workload. Also, it launches two pipelines: one data generation pipeline that pushes events to the Pub/Sub topic, and an ingestion pipeline that demonstrates the potential improvement points.&lt;/p>
&lt;p>In all cases, the pipelines start with an empty Pub/Sub topic and subscription and an empty BigQuery table. The plan is to generate one million events per second and, after a few minutes, review how the ingestion pipeline scales with time. The data being autogenerated is based on provided schemas or IDL (or Interface Description Language) given the configuration, and the goal is to have messages ranging between 800 bytes and 2 KB, adding up to approximately 1 GB/s volume throughput. Also, the ingestion pipelines are using the same worker type configuration on all runs (&lt;code>n2d-standard-4&lt;/code> GCE machines) and are capping the maximum workers number to avoid very large fleets.&lt;/p>
&lt;p>All of the executions run on Google Cloud using Dataflow, but you can apply all of the configurations and format changes to the suite while executing on other supported Apache Beam runners. Changes and recommendations are not runner specific.&lt;/p>
&lt;h3 id="local-environment-requirements">Local environment requirements&lt;/h3>
&lt;p>Before launching the startup scripts, install the following items in your local environment:&lt;/p>
&lt;ul>
&lt;li>&lt;code>gcloud&lt;/code>, along with the correct permissions&lt;/li>
&lt;li>Terraform&lt;/li>
&lt;li>JDK 17 or later&lt;/li>
&lt;li>Maven 3.6 or later&lt;/li>
&lt;/ul>
&lt;p>For more information, see the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests#requisites">requirements&lt;/a> section in the GitHub repository.&lt;/p>
&lt;p>Also, review the service quotas and resources available in your Google Cloud project. Specifically: Pub/Sub regional capacity, BigQuery ingestion quota, and Compute Engine instances available in the selected region for the tests.&lt;/p>
&lt;h3 id="workload-description">Workload description&lt;/h3>
&lt;p>Focusing on the ingestion pipeline, our &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/canonical-streaming-pipelines/src/main/java/com/google/cloud/pso/beam/pipelines/StreamingSourceToBigQuery.java#L55">workload&lt;/a> is straightforward. It completes the following steps:&lt;/p>
&lt;ol>
&lt;li>reads data in a specific format from Pub/Sub (Apache Thrift in this case)&lt;/li>
&lt;li>deals with potential compression and batching settings (not enabled by default)&lt;/li>
&lt;li>executes a UDF (identity function by default)&lt;/li>
&lt;li>transforms the input format to one of the formats supported by the &lt;code>BigQueryIO&lt;/code> transform&lt;/li>
&lt;li>writes the data to the configured table&lt;/li>
&lt;/ol>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/0-pipeline.png"
alt="Example Workload">&lt;/p>
&lt;p>The pipeline we used for the tests is highly configurable. For more details about how to tweak the ingestion, see the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/canonical-streaming-pipelines/src/main/java/com/google/cloud/pso/beam/pipelines/StreamingSourceToBigQuery.java#L39">options&lt;/a> in the file. No code changes are needed on any of our steps. The execution scripts take care of the configurations needed.&lt;/p>
&lt;p>Although these tests are focused on reading data from Pub/Sub, the ingestion pipeline is capable of reading data from a generic streaming source. The repository contains other &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/example-suite-scripts">examples&lt;/a> that show how to launch this same test suite reading data from Pub/Sub Lite and Kafka. In all cases, the pipeline automation sets up the streaming infrastructure.&lt;/p>
&lt;p>Finally, you can see in the &lt;a href="https://github.com/prodriguezdefino/apache-beam-ptransforms/blob/a0dd229081625c7b593512543614daf995a9f870/common/src/main/java/com/google/cloud/pso/beam/common/formats/options/TransportFormatOptions.java">configuration options&lt;/a> that the pipeline supports many transport format options for the input, such as Thrift, Avro, and JSON. This suite focuses on Thrift, because it is a common open source format, and because it generates a format transformation need. The intent is to put some strain in the workload processing. You can run similar tests for Avro and JSON input data. The streaming data generator pipeline can generate random data for the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/streaming-data-generator/src/main/java/com/google/cloud/pso/beam/generator/formats">three supported formats&lt;/a> by walking directly on the schema (Avro and JSON) or IDL (Thrift) provided for execution.&lt;/p>
&lt;h2 id="first-run-default-settings">First run: default settings&lt;/h2>
&lt;p>The default values for the execution writes the data to BigQuery using &lt;code>STREAMING_INSERTS&lt;/code> mode for &lt;code>BigQueryIO&lt;/code>. This mode correlates with the &lt;a href="https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll">&lt;code>tableData insertAll&lt;/code> API&lt;/a> for BigQuery. This API supports data in JSON format. From the Apache Beam perspective, using the &lt;code>BigQueryIO.writeTableRows&lt;/code> method lets us resolve the writes into BigQuery.&lt;/p>
&lt;p>For our ingestion pipeline, the Thrift format needs to be transformed into &lt;code>TableRow&lt;/code>. To do that, we need to translate the Thrift IDL into a BigQuery table schema. That can be achieved by translating the Thrift IDL into an Avro schema, and then using Beam utilities to translate the table schema for BigQuery. We can do this at bootstrap. The schema transformation is cached at the &lt;code>DoFn&lt;/code> level.&lt;/p>
&lt;p>After setting up the data generation and ingestion pipelines, and after letting the pipelines run for some minutes, we see that the pipeline is unable to sustain the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/1-default-ps.png"
alt="PubSub metrics">&lt;/p>
&lt;p>The previous image shows that the number of messages that are not being processed by the ingestion pipeline start to show as unacknowledged messages in Pub/Sub metrics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/1-default-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Reviewing the per stage performance metrics, we see that the pipeline shows a saw-like shape, which is often associated with the throttling mechanisms the Dataflow runner uses when some of the stages are acting as bottlenecks for the throughput. Also, we see that the &lt;code>Reshuffle&lt;/code> step on the &lt;code>BigQueryIO&lt;/code> write transform does not scale as expected.&lt;/p>
&lt;p>This behavior happens because by default the &lt;a href="https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java#L57">&lt;code>BigQueryOptions&lt;/code>&lt;/a> uses 50 different keys to shuffle data to workers before the writes happen on BigQuery. To solve this problem, we can add a configuration to our launch script that enables the write operations to scale to a larger number of workers, which improves performance.&lt;/p>
&lt;h2 id="second-run-improve-the-write-bottleneck">Second run: improve the write bottleneck&lt;/h2>
&lt;p>After increasing the number of streaming keys to a higher number, 512 keys in our case, we restarted the test suite. The Pub/Sub metrics started to improve. After an initial ramp on the size of the backlog, the curve started to ease out.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/2-skeys-ps.png"
alt="PubSub metrics">&lt;/p>
&lt;p>This is good, but we should take a look at the throughput per stage numbers to understand if we are achieving the goal we set up for this exercise.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/2-skeys-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Although the performance has clearly improved, and the Pub/Sub backlog no longer increases monotonically, we are still far from the goal of processing one million events per second (1 GB/s) for our ingestion pipeline. In fact, the throughput metrics jump all over, indicating that bottlenecks are preventing the processing from scaling further.&lt;/p>
&lt;h2 id="third-run-unleash-autoscale">Third run: unleash autoscale&lt;/h2>
&lt;p>Luckily for us, when writing into BigQuery, we can autoscale the writes. This step simplifies the configuration so that we don&amp;rsquo;t have to guess the right number of shards. We switched the pipeline’s configuration and enabled this setting for the next &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/scaling-streaming-workload-blog/3-ps2bq-si-tr-streamingautoshard.sh">launch script&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-parallelism.png"
alt="Key Parallelism">&lt;/p>
&lt;p>Immediately, we see that the autosharding mechanism tweaks the number of keys very aggressively and in a dynamic way. This change is good, because different moments in time might have different scale needs, such as early backlog recoveries and spikes in the execution.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-throughput-tr.png"
alt="Throughput">&lt;/p>
&lt;p>Inspecting the throughput performance per stage, we see that as the number of keys increases, the performance of the writes also increases. In fact, it reaches very large numbers!&lt;/p>
&lt;p>After the initial backlog was consumed and the pipeline stabilized, we saw that the desired performance numbers were reached. The pipeline can sustain processing many more than a million events per second from Pub/Sub and several GB/s of BigQuery ingestion. Yay!&lt;/p>
&lt;p>Still, we want to see if we can do better. We can introduce several improvements to the pipeline to make the execution more efficient. In most cases, the improvements are configuration changes. We just need to know where to focus next.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-autoscale.png"
alt="Resources">&lt;/p>
&lt;p>The previous image shows that the number of workers needed to sustain this throughput is still quite high. The workload itself is not CPU intensive. Most of the cost is spent on transforming formats and on I/O interactions, such as shuffles and the actual writes. To understand what to improve, we first investigate the transport formats.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-input.png"
alt="Thrift Input Size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-output.png"
alt="TableRow Output Size">&lt;/p>
&lt;p>Looking at the input size, right before the identity UDF execution, the data format is binary Thrift, which is a decently compact format even when no compression is used. However, while comparing the &lt;code>PCollection&lt;/code> approximated size with the &lt;code>TableRow&lt;/code> format needed for BigQuery ingestion, a clear size increase is visible. This is something we can improve by changing the BigQuery write API in use.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-overhead.png"
alt="Translation Overhead">&lt;/p>
&lt;p>When we inspect the &lt;code>StoreInBigQuery&lt;/code> transform, we see that the majority of the wall time is spent on the actual writes. Also, the wall time spent converting data to the destination format (&lt;code>TableRows&lt;/code>) compared with how much is spent in the actual writes is quite large: 13 times bigger for the writes. To improve this behavior, we can switch the pipeline write mode.&lt;/p>
&lt;h2 id="fourth-run-in-with-the-new">Fourth run: in with the new&lt;/h2>
&lt;p>In this run, we use the &lt;code>StorageWrite&lt;/code> API. Enabling the &lt;code>StorageWrite&lt;/code> API for this pipeline is straightforward. We set the write mode as &lt;code>STORAGE_WRITE_API&lt;/code> and define a write triggering frequency. For this test, we write data at most every ten seconds. The write triggering frequency controls how long the per-stream data accumulate. A higher number defines a larger output to be written after the stream assignment but also imposes a larger end-to-end latency for every element read from Pub/Sub. Similar to the &lt;code>STREAMING_WRITES&lt;/code> configuration, &lt;code>BigQueryIO&lt;/code> can handle autosharding for the writes, which we already demonstrated to be the best setting for performance.&lt;/p>
&lt;p>After both pipelines become stable, the performance benefits seen when using the &lt;code>StorageWrite&lt;/code> API in &lt;code>BigQueryIO&lt;/code> are apparent. After enabling the new implementation, the wall time rate between the format transformation and write operation decreases. The wall time spent on writes is only about 34 percent larger than the format transformation.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-format-transformation.png"
alt="Translation Overhead">&lt;/p>
&lt;p>After stabilization, the pipeline throughput is also quite smooth. The runner can quickly and steadily downscale the pipeline resources needed to sustain the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Looking at the resource scale needed to process the data, another dramatic improvement is visible. Whereas the streaming inserts-based pipeline needed more than 80 workers to sustain the throughput, the storage writes pipeline only needs 49, a 40 percent improvement.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>We can use the data generation pipeline as reference. This pipeline only needs to randomly generate data and write the events to Pub/Sub. It runs steadily with an average of 40 workers. The improvements on the ingestion pipeline using the right configuration for the workload makes it closer to those resources needed for the generation.&lt;/p>
&lt;p>Similar to the streaming inserts-based pipeline, writing the data into BigQuery requires running a format translation, from Thrift to &lt;code>TableRow&lt;/code> in the former and from Thrift to Protocol Buffers (protobuf) in the latter. Because we are using the &lt;code>BigQueryIO.writeTableRows&lt;/code> method, we add another step in the format translation. Because the &lt;code>TableRow&lt;/code> format also increases the size of the &lt;code>PCollection&lt;/code> being processed, we want to see if we can improve this step.&lt;/p>
&lt;h2 id="fifth-run-a-better-write-format">Fifth run: a better write format&lt;/h2>
&lt;p>When using &lt;code>STORAGE_WRITE_API&lt;/code>, the &lt;code>BigQueryIO&lt;/code> transform exposes a method that we can use to write the Beam row type directly into BigQuery. This step is useful because of the flexibility that the row type provides for interoperability and schema management. Also, it&amp;rsquo;s both efficient for shuffling and denser than &lt;code>TableRow&lt;/code>, so our pipeline will have smaller &lt;code>PCollection&lt;/code> sizes.&lt;/p>
&lt;p>For the next run, because our data volume is not small, we decrease the triggering frequency when writing to BigQuery. Because we use a different format, slightly different code runs. For this change, the test pipeline script is configured with the flag &lt;code>--formatToStore=BEAM_ROW&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-input-size.png"
alt="Thrift input size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-output-size.png"
alt="Row output size">&lt;/p>
&lt;p>The &lt;code>PCollection&lt;/code> size written into BigQuery is considerably smaller than on previous executions. In fact, for this particular execution, the Beam row format is a smaller size than the Thrift format. A larger &lt;code>PCollection&lt;/code> conformed by bigger per-element sizes can put nontrivial memory pressure in smaller worker configurations, reducing the overall throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-format-trasformation.png"
alt="Translation overhead">&lt;/p>
&lt;p>The wall clock rate for the format transformation and the actual BigQuery writes also maintain a very similar rate. Handling the Beam row format does not impose a performance penalty in the format translation and subsequent writes. This is confirmed by the number of workers in use by the pipeline when throughput becomes stable, slightly smaller than the previous run but clearly in the same range.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>Although we are in a much better position than when we started, given our test pipeline input format, there&amp;rsquo;s still room for improvement.&lt;/p>
&lt;h2 id="sixth-run-further-reduce-the-format-translation-effort">Sixth run: further reduce the format translation effort&lt;/h2>
&lt;p>Another supported format for the input &lt;code>PCollection&lt;/code> in the &lt;code>BigQueryIO&lt;/code> transform might be advantageous for our input format. The method &lt;code>writeGenericRecords&lt;/code> enables the transform to transform Avro &lt;code>GenericRecords&lt;/code> directly into protobuf before the write operation. Apache Thrift can be transformed into Avro &lt;code>GenericRecords&lt;/code> very efficiently. We can make another test run configuring our test ingestion pipeline by setting the option &lt;code>--formatToStore=AVRO_GENERIC_RECORD&lt;/code> on our execution script.&lt;/p>
&lt;p>This time, the difference between format translation and writes increases significantly, improving performance. The translation to Avro &lt;code>GenericRecords&lt;/code> is only 20 percent of the write effort spent on writing those records into BigQuery. Given that the test pipelines had similar runtimes and that the wall clock seen in the &lt;code>WriteIntoBigQuery&lt;/code> stage is also aligned with other &lt;code>StorageWrite&lt;/code> related runs, using this format is appropriate for this workload.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-format-transformation.png"
alt="Translation overhead">&lt;/p>
&lt;p>We see further gains when we look at resource utilization. We need less CPU time to execute the format translations for our workload while achieving the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>This pipeline improves upon the previous run, running steadily on 42 workers when throughput is stable. Given the worker configuration used (&lt;code>nd2-standard-4&lt;/code>), and the volume throughput of the workload process (about 1 GB/s), we are achieving about 6 MB/s throughput per CPU core, which is quite impressive for a streaming pipeline with exactly-once semantics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-latencies.png"
alt="Latencies">&lt;/p>
&lt;p>When we add up all of the stages executed in the main path of the pipeline, the latency seen at this scale achieves sub-second end-to-end latencies during sustained periods of time.&lt;/p>
&lt;p>Given the workload requirements and the implemented pipeline code, this performance is the best that we can extract without further tuning the runner’s specific settings.&lt;/p>
&lt;h2 id="seventh-run--lets-just-relax-at-least-some-constraints">Seventh run : lets just relax (at least some constraints)&lt;/h2>
&lt;p>When using the &lt;code>STORAGE\_WRITE\_API&lt;/code> setting for &lt;code>BigQueryIO&lt;/code>, we enforce exactly-once semantics on the writes. This configuration is great for use cases that need strong consistency on the data that gets processed, but it imposes a performance and cost penalty.&lt;/p>
&lt;p>From a high-level perspective, writes into BigQuery are made in batches, which are released based on the current sharding and the triggering frequency. If a write fails during the execution of a particular bundle, it is retried. A bundle of data is committed into BigQuery only when all the data in that particular bundle is correctly appended to a stream. This implementation needs to shuffle the full volume of data to create the batches that are written, and also the information of the finished batches for later commit (although this last piece is very small compared with the first).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-previous-data-input.png"
alt="Read data size">&lt;/p>
&lt;p>Looking at the previous pipeline execution, the total data being processed for the pipeline by Streaming Engine is larger than the data being read from Pub/Sub. For example, 7 TB of data is read from Pub/Sub, whereas the processing of data for the whole execution of the pipeline moves 25 TB of data to and from Streaming Engine.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-previous-shuffle-total.png"
alt="Streamed data size">&lt;/p>
&lt;p>When data consistency is not a hard requirement for ingestion, you can use at-least-once semantics with &lt;code>BigQueryIO&lt;/code> write mode. This implementation avoids shuffling and grouping data for the writes. However, this change might cause a small number of repeated rows to be written into the destination table. This can happen with append errors, infrequent worker restarts, and other even less frequent errors.&lt;/p>
&lt;p>Therefore, we add the configuration to use &lt;code>STORAGE_API_AT_LEAST_ONCE&lt;/code> write mode. To instruct the &lt;code>StorageWrite&lt;/code> client to reuse connections while writing data, we also add the configuration flag &lt;code>–useStorageApiConnectionPool&lt;/code>. This configuration option only works with &lt;code>STORAGE_API_AT_LEAST_ONCE&lt;/code> mode, and it reduces the occurrences of warnings similars to &lt;code>Storage Api write delay more than 8 seconds&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-resources.png"
alt="Resources">&lt;/p>
&lt;p>When pipeline throughput stabilizes, we see a similar pattern for resource utilization for the workload. The number of workers in use reaches 40, a small improvement compared with the last run. However, the amount of data being moved from Streaming Engine is much closer to the amount of data read from Pub/Sub.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-current-input.png"
alt="Read data size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-current-shuffle-total.png"
alt="Streamed data size">&lt;/p>
&lt;p>Considering all of these factors, this change further optimizes the workload, achieving a throughput of 6.4 MB/s per CPU core. This improvement is small compared to the same workload when using consistent writes into BigQuery, but it uses less streaming data resources. This configuration represents the most optimal setup for our workload, with the highest throughput per resource and the lowest streaming data across workers.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-latency.png"
alt="Streamed data size">&lt;/p>
&lt;p>This configuration also has impressively low latency for the end-to-end processing. Given that the main path of our pipeline has been fused in a single execution stage from reads to writes, we see that even at p99, the latency tends to be below 300 milliseconds at a quite large volume throughput (as previously mentioned around 1 GB/s).&lt;/p>
&lt;h2 id="recap">Recap&lt;/h2>
&lt;p>Optimizing Apache Beam streaming workloads for low latency and efficient execution requires careful analysis and decision-making, and the right configurations.&lt;/p>
&lt;p>Considering the scenario discussed in this article, it is essential to consider factors like overall CPU utilization, throughput and latency per stage, &lt;code>PCollection&lt;/code> sizes, wall time per stage, write mode, and transport formats, in addition to writing the right pipeline for the workload.&lt;/p>
&lt;p>Our experiments revealed that using the &lt;code>StorageWrite&lt;/code> API, autosharding for writes, and Avro &lt;code>GenericRecords&lt;/code> as the transport format yielded the most efficient results. Relaxing the consistency for writes can further improve performance.&lt;/p>
&lt;p>The accompanying &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests">Github repository&lt;/a> contains a test suite that you can use to replicate the analysis on your Google Cloud project or with a different runner setup. Feel free to take it for a spin. Comments and PRs are always welcome.&lt;/p></description></item><item><title>Blog: Build a scalable, self-managed streaming infrastructure with Beam and Flink: Tackling Autoscaling Challenges - Part 2</title><link>/blog/apache-beam-flink-and-kubernetes-part2/</link><pubDate>Mon, 18 Dec 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes-part2/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink-tackling-autoscaling-challenges---part-2">Build a scalable, self-managed streaming infrastructure with Flink: Tackling Autoscaling Challenges - Part 2&lt;/h1>
&lt;p>Welcome to Part 2 of our in-depth series about building and managing a service for Apache Beam Flink on Kubernetes. In this segment, we&amp;rsquo;re taking a closer look at the hurdles we encountered while implementing autoscaling. These challenges weren&amp;rsquo;t just roadblocks. They were opportunities for us to innovate and enhance our system. Let’s break down these issues, understand their context, and explore the solutions we developed.&lt;/p>
&lt;h2 id="understand-apache-beam-backlog-metrics-in-the-flink-runner-environment">Understand Apache Beam backlog metrics in the Flink runner environment&lt;/h2>
&lt;p>&lt;strong>The Challenge:&lt;/strong> In our current setup, we are using Apache Flink for processing data streams. However, we&amp;rsquo;ve encountered a puzzling issue: our Flink job isn&amp;rsquo;t showing the backlog metrics from Apache Beam. These metrics are critical for understanding the state and performance of our data pipelines.&lt;/p>
&lt;p>&lt;strong>What We Found:&lt;/strong> Interestingly, we noticed that the metrics are actually being generated in &lt;code>KafkaIO&lt;/code>, which is a part of our data pipeline that handles Kafka streams. But when we try to monitor these metrics through the Apache Flink Metric system, we can&amp;rsquo;t find them. We suspected that there might be an issue with the integration (or &amp;lsquo;wiring&amp;rsquo;) between Apache Beam and Apache Flink.&lt;/p>
&lt;p>&lt;strong>Digging Deeper:&lt;/strong> On closer inspection, we found that the metrics should be emitted during the &amp;lsquo;Checkpointing&amp;rsquo; phase of the data stream processing. During this crucial step, the system takes a snapshot of the stream&amp;rsquo;s state, and the metrics are typically metrics that are generated for unbounded sources. Unbounded sources are sources that continuously stream data, like Kafka.&lt;/p>
&lt;p>&lt;strong>A Potential Solution:&lt;/strong> We believe the root of the problem lies in how the metric context is set during the checkpointing phase. A disconnect appears to prevent the Beam metrics from being properly captured in the Flink Metric system. We proposed a fix for this issue, which you can review and contribute to on our GitHub pull request: &lt;a href="https://github.com/apache/beam/pull/29793">Apache Beam PR #29793&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/flink-backlog-metrics.png"
alt="Apache Flink Beam Backlog Metrics">&lt;/p>
&lt;h2 id="overcoming-challenges-in-checkpoint-size-reduction-for-autoscaling-beam-jobs">Overcoming challenges in checkpoint size reduction for autoscaling Beam jobs&lt;/h2>
&lt;p>In this section we will discuss strategies for reducing the size of checkpoints in autoscaling Apache Beam jobs, focusing on efficient checkpointing in Apache Flink and optimizing bundle sizes and PipelineOptions to manage frequent checkpoint timeouts and large-scale job requirements.&lt;/p>
&lt;h3 id="understand-the-basics-of-checkpointing-in-apache-flink">Understand the basics of checkpointing in Apache Flink&lt;/h3>
&lt;p>In stream processing, maintaining state consistency and fault tolerance is crucial. Apache Flink achieves this through a process called &lt;em>checkpointing&lt;/em>. Checkpointing periodically captures the state of a job&amp;rsquo;s operators and stores it in a stable storage location, like Google Cloud Storage or AWS S3. Specifically, Flink checkpoints a job every ten seconds and allows up to one minute for this process to complete. This process is vital for ensuring that, in case of failures, the job can resume from the last checkpoint, providing exactly-once semantics and fault tolerance.&lt;/p>
&lt;h3 id="the-role-of-bundles-in-apache-beam">The role of bundles in Apache Beam&lt;/h3>
&lt;p>Apache Beam introduces the concept of a &lt;em>bundle&lt;/em>. A bundle is essentially a group of elements that are processed together. This step enhances processing efficiency and throughput by reducing the overhead of handling each element separately. For more information, see &lt;a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence">Bundling and persistence&lt;/a>. In the Flink runner &lt;a href="https://beam.apache.org/releases/javadoc/2.52.0/org/apache/beam/runners/flink/FlinkPipelineOptions.html#getMaxBundleSize--">default configuration&lt;/a>, a bundle&amp;rsquo;s default size is 1000 elements with a one-second timeout. However, based on our performance tests, we adjusted the bundle size to &lt;em>10,000 elements with a 10-second timeout&lt;/em>.&lt;/p>
&lt;h3 id="challenge-frequent-checkpoint-timeouts">Challenge: frequent checkpoint timeouts&lt;/h3>
&lt;p>When we configured checkpointing every 10 seconds, we faced frequent checkpoint timeouts, often exceeding 1 minute. This was due to the large size of the checkpoints.&lt;/p>
&lt;h3 id="solution-manage-checkpoint-size">Solution: Manage checkpoint size&lt;/h3>
&lt;p>In Apache Beam Flink jobs, the &lt;code>finishBundleBeforeCheckpointing&lt;/code> option plays a pivotal role. When enabled, it ensures that all bundles are completely processed before initiating a checkpoint. This results in checkpoints that only contain the state post-bundle completion, significantly reducing checkpoint size. Initially, our checkpoints were around 2 MB per pipeline. With this change, they consistently dropped to 150 KB.&lt;/p>
&lt;h3 id="address-the-checkpoint-size-in-large-scale-jobs">Address the checkpoint size in large-scale jobs&lt;/h3>
&lt;p>Despite reducing checkpoint sizes, a 150 KB checkpoint every ten seconds can still be substantial, especially in jobs that run multiple pipelines. For instance, with 100 pipelines in a single job, this size balloons to 15 MB per 10-second interval.&lt;/p>
&lt;h3 id="further-optimization-reduce-checkpoint-size-with-pipelineoptions">Further optimization: reduce checkpoint size with PipelineOptions&lt;/h3>
&lt;p>We discovered that due to a specific issue (BEAM-8577), our Flink runner was including our large &lt;code>PipelineOptions&lt;/code> objects in every checkpoint. We solved this problem by removing unnecessary application-related options from &lt;code>PipelineOptions&lt;/code>, further reducing the checkpoint size to a more manageable 10 KB per pipeline.&lt;/p>
&lt;h2 id="kafka-reader-wait-time-solving-autoscaling-challenges-in-beam-jobs">Kafka Reader wait time: solving autoscaling challenges in Beam jobs&lt;/h2>
&lt;h3 id="understand-unaligned-checkpointing">Understand unaligned checkpointing&lt;/h3>
&lt;p>In our system, we use unaligned checkpointing to speed up the process of checkpointing, which is essential for ensuring data consistency in distributed systems. However, when we activated the &lt;code>finishBundleBeforeCheckpointing&lt;/code> feature, we began facing checkpoint timeout issues and delays in checkpointing steps. Apache Beam leverages Apache Flink&amp;rsquo;s legacy source implementation for processing unbounded sources. In Flink, tasks are categorized into two types: source tasks and non-source tasks.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Source tasks&lt;/strong>: fetch data from external systems into a Flink job&lt;/li>
&lt;li>&lt;strong>Non-source tasks&lt;/strong>: process the incoming data&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/kafkaio-wait-reader.png"
alt="Apache Flink Task Types">&lt;/p>
&lt;p>In the standard configuration, non-source tasks check for an available buffer before pulling data. If source tasks don&amp;rsquo;t perform this check, they might experience checkpointing delays in writing data to the output buffer. This delay affects the efficiency of unaligned checkpoints, which are only recognized by legacy source tasks when an output buffer is available.&lt;/p>
&lt;h3 id="address-the-challenge-with-unboundedsourcewrapper-in-beam">Address the challenge with UnboundedSourceWrapper in Beam&lt;/h3>
&lt;p>To solve this problem, Apache Flink introduced a new source implementation that operates in a pull mode. In this mode, a task checks for a free buffer before fetching data, aligning with the approach of non-source tasks.&lt;/p>
&lt;p>However, the legacy source, still used by Apache Beam&amp;rsquo;s Flink Runner, operates in a push mode. It sends data to downstream tasks immediately. This setup might create bottlenecks when buffers are full, causing delays in detecting unaligned checkpoint barriers.&lt;/p>
&lt;h3 id="our-solution">Our solution&lt;/h3>
&lt;p>Despite its deprecation, Apache Beam&amp;rsquo;s Flink Runner still uses the legacy source implementation. To address its issues, we implemented our modifications and the quick workarounds suggested in &lt;a href="https://issues.apache.org/jira/browse/FLINK-26759">FLINK-26759&lt;/a>. These enhancements are detailed in our &lt;a href="#">Pull Request&lt;/a>. You can also find more information about unaligned checkpoint issues in the &lt;a href="https://blog.51cto.com/u_14286418/7000028">Flink Unaligned Checkpoint&lt;/a> blog post.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/checkpoint_monitoring-history-subtasks.png"
alt="Apache Flink UI Checkpoint History">&lt;/p>
&lt;h2 id="address-slow-reads-in-high-traffic-scenarios">Address slow reads in high-traffic scenarios&lt;/h2>
&lt;p>In our journey with Apache Beam and the Flink Runner, we encountered a significant challenge similar to one documented in the post &lt;a href="https://antonio-si.medium.com/how-intuit-debug-consumer-lags-in-apache-beam-22ca3b39602e">How Intuit Debug Consumer Lags in Apache Beam&lt;/a> by &lt;a href="https://antonio-si.medium.com/">Antonio Si&lt;/a> in his experience at Intuit. Their real-time data processing pipelines had increasing Kafka consumer lag, particularly with topics experiencing high message traffic. This issue was traced to Apache Beam&amp;rsquo;s handling of Kafka partitions through &lt;code>UnboundedSourceWrapper&lt;/code> and &lt;code>KafkaUnboundedReader&lt;/code>. Specifically, for topics with lower traffic, the processing thread paused unnecessarily, delaying the processing of high-traffic topics. We faced a parallel situation in our system, where the imbalance in processing speeds between high- and low-traffic topics led to inefficiencies.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/adaptive-timeout-kafka.png"
alt="UnboundedSourceWrapper Design">&lt;/p>
&lt;p>To resolve this issue, we developed an innovative solution: an adaptive timeout strategy in &lt;code>KafkaIO&lt;/code>. This strategy dynamically adjusts the timeout duration based on the traffic of each topic. For low-traffic topics, it shortens the timeout, preventing unnecessary delays. For high-traffic topics, it extends the timeout, providing more processing opportunities. This approach is detailed in our recent pull request.&lt;/p>
&lt;h2 id="unbalanced-partition-distribution-in-beam-job-autoscaling">Unbalanced partition distribution in Beam job autoscaling&lt;/h2>
&lt;p>At the heart of this system is the adaptive scheduler, a component designed for rapid resource allocation. It intelligently adjusts the number of parallel tasks (parallelism) a job performs based on the availability of computing slots. These slots are like individual workstations, each capable of handling certain parts of the job.&lt;/p>
&lt;p>However, we encountered a problem. Our jobs consist of multiple independent pipelines, each needing its own set of resources. Initially, the system tended to overburden the first few workers by assigning them more tasks, while others remained underutilized. This issue was due to the way Flink allocated tasks, favoring the first workers for each pipeline.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/flink-partition-assignment.png"
alt="Flink split assignment on slots">&lt;/p>
&lt;p>To address this issue, we developed a custom patch for Flink&amp;rsquo;s &lt;em>SlotSharingSlotAllocator&lt;/em>, a component responsible for task distribution. This patch ensures a more balanced workload distribution across all available workers, improving efficiency and preventing bottlenecks.
With this improvement, each worker gets a fair share of tasks, leading to better resource utilization and smoother operation of our Beam Jobs.&lt;/p>
&lt;h2 id="drain-support-in-kubernetes-operator-with-flink">Drain support in Kubernetes Operator with Flink&lt;/h2>
&lt;h3 id="the-challenge">The challenge&lt;/h3>
&lt;p>In the world of data processing with Apache Flink, a common task is to manage and update data-processing jobs. These jobs could be either stateful, where they remember past data, or stateless, where they don&amp;rsquo;t.&lt;/p>
&lt;p>In the past, when we needed to update or delete a Flink job managed by the Kubernetes Operator, the system saved the current state of the job using a savepoint or checkpoint. However, a crucial step was missing: the system didn&amp;rsquo;t stop the job from processing new data (this is what we mean by draining the job). This oversight could lead to two major issues:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>For stateful jobs:&lt;/strong> potential data inconsistencies, because the job might process new data that wasn&amp;rsquo;t accounted for in the savepoint&lt;/li>
&lt;li>&lt;strong>For stateless jobs:&lt;/strong> data duplication, because the job might reprocess data it already processed&lt;/li>
&lt;/ol>
&lt;h3 id="the-solution-drain-function">The solution: drain function&lt;/h3>
&lt;p>This is where the update referenced as &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> is needed. This update introduced a drain function. Think of it as telling the job, &amp;ldquo;Finish what you&amp;rsquo;re currently processing, but don&amp;rsquo;t take on anything new.&amp;rdquo; Here&amp;rsquo;s how it works:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Stop new data:&lt;/strong> The job stops reading new input.&lt;/li>
&lt;li>&lt;strong>Mark the source:&lt;/strong> The job marks the source with an infinite watermark. Think of this watermark as a marker that tells the system that there&amp;rsquo;s no more new data to process.&lt;/li>
&lt;li>&lt;strong>Propagate through the pipeline:&lt;/strong> This marker is then passed through the job&amp;rsquo;s processing pipeline, ensuring that every part of the job knows not to expect any new data.&lt;/li>
&lt;/ol>
&lt;p>This seemingly small change has a big impact. It ensures that when a job is updated or deleted, the data it processes remains consistent and accurate. This is crucial for any data-processing task, because it maintains the integrity and reliability of the data. Furthermore, in cases where the drainage fails, you can cancel the job without needing a savepoint, which adds a layer of flexibility and safety to the whole process.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>As we conclude Part 2 of our series on building and managing Apache Beam Flink services on Kubernetes, it&amp;rsquo;s evident that the journey of implementing autoscaling has been both challenging and enlightening. The obstacles we faced, from understanding Apache Beam backlog metrics in the Flink Runner environment to addressing slow reads in high-traffic scenarios, pushed us to develop innovative solutions and deepen our understanding of streaming infrastructure.&lt;/p>
&lt;p>Our exploration into the intricacies of checkpointing, Kafka Reader wait times, and unbalanced partition distribution revealed the complexities of autoscaling Beam jobs. These challenges prompted us to devise strategies like the adaptive timeout in &lt;code>KafkaIO&lt;/code> and the balanced workload distribution in Flink&amp;rsquo;s &lt;code>SlotSharingSlotAllocator&lt;/code>. Additionally, the introduction of the drain support in Kubernetes Operator with Flink marks a significant advancement in managing stateful and stateless jobs effectively.&lt;/p>
&lt;p>This journey has not only enhanced the robustness and efficiency of our system but has also contributed valuable insights to the broader community working with Apache Beam and Flink. We hope that our experiences and solutions will aid others facing similar challenges in their projects.&lt;/p>
&lt;p>Stay tuned for our next blog post, where we&amp;rsquo;ll delve into the specifics of autoscaling in Apache Beam. We&amp;rsquo;ll break down the concepts, strategies, and best practices to effectively scale your Beam jobs. Thank you for following our series, and we look forward to sharing more of our journey and learnings with you.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed, Flink-based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitik Dani, and David He.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Explore More:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">Part 1: Introduction to Building and Managing Apache Beam Flink Services on Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Join the conversation and share your experiences on our &lt;a href="https://beam.apache.org/community/">Community&lt;/a> or contribute to our ongoing projects on &lt;a href="https://github.com/apache/beam">GitHub&lt;/a>. Your feedback is invaluable. If you have any comments or questions about this series, please feel free to reach out to us via &lt;a href="https://beam.apache.org/community/contact-us/">User Mailist&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>Stay connected with us for more updates and insights into Apache Beam, Flink, and Kubernetes.&lt;/em>&lt;/p></description></item><item><title>Blog: Apache Beam 2.52.0</title><link>/blog/beam-2.52.0/</link><pubDate>Fri, 17 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.52.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.52.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2520-2023-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.52.0, check out the &lt;a href="https://github.com/apache/beam/milestone/16">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Previously deprecated Avro-dependent code (Beam Release 2.46.0) has been finally removed from Java SDK &amp;ldquo;core&amp;rdquo; package.
Please, use &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead. This will allow to easily update Avro version in user code without
potential breaking changes in Beam &amp;ldquo;core&amp;rdquo; since the Beam Avro extension already supports the latest Avro versions and
should handle this. (&lt;a href="https://github.com/apache/beam/issues/25252">#25252&lt;/a>).&lt;/li>
&lt;li>Publishing Java 21 SDK container images now supported as part of Apache Beam release process. (&lt;a href="https://github.com/apache/beam/issues/28120">#28120&lt;/a>)
&lt;ul>
&lt;li>Direct Runner and Dataflow Runner support running pipelines on Java21 (experimental until tests fully setup). For other runners (Flink, Spark, Samza, etc) support status depend on runner projects.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add &lt;code>UseDataStreamForBatch&lt;/code> pipeline option to the Flink runner. When it is set to true, Flink runner will run batch
jobs using the DataStream API. By default the option is set to false, so the batch jobs are still executed
using the DataSet API.&lt;/li>
&lt;li>&lt;code>upload_graph&lt;/code> as one of the Experiments options for DataflowRunner is no longer required when the graph is larger than 10MB for Java SDK (&lt;a href="https://github.com/apache/beam/pull/28621">PR#28621&lt;/a>).&lt;/li>
&lt;li>state amd side input cache has been enabled to a default of 100 MB. Use &lt;code>--max_cache_memory_usage_mb=X&lt;/code> to provide cache size for the user state API and side inputs. (Python) (&lt;a href="https://github.com/apache/beam/issues/28770">#28770&lt;/a>).&lt;/li>
&lt;li>Beam YAML stable release. Beam pipelines can now be written using YAML and leverage the Beam YAML framework which includes a preliminary set of IO&amp;rsquo;s and turnkey transforms. More information can be found in the YAML root folder and in the &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/README.md">README&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>org.apache.beam.sdk.io.CountingSource.CounterMark&lt;/code> uses custom &lt;code>CounterMarkCoder&lt;/code> as a default coder since all Avro-dependent
classes finally moved to &lt;code>extensions/avro&lt;/code>. In case if it&amp;rsquo;s still required to use &lt;code>AvroCoder&lt;/code> for &lt;code>CounterMark&lt;/code>, then,
as a workaround, a copy of &amp;ldquo;old&amp;rdquo; &lt;code>CountingSource&lt;/code> class should be placed into a project code and used directly
(&lt;a href="https://github.com/apache/beam/issues/25252">#25252&lt;/a>).&lt;/li>
&lt;li>Renamed &lt;code>host&lt;/code> to &lt;code>firestoreHost&lt;/code> in &lt;code>FirestoreOptions&lt;/code> to avoid potential conflict of command line arguments (Java) (&lt;a href="https://github.com/apache/beam/pull/29201">#29201&lt;/a>).&lt;/li>
&lt;li>Transforms which use &lt;code>SnappyCoder&lt;/code> are update incompatible with previous versions of the same transform (Java) on some runners. This includes PubSubIO&amp;rsquo;s read (&lt;a href="https://github.com/apache/beam/pull/28655#issuecomment-2407839769">#28655&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &amp;ldquo;Desired bundle size 0 bytes must be greater than 0&amp;rdquo; in Java SDK&amp;rsquo;s BigtableIO.BigtableSource when you have more cores than bytes to read (Java) &lt;a href="https://github.com/apache/beam/issues/28793">#28793&lt;/a>.&lt;/li>
&lt;li>&lt;code>watch_file_pattern&lt;/code> arg of the &lt;a href="https://github.com/apache/beam/blob/104c10b3ee536a9a3ea52b4dbf62d86b669da5d9/sdks/python/apache_beam/ml/inference/base.py#L997">RunInference&lt;/a> arg had no effect prior to 2.52.0. To use the behavior of arg &lt;code>watch_file_pattern&lt;/code> prior to 2.52.0, follow the documentation at &lt;a href="https://beam.apache.org/documentation/ml/side-input-updates/">https://beam.apache.org/documentation/ml/side-input-updates/&lt;/a> and use &lt;code>WatchFilePattern&lt;/code> PTransform as a SideInput. (&lt;a href="https://github.com/apache/beam/pulls/28948">#28948&lt;/a>)&lt;/li>
&lt;li>&lt;code>MLTransform&lt;/code> doesn&amp;rsquo;t output artifacts such as min, max and quantiles. Instead, &lt;code>MLTransform&lt;/code> will add a feature to output these artifacts as human readable format - &lt;a href="https://github.com/apache/beam/issues/29017">#29017&lt;/a>. For now, to use the artifacts such as min and max that were produced by the eariler &lt;code>MLTransform&lt;/code>, use &lt;code>read_artifact_location&lt;/code> of &lt;code>MLTransform&lt;/code>, which reads artifacts that were produced earlier in a different &lt;code>MLTransform&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/29016/">#29016&lt;/a>)&lt;/li>
&lt;li>Fixed a memory leak, which affected some long-running Python pipelines: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &lt;a href="https://www.cve.org/CVERecord?id=CVE-2023-39325">CVE-2023-39325&lt;/a> (Java/Python/Go) (&lt;a href="https://github.com/apache/beam/issues/29118">#29118&lt;/a>).&lt;/li>
&lt;li>Mitigated &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47248">CVE-2023-47248&lt;/a> (Python) &lt;a href="https://github.com/apache/beam/issues/29392">#29392&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known issues&lt;/h2>
&lt;ul>
&lt;li>MLTransform drops the identical elements in the output PCollection. For any duplicate elements, a single element will be emitted downstream. (&lt;a href="https://github.com/apache/beam/issues/29600">#29600&lt;/a>).&lt;/li>
&lt;li>Some Python pipelines that run with 2.52.0-2.54.0 SDKs and use large materialized side inputs might be affected by a performance regression. To restore the prior behavior on these SDK versions, supply the &lt;code>--max_cache_memory_usage_mb=0&lt;/code> pipeline option. (Python) (&lt;a href="https://github.com/apache/beam/issues/30360">#30360&lt;/a>).&lt;/li>
&lt;li>Users who lauch Python pipelines in an environment without internet access and use the &lt;code>--setup_file&lt;/code> pipeline option might experience an increase in pipeline submission time. This has been fixed in 2.56.0 (&lt;a href="https://github.com/apache/beam/pull/31070">#31070&lt;/a>).&lt;/li>
&lt;li>Transforms which use &lt;code>SnappyCoder&lt;/code> are update incompatible with previous versions of the same transform (Java) on some runners. This includes PubSubIO&amp;rsquo;s read (&lt;a href="https://github.com/apache/beam/pull/28655#issuecomment-2407839769">#28655&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.52.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Devansh Modi&lt;/p>
&lt;p>Dominik Dębowczyk&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Jiangjie Qin&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>Joar Wandborg&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Luís Bianchin&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vivek Sumanth&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>aku019&lt;/p>
&lt;p>brucearctor&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>ddebowczyk92&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>dpcollins-google&lt;/p>
&lt;p>edman124&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>illoise&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p></description></item><item><title>Blog: Contributor Spotlight: Johanna Öjeling</title><link>/blog/contributor-spotlight-johanna-ojeling/</link><pubDate>Sat, 11 Nov 2023 15:00:00 -0800</pubDate><guid>/blog/contributor-spotlight-johanna-ojeling/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Johanna Öjeling is a Senior Software Engineer at &lt;a href="https://normative.io/">Normative&lt;/a>. She started using Apache Beam in 2020 at her previous company &lt;a href="http://datatonic.com">Datatonic&lt;/a> and began contributing in 2022 at a personal capacity. We interviewed Johanna to learn more about her interests and we hope that this will inspire new, future, diverse set of contributors to participate in OSS projects.&lt;/p>
&lt;p>&lt;strong>What areas of interest are you passionate about in your career?&lt;/strong>&lt;/p>
&lt;p>My core interest lies in distributed and data-intensive systems, and I enjoy working on challenges related to performance, scalability and maintainability. I also feel strongly about developer experience, and like to build tools and frameworks that make developers happier and more productive. Aside from that, I take pleasure in mentoring and coaching other software engineers to grow their skills and pursue a fulfilling career.&lt;/p>
&lt;p>&lt;strong>What motivated you to make your first contribution?&lt;/strong>&lt;/p>
&lt;p>I was already a user of the Apache Beam Java and Python SDKs and Google Cloud Dataflow in my previous job, and had started to play around with the Go SDK to learn Go. When I noticed that a feature I wanted was missing, it seemed like a great opportunity to implement it. I had been curious about developing open source software for some time, but did not have a good idea until then of what to contribute with.&lt;/p>
&lt;p>&lt;strong>In which way have you contributed to Apache Beam?&lt;/strong>&lt;/p>
&lt;p>I have primarily worked on the Go SDK with implementation of new features, bug fixes, tests, documentation and code reviews. Some examples include a MongoDB I/O connector with dynamically scalable reads and writes, a file I/O connector supporting continuous file discovery, and an Amazon S3 file system implementation.&lt;/p>
&lt;p>&lt;strong>How has your open source engagement impacted your personal or professional growth?&lt;/strong>&lt;/p>
&lt;p>Contributing to open source is one of the best decisions I have taken professionally. The Beam community has been incredibly welcoming and appreciative, and it has been rewarding to collaborate with talented people around the world to create software that is free for anyone to benefit from. Open source has opened up new opportunities to challenge myself, dive deeper into technologies I like, and learn from highly skilled professionals. To me, it has served as an outlet for creativity, problem solving and purposeful work.&lt;/p>
&lt;p>&lt;strong>How have you noticed contributing to open source is different from contributing to closed source/proprietary software?&lt;/strong>&lt;/p>
&lt;p>My observation has been that there are higher requirements for software quality in open source, and it is more important to get things right the first time. My closed source software experience is from startups/scale-ups where speed is prioritized. When not working on public facing APIs or libraries, one can also more easily change things, whereas we need to be mindful about breaking changes in Beam. I care for software quality and value the high standards the Beam committers hold.&lt;/p>
&lt;p>&lt;strong>What do you like to do with your spare time when you&amp;rsquo;re not contributing to Beam?&lt;/strong>&lt;/p>
&lt;p>Coding is a passion of mine so I tend to spend a lot of my free time on hobby projects, reading books and articles, listening to talks and attending events. When I was younger I loved learning foreign languages and studied English, French, German and Spanish. Later I discovered an interest in computer science and switched focus to programming languages. I decided to change careers to software engineering and have tried to learn as much as possible ever since. I love that it never ends.&lt;/p>
&lt;p>&lt;strong>What future features/improvements are you most excited about, or would you like to see on Beam?&lt;/strong>&lt;/p>
&lt;p>The multi-language pipeline support is an impressive feature of Beam, and I like that new SDKs such as TypeScript and Swift are emerging, which enables developers to write pipelines in their preferred language. Naturally, I am also excited to see where the Go SDK is headed and how we can make use of newer features of the Go language.&lt;/p>
&lt;p>&lt;strong>What types of contributions or support do you think the Beam community needs more of?&lt;/strong>&lt;/p>
&lt;p>Many data and machine learning engineers feel more comfortable with Python than Java and wish the Python SDK were as feature rich as the Java SDK. This presents great opportunities for Python developers to start contributing to Beam. As an SDK author, one can take advantage of Beam&amp;rsquo;s multiple SDKs. When I have developed in Go I have often studied the Java and Python implementations to get ideas for how to solve specific problems and make sure the Go SDK follows a similar pattern.&lt;/p>
&lt;p>&lt;strong>What advice would you give to someone who wants to contribute but does not know where to begin?&lt;/strong>&lt;/p>
&lt;p>Start with asking yourself what prior knowledge you have and what you would like to learn, then look for opportunities that match that. The contribution guidelines will tell you where to find open issues and what the process looks like. There are tasks labeled as &amp;ldquo;good first issue&amp;rdquo; which can be a good starting point. I was quite nervous about making my first contribution and had my mentor pre-review my PR. There was no need to worry though, as people will be grateful for your effort to improve the project. The pride I felt when a committer approved my PR and welcomed me to Beam is something I still remember.&lt;/p>
&lt;p>&lt;strong>What advice would you give to the Beam community? What could we improve?&lt;/strong>&lt;/p>
&lt;p>We can make it easier for new community members to get involved by providing more examples of tasks that we need help with, both in the form of code and non-code contributions. I will take it as an action point myself to label more issues accordingly and tailor the descriptions for newcomers. However, this is contingent on community members visiting the GitHub project. To address this, we could also proactively promote opportunities through social channels and the user mailing list.&lt;/p>
&lt;p>&lt;em>We thank Johanna for the interview and for her contributions! If you would like to learn more about contributing to Beam you can learn more about it here: &lt;a href="https://beam.apache.org/contribute/">https://beam.apache.org/contribute/&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Blog: Build a scalable, self-managed streaming infrastructure with Beam and Flink</title><link>/blog/apache-beam-flink-and-kubernetes/</link><pubDate>Fri, 03 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog series, &lt;a href="https://www.linkedin.com/in/talatuyarer/">Talat Uyarer (Architect / Senior Principal Engineer)&lt;/a>, &lt;a href="https://www.linkedin.com/in/rishabhkedia/">Rishabh Kedia (Principal Engineer)&lt;/a>, and &lt;a href="https://www.linkedin.com/in/davidqhe/">David He (Engineering Director)&lt;/a> describe how we built a self-managed streaming platform by using Apache Beam and Flink. In this part of the series, we describe why and how we built a large-scale, self-managed streaming infrastructure and services based on Flink by migrating from a cloud managed streaming service. We also outline the learnings for operational scalability and observability, performance, and cost effectiveness. We summarize techniques that we found useful in our journey.&lt;/p>
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink---part-1">Build a scalable, self-managed streaming infrastructure with Flink - part 1&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Palo Alto Networks (PANW) is a leader in cybersecurity, providing products, services and solutions to our customers. Data is the center of our products and services. We stream and store exabytes of data in our data lake, with near real-time ingestion, data transformation, data insertion to data store, and forwarding data to our internal ML-based systems and external SIEM’s. We support multi-tenancy in each component so that we can isolate tenants and provide optimal performance and SLA. Streaming processing plays a critical role in the pipelines.&lt;/p>
&lt;p>In the second part of the series, we provide a more thorough description of the core building blocks of our streaming infrastructure, such as autoscaler. We also give more details about our customizations, which enabled us to build a high-performance, large-scale streaming system. Finally, we explain how we solved challenging problems.&lt;/p>
&lt;h2 id="the-importance-of-self-managed-streaming-infrastructure">The importance of self-managed streaming Infrastructure&lt;/h2>
&lt;p>We built a large-scale data platform on Google Cloud. We used Dataflow as a managed streaming service. With Dataflow, we used the streaming engine running our application using Apache Beam and observability tools such as Cloud Logging and Cloud Monitoring. For more details, see [1]. The system can handle 15 million of events per second and one trillion events daily, at four petabytes of data volume daily. We run about 30,000 Dataflow jobs. Each job can have one or hundreds of workers, depending on the customer’s event throughputs.&lt;/p>
&lt;p>We support various applications using different endpoints: BigQuery data store, HTTPS-based external SIEMs or internal endpoints, Syslog based SIEMs, and Google Cloud Storage endpoints. Our customers and products rely on this data platform to handle cybersecurity postures and reactions. Our streaming infrastructure is highly flexible to add, update, and delete use cases through a streaming job subscription. For example, a customer wants to ingest log events from a firewall device into the data lake buffered in Kafka topics. A streaming job is subscribed to extract and filter the data, transform the data format, and do a streaming insert to our BigQuery data warehouse endpoint in real-time. The customer can use our visualization and dashboard products to view traffic or threads captured by this firewall. The following diagram illustrates the event producer, the use case subscription workflow, and the key components of the streaming platform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/image1.png"
alt="Streaming service design">&lt;/p>
&lt;p>This managed, Dataflow-based streaming infrastructure runs fine, but with some caveats:&lt;/p>
&lt;ol>
&lt;li>Cost is high, because it is a managed service. For the same resources used in a Dataflow application, such as vCPU and memory, the cost is much more expensive than using an open source streaming engine such as Flink running the same Beam application code.&lt;/li>
&lt;li>It&amp;rsquo;s not easy to achieve our latency and SLA goals, because it&amp;rsquo;s difficult to extend features, such as autoscaling based on different applications, endpoints, or different parameters within one application.&lt;/li>
&lt;li>The pipeline only runs on Google Cloud.&lt;/li>
&lt;/ol>
&lt;p>The uniqueness of PANW’s streaming use cases is another reason that we use a self-managed service. We support multi-tenancy. A tenant (a customer) can ingest data at a very high rate (&amp;gt;100k requests per second), or at a very low rate (&amp;lt; 100 requests per second). A Dataflow job runs on VMs instead of Kubernetes, requiring a minimal one vCPU core. With a small tenant, this wastes resources. Our streaming infrastructure supports thousands of jobs, and the CPU utilization is more efficient if we do not have to use one core for a job. It is natural for us to use a streaming engine running on Kubernetes, so that we can allocate minimal resources for a small tenant, for example, using a Google Kubernetes Engine (GKE) pod with ½ or less vCPU core.&lt;/p>
&lt;h2 id="the-choice-of-apache-flink-and-kubernetes">The choice of Apache Flink and Kubernetes&lt;/h2>
&lt;p>In an effort to handle the problems already stated and to find the most efficient solution, we evaluated various streaming frameworks, including Apache Samza, Apache Flink, and Apache Spark, against Dataflow.&lt;/p>
&lt;h3 id="performance">Performance&lt;/h3>
&lt;ul>
&lt;li>One notable factor was Apache Flink’s native Kubernetes support. Unlike Samza, which lacked native Kubernetes support and required Apache Zookeeper for coordination, Flink seamlessly integrated with Kubernetes. This integration eliminated unnecessary complexities. In terms of performance, both Samza and Flink were close competitors.&lt;/li>
&lt;li>Apache Spark, while popular, proved to be significantly slower in our tests. A presentation at the Beam Summit revealed that Apache Beam’s Spark Runner was approximately ten times slower than Native Apache Spark [3]. We could not afford such a drastic performance hit. Rewriting our entire Beam codebase with native Spark was not a viable option, especially given the extensive codebase we had built over the past four years with Apache Beam.&lt;/li>
&lt;/ul>
&lt;h3 id="community">Community&lt;/h3>
&lt;p>The robustness of community support played a pivotal role in our decision making. Dataflow provided excellent support, but we needed assurance in our choice of an open-source framework. Apache Flink’s vibrant community and active contributions from multiple companies offered a level of confidence that was unmatched. This collaborative environment meant that bug identification and fixes were ongoing processes. In fact, in our journey, we have patched our system using many Flink fixes from the community:&lt;/p>
&lt;ul>
&lt;li>We fixed the Google Cloud Storage file reading exceptions by merging Flink 1.15 open source fix &lt;a href="https://issues.apache.org/jira/browse/FLINK-26063?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&amp;amp;focusedCommentId=17504555#comment-17504555">FLINK-26063&lt;/a> (we are using 1.13).&lt;/li>
&lt;li>We fixed an issue with workers restarting for stateful jobs from &lt;a href="https://issues.apache.org/jira/browse/FLINK-31963">FLINK-31963&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>We also contributed to the community during our journey by founding and fixing bugs in the open source code. For details, see &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> for Flink Kubernetes Operator. We also created a new GKE Auth support for Kubernetes clients and merged it to GitHub at [4].&lt;/p>
&lt;h3 id="integration">Integration&lt;/h3>
&lt;p>The seamless integration of Apache Flink with Kubernetes provided us with a flexible and scalable platform for orchestration. The synergy between Apache Flink and Kubernetes not only optimized our data processing workflows but also future-proofed our system.&lt;/p>
&lt;h2 id="architecture-and-deployment-workflow">Architecture and deployment workflow&lt;/h2>
&lt;p>In the realm of real-time data processing and analytics, Apache Flink distinguishes itself as a powerful and versatile framework. When combined with Kubernetes, the industry-standard container orchestration system, Flink applications can scale horizontally and have robust management capabilities. We explore a cutting-edge design where Apache Flink and Kubernetes synergize seamlessly, thanks to the Apache Flink Kubernetes Operator.&lt;/p>
&lt;p>At its core, the Flink Kubernetes Operator serves as a control plane, mirroring the knowledge and actions of a human operator managing Flink deployments. Unlike traditional methods, the Operator automates critical activities, from starting and stopping applications to handling upgrades and errors. Its versatile feature set includes fully-automated job lifecycle management, support for different Flink versions, and multiple deployment modes, such as application clusters and session jobs. Moreover, the Operator&amp;rsquo;s operational prowess extends to metrics, logging, and even dynamic scaling by using the Job Autoscaler.&lt;/p>
&lt;h3 id="build-a-seamless-deployment-workflow">Build a seamless deployment workflow&lt;/h3>
&lt;p>Imagine a robust system where Flink jobs are deployed effortlessly, monitored diligently, and managed proactively. Our team created this workflow by integrating Apache Flink, Apache Flink Kubernetes Operator, and Kubernetes. Central to this setup is our custom-built Apache Flink Kubernetes Operator Client Library. This library acts as a bridge, enabling atomic operations such as starting, stopping, updating, and canceling Flink jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/stream-service-changes.png"
alt="Streaming service changes">&lt;/p>
&lt;h3 id="the-deployment-process">The deployment process&lt;/h3>
&lt;p>In our code, the client provides Apache Beam pipeline options, which include essential information such as the Kubernetes cluster&amp;rsquo;s API endpoint, authentication details, the Google Cloud/S3 temporary location for uploading the JAR file, and worker type specifications. The Kubernetes Operator Library uses this information to orchestrate a seamless deployment process. The following sections explain the steps taken. Most of the core steps are automated in our code base.&lt;/p>
&lt;p>&lt;strong>Step 1:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>The client wants to start a job for a customer and a specific application.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 2:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Generate a unique job ID:&lt;/strong> The library generates a unique job ID, which is set as a Kubernetes label. This identifier helps track and manage the deployed Flink job.&lt;/li>
&lt;li>&lt;strong>Configuration and code upload:&lt;/strong> The library uploads all necessary configurations and user code to a designated location on Google Cloud Storage or Amazon S3. This step ensures that the Flink application&amp;rsquo;s resources are available for deployment.&lt;/li>
&lt;li>&lt;strong>YAML payload generation:&lt;/strong> After the upload process completes, the library constructs a YAML payload. This payload contains crucial deployment information, including resource settings based on the specified worker type.&lt;/li>
&lt;/ul>
&lt;p>We used a convention for naming our worker VM instance types. Our convention is similar to the naming convention that Google Cloud uses. The name &lt;code>n1-standard-1&lt;/code> refers to a specific, predefined VM machine type. Let’s break down what each component of the name means:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>n1&lt;/strong> indicates the CPU type of the instance. In this case, it refers to the Intel based on instances in the N1 series. Google Cloud has multiple generations of instances with varying hardware and performance characteristics.&lt;/li>
&lt;li>&lt;strong>standard&lt;/strong> signifies the machine type family. Standard machine types offer a balanced ratio of 1 virtual CPU (vCPUs) and 4 GB of memory for Task Manager, and 0.5 vCPU and 2 GB memory for Job Manager.&lt;/li>
&lt;li>&lt;strong>1&lt;/strong> represents the number of vCPUs available in the instance. In the case of n1-standard-1, it means the instance has 1 vCPU.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 3:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Calling the Kubernetes API with Fabric8&lt;/strong>: To initiate the deployment, the library interacts with the Kubernetes API using Fabric8. Fabric8 initially lacked support for authentication in Google Kubernetes Engine or Amazon Elastic Kubernetes Service (EKS). To address this limitation, our team implemented the necessary authentication support, which can be found in our merge request on GitHub PR [4].&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 4:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flink Operator deployment&lt;/strong>: When it receives the YAML payload, the Flink Operator takes charge of deploying the various components of the Flink job. Tasks include provisioning resources and managing the deployment of the Flink Job Manager, Task Manager, and Job Service.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 5:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Job submission and execution&lt;/strong>: When the Flink Job Manager is running, it fetches the JAR file and configurations from the designated Google Cloud Storage or S3 location. With all necessary resources in place, it submits the Flink job to the standalone Flink cluster for execution.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 6&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Continuous monitoring&lt;/strong>: Post-deployment, our operator continuously monitors the status of the running Flink job. This real-time feedback loop enables us to promptly address any issues that arise, ensuring the overall health and optimal performance of our Flink applications.&lt;/li>
&lt;/ul>
&lt;p>In summary, our deployment process leverages Apache Beam pipeline options, integrates seamlessly with Kubernetes and the Flink Operator, and employs custom logic to handle configuration uploads and authentication. This end-to-end workflow ensures a reliable and efficient deployment of Flink applications in Kubernetes clusters while maintaining vigilant monitoring for smooth operation. The following sequence diagram shows the steps.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-start-activity-diagram.png"
alt="Job Start Activity Diagram">&lt;/p>
&lt;h2 id="develope-an-autoscaler">Develope an autoscaler&lt;/h2>
&lt;p>Having an autoscaler is critical to having a self-managed streaming service. There are not enough resources available on the internet for us to learn to build our own autoscaler, which makes this part of the workflow difficult.&lt;/p>
&lt;p>The autoscaler scales up the number of task managers to drain the lag and to keep up with the throughput. It also scales down the minimum number of resources required to process the incoming traffic to reduce costs. We need to do this frequently while keeping the processing disruption to minimum.&lt;/p>
&lt;p>We extensively tuned the autoscaler to meet the SLA for latency. This tuning involved a cost trade off. We also made the autoscaler application-specific to meet specific needs for certain applications. Every decision has a hidden cost. The second part of this blog provides more details about the autoscaler.&lt;/p>
&lt;h2 id="create-a-client-library-for-steaming-job-development">Create a client library for steaming job development&lt;/h2>
&lt;p>To deploy the job using the Flink Kubernetes Operator, you need to know about how Kubernetes works. The following steps explain how to create a single Flink job.&lt;/p>
&lt;ol>
&lt;li>Define a YAML file with proper specifications. The following image provides an example.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink.apache.org/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FlinkDeployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">basic-reactive-example&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink:1.13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1_13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">scheduler-mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">REACTIVE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskmanager.numberOfTaskSlots&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.savepoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/savepoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.checkpoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/checkpoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability.storageDir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/ha&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">serviceAccount&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jobManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-main-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeMounts&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/flink-data&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hostPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># directory location on host&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/tmp/flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># this field is optional&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Directory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">job&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jarURI&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">local:///opt/flink/examples/streaming/StateMachineExample.jar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">parallelism&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">upgradeMode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">savepoint&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">running&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">savepointTriggerNonce&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">standalone&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>SSH into your Flink cluster and run the command following command:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl create -f job1.yaml
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Use the following command to check the status of the job:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl get flinkdeployment job1
&lt;/code>&lt;/pre>&lt;p>This process impacts our scalability. Because we frequently update our jobs, we can&amp;rsquo;t manually follow these steps for every running job. To do so would be highly error prone and time consuming. One wrong space in the YAML can fail the deployment. This approach also acts as a barrier to innovation, because you need to know Kubernetes to interact with Flink jobs.&lt;/p>
&lt;p>We built a library to provide an interface for any teams and applications that want to to start, delete, update, or get the status of their jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/fko-library.png"
alt="Flink Kubernetes Operator Library">&lt;/p>
&lt;p>This library extends the Fabric8 client and FlinkDeployment CRD. FlinkDeployment CRD is exposed by the Flink Kubernetes Operator. CRD lets you store and retrieve structured data. By extending the CRD, we get access to POJO, making it easier to manipulate the YAML file.&lt;/p>
&lt;p>The library supports the following tasks:&lt;/p>
&lt;ol>
&lt;li>Authentication to ensure that you are allowed to perform actions on the Flink cluster.&lt;/li>
&lt;li>Validation (fetches the template from AWS/Google Cloud Storage for validation) takes user variable input and validates it against the policy, rules, YAML format.&lt;/li>
&lt;li>Action execution converts the Java call to invoke the Kubernetes operation.&lt;/li>
&lt;/ol>
&lt;p>During this process, we learned the following lessons:&lt;/p>
&lt;ol>
&lt;li>App specific operator service: At our large scale, the operator was unable to handle such a large number of jobs. Kubernetes calls started to time out and fail. To solve this problem, we created multiple operators (about 4) in high-traffic regions to handle each application.&lt;/li>
&lt;li>Kube call caching: To prevent overloading, we cached the results of Kubernetes calls for thirty to sixty seconds.&lt;/li>
&lt;li>Label support: Providing label support to search jobs using client-specific variables reduced the load on Kube and improved the job search speed by 5x.&lt;/li>
&lt;/ol>
&lt;p>The following are some of the biggest wins we achieved by exposing the library:&lt;/p>
&lt;ol>
&lt;li>Standardized job management: Users can start, delete, and get status updates for their Flink jobs in a Kubernetes environment using a single library.&lt;/li>
&lt;li>Abstracted Kubernetes complexity: Teams no longer need to worry about the inner workings of Kubernetes or the formatting job deployment YAML files. The library handles these details internally.&lt;/li>
&lt;li>Simplified upgrades: With the underlying Kubernetes infrastructure, the library brings robustness and fault tolerance to Flink job management, ensuring minimal downtime and efficient recovery.&lt;/li>
&lt;/ol>
&lt;h2 id="observability-and-alerting">Observability and alerting&lt;/h2>
&lt;p>Observability is important when runing a production system at a large scale. We have about 30,000 streaming jobs in PANW. Each job serves a customer for a specific application. Each job also reads data from multiple topics in Kafka, performs transformations, and then writes the data to various sinks and endpoints.&lt;/p>
&lt;p>Constraints can occur anywhere in the pipeline or its endpoints, such as the customer API, BigQuery, and so on. We want to make sure the latency of streaming meets the SLA. Therefore, understanding if a job is healthy, meeting SLA, and alerting and intervening when needed is very challenging.&lt;/p>
&lt;p>To achieve our operational goals, we built a sophisticated observability and alerting capability. We provide three kinds of observability and debugging tools, described in the following sections.&lt;/p>
&lt;h3 id="flink-job-list-and-job-insights-from-prometheus-and-grafana">Flink job list and job insights from Prometheus and Grafana&lt;/h3>
&lt;p>Each Flink job sends various metrics to our Prometheus with cardinality details, such as application name, customer Id, and regions, so that we can look at each job. Critical metrics include the input traffic rate, output throughput, backlogs in Kafka, timestamp-based latency, task CPU usage, task numbers, OOM counts, and so on.&lt;/p>
&lt;p>The following charts provide a few examples. The charts provide details about the ingestion traffic rate to Kafka for a specific customer, the streaming job’s overall throughput, each vCPU’s throughput, backlogs in Kafka, and worker autoscaling based on the observed backlog.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-metrics.png"
alt="Flink Job Metrics">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/autoscaling-metrics.png"
alt="Flink Job Autoscaling Metrics">&lt;/p>
&lt;p>The following chart shows streaming latency based on the timestamp watermark. In addition to the numbers of events in Kafka as backlogs, it is important to know the time latency for end-to-end streaming so that we can define and monitor the SLA. The latency is defined as the time taken for the streaming processing, starting from ingestion timestamp, to the timestamp sending to the streaming endpoint. A watermark is the last processed event’s time. With the watermark, we are tracking P100 latency. We track each event’s stream latency, so that we can understand each Kafka topic and partition or Flink job pipeline issue. The following example shows each event stream and its latency:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/watermark-metrics.png"
alt="Apache Beam Watermark Metrics">&lt;/p>
&lt;h3 id="flink-open-source-ui">Flink open source UI&lt;/h3>
&lt;p>We use and extend the Apache Flink dashboard UI to monitor jobs and tasks, such as the checkpoint duration, size, and failure. One important extension we used is a job history page that lets us see a job&amp;rsquo;s start and update timeline and details, which helps us to debug issues.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/flink-checkpoint-ui.png"
alt="Flink Checkpoint UI">&lt;/p>
&lt;h3 id="dashboards-and-alerting-for-backlog-and-latency">Dashboards and alerting for backlog and latency&lt;/h3>
&lt;p>We have about 30,000 jobs, and we want to closely monitor the jobs and receive alerts for jobs in abnormal states so that we can intervene. We created dashboards for each application so that we can show the list of jobs with the highest latency and create thresholds for alerts. The following example shows the timestamp-based latency dashboard for one application. We can set the alerting if the latency is larger than a threshold, such as 10 minutes, for a certain time continuously:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/latency-graph.png"
alt="Latency Graph">&lt;/p>
&lt;p>The following example shows more backlog-based dashboards:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/backlog-graph.png"
alt="Backlog Graph">&lt;/p>
&lt;p>The alerts are based on thresholds, and we frequently check metrics. If a threshold is met and continues for a certain amount of times, we alert our internal Slack channels or PagerDuty for immediate attention. We tune the alerting so that the accuracy is high.&lt;/p>
&lt;h2 id="cost-optimization-strategies-and-tuning">Cost optimization strategies and tuning&lt;/h2>
&lt;p>We also moved to a self-managed streaming service to improve cost efficiency. Several minor tunings have allowed us to reduce costs by half, and we have more opportunities for improvement.&lt;/p>
&lt;p>The following list includes a few tips that have helped us:&lt;/p>
&lt;ul>
&lt;li>Use Google Cloud Storage as checkpointing storage.&lt;/li>
&lt;li>Reduce the write frequency to Google Cloud Storage.&lt;/li>
&lt;li>Use appropriate machine types. For example, in Google Cloud, N2D machines are 15% less expensive than N2 machines.&lt;/li>
&lt;li>Autoscale tasks to use optimal resources while maintaining the latency SLA.&lt;/li>
&lt;/ul>
&lt;p>The following sections provide more details about the first two tips.&lt;/p>
&lt;h3 id="google-cloud-storage-and-checkpointing">Google Cloud Storage and checkpointing&lt;/h3>
&lt;p>We use Google Cloud Storage as our checkpoint store because it is cost-effective, scalable, and durable. When working with Google Cloud Storage, the following design considerations and best practices can help you optimize scaling and performance:&lt;/p>
&lt;ul>
&lt;li>Use data partitioning methods like range partitioning, which divides data based on specific attributes, and hash partitioning, which distributes data evenly using hash functions.&lt;/li>
&lt;li>Avoid sequential key names, especially timestamps, to avoid hotspots and uneven data distribution. Instead, introduce random prefixes for object distribution.&lt;/li>
&lt;li>Use a hierarchical folder structure to improve data management and reduce the number of objects in a single directory.&lt;/li>
&lt;li>Combine small files into larger ones to improve read throughput. Minimizing the number of small files reduces inefficient storage use and metadata operations.&lt;/li>
&lt;/ul>
&lt;h3 id="tune-the-frequency-of-writing-to-google-cloud-storage">Tune the frequency of writing to Google Cloud Storage&lt;/h3>
&lt;p>Scaling jobs efficiently was one of our primary challenges. Stateless jobs, which are relatively simpler, still present hurdles, especially in scenarios where Flink needed to process an overwhelming number of workers. To overcome this challenge, We increased the &lt;code>state.storage.fs.memory-threshold&lt;/code> settings to 1 MB from 20KB (??). This configuration allowed us to combine small checkpoint files into larger ones at the Job Manager level and to reduce metadata calls.&lt;/p>
&lt;p>Optimizing the performance of Google Cloud operations was another challenge. Although Google Cloud Storage is excellent for streaming large amounts of data, it has limitations when it comes to handling high-frequency I/O requests. To mitigate this issue, we introduced random prefixes in key names, avoided sequential key names, and optimized our Google Cloud Storage sharding techniques. These methods significantly enhanced our Google Cloud Storage performance, enabling the smooth operation of our stateless jobs.&lt;/p>
&lt;p>The following chart shows the Google Cloud Storage writes reduction after changing the memory-threshold:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/gcs-write-graph.png"
alt="GCS write Graph">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Palo Alto Networks® Cortex Data Lake is fully migrated from Dataflow streaming engine to Flink self managed streaming engine infrastructure. We have achieved our goals to run the system more cost efficiently (more than half cost cut), and run the infrastructure on multiple clouds such as GCP and AWS. We have learned how to build a large scale reliable production system based on open sources. We see large potentials to customize the system based on our specific needs as we have a lot of freedom to customize the open source code and configuration. In the next Part 2 post we will give more details on autoscaling and performance tuning parts. We hope our experience will be helpful for readers who will explore similar solutions for their own organizations.&lt;/p>
&lt;h1 id="additional-resources">Additional Resources&lt;/h1>
&lt;p>We provide links here for related presentations as further reading for readers interested in implementing similar solutions. By adding this section, we hope you can find more details to build a fully managed streaming infrastructure, making it easier for readers to follow our stories and learnings.&lt;/p>
&lt;p>[1] Streaming framework at PANW published at Apache Beam: &lt;a href="https://beam.apache.org/case-studies/paloalto/">https://beam.apache.org/case-studies/paloalto/&lt;/a>&lt;/p>
&lt;p>[2] PANW presentation at Beam Summit 2023: &lt;a href="https://youtu.be/IsGW8IU3NfA?feature=shared">https://youtu.be/IsGW8IU3NfA?feature=shared&lt;/a>&lt;/p>
&lt;p>[3] Benchmark presented at Beam Summit 2021: &lt;a href="https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/">https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/&lt;/a>&lt;/p>
&lt;p>[4] PANW open source contribution to Flink for GKE Auth support: &lt;a href="https://github.com/fabric8io/kubernetes-client/pull/4185">https://github.com/fabric8io/kubernetes-client/pull/4185&lt;/a>&lt;/p>
&lt;h1 id="acknowledgements">Acknowledgements&lt;/h1>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed Flink based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitik Dani, and David He.&lt;/p></description></item><item><title>Blog: Apache Beam 2.51.0</title><link>/blog/beam-2.51.0/</link><pubDate>Wed, 11 Oct 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.51.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.51.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2510-2023-10-03">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.51.0, check out the &lt;a href="https://github.com/apache/beam/milestone/15">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>In Python, &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#why-use-the-runinference-api">RunInference&lt;/a> now supports loading many models in the same transform using a &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#use-a-keyed-modelhandler">KeyedModelHandler&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/27628">#27628&lt;/a>).&lt;/li>
&lt;li>In Python, the &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.vertex_ai_inference.html#apache_beam.ml.inference.vertex_ai_inference.VertexAIModelHandlerJSON">VertexAIModelHandlerJSON&lt;/a> now supports passing in inference_args. These will be passed through to the Vertex endpoint as parameters.&lt;/li>
&lt;li>Added support to run &lt;code>mypy&lt;/code> on user pipelines (&lt;a href="https://github.com/apache/beam/issues/27906">#27906&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Removed fastjson library dependency for Beam SQL. Table property is changed to be based on jackson ObjectNode (Java) (&lt;a href="https://github.com/apache/beam/issues/24154">#24154&lt;/a>).&lt;/li>
&lt;li>Removed TensorFlow from Beam Python container images &lt;a href="https://github.com/apache/beam/pull/28424">PR&lt;/a>. If you have been negatively affected by this change, please comment on &lt;a href="https://github.com/apache/beam/issues/20605">#20605&lt;/a>.&lt;/li>
&lt;li>Removed the parameter &lt;code>t reflect.Type&lt;/code> from &lt;code>parquetio.Write&lt;/code>. The element type is derived from the input PCollection (Go) (&lt;a href="https://github.com/apache/beam/issues/28490">#28490&lt;/a>)&lt;/li>
&lt;li>Refactor BeamSqlSeekableTable.setUp adding a parameter joinSubsetType. &lt;a href="https://github.com/apache/beam/issues/28283">#28283&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed exception chaining issue in GCS connector (Python) (&lt;a href="https://github.com/apache/beam/issues/26769#issuecomment-1700422615">#26769&lt;/a>).&lt;/li>
&lt;li>Fixed streaming inserts exception handling, GoogleAPICallErrors are now retried according to retry strategy and routed to failed rows where appropriate rather than causing a pipeline error (Python) (&lt;a href="https://github.com/apache/beam/issues/21080">#21080&lt;/a>).&lt;/li>
&lt;li>Fixed a bug in Python SDK&amp;rsquo;s cross-language Bigtable sink that mishandled records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Python containers updated, fixing &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>&lt;/li>
&lt;li>Used go 1.21.1 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39320">CVE-2023-39320&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines using BigQuery Storage Read API must pin &lt;code>fastavro&lt;/code> dependency to 1.8.3
or earlier: &lt;a href="https://github.com/apache/beam/issues/28811">#28811&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Whitmore&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hao Xu&lt;/p>
&lt;p>Haruka Abe&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Joey Tran&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Melissa Pashniak&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruwann&lt;/p>
&lt;p>Ryan Tam&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sereana Seim&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tim Grein&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zbynek Konecny&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>gDuperran&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kberezin-nshl&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lostluck&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>olalamichelle&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p>
&lt;p>Łukasz Spyra&lt;/p></description></item><item><title>Blog: DIY GenAI Content Discovery Platform with Apache Beam</title><link>/blog/dyi-content-discovery-platform-genai-beam/</link><pubDate>Mon, 02 Oct 2023 00:00:01 -0800</pubDate><guid>/blog/dyi-content-discovery-platform-genai-beam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="diy-genai-content-discovery-platform-with-apache-beam">DIY GenAI Content Discovery Platform with Apache Beam&lt;/h1>
&lt;p>Your digital assets, such as documents, PDFs, spreadsheets, and presentations, contain a wealth of valuable information, but sometimes it&amp;rsquo;s hard to find what you&amp;rsquo;re looking for. This blog post explains how to build a DIY starter architecture, based on near real-time ingestion processing and large language models (LLMs), to extract meaningful information from your assets. The model makes the information available and discoverable through a simple natural language query.&lt;/p>
&lt;p>Building a near real-time processing pipeline for content ingestion might seem like a complex task, and it can be. To make pipeline building easier, the Apache Beam framework exposes a set of powerful constructs. These constructs remove the following complexities: interacting with multiple types of content sources and destinations, error handling, and modularity. They also maintain resiliency and scalability with minimal effort. You can use an Apache Beam streaming pipeline to complete the following tasks:&lt;/p>
&lt;ul>
&lt;li>Connect to the many components of a solution.&lt;/li>
&lt;li>Quickly process content ingestion requests of documents.&lt;/li>
&lt;li>Make the information in the documents available a few seconds after ingestion.&lt;/li>
&lt;/ul>
&lt;p>LLMs are often used to extract content and summarize information stored in many different places. Organizations can use LLMs to quickly find relevant information disseminated in multiple documents written across the years. The information might be in different formats, or the documents might be too long and complex to read and understand quickly. Use LLMs to process this content to make it easier for people to find the information that they need.&lt;/p>
&lt;p>Follow the steps in this guide to create a custom scalable solution for data extraction, content ingestion, and storage. Learn how to kickstart the development of a LLM-based solution using Google Cloud products and generative AI offerings. Google Cloud is designed to be simple to use, scalable, and flexible, so you can use it as a starting point for further expansion or experimentation.&lt;/p>
&lt;h3 id="high-level-flow">High-level Flow&lt;/h3>
&lt;p>In this workflow, content uptake and query interactions are completely separated. An external content owner can send documents stored in Google Docs or in a binary text format and receive a tracking ID for the ingestion request. The ingestion process gets the content of the document and creates chunks that are configurable in size. Each document chunk is used to generate embeddings. These embeddings represent the content semantics, in the form of a vector of 768 dimensions. Given the document identifier and the chunk identifier, you can store the embeddings in a Vector database for semantic matching. This process is central to contextualizing user inquiries.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-highlevel.png"
alt="Content Discovery Platform Overview">&lt;/p>
&lt;p>The query resolution process doesn&amp;rsquo;t depend directly on information ingestion. The user receives relevant answers based on the content ingested until the moment of the query request. Even if the platform doesn&amp;rsquo;t have any relevant content stored, the platform returns an answer stating that it doesn&amp;rsquo;t have relevant content. Therefore, the query resolution process first generates embeddings from the query content and from the previously existing context, like previous exchanges with the platform, then matches these embeddings with the existing embedding vectors stored from the content. When the platform has positive matches, it retrieves the plain-text content represented by the content embeddings. Finally, by using the textual representation of the query and the textual representation of the matched content, the platform formulates a request to the LLM to provide a final answer to the original user inquiry.&lt;/p>
&lt;h2 id="components-of-the-solution">Components of the solution&lt;/h2>
&lt;p>Use the low-ops capabilities of the Google Cloud services to create a set of highly scalable features. You can separate the solution into two main components: the service layer and the content ingestion pipeline. The service layer acts as the entry point for document ingestion and user queries. It’s a simple set of REST resources exposed through Cloud Run and implemented by using &lt;a href="https://quarkus.io/">Quarkus&lt;/a> and the client libraries to access other services (Vertex AI models, Cloud Bigtable and Pub/Sub). The content ingestion pipeline includes the following components:&lt;/p>
&lt;ul>
&lt;li>A streaming pipeline that captures user content from wherever it resides.&lt;/li>
&lt;li>A process that extracts meaning from this content as a set of multi-dimensional vectors (text embeddings).&lt;/li>
&lt;li>A storage system that simplifies context matching between knowledge content and user inquiries (a Vector Database).&lt;/li>
&lt;li>Another storage system that maps knowledge representation with the actual content, forming the aggregated context of the inquiry.&lt;/li>
&lt;li>A model capable of understanding the aggregated context and, through prompt engineering, delivering meaningful answers.&lt;/li>
&lt;li>HTTP and gRPC-based services.&lt;/li>
&lt;/ul>
&lt;p>Together, these components provide a comprehensive and simple implementation for a content discovery platform.&lt;/p>
&lt;h2 id="workflow-architecture">Workflow Architecture&lt;/h2>
&lt;p>This section explains how the different components interact.&lt;/p>
&lt;h3 id="dependencies-of-the-components">Dependencies of the components&lt;/h3>
&lt;p>The following diagram shows all of the components that the platform integrates with. It also shows all of the dependencies that exist between the components of the solution and the Google Cloud services.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-arch.png"
alt="Content Discovery Platform Interactions">&lt;/p>
&lt;p>As seen in the diagram, the context-extraction component is the central aspect in charge of retrieving the document’s content, also their semantic meaning from the embedding’s model and storing the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. PubSub resources are the glue between the streaming pipeline and the asynchronous processing, capturing the user ingestion requests, retries from potential errors from the ingestion pipeline (like the cases on where documents have been sent for ingestion but the permission has not been granted yet, triggering a retry after some minutes) and content refresh events (periodically the pipeline will scan the ingested documents, review the latest editions and define if a content refresh should be triggered).&lt;/p>
&lt;p>The context-extraction component retrieves the content of the documents, diving it in chunks. It also computes embeddings, using the LLM interaction, from the extracted content. Then it stores the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. Pub/Sub resources connect the streaming pipeline and the asynchronous processing, capturing the following actions:&lt;/p>
&lt;ul>
&lt;li>user ingestion requests&lt;/li>
&lt;li>retries from errors from the ingestion pipeline, such as when documents are sent for ingestion but access permissions are missing&lt;/li>
&lt;li>content refresh events (periodically the pipeline scans the ingested documents, reviews the latest editions, and decides whether to trigger a content refresh)&lt;/li>
&lt;/ul>
&lt;p>Also, CloudRun plays an important role exposing the services, interacting with many Google Cloud services to resolve the user query or ingestion requests. For example, while resolving a query request the service will:&lt;/p>
&lt;ul>
&lt;li>Request the computation of embeddings from the user’s query by interacting with the embeddings model&lt;/li>
&lt;li>Find near neighbor matches from the Vertex AI Vector Search (formerly Matching Engine) using the query embeddings representation&lt;/li>
&lt;li>Retrieve the text content from BigTable for those matched vectors, using their identifier, in order contextualize a LLM prompt&lt;/li>
&lt;li>And finally create a request to the VertexAI Chat-Bison model, generating the response the system will delivery to the user’s query.&lt;/li>
&lt;/ul>
&lt;h3 id="google-cloud-products">Google Cloud products&lt;/h3>
&lt;p>This section describes the Google Cloud products and services used in the solution and what purpose they serve.&lt;/p>
&lt;p>&lt;strong>Cloud Build:&lt;/strong> All container images, including services and pipelines, are built directly from source code by using Cloud Build. Using Cloud Build simplifies code distribution during the deployment of the solution.&lt;/p>
&lt;p>&lt;strong>CloudRun:&lt;/strong> The solution&amp;rsquo;s service entry points are deployed and automatically scaled by CloudRun.&lt;/p>
&lt;p>&lt;strong>Pub/Sub:&lt;/strong> A Pub/Sub topic and subscription queue all of the ingestion requests for Google Drive or self-contained content and deliver the requests to the pipeline.&lt;/p>
&lt;p>&lt;strong>Dataflow:&lt;/strong> A multi-language, streaming Apache Beam pipeline processes the ingestion requests. These requests are sent to the pipeline from the Pub/Sub subscription. The pipeline extracts content from Google Docs, Google Drive URLs, and self-contained binary encoded text content. It then produces content chunks. These chunks are sent to one of the Vertex AI foundational models for the embedding representation. The embeddings and chunks from the documents are sent to Vertex AI Vector Search and to Cloud Bigtable for indexing and rapid access. Finally, the ingested documentation is stored in Google Cloud Storage in JSON-L format, which can be used to fine-tune the Vertex AI models. By using Dataflow to run the Apache Beam streaming pipeline, you minimize the ops needed to scale resources. If you have a burst on ingestion requests, Dataflow can keep the latency less than a minute.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Vector Search:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/matching-engine/overview">Vector Search&lt;/a> is a high-performance, low-latency vector database. These vector databases are often called vector similarity search or approximate nearest neighbor (ANN) services. We use a Vector Search Index to store all the ingested documents embeddings as a meaning representation. These embeddings are indexed by chunk and document id. Later on, these identifiers can be used to contextualize the user queries and enrich the requests made to a LLM by providing knowledge extracted directly from the document’s content mappings stored on BigTable (using the same chunk-document identifiers).&lt;/p>
&lt;p>&lt;strong>Cloud BigTable:&lt;/strong> This storage system provides a low latency search by identifier at a predictable scale. Is a perfect fit, given the low latency of the requests resolution, for online exchanges between user queries and the platform component interactions. It used to store the content extracted from the documents since it&amp;rsquo;s indexed by chunk and document identifier. Every time a user makes a request to the query service, and after the query text embeddings are resolved and matched with the existing context, the document and chunk ids are used to retrieve the document’s content that will be used as context to request an answer to the LLM in use. Also, BigTable is used to keep track of the conversational exchanges between users and the platform, furthermore enriching the context included on the requests sent to the LLMs (embeddings, summarization, chat Q&amp;amp;A).&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Embedding Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings">Text embeddings&lt;/a> are a condensed vector (numeric) representation of a piece of text. If two pieces of text are semantically similar, their corresponding embeddings will be located close together in the embedding vector space. For more details please see &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings">get text embeddings&lt;/a>. These embeddings are directly used by the ingestion pipeline when processing the document’s content and the query service as an input to match the users query semantic with existing content indexed in Vector Search.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Summarization Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text">Text-bison&lt;/a> is the name of the PaLM 2 LLM that understands, summarizes and generates text. The types of content that text-bison can create include document summaries, answers to questions, and labels that classify the provided input content. We used this LLM to summarize the previously maintained conversation with the goal of enriching the user’s queries and better embedding matching. In summary, the user does not have to include all the context of his question, we extract and summarize it from the conversation history.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Chat Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat">Chat-bison&lt;/a> is the PaLM 2 LLM that excels at language understanding, language generation, and conversations. This chat model is fine-tuned to conduct natural multi-turn conversations, and is ideal for text tasks about code that require back-and-forth interactions. We use this LLM to provide answers to the queries made by users of the solution, including the conversation history between both parties and enriching the model’s context with the content stored in the solution.&lt;/p>
&lt;h3 id="extraction-pipeline">Extraction Pipeline&lt;/h3>
&lt;p>The content extraction pipeline is the platform&amp;rsquo;s centerpiece. It takes care of handling content ingestion requests, extracting documents content and computing embeddings from that content, to finally store the data in specialized storage systems that will be used in the query service components for rapid access.&lt;/p>
&lt;h4 id="high-level-view">High Level View&lt;/h4>
&lt;p>As previously mentioned the pipeline is implemented using Apache Beam framework and runs in streaming fashion on GCP&amp;rsquo;s &lt;a href="https://cloud.google.com/dataflow">Dataflow&lt;/a> service.&lt;/p>
&lt;p>By using Apache Beam and Dataflow we can ensure minimal latency (sub minute processing times), low ops (no need to manually scale up or down the pipeline when traffic spikes occur with time, worker recycle, updates, etc.) and with high level of observability (clear and abundant performance metrics are available).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-1.png"
alt="Apache Beam Pipeline">&lt;/p>
&lt;p>On a high level, the pipeline separates the extraction, computing, error handling and storage responsibilities on different components or PTransforms. As seen in the diagram, the messages are read from a PubSub subscription and immediately afterwards are included in the window definition before the content extraction.&lt;/p>
&lt;p>Each of those PTransforms can be expanded to reveal more details regarding the underlying stages for the implementation. We will dive into each in the following sections.&lt;/p>
&lt;p>The pipeline was implemented using a multi-language approach, with the main components written in the Java language (JDK version 17) and those related with the embeddings computations implemented in Python (version 3.11) since the Vertex AI API clients are available for this language.&lt;/p>
&lt;h4 id="content-extraction">Content Extraction&lt;/h4>
&lt;p>The content extraction component is in charge of reviewing the ingestion request payload and deciding (given the event properties) if it will need to retrieve the content from the event itself (self-contained content, text based document binary encoded) or retrieve it from Google Drive.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-2-extractcontent.png"
alt="Pipeline's Content Extraction">&lt;/p>
&lt;p>In case of a self-contained document, the pipeline will extract the document id and format the document in paragraphs for later embedding processing.&lt;/p>
&lt;p>When in need of retrieval from Google Drive, the pipeline will inspect if the provided URL in the event refers to a Google Drive folder or a single file format (supported formats are Documents, Spreadsheets and Presentations). In the case of a folder, the pipeline will crawl the folder’s content recursively extracting all the files for the supported formats, in case of a single document will just return that one.&lt;/p>
&lt;p>Finally, with all the file references retrieved from the ingestion request, textual content is extracted from the files (no image support implemented for this PoC). That content will also be passed to the embedding processing stages including the document’s identifier and the content as paragraphs.&lt;/p>
&lt;h4 id="error-handling">Error Handling&lt;/h4>
&lt;p>On every stage of the content extraction process multiple errors can be encountered, malformed ingestion requests, non-conformant URLs, lack of permissions for Drive resources, lack of permissions for File data retrieval.&lt;/p>
&lt;p>In all those cases a dedicated component will capture those potential errors and define, given the nature of the error, if the event should be retried or sent to a dead letter GCS bucket for later inspection.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-3-errorhandling.png"
alt="Pipeline's Error Handling">&lt;/p>
&lt;p>The final errors, or those which won’t be retried, are those errors related with bad request formats (the event itself or the properties content, like malformed or wrong URLs, etc.).&lt;/p>
&lt;p>The retryable errors are those related with content access and lack of permissions. A request may have been resolved faster than the manual process of providing the right permissions to the Service Account that runs the pipeline to access the resources included in the ingestion request (Google Drive folders or files). In case of detecting a retryable error, the pipeline will hold the retry for 10 minutes before re-sending the message to the upstream PubSub topic; each error is retried at most 5 times before being sent to the dead letter GCS bucket.&lt;/p>
&lt;p>In all cases of events ending on the dead letter destination, the inspection and re-processing must be done in a manual process.&lt;/p>
&lt;h4 id="process-embeddings">Process Embeddings&lt;/h4>
&lt;p>Once the content has been extracted from the request, or captured from Google Drive files, the pipeline will trigger the embeddings computation process. As previously mentioned the interactions with the Vertex AI Foundational Models API is implemented in Python language. For this reason we need to format the extracted content in Java types that have a direct translation to those existing in the Python world. Those are key-values (in Python those are 2-element tuples), Strings (available in both languages), and iterables (also available in both languages). We could have implemented coders in both languages to support custom transport types, but we opted out of that in favor of clarity and simplicity.&lt;/p>
&lt;p>Before computing the content’s embeddings we decided to introduce a Reshuffle step, making the output consistent to downstream stages, with the idea of avoiding the content extraction step being repeated in case of errors. This should avoid putting pressure on existing access quotas on Google Drive related APIs.&lt;/p>
&lt;p>The pipeline will then chunk the content in configurable sizes and also configurable overlapping, good parameters are hard to get for generic effective data extraction, so we opted to use smaller chunks with small overlapping factor as the default settings to favor diversity on the document results (at least that’s what we see from the empirical results obtained).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings1.png"
alt="Embeddings Processing">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings2.png"
alt="Embeddings Processing">
&lt;/p>
&lt;p>Once the embeddings vectors are retrieved from the embeddings Vertex AI LLM, we will consolidate them again avoiding repetition of this step in case of downstream errors.&lt;/p>
&lt;p>Worth to notice that this pipeline is interacting directly with Vertex AI models using the client SDKs, Apache Beam already provides supports for this interactions through the RunInference PTransform (see an example &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/vertex_ai_llm_text_classification.py">here&lt;/a>).&lt;/p>
&lt;h4 id="content-storage">Content Storage&lt;/h4>
&lt;p>Once the embeddings are computed for the content chunks extracted from the ingested documents, we need to store the vectors in a searchable storage and also the textual content that correlates with those embeddings. We will be using the embeddings vectors as a semantic match later from the query service, and the textual content that corresponds to those embeddings for LLM context as a way to improve and guide the response expectations.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-5-storecontent.png"
alt="Content Storage">&lt;/p>
&lt;p>With that in mind is that in mind we split the consolidated embeddings into 3 paths, one that stores the vectors into Vertex AI Vector Search (using simple REST calls), another storing the textual content into BigTable (for low latency retrieval after semantic matching) and the final one as a potential clean up of content refresh or re ingestion (more on that later). The three paths are using the ingested document identifier as the correlating data on the actions, this key is formed by the document name (in case of available), the document identifier and the chunk sequence number. The reason for using identifiers for the chunk comes behind the idea of subsequent updates. An increase in the content will generate a larger number of chunks, and upserting all the chunks will enable always fresh data; on the contrary, a decrease in content will generate a smaller chunk count for the document’s content, this number difference can be used to delete the remaining orphan indexed chunks (from content no longer existing in the latest version of the document).&lt;/p>
&lt;h4 id="content-refresh">Content Refresh&lt;/h4>
&lt;p>The last pipeline component is the simplest, at least conceptually. After the documents from Google Drive gets ingested, an external user can produce updates in them, causing the indexed content to become out of date. We implemented a simple periodic process, inside the same streaming pipeline, that will take care of the review of already ingested documents and see if there are content updates needed. We use a GenerateSequence transform to produce a periodic impulse (every 6 hours by default), that will trigger a scan on BigTable retrieving all the ingested document identifiers. Given those identifiers we can then query Google Drive for the latest update timestamp of each document and use that marker to decide if an update is needed.&lt;/p>
&lt;p>In case of needing to update the document’s content, we can simply send an ingestion request to the upstream PubSub topic and let the pipeline run its course for this new event. Since we are taking care of upserting embeddings and cleaning up those that no longer exist, we should be capable of taking care of the majority of the additions (as long those are text updates, image based content is not being processed as of now).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh1.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh2.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh3.png"
alt="Content Refresh">
&lt;/p>
&lt;p>This task could be performed as a separate job, possibly one that is periodically scheduled in batch form. This would result in lower costs, a separate error domain, and more predictable auto scaling behavior. However, for the purposes of this demonstration, it is simpler to have a single job.&lt;/p>
&lt;p>Next, we will be focusing on how the solution interacts with external clients for ingestion and content discovery use cases.&lt;/p>
&lt;h2 id="interaction-design">Interaction Design&lt;/h2>
&lt;p>The solution aims to make the interactions for ingesting and querying the platform as simple as possible. Also, since the ingestion part may imply interacting with several services and imply retries or content refresh, we decided to make both separated and asynchronous, freeing the external users of blocking themselves while waiting for requests resolutions.&lt;/p>
&lt;h3 id="example-interactions">Example Interactions&lt;/h3>
&lt;p>Once the platform is deployed in a GCP project, a simple way to interact with the services is through the use of a web client, curl is a good example. Also, since the endpoints are authenticated, a client needs to include its credentials in the request header to have its access granted.&lt;/p>
&lt;p>Here is an example of an interaction for content ingestion:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST -H &amp;#34;Content-Type: application/json&amp;#34; -H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; https://&amp;lt;service-address&amp;gt;/ingest/content/gdrive -d $&amp;#39;{&amp;#34;url&amp;#34;:&amp;#34;https://drive.google.com/drive/folders/somefolderid&amp;#34;}&amp;#39; | jq .
# response from service
{
&amp;#34;status&amp;#34;: &amp;#34;Ingestion trace id: &amp;lt;some identifier&amp;gt;&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>In this case, after the ingestion request has been sent to the PubSub topic for processing, the service will return the tracking identifier, which maps with the PubSub message identifier. Note the provided URL can be one of a Google Doc or a Google Drive folder, in the later case the ingestion process will crawl the folder’s content recursively to retrieve all the contained documents and their contents.&lt;/p>
&lt;p>Next, an example of a content query interaction, very similar to the previous one:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundation Models are a set of pre-trained models that can be used to accelerate the development of machine learning applications. They are available for a variety of tasks, including natural language processing, computer vision, and recommendation systems.\n\nVertexAI Foundation Models can be used to improve the performance of Generative AI applications by providing a starting point for model development. They can also be used to reduce the amount of time and effort required to train a model.\n\nIn addition, VertexAI Foundation Models can be used to improve the accuracy and robustness of Generative AI applications. This is because they are trained on large datasets and are subject to rigorous quality control.\n\nOverall, VertexAI Foundation Models can be a valuable resource for developers who are building Generative AI applications. They can help to accelerate the development process, reduce the cost of development, and improve the performance and accuracy of applications.&amp;#34;,
&amp;#34;previousConversationSummary&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;sourceLinks&amp;#34;: [
{
&amp;#34;link&amp;#34;: &amp;#34;&amp;lt;possibly some ingested doc url/id&amp;gt;&amp;#34;,
&amp;#34;distance&amp;#34;: 0.7233397960662842
}
],
&amp;#34;citationMetadata&amp;#34;: [
{
&amp;#34;citations&amp;#34;: []
}
],
&amp;#34;safetyAttributes&amp;#34;: [
{
&amp;#34;categories&amp;#34;: [],
&amp;#34;scores&amp;#34;: [],
&amp;#34;blocked&amp;#34;: false
}
]
}
&lt;/code>&lt;/pre>&lt;p>The platform will answer the request with a textual response from the LLM and include as well information about the categorization, citation metadata and source links (if available) of the content used to generate the response (this are for example, Google Docs links of the documents previously ingested by the platform).&lt;/p>
&lt;p>When interacting with the services, a good query will generally return good results, the clearer the query the easier it will be to contextualize its meaning and more accurate information will be sent to the LLMs to retrieve answers. But having to include all the details of the query context in a phrase on every exchange with the service can be very cumbersome and difficult. For that case the platform can use a provided session identifier that will be used to store all the previous exchanges between a user and the platform. This should help the implementation to better contextualize the initial query embeddings matching and even provide more concise contextual information in the model requests. Here is an example of a contextual exchange:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundational Models are a suite of pre-trained models that can be used to accelerate the development of Generative AI applications. These models are available in a variety of languages and domains, and they can be used to generate text, images, audio, and other types of content.\n\nUsing VertexAI Foundational Models can help you to:\n\n* Reduce the time and effort required to develop Generative AI applications\n* Improve the accuracy and quality of your models\n* Access the latest research and development in Generative AI\n\nVertexAI Foundational Models are a powerful tool for developers who want to create innovative and engaging Generative AI applications.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;describe the available LLM models?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models suite includes a variety of LLM models, including:\n\n* Text-to-text LLMs: These models can generate text based on a given prompt. They can be used for tasks such as summarization, translation, and question answering.\n* Image-to-text LLMs: These models can generate text based on an image. They can be used for tasks such as image captioning and description generation.\n* Audio-to-text LLMs: These models can generate text based on an audio clip. They can be used for tasks such as speech recognition and transcription.\n\nThese models are available in a variety of languages, including English, Spanish, French, German, and Japanese. They can be used to create a wide range of Generative AI applications, such as chatbots, customer service applications, and creative writing tools.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;do rate limit apply for those LLMs?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;Yes, there are rate limits for the VertexAI Foundational Models. The rate limits are based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models documentation](https://cloud.google.com/vertex-ai/docs/foundational-models#rate-limits).&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;care to share the price?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models are priced based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models pricing page](https://cloud.google.com/vertex-ai/pricing#foundational-models).&amp;#34;,
…
}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Usage Tip:&lt;/strong> in case of abruptly changing topics, sometimes is better to use a new session identifier.&lt;/p>
&lt;h3 id="deployment">Deployment&lt;/h3>
&lt;p>As part of the platform solution, there are a set of scripts that help with the deployment of all the different components. By running the &lt;code>start.sh&lt;/code> and setting the right parameters (GCP project, terraform state bucket and name for the platform instance) the script will take care of building the code, deploying the needed containers (service endpoint container and Dataflow python custom container), deploying all the GCP resources using Terraform and finally deploying the pipeline. There is also the possibility of modifying the pipeline’s execution by passing an extra parameter to the startup script, for example: &lt;code>start.sh &amp;lt;gcp project&amp;gt; &amp;lt;state-bucket-name&amp;gt; &amp;lt;a run name&amp;gt; &amp;quot;--update&amp;quot;&lt;/code> will update the content extraction pipeline in-place.&lt;/p>
&lt;p>Also, in case of wanting to focus only on the deployment of specific components other scripts have been included to help with those specific tasks (build the solution, deploy the infrastructure, deploy the pipeline, deploy the services, etc.).&lt;/p>
&lt;h3 id="solutions-notes">Solution&amp;rsquo;s Notes&lt;/h3>
&lt;p>This solution is designed to serve as an example for learning purposes. Many of the configuration values for the extraction pipeline and security restrictions are provided only as examples. The solution doesn&amp;rsquo;t propagate the existing access control lists (ACLs) of the ingested content. As a result, all users that have access to the service endpoints have access to summarizations of the ingested content from those original documents.&lt;/p>
&lt;h3 id="notes-about-the-source-code">Notes about the source code&lt;/h3>
&lt;p>The source code for the content discovery platform is available in &lt;a href="https://github.com/prodriguezdefino/content-dicovery-platform-gcp">Github&lt;/a>. You can run it in any Google Cloud project. The repository includes the source code for the integration services, the multi-language ingestion pipeline, and the deployment automation through Terraform. If you deploy this example, it might take up to 90 minutes to create and configure all the needed resources. The README file contains additional documentation about the deployment prerequisites and example REST interactions.&lt;/p></description></item><item><title>Blog: Apache Beam 2.50.0</title><link>/blog/beam-2.50.0/</link><pubDate>Wed, 30 Aug 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.50.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.50.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2500-2023-08-30">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.50.0, check out the &lt;a href="https://github.com/apache/beam/milestone/14">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark 3.2.2 is used as default version for Spark runner (&lt;a href="https://github.com/apache/beam/issues/23804">#23804&lt;/a>).&lt;/li>
&lt;li>The Go SDK has a new default local runner, called Prism (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java KafkaIO now supports picking up topics via topicPattern (&lt;a href="https://github.com/apache/beam/pull/26948">#26948&lt;/a>)&lt;/li>
&lt;li>Support for read from Cosmos DB Core SQL API (&lt;a href="https://github.com/apache/beam/issues/23604">#23604&lt;/a>)&lt;/li>
&lt;li>Upgraded to HBase 2.5.5 for HBaseIO. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">#27711&lt;/a>)&lt;/li>
&lt;li>Added support for GoogleAdsIO source (Java) (&lt;a href="https://github.com/apache/beam/pull/27681">#27681&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Go SDK now requires Go 1.20 to build. (&lt;a href="https://github.com/apache/beam/issues/27558">#27558&lt;/a>)&lt;/li>
&lt;li>The Go SDK has a new default local runner, Prism. (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).
&lt;ul>
&lt;li>Prism is a portable runner that executes each transform independantly, ensuring coders.&lt;/li>
&lt;li>At this point it supercedes the Go direct runner in functionality. The Go direct runner is now deprecated.&lt;/li>
&lt;li>See &lt;a href="https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md">https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md&lt;/a> for the goals and features of Prism.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hugging Face Model Handler for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/26632">#26632&lt;/a>)&lt;/li>
&lt;li>Hugging Face Pipelines support for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/27399">#27399&lt;/a>)&lt;/li>
&lt;li>Vertex AI Model Handler for RunInference now supports private endpoints (&lt;a href="https://github.com/apache/beam/pull/27696">#27696&lt;/a>)&lt;/li>
&lt;li>MLTransform transform added with support for common ML pre/postprocessing operations (&lt;a href="https://github.com/apache/beam/pull/26795">#26795&lt;/a>)&lt;/li>
&lt;li>Upgraded the Kryo extension for the Java SDK to Kryo 5.5.0. This brings in bug fixes, performance improvements, and serialization of Java 14 records. (&lt;a href="https://github.com/apache/beam/issues/27635">#27635&lt;/a>)&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures. (&lt;a href="https://github.com/apache/beam/issues/27674">#27674&lt;/a>). The multi-arch container images include:
&lt;ul>
&lt;li>All versions of Go, Python, Java and Typescript SDK containers.&lt;/li>
&lt;li>All versions of Flink job server containers.&lt;/li>
&lt;li>Java and Python expansion service containers.&lt;/li>
&lt;li>Transform service controller container.&lt;/li>
&lt;li>Spark3 job server container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for batched writes to AWS SQS for improved throughput (Java, AWS 2).(&lt;a href="https://github.com/apache/beam/issues/21429">#21429&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python SDK: Legacy runner support removed from Dataflow, all pipelines must use runner v2.&lt;/li>
&lt;li>Python SDK: Dataflow Runner will no longer stage Beam SDK from PyPI in the &lt;code>--staging_location&lt;/code> at pipeline submission. Custom container images that are not based on Beam&amp;rsquo;s default image must include Apache Beam installation.(&lt;a href="https://github.com/apache/beam/issues/26996">#26996&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>The Go Direct Runner is now Deprecated. It remains available to reduce migration churn.
&lt;ul>
&lt;li>Tests can be set back to the direct runner by overriding TestMain: &lt;code>func TestMain(m *testing.M) { ptest.MainWithDefault(m, &amp;quot;direct&amp;quot;) }&lt;/code>&lt;/li>
&lt;li>It&amp;rsquo;s recommended to fix issues seen in tests using Prism, as they can also happen on any portable runner.&lt;/li>
&lt;li>Use the generic register package for your pipeline DoFns to ensure pipelines function on portable runners, like prism.&lt;/li>
&lt;li>Do not rely on closures or using package globals for DoFn configuration. They don&amp;rsquo;t function on portable runners.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed DirectRunner bug in Python SDK where GroupByKey gets empty PCollection and fails when pipeline option &lt;code>direct_num_workers!=1&lt;/code>.(&lt;a href="https://github.com/apache/beam/pull/27373">#27373&lt;/a>)&lt;/li>
&lt;li>Fixed BigQuery I/O bug when estimating size on queries that utilize row-level security (&lt;a href="https://github.com/apache/beam/pull/27474">#27474&lt;/a>)&lt;/li>
&lt;li>Beam Python containers rely on a version of Debian/aom that has several security vulnerabilities: &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python Pipelines using BigQuery IO or &lt;code>orjson&lt;/code> dependency might experience segmentation faults or get stuck: &lt;a href="https://github.com/apache/beam/issues/28318">#28318&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>acejune&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>al97&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anton Shalkovich&lt;/p>
&lt;p>ArjunGHUB&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brett Morgan&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Connor Brett&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Hansen&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmytro Sadovnychyi&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Gabriel Lacroix&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hong Liang Teoh&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jon esperanza&lt;/p>
&lt;p>Josef Šimánek&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>Mahmud Ridwan&lt;/p>
&lt;p>Manav Garg&lt;/p>
&lt;p>Marco Vela&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>Peter Sobot&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Saba Sathya&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Steven Niemitz&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p></description></item><item><title>Blog: Apache Beam 2.49.0</title><link>/blog/beam-2.49.0/</link><pubDate>Mon, 17 Jul 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.49.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.49.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2490-2023-07-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.49.0, check out the &lt;a href="https://github.com/apache/beam/milestone/13">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable Change Streams added in Java &lt;code>BigtableIO.ReadChangeStream&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/27183">#27183&lt;/a>).&lt;/li>
&lt;li>Added Bigtable Read and Write cross-language transforms to Python SDK ((&lt;a href="https://github.com/apache/beam/issues/26593">#26593&lt;/a>), (&lt;a href="https://github.com/apache/beam/issues/27146">#27146&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow prebuilding large images when using &lt;code>--prebuild_sdk_container_engine=cloud_build&lt;/code>, like images depending on &lt;code>tensorflow&lt;/code> or &lt;code>torch&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/27023">#27023&lt;/a>).&lt;/li>
&lt;li>Disabled &lt;code>pip&lt;/code> cache when installing packages on the workers. This reduces the size of prebuilt Python container images (&lt;a href="https://github.com/apache/beam/pull/27035">#27035&lt;/a>).&lt;/li>
&lt;li>Select dedicated avro datum reader and writer (Java) (&lt;a href="https://github.com/apache/beam/issues/18874">#18874&lt;/a>).&lt;/li>
&lt;li>Timer API for the Go SDK (Go) (&lt;a href="https://github.com/apache/beam/issues/22737">#22737&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Python 3.7 support. (&lt;a href="https://github.com/apache/beam/issues/26447">#26447&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed KinesisIO &lt;code>NullPointerException&lt;/code> when a progress check is made before the reader is started (IO) (&lt;a href="https://github.com/apache/beam/issues/23868">#23868&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;li>Python pipelines using the &lt;code>--impersonate_service_account&lt;/code> option with BigQuery IOs might fail on Dataflow (&lt;a href="https://github.com/apache/beam/issues/32030">#32030&lt;/a>). This is fixed in 2.59.0 release.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.49.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Dopierała&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Gavin McDonald&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>John Gill&lt;/p>
&lt;p>Joseph Crowley&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Galloway&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Masato Nakamura&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nathaniel Young&lt;/p>
&lt;p>Nelson Osacky&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Smeet nagda&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>WuA&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Xianhua Liu&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zachary Houfek&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>bigduu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>jubebo&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>sultanalieva-s&lt;/p>
&lt;p>vitaly.terentyev&lt;/p></description></item><item><title>Blog: Managing Beam dependencies in Java</title><link>/blog/managing-beam-dependencies-in-java/</link><pubDate>Fri, 23 Jun 2023 09:00:00 -0700</pubDate><guid>/blog/managing-beam-dependencies-in-java/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Managing your Java dependencies can be challenging, and if not done correctly,
it may cause a variety of problems, as incompatibilities may arise when using
specific and previously untested combinations.&lt;/p>
&lt;p>To make that process easier, Beam now
provides &lt;a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#bill-of-materials-bom-poms">Bill of Materials (BOM)&lt;/a>
artifacts that will help dependency management tools to select compatible
combinations.&lt;/p>
&lt;p>We hope this will make it easier for you to use Apache Beam, and have a simpler
transition when upgrading to newer versions.&lt;/p>
&lt;p>When bringing incompatible classes and libraries, the code is susceptible to
errors such
as &lt;code>NoClassDefFoundError&lt;/code>, &lt;code>NoSuchMethodError&lt;/code>, &lt;code>NoSuchFieldError&lt;/code>, &lt;code>FATAL ERROR in native method&lt;/code>.&lt;/p>
&lt;p>When importing Apache Beam, the recommended way is to use Bill of Materials
(BOMs). The way BOMs work is by providing hints to the dependency management
resolution tool, so when a project imports unspecified or ambiguous dependencies,
it will know what version to use.&lt;/p>
&lt;p>There are currently two BOMs provided by Beam:&lt;/p>
&lt;ul>
&lt;li>&lt;code>beam-sdks-java-bom&lt;/code>, which manages what dependencies of Beam will be used, so
you can specify the version only once.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>, a more comprehensive list,
which manages Beam, along with GCP client and third-party dependencies.&lt;/li>
&lt;/ul>
&lt;p>Since errors are more likely to arise when using third-party dependencies,
that&amp;rsquo;s the one that is recommended to use to minimize any conflicts.&lt;/p>
&lt;p>In order to use BOM, the artifact has to be imported to your Maven or Gradle
dependency configurations. For example, to
use &lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>,
the following changes have to be done (and make sure that &lt;em>BEAM_VERSION&lt;/em> is
replaced by a valid version):&lt;/p>
&lt;p>&lt;strong>Maven&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-google-cloud-platform-bom&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;version&amp;gt;&lt;/span>BEAM_VERSION&lt;span class="nt">&amp;lt;/version&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;type&amp;gt;&lt;/span>pom&lt;span class="nt">&amp;lt;/type&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;scope&amp;gt;&lt;/span>import&lt;span class="nt">&amp;lt;/scope&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Gradle&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>dependencies {
implementation(platform(&amp;#34;org.apache.beam:beam-sdks-java-google-cloud-platform-bom:BEAM_VERSION&amp;#34;))
}
&lt;/code>&lt;/pre>&lt;p>After importing the BOM, specific version pinning of dependencies, for example,
anything for &lt;code>org.apache.beam&lt;/code>, &lt;code>io.grpc&lt;/code>, &lt;code>com.google.cloud&lt;/code> (
including &lt;code>libraries-bom&lt;/code>) may be removed.&lt;/p>
&lt;p>Do not entirely remove the dependencies, as they are not automatically imported
by the BOM. It is important to keep the dependency without specifying a version.
For example, in Maven:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-core&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or Gradle:&lt;/p>
&lt;pre tabindex="0">&lt;code>implementation(&amp;#34;org.apache.beam:beam-sdks-java-core&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>For a full list of dependency versions that are managed by a specific BOM, the
Maven tool &lt;code>help:effective-pom&lt;/code> can be used. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">mvn help:effective-pom -f ~/.m2/repository/org/apache/beam/beam-sdks-java-google-cloud-platform-bom/BEAM_VERSION/beam-sdks-java-google-cloud-platform-bom-BEAM_VERSION.pom
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The third-party
website &lt;a href="https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-google-cloud-platform-bom/">mvnrepository.com&lt;/a>
can also be used to display such version information.&lt;/p>
&lt;p>We hope you find this
useful. &lt;a href="https://beam.apache.org/community/contact-us/">Feedback&lt;/a> and
contributions are always welcome! So feel free to create a GitHub issue, or open
a Pull Request if you encounter any problem when using those artifacts.&lt;/p></description></item><item><title>Blog: Getting started with Apache Beam: An open source proficiency credential sponsored by Google Cloud</title><link>/blog/beamquest/</link><pubDate>Tue, 06 Jun 2023 00:00:01 -0800</pubDate><guid>/blog/beamquest/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-badge-image-scaled.png"
alt="Quest image">&lt;/p>
&lt;p>We’re excited to announce the release of the &lt;a href="https://www.cloudskillsboost.google/course_templates/724">“Getting Started with Apache Beam” quest&lt;/a>, a series of four online labs that venture into different Apache Beam concepts. When you complete all four labs, you’ll earn a Google Cloud badge that you can share on platforms like LinkedIn. Earning this badge should take less than seven hours total, and signing up for the quest costs $20 (there are often free specials for people who attend Beam events, such as &lt;a href="https://www.meetup.com/topics/apache-beam/">Meetups&lt;/a>, &lt;a href="https://beamsummit.org/">Beam Summit&lt;/a>, and &lt;a href="https://beamcollege.dev/">Beam College&lt;/a>).&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>&amp;ldquo;I really like the workshop to be honest. I learnt a lot from doing those labs. I would suggest we offer that for any new members to Beam&amp;rdquo;&lt;/em> &amp;ndash; Shunping Huang, Software Engineer&lt;/p>
&lt;/blockquote>
&lt;p>Beam is one of the largest big data open source projects actively in development. Over the past six years, the Apache Beam community has seen tremendous growth in the number of contributors, committers, and users. If you’re a long time Beam user, you can now earn a badge to show your skills to potential employers. If you’re new to Beam, you can begin your learning journey with this quest. To attempt this quest, you don’t need any prior knowledge of data processing or distributed systems. All you need is elementary knowledge about programming.&lt;/p>
&lt;p>Individuals aren’t the only ones who can benefit from completing this quest - organizations can too! Because earning this badge represents deep knowledge of an industry leading big data library, having the badge validates your organization’s understanding of Beam. In addition, you can run the Beam library on a wide variety of runners, including Google Cloud Dataflow, Flink, Spark, and more, making knowledge about this library highly transferable. Finally, your organization can use this quest as onboarding material for new hires on big data teams, allowing teams and organizations to get their newest employees up-to-date on the latest and greatest that Apache Beam has to offer.&lt;/p>
&lt;p>Data Processing is a key part of AI/ML workflows. Given the recent advancements in artificial intelligence, now’s the time to jump into the world of data processing! Get started on your journey &lt;a href="https://www.cloudskillsboost.google/quests/310">here&lt;/a>.&lt;/p>
&lt;p>We are currently offering this quest &lt;strong>FREE OF CHARGE&lt;/strong>. To obtain your badge for &lt;strong>FREE&lt;/strong>, use the &lt;a href="https://www.cloudskillsboost.google/catalog?qlcampaign=1h-swiss-19">Access Code&lt;/a>, create an account, and search &lt;a href="https://www.cloudskillsboost.google/course_templates/724">&amp;ldquo;Getting Started with Apache Beam&amp;rdquo;&lt;/a>. If the code does not work, please email &lt;a href="dev@beam.apache.org">dev@beam.apache.org&lt;/a> to obtain a free code.&lt;/p>
&lt;p>PS: Once you earn your badge, please &lt;a href="https://support.google.com/qwiklabs/answer/9222527?hl=en&amp;amp;sjid=14905615709060962899-NA">share it on social media&lt;/a>!&lt;/p></description></item><item><title>Blog: Apache Beam 2.48.0</title><link>/blog/beam-2.48.0/</link><pubDate>Wed, 31 May 2023 11:30:00 -0400</pubDate><guid>/blog/beam-2.48.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.48.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2480-2023-05-31">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.48.0, check out the &lt;a href="https://github.com/apache/beam/milestone/12">detailed release notes&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Note: The release tag for Go SDK for this release is sdks/v2.48.2 instead of sdks/v2.48.0 because of incorrect commit attached to the release tag sdks/v2.48.0.&lt;/strong>&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Experimental&amp;rdquo; annotation cleanup: the annotation and concept have been removed from Beam to avoid
the misperception of code as &amp;ldquo;not ready&amp;rdquo;. Any proposed breaking changes will be subject to
case-by-case pro/con decision making (and generally avoided) rather than using the &amp;ldquo;Experimental&amp;rdquo;
to allow them.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added rename for GCS and copy for local filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/26064">#25779&lt;/a>).&lt;/li>
&lt;li>Added support for enhanced fan-out in KinesisIO.Read (Java) (&lt;a href="https://github.com/apache/beam/issues/19967">#19967&lt;/a>).
&lt;ul>
&lt;li>This change is not compatible with Flink savepoints created by Beam 2.46.0 applications which had KinesisIO sources.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added textio.ReadWithFilename transform (Go) (&lt;a href="https://github.com/apache/beam/issues/25812">#25812&lt;/a>).&lt;/li>
&lt;li>Added fileio.MatchContinuously transform (Go) (&lt;a href="https://github.com/apache/beam/issues/26186">#26186&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow passing service name for google-cloud-profiler (Python) (&lt;a href="https://github.com/apache/beam/issues/26280">#26280&lt;/a>).&lt;/li>
&lt;li>Dead letter queue support added to RunInference in Python (&lt;a href="https://github.com/apache/beam/issues/24209">#24209&lt;/a>).&lt;/li>
&lt;li>Support added for defining pre/postprocessing operations on the RunInference transform (&lt;a href="https://github.com/apache/beam/issues/26308">#26308&lt;/a>)&lt;/li>
&lt;li>Adds a Docker Compose based transform service that can be used to discover and use portable Beam transforms (&lt;a href="https://github.com/apache/beam/pull/26023">#26023&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Passing a tag into MultiProcessShared is now required in the Python SDK (&lt;a href="https://github.com/apache/beam/issues/26168">#26168&lt;/a>).&lt;/li>
&lt;li>CloudDebuggerOptions is removed (deprecated in Beam v2.47.0) for Dataflow runner as the Google Cloud Debugger service is &lt;a href="https://cloud.google.com/debugger/docs/deprecations">shutting down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;li>AWS 2 client providers (deprecated in Beam &lt;a href="#2380---2022-04-20">v2.38.0&lt;/a>) are finally removed (&lt;a href="https://github.com/apache/beam/issues/26681">#26681&lt;/a>).&lt;/li>
&lt;li>AWS 2 SnsIO.writeAsync (deprecated in Beam v2.37.0 due to risk of data loss) was finally removed (&lt;a href="https://github.com/apache/beam/issues/26710">#26710&lt;/a>).&lt;/li>
&lt;li>AWS 2 coders (deprecated in Beam v2.43.0 when adding Schema support for AWS Sdk Pojos) are finally removed (&lt;a href="https://github.com/apache/beam/issues/23315">#23315&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java bootloader failing with Too Long Args due to long classpaths, with a pathing jar. (Java) (&lt;a href="https://github.com/apache/beam/issues/25582">#25582&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>PubsubIO writes will throw &lt;em>SizeLimitExceededException&lt;/em> for any message above 100 bytes, when used in batch (bounded) mode. (Java) (&lt;a href="https://github.com/apache/beam/issues/27000">#27000&lt;/a>).&lt;/li>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.48.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bazyli Polednia&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Arn&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Novitskiy&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Kuba Rauch&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Pranjal Joshi&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>RuiLong J&lt;/p>
&lt;p>RyujiTamaki&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vishal Bhise&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>kellen&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>mokamoka03210120&lt;/p>
&lt;p>psolomin&lt;/p></description></item><item><title>Blog: Apache Beam 2.47.0</title><link>/blog/beam-2.47.0/</link><pubDate>Wed, 10 May 2023 12:00:00 -0500</pubDate><guid>/blog/beam-2.47.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.47.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2470-2023-05-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.47.0, check out the &lt;a href="https://github.com/apache/beam/milestone/10">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam adds Python 3.11 support (&lt;a href="https://github.com/apache/beam/issues/23848">#23848&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>BigQuery Storage Write API is now available in Python SDK via cross-language (&lt;a href="https://github.com/apache/beam/issues/21961">#21961&lt;/a>).&lt;/li>
&lt;li>Added HbaseIO support for writing RowMutations (ordered by rowkey) to Hbase (Java) (&lt;a href="https://github.com/apache/beam/issues/25830">#25830&lt;/a>).&lt;/li>
&lt;li>Added fileio transforms MatchFiles, MatchAll and ReadMatches (Go) (&lt;a href="https://github.com/apache/beam/issues/25779">#25779&lt;/a>).&lt;/li>
&lt;li>Add integration test for JmsIO + fix issue with multiple connections (Java) (&lt;a href="https://github.com/apache/beam/issues/25887">#25887&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Flink runner now supports Flink 1.16.x (&lt;a href="https://github.com/apache/beam/issues/25046">#25046&lt;/a>).&lt;/li>
&lt;li>Schema&amp;rsquo;d PTransforms can now be directly applied to Beam dataframes just like PCollections.
(Note that when doing multiple operations, it may be more efficient to explicitly chain the operations
like &lt;code>df | (Transform1 | Transform2 | ...)&lt;/code> to avoid excessive conversions.)&lt;/li>
&lt;li>The Go SDK adds new transforms periodic.Impulse and periodic.Sequence that extends support
for slowly updating side input patterns. (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>Several Google client libraries in Python SDK dependency chain were updated to latest available major versions. (&lt;a href="https://github.com/apache/beam/pull/24599">#24599&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>If a main session fails to load, the pipeline will now fail at worker startup. (&lt;a href="https://github.com/apache/beam/issues/25401">#25401&lt;/a>).&lt;/li>
&lt;li>Python pipeline options will now ignore unparsed command line flags prefixed with a single dash. (&lt;a href="https://github.com/apache/beam/issues/25943">#25943&lt;/a>).&lt;/li>
&lt;li>The SmallestPerKey combiner now requires keyword-only arguments for specifying optional parameters, such as &lt;code>key&lt;/code> and &lt;code>reverse&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/25888">#25888&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Cloud Debugger support and its pipeline options are deprecated and will be removed in the next Beam version,
in response to the Google Cloud Debugger service &lt;a href="https://cloud.google.com/debugger/docs/deprecations">turning down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>BigQuery sink in STORAGE_WRITE_API mode in batch pipelines might result in data consistency issues during the handling of other unrelated transient errors for Beam SDKs 2.35.0 - 2.46.0 (inclusive). For more details see: &lt;a href="https://github.com/apache/beam/issues/26521">https://github.com/apache/beam/issues/26521&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>BigQueryIO Storage API write with autoUpdateSchema may cause data corruption for Beam SDKs 2.45.0 - 2.47.0 (inclusive) (&lt;a href="https://github.com/apache/beam/issues/26789">#26789&lt;/a>)&lt;/li>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.47.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amir Fayazi&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Andrey Kot&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>ChangyuLi28&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Ma&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Jiangjie (Becket) Qin&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Weaver&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Reza Rokni&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Saifuddin53&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Thomas Gaddy&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yanan Hao&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yuvi Panda&lt;/p>
&lt;p>andres-vv&lt;/p>
&lt;p>bochap&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>hnnsgstfssn&lt;/p>
&lt;p>jrmccluskey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xianhualiu&lt;/p>
&lt;p>zhangskz&lt;/p></description></item><item><title>Blog: Apache Beam 2.46.0</title><link>/blog/beam-2.46.0/</link><pubDate>Fri, 10 Mar 2023 13:00:00 -0500</pubDate><guid>/blog/beam-2.46.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.46.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2460-2023-03-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.46.0, check out the &lt;a href="https://github.com/apache/beam/milestone/9?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java SDK containers migrated to &lt;a href="https://hub.docker.com/_/eclipse-temurin">Eclipse Temurin&lt;/a>
as a base. This change migrates away from the deprecated &lt;a href="https://hub.docker.com/_/openjdk">OpenJDK&lt;/a>
container. Eclipse Temurin is currently based upon Ubuntu 22.04 while the OpenJDK
container was based upon Debian 11.&lt;/li>
&lt;li>RunInference PTransform will accept model paths as SideInputs in Python SDK. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>RunInference supports ONNX runtime in Python SDK (&lt;a href="https://github.com/apache/beam/issues/22972">#22972&lt;/a>)&lt;/li>
&lt;li>Tensorflow Model Handler for RunInference in Python SDK (&lt;a href="https://github.com/apache/beam/issues/25366">#25366&lt;/a>)&lt;/li>
&lt;li>Java SDK modules migrated to use &lt;code>:sdks:java:extensions:avro&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/24748">#24748&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added in JmsIO a retry policy for failed publications (Java) (&lt;a href="https://github.com/apache/beam/issues/24971">#24971&lt;/a>).&lt;/li>
&lt;li>Support for &lt;code>LZMA&lt;/code> compression/decompression of text files added to the Python SDK (&lt;a href="https://github.com/apache/beam/issues/25316">#25316&lt;/a>)&lt;/li>
&lt;li>Added ReadFrom/WriteTo Csv/Json as top-level transforms to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;li>Option for SparkRunner to avoid the need of SDF output to fit in memory (&lt;a href="https://github.com/apache/beam/issues/23852">#23852&lt;/a>).
This helps e.g. with ParquetIO reads. Turn the feature on by adding experiment &lt;code>use_bounded_concurrent_output_for_sdf&lt;/code>.&lt;/li>
&lt;li>Add &lt;code>WatchFilePattern&lt;/code> transform, which can be used as a side input to the RunInference PTransfrom to watch for model updates using a file pattern. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>Add support for loading TorchScript models with &lt;code>PytorchModelHandler&lt;/code>. The TorchScript model path can be
passed to PytorchModelHandler using &lt;code>torch_script_model_path=&amp;lt;path_to_model&amp;gt;&lt;/code>. (&lt;a href="https://github.com/apache/beam/pull/25321">#25321&lt;/a>)&lt;/li>
&lt;li>The Go SDK now requires Go 1.19 to build. (&lt;a href="https://github.com/apache/beam/pull/25545">#25545&lt;/a>)&lt;/li>
&lt;li>The Go SDK now has an initial native Go implementation of a portable Beam Runner called Prism. (&lt;a href="https://github.com/apache/beam/pull/24789">#24789&lt;/a>)
&lt;ul>
&lt;li>For more details and current state see &lt;a href="https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism">https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The deprecated SparkRunner for Spark 2 (see &lt;a href="#2410---2022-08-23">2.41.0&lt;/a>) was removed (&lt;a href="https://github.com/apache/beam/pull/25263">#25263&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s BatchElements performs more aggressive batching in some cases,
capping at 10 second rather than 1 second batches by default and excluding
fixed cost in this computation to better handle cases where the fixed cost
is larger than a single second. To get the old behavior, one can pass
&lt;code>target_batch_duration_secs_including_fixed_cost=1&lt;/code> to BatchElements.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Avro related classes are deprecated in module &lt;code>beam-sdks-java-core&lt;/code> and will be eventually removed. Please, migrate to a new module &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead by importing the classes from &lt;code>org.apache.beam.sdk.extensions.avro&lt;/code> package.
For the sake of migration simplicity, the relative package path and the whole class hierarchy of Avro related classes in new module is preserved the same as it was before.
For example, import &lt;code>org.apache.beam.sdk.extensions.avro.coders.AvroCoder&lt;/code> class instead of&lt;code>org.apache.beam.sdk.coders.AvroCoder&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/24749">#24749&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.46.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Katz&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Egbert van der Wal&lt;/p>
&lt;p>Elizaveta Lomteva&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Herman Mak&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo E&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruslan Altynnikov&lt;/p>
&lt;p>Ryan Zhang&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sam sam&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shivam&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vachan&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vincent Devillers&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>William Ross Morrow&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>Ziqi Ma&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>aliftadvantage&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kamrankoupayi&lt;/p>
&lt;p>kileys&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>nickuncaged1201&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p></description></item><item><title>Blog: Apache Beam 2.45.0</title><link>/blog/beam-2.45.0/</link><pubDate>Wed, 15 Feb 2023 09:00:00 -0700</pubDate><guid>/blog/beam-2.45.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.45.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2023-01-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.45.0, check out the &lt;a href="https://github.com/apache/beam/milestone/8?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>MongoDB IO connector added (Go) (&lt;a href="https://github.com/apache/beam/issues/24575">#24575&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>RunInference Wrapper with Sklearn Model Handler support added in Go SDK (&lt;a href="https://github.com/apache/beam/issues/23382">#24497&lt;/a>).&lt;/li>
&lt;li>Adding override of allowed TLS algorithms (Java), now maintaining the disabled/legacy algorithms
present in 2.43.0 (up to 1.8.0_342, 11.0.16, 17.0.2 for respective Java versions). This is accompanied
by an explicit re-enabling of TLSv1 and TLSv1.1 for Java 8 and Java 11.&lt;/li>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Portable Java pipelines, Go pipelines, Python streaming pipelines, and portable Python batch
pipelines on Dataflow are required to use Runner V2. The &lt;code>disable_runner_v2&lt;/code>,
&lt;code>disable_runner_v2_until_2023&lt;/code>, &lt;code>disable_prime_runner_v2&lt;/code> experiments will raise an error during
pipeline construction. You can no longer specify the Dataflow worker jar override. Note that
non-portable Java jobs and non-portable Python batch jobs are not impacted. (&lt;a href="https://github.com/apache/beam/issues/24515">#24515&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Avoids Cassandra syntax error when user-defined query has no where clause in it (Java) (&lt;a href="https://github.com/apache/beam/issues/24829">#24829&lt;/a>).&lt;/li>
&lt;li>Fixed JDBC connection failures (Java) during handshake due to deprecated TLSv1(.1) protocol for the JDK. (&lt;a href="https://github.com/apache/beam/issues/24623">#24623&lt;/a>)&lt;/li>
&lt;li>Fixed Python BigQuery Batch Load write may truncate valid data when deposition sets to WRITE_TRUNCATE and incoming data is large (Python) (&lt;a href="https://github.com/apache/beam/issues/24535">#24623&lt;/a>).&lt;/li>
&lt;li>Fixed Kafka watermark issue with sparse data on many partitions (&lt;a href="https://github.com/apache/beam/pull/24205">#24205&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.45.0 release. Thank you to all contributors!&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrea Nardelli&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Benjamin Gonzalez&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniela Martín&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dejan Spasic&lt;/p>
&lt;p>Diego Gomez&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Elias Segundo Antonio&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Evgeny Antyshev&lt;/p>
&lt;p>Fernando Morales&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Lucas Marques&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Marco Robles&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>Melanie&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Philippe Moussalli&lt;/p>
&lt;p>Piyush Sagar&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Rick Viscomi&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Seung Jin An&lt;/p>
&lt;p>Shane Hansen&lt;/p>
&lt;p>Sho Nakatani&lt;/p>
&lt;p>Shunya Ueta&lt;/p>
&lt;p>Siddharth Agrawal&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinbin Huang&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Xinyue Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>andoni-guzman&lt;/p>
&lt;p>andthezhang&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>camphillips22&lt;/p>
&lt;p>gabihodoroaga&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>yyy1000&lt;/p>
&lt;p>zhengbuqian&lt;/p></description></item><item><title>Blog: Apache Beam 2.44.0</title><link>/blog/beam-2.44.0/</link><pubDate>Tue, 17 Jan 2023 09:00:00 -0700</pubDate><guid>/blog/beam-2.44.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.44.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2023-01-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.44.0, check out the &lt;a href="https://github.com/apache/beam/milestone/7?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable sink (Write and WriteBatch) added (Go) (&lt;a href="https://github.com/apache/beam/issues/23324">#23324&lt;/a>).&lt;/li>
&lt;li>S3 implementation of the Beam filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/23991">#23991&lt;/a>).&lt;/li>
&lt;li>Support for SingleStoreDB source and sink added (Java) (&lt;a href="https://github.com/apache/beam/issues/22617">#22617&lt;/a>).&lt;/li>
&lt;li>Added support for DefaultAzureCredential authentication in Azure Filesystem (Python) (&lt;a href="https://github.com/apache/beam/issues/24210">#24210&lt;/a>).&lt;/li>
&lt;li>Added new CdapIO for CDAP Batch and Streaming Source/Sinks (Java) (&lt;a href="https://github.com/apache/beam/issues/24961">#24961&lt;/a>).&lt;/li>
&lt;li>Added new SparkReceiverIO for Spark Receivers 2.4.* (Java) (&lt;a href="https://github.com/apache/beam/issues/24960">#24960&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Beam now provides a portable &amp;ldquo;runner&amp;rdquo; that can render pipeline graphs with
graphviz. See &lt;code>python -m apache_beam.runners.render --help&lt;/code> for more details.&lt;/li>
&lt;li>Local packages can now be used as dependencies in the requirements.txt file, rather
than requiring them to be passed separately via the &lt;code>--extra_package&lt;/code> option
(Python) (&lt;a href="https://github.com/apache/beam/pull/23684">#23684&lt;/a>).&lt;/li>
&lt;li>Pipeline Resource Hints now supported via &lt;code>--resource_hints&lt;/code> flag (Go) (&lt;a href="https://github.com/apache/beam/pull/23990">#23990&lt;/a>).&lt;/li>
&lt;li>Make Python SDK containers reusable on portable runners by installing dependencies to temporary venvs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12792">BEAM-12792&lt;/a>).&lt;/li>
&lt;li>RunInference model handlers now support the specification of a custom inference function in Python (&lt;a href="https://github.com/apache/beam/issues/22572">#22572&lt;/a>)&lt;/li>
&lt;li>Support for &lt;code>map_windows&lt;/code> urn added to Go SDK (&lt;a href="https://github.apache/beam/pull/24307">#24307&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>ParquetIO.withSplit&lt;/code> was removed since splittable reading has been the default behavior since 2.35.0. The effect of
this change is to drop support for non-splittable reading (Java)(&lt;a href="https://github.com/apache/beam/issues/23832">#23832&lt;/a>).&lt;/li>
&lt;li>&lt;code>beam-sdks-java-extensions-google-cloud-platform-core&lt;/code> is no longer a
dependency of the Java SDK Harness. Some users of a portable runner (such as Dataflow Runner v2)
may have an undeclared dependency on this package (for example using GCS with
TextIO) and will now need to declare the dependency.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-core&lt;/code> is no longer a dependency of the Java SDK Harness. Users of a portable
runner (such as Dataflow Runner v2) will need to provide this package and its dependencies.&lt;/li>
&lt;li>Slices now use the Beam Iterable Coder. This enables cross language use, but breaks pipeline updates
if a Slice type is used as a PCollection element or State API element. (Go)&lt;a href="https://github.com/apache/beam/issues/24339">#24339&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed JmsIO acknowledgment issue (Java) (&lt;a href="https://github.com/apache/beam/issues/20814">#20814&lt;/a>)&lt;/li>
&lt;li>Fixed Beam SQL CalciteUtils (Java) and Cross-language JdbcIO (Python) did not support JDBC CHAR/VARCHAR, BINARY/VARBINARY logical types (&lt;a href="https://github.com/apache/beam/issues/23747">#23747&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/23526">#23526&lt;/a>).&lt;/li>
&lt;li>Ensure iterated and emitted types are used with the generic register package are registered with the type and schema registries.(Go) (&lt;a href="https://github.com/apache/beam/pull/23889">#23889&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.44.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alex Merose&lt;/p>
&lt;p>Alexey Inkin&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrej Galad&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Ayush Sharma&lt;/p>
&lt;p>Benjamin Gonzalez&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat Safiullin&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Danielle Syse&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Elias Segundo Antonio&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Evgeny Antyshev&lt;/p>
&lt;p>Heejong Lee&lt;/p>
&lt;p>Henrik Heggelund-Berg&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Janek Bevendorff&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John J. Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nancy Xu&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Nivaldo Tokuda&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Philippe Moussalli&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Rick Smit&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ryan Thompson&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Scott Strong&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Thiago Nunes&lt;/p>
&lt;p>Tianyang Hu&lt;/p>
&lt;p>Trevor Gevers&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vladislav Chunikhin&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>agvdndor&lt;/p>
&lt;p>andremissaglia&lt;/p>
&lt;p>arne-alex&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>camphillips22&lt;/p>
&lt;p>capthiron&lt;/p>
&lt;p>creste&lt;/p>
&lt;p>fab-jul&lt;/p>
&lt;p>illoise&lt;/p>
&lt;p>kn1kn1&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>peridotml&lt;/p>
&lt;p>shinannegans&lt;/p>
&lt;p>smeet07&lt;/p></description></item><item><title>Blog: Apache Beam Playground: An interactive environment to try transforms and examples</title><link>/blog/apacheplayground/</link><pubDate>Wed, 30 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/apacheplayground/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h3 id="what-is-apache-beam-playground">&lt;strong>What is Apache Beam Playground?&lt;/strong>&lt;/h3>
&lt;p>&lt;a href="https://play.beam.apache.org/">Apache Beam Playground&lt;/a> is an interactive environment to try Apache Beam transforms and examples without requiring to install or set up a Beam environment.&lt;/p>
&lt;h3 id="apache-beam-playground-features">&lt;strong>Apache Beam Playground Features&lt;/strong>&lt;/h3>
&lt;p>&lt;img class="center-block"
src="/images/blog/BeamPlayground.gif"
alt="Apache Beam Playground">&lt;/p>
&lt;ul>
&lt;li>Discover transform examples that you can try right away by browsing or searching Catalog that is sourced from Apache Beam GitHub&lt;/li>
&lt;li>Supports Java, Python, Go SDKs, and Scio to execute the example in Beam Direct Runner&lt;/li>
&lt;li>Displays pipeline execution graph (DAG)&lt;/li>
&lt;li>Code editor to modify examples or try your own custom pipeline with a Direct Runner&lt;/li>
&lt;li>Code editor with code highlighting, flexible layout, color schemes, and other features to provide responsive UX in desktop browsers&lt;/li>
&lt;li>Embedding a Playground example on a web page prompts the web page readers to try the example pipeline in the Playground - e.g., &lt;a href="/get-started/try-beam-playground/">Playground Quickstart&lt;/a> page&lt;/li>
&lt;/ul>
&lt;h3 id="whats-next">&lt;strong>What’s Next&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Try examples in &lt;a href="https://play.beam.apache.org/">Apache Beam Playground&lt;/a>&lt;/li>
&lt;li>Submit your feedback using “Enjoying Playground?” in Apache Beam Playground or via &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSd5_5XeOwwW2yjEVHUXmiBad8Lxk-4OtNcgG45pbyAZzd4EbA/viewform?usp=pp_url">this form&lt;/a>&lt;/li>
&lt;li>Join the Beam &lt;a href="/community/contact-us">users@&lt;/a> mailing list&lt;/li>
&lt;li>Contribute to the Apache Beam Playground codebase by following a few steps in this &lt;a href="/contribute">Contribution Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Please &lt;a href="/community/contact-us">reach out&lt;/a> if you have any feedback or encounter any issues!&lt;/p></description></item><item><title>Blog: Apache Beam 2.43.0</title><link>/blog/beam-2.43.0/</link><pubDate>Thu, 17 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.43.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.43.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2022-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.43.0, check out the &lt;a href="https://github.com/apache/beam/milestone/5?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python 3.10 support in Apache Beam (&lt;a href="https://github.com/apache/beam/issues/21458">#21458&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. Try it out and give us feedback! (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Decreased TextSource CPU utilization by 2.3x (Java) (&lt;a href="https://github.com/apache/beam/issues/23193">#23193&lt;/a>).&lt;/li>
&lt;li>Fixed bug when using SpannerIO with RuntimeValueProvider options (Java) (&lt;a href="https://github.com/apache/beam/issues/22146">#22146&lt;/a>).&lt;/li>
&lt;li>Fixed issue for unicode rendering on WriteToBigQuery (&lt;a href="https://github.com/apache/beam/issues/22312">#22312&lt;/a>)&lt;/li>
&lt;li>Remove obsolete variants of BigQuery Read and Write, always using Beam-native variant
(&lt;a href="https://github.com/apache/beam/issues/23564">#23564&lt;/a> and &lt;a href="https://github.com/apache/beam/issues/23559">#23559&lt;/a>).&lt;/li>
&lt;li>Bumped google-cloud-spanner dependency version to 3.x for Python SDK (&lt;a href="https://github.com/apache/beam/issues/21198">#21198&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Dataframe wrapper added in Go SDK via Cross-Language (with automatic expansion service). (Go) (&lt;a href="https://github.com/apache/beam/issues/23384">#23384&lt;/a>).&lt;/li>
&lt;li>Name all Java threads to aid in debugging (&lt;a href="https://github.com/apache/beam/issues/23049">#23049&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;li>Allow configuring GCP OAuth scopes via pipeline options. This unblocks usages of Beam IOs that require additional scopes.
For example, this feature makes it possible to access Google Drive backed tables in BigQuery (&lt;a href="https://github.com/apache/beam/issues/23290">#23290&lt;/a>).&lt;/li>
&lt;li>An example for using Python RunInference from Java (&lt;a href="https://github.com/apache/beam/pull/23619">#23290&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>CoGroupByKey transform in Python SDK has changed the output typehint. The typehint component representing grouped values changed from List to Iterable,
which more accurately reflects the nature of the arbitrarily large output collection. &lt;a href="https://github.com/apache/beam/issues/21556">#21556&lt;/a> Beam users may see an error on transforms downstream from CoGroupByKey. Users must change methods expecting a List to expect an Iterable going forward. See &lt;a href="https://docs.google.com/document/d/1RIzm8-g-0CyVsPb6yasjwokJQFoKHG4NjRUcKHKINu0">document&lt;/a> for information and fixes.&lt;/li>
&lt;li>The PortableRunner for Spark assumes Spark 3 as default Spark major version unless configured otherwise using &lt;code>--spark_version&lt;/code>.
Spark 2 support is deprecated and will be removed soon (&lt;a href="https://github.com/apache/beam/issues/23728">#23728&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Numeric/Decimal type values (&lt;a href="https://github.com/apache/beam/issues/19817">#19817&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.43.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
AlexZMLyu
Alexey Romanenko
Anand Inguva
Andrew Pilloud
Andy Ye
Arnout Engelen
Benjamin Gonzalez
Bharath Kumarasubramanian
BjornPrime
Brian Hulette
Bruno Volpato
Chamikara Jayalath
Colin Versteeg
Damon
Daniel Smilkov
Daniela Martín
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Denis Pyshev
Dmitry Repin
Evan Galpin
Evgeny Antyshev
Fernando Morales
Geddy05
Harshit Mehrotra
Iñigo San Jose Visiers
Ismaël Mejía
Israel Herraiz
Jan Lukavský
Juta Staes
Kanishk Karanawat
Kenneth Knowles
KevinGG
Kiley Sok
Liam Miller-Cushon
Luke Cwik
Mc
Melissa Pashniak
Moritz Mack
Ning Kang
Pablo Estrada
Philippe Moussalli
Pranav Bhandari
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Ryohei Nagao
Sam Rohde
Sam Whittle
Sanil Jain
Seunghwan Hong
Shane Hansen
Shubham Krishna
Shunsuke Otani
Steve Niemitz
Steven van Rossum
Svetak Sundhar
Thiago Nunes
Toran Sahu
Veronica Wasson
Vitaly Terentyev
Vladislav Chunikhin
Xinyu Liu
Yi Hu
Yixiao Shen
alexeyinkin
arne-alex
azhurkevich
bulat safiullin
bullet03
coldWater
dpcollins-google
egalpin
johnjcasey
liferoad
rvballada
shaojwu
tvalentyn&lt;/p></description></item><item><title>Blog: New Resources Available for Beam ML</title><link>/blog/ml-resources/</link><pubDate>Wed, 09 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/ml-resources/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>If you&amp;rsquo;ve been paying attention, over the past year you&amp;rsquo;ve noticed that
Beam has released a number of features designed to make Machine Learning
easy. Ranging from things like the introduction of the &lt;code>RunInference&lt;/code>
transform to the continued refining of &lt;code>Beam Dataframes&lt;/code>, this has been
an area where we&amp;rsquo;ve seen Beam make huge strides. While development has
advanced quickly, however, until recently there has been a lack of
resources to help people discover and use these new features.&lt;/p>
&lt;p>Over the past several months, we&amp;rsquo;ve been hard at work building out
documentation and notebooks to make it easier to use these new features
and to show how Beam can be used to solve common Machine Learning problems.
We&amp;rsquo;re now happy to present this new and improved Beam ML experience!&lt;/p>
&lt;p>To get started, we encourage you to visit Beam&amp;rsquo;s new &lt;a href="/documentation/ml/overview/">AI/ML landing page&lt;/a>.
We&amp;rsquo;ve got plenty of content on things like &lt;a href="/documentation/ml/multi-model-pipelines/">multi-model pipelines&lt;/a>,
&lt;a href="/documentation/ml/runinference-metrics/">performing inference with metrics&lt;/a>,
&lt;a href="/documentation/ml/online-clustering/">online training&lt;/a>, and much more.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ml-landing.png"
alt="ML landing page">&lt;/p>
&lt;p>We&amp;rsquo;ve also introduced a number of example &lt;a href="https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml">Jupyter Notebooks&lt;/a>
showing how to use built in beam transforms like &lt;code>RunInference&lt;/code> and &lt;code>Beam Dataframes&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ensemble-model-notebook.png"
alt="Example ensemble notebook with RunInference">&lt;/p>
&lt;p>Adding more examples and notebooks will be a point of emphasis going forward.
For our next round of improvements, we are planning on adding examples of
using RunInference with &amp;gt;30GB models, with multi-language pipelines, with
common Beam concepts, and with TensorRT. We will also add examples showing
other pieces of the Machine Learning lifecycle like model evaluation with TFMA,
per-entity training, and more online training.&lt;/p>
&lt;p>We hope you find this useful! As always, if you see any areas for improvement, please &lt;a href="https://github.com/apache/beam/issues/new/choose">open an issue&lt;/a>
or a &lt;a href="https://github.com/apache/beam/pulls">pull request&lt;/a>!&lt;/p></description></item><item><title>Blog: Beam starter projects</title><link>/blog/beam-starter-projects/</link><pubDate>Thu, 03 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-starter-projects/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re happy to announce that we&amp;rsquo;re providing new Beam starter projects! 🎉&lt;/p>
&lt;p>Setting up and configuring a new project can be time consuming, and varies in different languages. We hope this will make it easier for you to get started in creating new Apache Beam projects and pipelines.&lt;/p>
&lt;p>All the starter projects come in their own GitHub repository, so you can simply clone a repo and you&amp;rsquo;re ready to go. Each project comes with a README with how to use it, a simple &amp;ldquo;Hello World&amp;rdquo; pipeline, and a test for the pipeline. The GitHub repositories come pre-configured with GitHub Actions to automatically run tests when pull requests are opened or modified, and Dependabot is enabled to make sure all the dependencies are up to date. This all comes out of the box, so you can start playing with your Beam pipeline without a hassle.&lt;/p>
&lt;p>For example, here&amp;rsquo;s how to get started with Java:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/apache/beam-starter-java
cd beam-starter-java
# Install Java and Gradle with sdkman.
curl -s &amp;#34;https://get.sdkman.io&amp;#34; | bash
sdk install java 11.0.12-tem
sdk install gradle
# To run the pipeline.
gradle run
# To run the tests.
gradle test
&lt;/code>&lt;/pre>&lt;p>And here&amp;rsquo;s how to get started with Python:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/apache/beam-starter-python
cd beam-starter-python
# Set up a virtual environment with the dependencies.
python -m venv env
source env/bin/activate
pip install -r requirements.txt
# To run the pipeline.
python main.py
# To run the tests.
python -m unittest
&lt;/code>&lt;/pre>&lt;p>Here are the starter projects; you can choose your favorite language:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[Java]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-java">github.com/apache/beam-starter-java&lt;/a> – Includes both Gradle and Maven configurations.&lt;/li>
&lt;li>&lt;strong>[Python]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-python">github.com/apache/beam-starter-python&lt;/a> – Includes a setup.py file to allow multiple files in your pipeline.&lt;/li>
&lt;li>&lt;strong>[Go]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-go">github.com/apache/beam-starter-go&lt;/a> – Includes how to register different types of functions for ParDo.&lt;/li>
&lt;li>&lt;strong>[Kotlin]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-kotlin">github.com/apache/beam-starter-kotlin&lt;/a> – Adapted to idiomatic Kotlin&lt;/li>
&lt;li>&lt;strong>[Scala]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-scala">github.com/apache/beam-starter-scala&lt;/a> – Coming soon!&lt;/li>
&lt;/ul>
&lt;p>We have updated the &lt;a href="/get-started/quickstart/java/">Java quickstart&lt;/a> to use the new starter project, and we&amp;rsquo;re working on updating the Python and Go quickstarts as well.&lt;/p>
&lt;p>We hope you find this useful. Feedback and contributions are always welcome! So feel free to create a GitHub issue, or open a Pull Request to any of the starter project repositories.&lt;/p></description></item><item><title>Blog: Apache Beam 2.42.0</title><link>/blog/beam-2.42.0/</link><pubDate>Mon, 17 Oct 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.42.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.42.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2420-2022-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.42.0, check out the &lt;a href="https://github.com/apache/beam/milestone/4?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;li>Added support for &lt;a href="/documentation/programming-guide/#batched-dofns">Batched
DoFns&lt;/a>
to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for Zstd compression to the Python SDK.&lt;/li>
&lt;li>Added support for Google Cloud Profiler to the Go SDK.&lt;/li>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go SDK&amp;rsquo;s Row Coder now uses a different single-precision float encoding for float32 types to match Java&amp;rsquo;s behavior (&lt;a href="https://github.com/apache/beam/issues/22629">#22629&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Timestamp type values &lt;a href="https://github.com/apache/beam/issues/19817">19817&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Go SDK doesn&amp;rsquo;t yet support Slowly Changing Side Input pattern (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/4">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.42.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abirdcfly
Ahmed Abualsaud
Alexander Zhuravlev
Alexey Inkin
Alexey Romanenko
Anand Inguva
Andrej Galad
Andrew Pilloud
Andy Ye
Balázs Németh
Brian Hulette
Bruno Volpato
bulat safiullin
bullet03
Chamikara Jayalath
ChangyuLi28
Clément Guillaume
Damon
Danny McCormick
Darkhan Nausharipov
David Huntsperger
dpcollins-google
Evgeny Antyshev
grufino
Heejong Lee
Ismaël Mejía
Jack McCluskey
johnjcasey
Jonathan Shen
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
liferoad
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manit Gupta
masahitojp
Michael Hu
Michel Davit
Moritz Mack
Naireen Hussain
nancyxu123
Nikhil Nadig
oborysevych
Pablo Estrada
Pranav Bhandari
Rajat Bhatta
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sergey Pronin
Shivam
Shunsuke Otani
Shunya Ueta
Steven Niemitz
Stuart
Svetak Sundhar
Valentyn Tymofieiev
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yixiao Shen&lt;/p></description></item></channel></rss>