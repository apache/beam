<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Beam starter projects</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re happy to announce that we&amp;rsquo;re providing new Beam starter projects! ðŸŽ‰&lt;/p>
&lt;p>Setting up and configuring a new project can be time consuming, and varies in different languages. We hope this will make it easier for you to get started in creating new Apache Beam projects and pipelines.&lt;/p>
&lt;p>All the starter projects come in their own GitHub repository, so you can simply clone a repo and you&amp;rsquo;re ready to go. Each project comes with a README with how to use it, a simple &amp;ldquo;Hello World&amp;rdquo; pipeline, and a test for the pipeline. The GitHub repositories come pre-configured with GitHub Actions to automatically run tests when pull requests are opened or modified, and Dependabot is enabled to make sure all the dependencies are up to date. This all comes out of the box, so you can start playing with your Beam pipeline without a hassle.&lt;/p>
&lt;p>For example, here&amp;rsquo;s how to get started with Java:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/apache/beam-starter-java
cd beam-starter-java
# Install Java and Gradle with sdkman.
curl -s &amp;quot;https://get.sdkman.io&amp;quot; | bash
sdk install java 11.0.12-tem
sdk install gradle
# To run the pipeline.
gradle run
# To run the tests.
gradle test
&lt;/code>&lt;/pre>&lt;p>And here&amp;rsquo;s how to get started with Python:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/apache/beam-starter-python
cd beam-starter-python
# Set up a virtual environment with the dependencies.
python -m venv env
source env/bin/activate
pip install -r requirements.txt
# To run the pipeline.
python main.py
# To run the tests.
python -m unittest
&lt;/code>&lt;/pre>&lt;p>Here are the starter projects; you can choose your favorite language:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[Java]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-java">github.com/apache/beam-starter-java&lt;/a> â€“ Includes both Gradle and Maven configurations.&lt;/li>
&lt;li>&lt;strong>[Python]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-python">github.com/apache/beam-starter-python&lt;/a> â€“ Includes a setup.py file to allow multiple files in your pipeline.&lt;/li>
&lt;li>&lt;strong>[Go]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-go">github.com/apache/beam-starter-go&lt;/a> â€“ Includes how to register different types of functions for ParDo.&lt;/li>
&lt;li>&lt;strong>[Kotlin]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-kotlin">github.com/apache/beam-starter-kotlin&lt;/a> â€“ Adapted to idiomatic Kotlin&lt;/li>
&lt;li>&lt;strong>[Scala]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-scala">github.com/apache/beam-starter-scala&lt;/a> â€“ Coming soon!&lt;/li>
&lt;/ul>
&lt;p>We have updated the &lt;a href="https://beam.apache.org/get-started/quickstart/java/">Java quickstart&lt;/a> to use the new starter project, and we&amp;rsquo;re working on updating the Python and Go quickstarts as well.&lt;/p>
&lt;p>We hope you find this useful. Feedback and contributions are always welcome! So feel free to create a GitHub issue, or open a Pull Request to any of the starter project repositories.&lt;/p></description><link>/blog/beam-starter-projects/</link><pubDate>Thu, 03 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-starter-projects/</guid><category>blog</category></item><item><title>Apache Beam 2.42.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.42.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2420-2022-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.42.0, check out the &lt;a href="https://github.com/apache/beam/milestone/4?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;li>Added support for &lt;a href="https://beam.apache.org/documentation/programming-guide/#batched-dofns">Batched
DoFns&lt;/a>
to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for Zstd compression to the Python SDK.&lt;/li>
&lt;li>Added support for Google Cloud Profiler to the Go SDK.&lt;/li>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go SDK&amp;rsquo;s Row Coder now uses a different single-precision float encoding for float32 types to match Java&amp;rsquo;s behavior (&lt;a href="https://github.com/apache/beam/issues/22629">#22629&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Timestamp type values &lt;a href="https://github.com/apache/beam/issues/19817">19817&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Go SDK doesn&amp;rsquo;t yet support Slowly Changing Side Input pattern (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/4">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.42.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abirdcfly
Ahmed Abualsaud
Alexander Zhuravlev
Alexey Inkin
Alexey Romanenko
Anand Inguva
Andrej Galad
Andrew Pilloud
Andy Ye
BalÃ¡zs NÃ©meth
Brian Hulette
Bruno Volpato
bulat safiullin
bullet03
Chamikara Jayalath
ChangyuLi28
ClÃ©ment Guillaume
Damon
Danny McCormick
Darkhan Nausharipov
David Huntsperger
dpcollins-google
Evgeny Antyshev
grufino
Heejong Lee
IsmaÃ«l MejÃ­a
Jack McCluskey
johnjcasey
Jonathan Shen
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
liferoad
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manit Gupta
masahitojp
Michael Hu
Michel Davit
Moritz Mack
Naireen Hussain
nancyxu123
Nikhil Nadig
oborysevych
Pablo Estrada
Pranav Bhandari
Rajat Bhatta
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sergey Pronin
Shivam
Shunsuke Otani
Shunya Ueta
Steven Niemitz
Stuart
Svetak Sundhar
Valentyn Tymofieiev
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yixiao Shen&lt;/p></description><link>/blog/beam-2.42.0/</link><pubDate>Mon, 17 Oct 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.42.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Hop web version with Cloud Dataflow</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Hop is a codeless visual development environment for Apache Beam pipelines that
can run jobs in any Beam runner, such as Dataflow, Flink or Spark. &lt;a href="https://beam.apache.org/blog/apache-hop-with-dataflow/">In a
previous post&lt;/a>, we
introduced the desktop version of Apache Hop. Hop also has a web environment,
Hop Web, that you can run from a container, so you don&amp;rsquo;t have to install
anything on your computer to use it.&lt;/p>
&lt;p>In this detailed tutorial, you access Hop through the internet using a web
browser and point to a container running in a virtual machine on Google
Cloud. That container will launch jobs in Dataflow and report back the results
of those jobs. Because we don&amp;rsquo;t want just anyone to access your Hop instance,
weâ€™re going to secure it so that only you can access that virtual machine. The
following diagram illustrates the setup:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image2.png" alt="Architecture deployed with this tutorial">&lt;/p>
&lt;p>We will show how to do the deployment described previously, creating a web and
visual development environment that builds Beam pipelines using just a web
browser. When complete, you will have a secure web environment that you can use
to create pipelines with your web browser and launch them using Google Cloud
Dataflow.&lt;/p>
&lt;h2 id="what-do-you-need-to-run-this-example">What do you need to run this example?&lt;/h2>
&lt;p>We are using Google Cloud, so the first thing you need is a Google Cloud
project. If needed, you can sign up for the free trial of Google Cloud at
&lt;a href="https://cloud.google.com/free">https://cloud.google.com/free&lt;/a>.&lt;/p>
&lt;p>When you have a project, you can use &lt;a href="https://cloud.google.com/shell">Cloud
Shell&lt;/a> in your web browser with no additional
setup. In Cloud Shell, the Google Cloud SDK is automatically configured for your
project and credentials. That&amp;rsquo;s the option we use here. Alternatively, you can
configure the Google Cloud SDK in your local computer. For instructions, see
&lt;a href="https://cloud.google.com/sdk/docs/install">https://cloud.google.com/sdk/docs/install&lt;/a>.&lt;/p>
&lt;p>To open Cloud Shell, go to the [Google Cloud console]
(&lt;a href="http://console.cloud.google.com">http://console.cloud.google.com&lt;/a>), make sure your project is selected, and click
the Cloud Shell button &lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image1.png" alt="Cloud Shellbutton">. Cloud Shell opens,
and you can use it to run the commands shown in this post.&lt;/p>
&lt;p>The commands that we are going to use in the next steps are &lt;a href="https://gist.github.com/iht/6219b227424ada477462c7b9d9d93c57">available in a Gist
in Github&lt;/a>, just
in case you prefer to run that script instead of copying the commands from this
tutorial.&lt;/p>
&lt;h2 id="permissions-and-accounts">Permissions and accounts&lt;/h2>
&lt;p>When we run a Dataflow pipeline, we can use our personal Google Cloud
credentials to run the job. But Hop web will be running in a virtual machine,
and in Google Cloud, virtual machines run using service accounts as
credentials. So we need to make sure that we have a service account that has
permission to run Dataflow jobs.&lt;/p>
&lt;p>By default, virtual machines use the service account called &lt;em>Compute Engine
default service account&lt;/em>. For the sake of simplicity, we will use this
account. Still, we need to add some permissions to run Dataflow jobs with that
service account.&lt;/p>
&lt;p>First, let&amp;rsquo;s make sure that you have enabled all the required Google Cloud
APIs. &lt;a href="https://console.cloud.google.com/flows/enableapi?apiid=dataflow,compute_component,logging,storage_component,storage_api,bigquery,pubsub">Click this link to enable Dataflow, BigQuery and
Pub/Sub&lt;/a>,
which weâ€™ll use in this workflow. The link takes you to your project in the
Google Cloud console, where you can enable the APIs.&lt;/p>
&lt;p>Let&amp;rsquo;s now give permissions to the VM account. First, find the ID of the service
account. Open Cloud Shell, and run the following command.&lt;/p>
&lt;pre>&lt;code>gcloud iam service-accounts list | grep compute
&lt;/code>&lt;/pre>&lt;p>The output is similar to the following, with &lt;code>&amp;lt;PROJECT_NUMBER&amp;gt;&lt;/code> replaced by your
project number:&lt;/p>
&lt;pre>&lt;code>EMAIL: &amp;lt;PROJECT_NUMBER&amp;gt;-compute@developer.gserviceaccount.com
&lt;/code>&lt;/pre>&lt;p>Copy that service account ID, because we use it in the next step. Run the
following command to grant the &lt;a href="https://cloud.google.com/dataflow/docs/concepts/access-control">Dataflow Admin
role&lt;/a> to the
service account. This role is required to run jobs:&lt;/p>
&lt;pre>&lt;code>gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member=&amp;quot;serviceAccount:&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&amp;quot; --role=&amp;quot;roles/dataflow.admin&amp;quot;
&lt;/code>&lt;/pre>&lt;p>where &lt;code>&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&lt;/code> is the ID that you retrieved previously. If you are
running these commands in Cloud Shell, the environment variable
&lt;code>GOOGLE_CLOUD_PROJECT&lt;/code> is already set to your project ID. If you are running
this from any other place, set the &lt;code>$GOOGLE_CLOUD_PROJECT&lt;/code> variable with the ID
of your project.&lt;/p>
&lt;p>Now your &amp;ldquo;user&amp;rdquo; for Dataflow is that service account. If your jobs are accessing
data in BigQuery, Cloud Storage, Pub/Sub, and so on, you also need to grant
roles for those services to the service account.&lt;/p>
&lt;h2 id="disk-and-virtual-machine">Disk and virtual machine&lt;/h2>
&lt;p>Let&amp;rsquo;s create a virtual machine (VM) in Compute Engine to run the Docker
container of Apache Hop.&lt;/p>
&lt;p>In Compute Engine, it is possible to run a container directly in a VM. There are
other options to run containers in Google Cloud, but a VM is probably the
simplest and most straightforward. The full details are in the &lt;a href="https://cloud.google.com/compute/docs/containers/deploying-containers">Deploying
containers on VMs and
MIGs&lt;/a>
page of the Google Cloud documentation.&lt;/p>
&lt;p>In this tutorial, we will always be working in the zone &lt;code>europe-west1-b&lt;/code>, so you
will see that zone in a lot of the commands. However, you can choose any Google
Cloud zone; just remember to use the value for your zone instead of
&lt;code>europe-west1-b&lt;/code>. Always use the same zone for all the resources, such as disks
and VMs. To minimize the latency when using Hop web, choose a zone that is
geographically close to your location. Let&amp;rsquo;s define the zone now and use this
variable for the rest of the commands:&lt;/p>
&lt;pre>&lt;code>ZONE=europe-west1-b
&lt;/code>&lt;/pre>&lt;p>Containers have ephemeral storage: when you restart the container, the disk of
the container returns to its original state. Therefore, if we restart the Hop
web container, we lose all our precious pipelines. To avoid that, we are going
to create a persistent disk, where we will store all our work with Hop web. Run
the following command to create the disk:&lt;/p>
&lt;pre>&lt;code>gcloud compute disks create my-hop-disk \
--type=pd-balanced \
--size=10GB \
--zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Thanks to this disk, weâ€™re able to stop the virtual machine and still keep all
our personal files in Hop web intact.&lt;/p>
&lt;p>Let&amp;rsquo;s now create the VM. For the VM, we need to select the network (&lt;code>default&lt;/code> in
the, well, default case) so the VM will not have a public IP address. This is
important for security reasons, but it wonâ€™t stop us from using the VM from our
web browser thanks to the Identity Aware Proxy. More on this later; for now
let&amp;rsquo;s create the VM:&lt;/p>
&lt;pre>&lt;code>gcloud compute instances create-with-container my-hop-vm \
--zone=$ZONE \
--network-interface=subnet=default,no-address \
--scopes=https://www.googleapis.com/auth/cloud-platform \
--tags=http-server,https-server,ssh \
--container-image=apache/hop-web:2.0.1 \
--container-restart-policy=on-failure \
--container-mount-disk=mode=rw,mount-path=/root,name=my-hop-disk,partition=0 \
--disk=boot=no,device-name=my-hop-disk,mode=rw,name=my-hop-disk
&lt;/code>&lt;/pre>&lt;p>You might be wondering what those additional options are. They are required for
the VM to work properly with Hop web. For instance, the &lt;code>scopes&lt;/code> option is what
allows the VM to use Dataflow, and the &lt;code>tags&lt;/code> option lets your browser reach the
Hop web address through the network firewall.&lt;/p>
&lt;p>Apache Hop listens on port 8080 for HTTP connections, so if you have additional
custom firewall rules in your project, make sure you are not stopping TCP
traffic on port 8080.&lt;/p>
&lt;p>But wait a minute; we have created a machine with only private IPs. How can we
reach Hop web from the web browser on our computer? Don&amp;rsquo;t we need a public IP
address for that?&lt;/p>
&lt;p>Google Cloud has a feature called the Identity Aware Proxy (IAP) that can be
used to wrap services with an authorization layer, allowing connections to
resources with only internal IPs.&lt;/p>
&lt;p>We can use the IAP to wrap our Apache Hop web server. With the following
command, we create a tunnel listening on local port 8080 that connects to port
8080 on the VM:&lt;/p>
&lt;pre>&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>To keep the tunnel open, leave that command running. If the command fails right
after creating the VM, wait a few seconds and try again; the container might
still be booting up.&lt;/p>
&lt;p>We now have a tunnel that we can connect to using our web browser. If youâ€™re
running these commands on your local computer and not in Cloud Shell, point your
browser to &lt;code>localhost:8080&lt;/code>. The Hop UI should load.&lt;/p>
&lt;p>If you are running these command in Cloud Shell, where do we point the browser
to? Cloud Shell comes with an utility for situations like this one. In Cloud
Shell, locate the &lt;strong>Web Preview&lt;/strong> button:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image3.png" alt="Web preview options">&lt;/p>
&lt;p>If the preview isnâ€™t using port 8080, click &lt;strong>Change port&lt;/strong>, and switch to
port 8080. When you click &lt;strong>Preview on port&lt;/strong>, Cloud Shell opens a new tab in
your browser that points to the tunnel address.&lt;/p>
&lt;p>The &lt;strong>Identity Aware Proxy&lt;/strong> will ask you to identify yourself using your Google
account.&lt;/p>
&lt;p>After that, the Apache Hop web interface loads:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image4.png" alt="Hop web UI">&lt;/p>
&lt;p>That URL is authenticated using your Google account, the same one that you are
using for Google Cloud (the one you are authenticated with in the Google Cloud
SDK). So even if another person gets that URL address, they wonâ€™t be able to
access your Apache Hop instance.&lt;/p>
&lt;p>You are now ready to use Apache Hop in a web browser!&lt;/p>
&lt;p>You can try to replicate the example that was given &lt;a href="https://beam.apache.org/blog/apache-hop-with-dataflow/">in a previous
post&lt;/a> using Hop web, or
just try to launch any other project from the samples included with Hop:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image5.png" alt="Sample projects in Hop">&lt;/p>
&lt;h2 id="where-should-i-store-my-stuff">Where should I store my stuff?&lt;/h2>
&lt;p>The directories in the file system of a container are ephemeral. How can you be
sure that you store your pipelines and JARs in a persistent location?&lt;/p>
&lt;p>The home directory container is &lt;code>/root&lt;/code>, and it is the only &lt;strong>persistent&lt;/strong>
directory in the container (thanks to the disk we created previously). When you
restart the VM for whatever reason, any file included in that directory is
retained. But the rest of the directories reset to their original state. So make
sure you save your stuff, such as your pipelines, the fat JAR generated for
Dataflow, and so on, in the &lt;code>/root&lt;/code> directory or its subdirectories.&lt;/p>
&lt;p>In the Hop file dialogs, when you click the home icon, you are directed to the
&lt;code>/root&lt;/code> directory, so it is very straightforward to use it to store
everything. In the example in the picture, we clicked the &lt;strong>Home&lt;/strong> button and
are storing a JAR in that persistent directory:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image6.png" alt="Hop file dialog">&lt;/p>
&lt;h2 id="turning-off-the-virtual-machine">Turning off the virtual machine&lt;/h2>
&lt;p>If you want to save some money when you are not using the virtual machine, stop
the VM and launch it again when needed. The content of the &lt;em>/root&lt;/em> directory is
saved when you stop the virtual machine.&lt;/p>
&lt;p>To stop the VM, run the following command (or in the console, on the Compute
Engine VM page, click &lt;strong>Stop&lt;/strong>):&lt;/p>
&lt;pre>&lt;code>gcloud compute instances stop my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>And to start it again, run the following command:&lt;/p>
&lt;pre>&lt;code>gcloud compute instances start my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Remember that you need to have the Identity Aware Proxy running in order to
access Hop web, so after starting the VM, don&amp;rsquo;t forget to run the command to
start the Identity Aware Proxy (and if it fails right after starting, wait a few
seconds and run it again):&lt;/p>
&lt;pre>&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>This post has shown that all that you need to run Hop is a web browser. And,
well, a Google Cloud project too.&lt;/p>
&lt;p>We deployed the container to a virtual machine in Google Cloud, so you can
access Hop from anywhere, and we created a persistent disk, so you can have
permanent storage for your pipelines. Now you can use your web browser to create
your pipelines and to run Dataflow jobs without having to install anything
locally in your computer: not Java, not Docker, not the Google Cloud SDK;
nothing, just your favourite web browser.&lt;/p>
&lt;p>If you followed the instructions in this post, head over to the post &lt;a href="https://beam.apache.org/blog/apache-hop-with-dataflow/">Running
Apache Hop visual pipelines with Google Cloud
Dataflow&lt;/a> to run a
Dataflow pipeline right from your web browser!&lt;/p></description><link>/blog/hop-web-cloud/</link><pubDate>Sat, 15 Oct 2022 00:00:01 -0800</pubDate><guid>/blog/hop-web-cloud/</guid><category>blog</category></item><item><title>Apache Beam 2.41.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.41.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2410-2022-08-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.41.0, check out the &lt;a href="https://github.com/apache/beam/milestone/3?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer is now on by default for streaming, matching the behavior of batch pipelines since 2.38.0. If you encounter a bug with the optimizer, please file an issue and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Previously available in Java sdk, Python sdk now also supports logging level overrides per module. (&lt;a href="https://github.com/apache/beam/issues/18222">#18222&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer may break Dataflow upgrade compatibility for optimized pipelines when it removes unused fields. If you need to upgrade and encounter a compatibility issue, disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Support for Spark 2.4.x is deprecated and will be dropped with the release of Beam 2.44.0 or soon after (Spark runner) (&lt;a href="https://github.com/apache/beam/issues/22094">#22094&lt;/a>).&lt;/li>
&lt;li>The modules &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services">amazon-web-services&lt;/a> and
&lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/kinesis">kinesis&lt;/a> for AWS Java SDK v1 are deprecated
in favor of &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services2">amazon-web-services2&lt;/a>
and will be eventually removed after a few Beam releases (Java) (&lt;a href="https://github.com/apache/beam/issues/21249">#21249&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed a condition where retrying queries would yield an incorrect cursor in the Java SDK Firestore Connector (&lt;a href="https://github.com/apache/beam/issues/22089">#22089&lt;/a>).&lt;/li>
&lt;li>Fixed plumbing allowed lateness in Go SDK. It was ignoring the user set value earlier and always used to set to 0. (&lt;a href="https://github.com/apache/beam/issues/22474">#22474&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/3">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.41.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
akashorabek
Alexey Inkin
Alexey Romanenko
Anand Inguva
andoni-guzman
Andrew Pilloud
Andrey
Andy Ye
BalÃ¡zs NÃ©meth
Benjamin Gonzalez
BjornPrime
Brian Hulette
bulat safiullin
bullet03
Byron Ellis
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Daniel Thevessen
Danny McCormick
David Huntsperger
Dheeraj Gharde
Etienne Chauchot
Evan Galpin
Fernando Morales
Heejong Lee
Jack McCluskey
johnjcasey
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manu Zhang
Minbo Bae
Moritz Mack
Naireen Hussain
Ning Kang
Oleh Borysevych
Pablo Estrada
pablo rodriguez defino
Pranav Bhandari
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Steven Niemitz
Valentyn Tymofieiev
Vincent Marquez
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
yirutang
Yixiao Shen
Yu Feng&lt;/p></description><link>/blog/beam-2.41.0/</link><pubDate>Tue, 23 Aug 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.41.0/</guid><category>blog</category><category>release</category></item><item><title>Big Improvements in Beam Go's 2.40 Release</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The 2.40 release is one of Beam Go&amp;rsquo;s biggest yet, and we wanted to highlight
some of the biggest changes coming with this important release!&lt;/p>
&lt;h1 id="native-streaming-support">Native Streaming Support&lt;/h1>
&lt;p>2.40 marks the release of one of our most anticipated feature sets yet:
native streaming Go pipelines. This includes adding support for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#user-initiated-checkpoint">Self Checkpointing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#watermark-estimation">Watermark Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#truncating-during-drain">Pipeline Drain/Truncation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#bundle-finalization">Bundle Finalization&lt;/a> (added in 2.39)&lt;/li>
&lt;/ul>
&lt;p>With all of these features, it is now possible to write your own streaming
pipeline source DoFns in Go without relying on cross-language transforms
from Java or Python. We encourage you to try out all of these new features
in your streaming pipelines! The &lt;a href="https://beam.apache.org/documentation/programming-guide/#splittable-dofns">programming guide&lt;/a>
has additional information on getting started with native Go streaming DoFns.&lt;/p>
&lt;h1 id="generic-registration-make-your-pipelines-3x-faster">Generic Registration (Make Your Pipelines 3x Faster)&lt;/h1>
&lt;p>The release of &lt;a href="https://go.dev/blog/intro-generics">Go Generics&lt;/a> in Go 1.18
unlocked significant performance improvements for Beam Go. With generics,
we were able to add simple registration functions that can massively reduce
your pipeline&amp;rsquo;s runtime and resource consumption. For example, registering
the ParDo&amp;rsquo;s in our load tests which are designed to simulate a basic pipeline
reduced execution time from around 25 minutes to around 7 minutes on average&lt;/p>
&lt;ul>
&lt;li>an over 70% reduction!&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/go-registration.png"
alt="Beam Registration Load Tests ParDo Improvements">&lt;/p>
&lt;p>To get started with registering your own DoFns and unlocking these performance
gains, check out the &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/go/pkg/beam/register">registration doc page&lt;/a>.&lt;/p>
&lt;h1 id="whats-next">What&amp;rsquo;s Next?&lt;/h1>
&lt;p>Moving forward, we remain focused on improving the streaming experience and
leveraging generics to improve the SDK. Specific improvements we are considering
include adding &lt;a href="https://beam.apache.org/documentation/programming-guide/#state-and-timers">State &amp;amp; Timers&lt;/a>
support, introducing a Go expansion service so that Go DoFns can be used in other
languages, and wrapping more Java and Python IOs so that they can be easily used
in Go. As always, please let us know what changes you would like to see by
&lt;a href="https://github.com/apache/beam/issues/new/choose">filing an issue&lt;/a>,
&lt;a href="dev@beam.apache.org">emailing the dev list&lt;/a>, or starting a &lt;a href="https://app.slack.com/client/T4S1WH2J3/C9H0YNP3P">slack thread&lt;/a>!&lt;/p></description><link>/blog/go-2.40/</link><pubDate>Wed, 06 Jul 2022 00:00:01 -0800</pubDate><guid>/blog/go-2.40/</guid><category>blog</category><category>go</category></item><item><title>Apache Beam 2.40.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.40.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2400-2022-06-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.40.0 check out the &lt;a href="https://github.com/apache/beam/releases/tag/v2.40.0">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added &lt;a href="https://s.apache.org/inference-sklearn-pytorch">RunInference&lt;/a> API, a framework agnostic transform for inference. With this release, PyTorch and Scikit-learn are supported by the transform.
See also example at apache_beam/examples/inference/pytorch_image_classification.py&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Upgraded to Hive 3.1.3 for HCatalogIO. Users can still provide their own version of Hive. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">Issue-19554&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Go SDK users can now use generic registration functions to optimize their DoFn execution. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>)&lt;/li>
&lt;li>Go SDK users may now write self-checkpointing Splittable DoFns to read from streaming sources. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11104">BEAM-11104&lt;/a>)&lt;/li>
&lt;li>Go SDK textio Reads have been moved to Splittable DoFns exclusively. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14489">BEAM-14489&lt;/a>)&lt;/li>
&lt;li>Pipeline drain support added for Go SDK has now been tested. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>)&lt;/li>
&lt;li>Go SDK users can now see heap usage, sideinput cache stats, and active process bundle stats in Worker Status. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13829">BEAM-13829&lt;/a>)&lt;/li>
&lt;li>The serialization (pickling) library for Python is dill==0.3.1.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11167">BEAM-11167&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go Sdk now requires a minimum version of 1.18 in order to support generics (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>).&lt;/li>
&lt;li>synthetic.SourceConfig field types have changed to int64 from int for better compatibility with Flink&amp;rsquo;s use of Logical types in Schemas (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14173">BEAM-14173&lt;/a>)&lt;/li>
&lt;li>Default coder updated to compress sources used with &lt;code>BoundedSourceAsSDFWrapperFn&lt;/code> and &lt;code>UnboundedSourceAsSDFWrapper&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java expansion service to allow specific files to stage (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14160">BEAM-14160&lt;/a>).&lt;/li>
&lt;li>Fixed Elasticsearch connection when using both ssl and username/password (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14000">BEAM-14000&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python&amp;rsquo;s &lt;code>beam.FlatMap&lt;/code> will raise &lt;code>AttributeError: 'builtin_function_or_method' object has no attribute '__func__'&lt;/code> when
constructed with some
&lt;a href="https://docs.python.org/3/library/functions.html">built-ins&lt;/a>, like &lt;code>sum&lt;/code>
and &lt;code>len&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/22091">#22091&lt;/a>).&lt;/li>
&lt;li>Java&amp;rsquo;s &lt;code>BigQueryIO.Write&lt;/code> can have an exception where it attempts to output a timestamp beyond the max timestamp range
&lt;code>Cannot output with timestamp 294247-01-10T04:00:54.776Z. Output timestamps must be no earlier than the timestamp of the current input or timer (294247-01-10T04:00:54.776Z) minus the allowed skew (0 milliseconds) and no later than 294247-01-10T04:00:54.775Z. See the DoFn#getAllowedTimestampSkew() Javadoc for details on changing the allowed skew.&lt;/code>
This happens when a sink is idle, causing the idle timeout to trigger, or when a specific table is idle long enough when using dynamic destinations.
When this happens, the job is no longer able to be drained. This has been fixed for the 2.41 release.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.40.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
Aizhamal Nurmamat kyzy
Alejandro Rodriguez-Morantes
Alexander Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
Andy Ye
BalÃ¡zs NÃ©meth
Benjamin Gonzalez
Brian Hulette
bulat safiullin
bullet03
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Diego Gomez
dpcollins-google
Ekaterina Tatanova
Elias Segundo
Etienne Chauchot
Evan Galpin
fbeevikm
Fernando Morales
Heejong Lee
Igor Krasavin
Ilion Beyst
Israel Herraiz
Jack McCluskey
Jan Kuehle
Jan LukavskÃ½
johnjcasey
Jonathan Lui
jrmccluskey
Julien Tournay
Kenneth Knowles
Kerry Donny-Clark
Kevin Puthusseri
Kiley Sok
Kyle Weaver
kynx
Lucas Nogueira
Luke Cwik
LuNing Wang
Marco Robles
masahitojp
Minbo Bae
Moritz Mack
Naireen Hussain
Nancy Xu
Niel Markwick
Ning Kang
nishant jain
nishantjain91
Oskar Firlej
Pablo Estrada
pablo rodriguez defino
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Thiago Nunes
Tom Stepp
vachan-shetty
Valentyn Tymofieiev
vikash2310
Vitaly Terentyev
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yiru Tang
yixiaoshen
zwestrick&lt;/p></description><link>/blog/beam-2.40.0/</link><pubDate>Sat, 25 Jun 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.40.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Beam 2.39.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.39.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2390-2022-05-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.39.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ConfigureReleaseNote.jspa?projectId=12319527&amp;amp;version=12351170">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>JmsIO gains the ability to map any kind of input to any subclass of &lt;code>javax.jms.Message&lt;/code> (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).&lt;/li>
&lt;li>JmsIO introduces the ability to write to dynamic topics (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).
&lt;ul>
&lt;li>A &lt;code>topicNameMapper&lt;/code> must be set to extract the topic name from the input value.&lt;/li>
&lt;li>A &lt;code>valueMapper&lt;/code> must be set to convert the input value to JMS message.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reduce number of threads spawned by BigqueryIO StreamingInserts (
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14283">BEAM-14283&lt;/a>).&lt;/li>
&lt;li>Implemented Apache PulsarIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8218">BEAM-8218&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for flink scala 2.12, because most of the libraries support version 2.12 onwards. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14386">beam-14386&lt;/a>)&lt;/li>
&lt;li>&amp;lsquo;Manage Clusters&amp;rsquo; JupyterLab extension added for users to configure usage of Dataproc clusters managed by Interactive Beam (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>Pipeline drain support added for Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>). &lt;strong>Note: this feature is not yet fully validated and should be treated as experimental in this release.&lt;/strong>&lt;/li>
&lt;li>&lt;code>DataFrame.unstack()&lt;/code>, &lt;code>DataFrame.pivot() &lt;/code> and &lt;code>Series.unstack()&lt;/code>
implemented for DataFrame API (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13948">BEAM-13948&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13966">BEAM-13966&lt;/a>).&lt;/li>
&lt;li>Support for impersonation credentials added to dataflow runner in the Java and Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14014">BEAM-14014&lt;/a>).&lt;/li>
&lt;li>Implemented Jupyterlab extension for managing Dataproc clusters (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>ExternalPythonTransform API added for easily invoking Python transforms from
Java (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14143">BEAM-14143&lt;/a>).&lt;/li>
&lt;li>Added Add support for Elasticsearch 8.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14003">BEAM-14003&lt;/a>).&lt;/li>
&lt;li>Shard aware Kinesis record aggregation (AWS Sdk v2), (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14104">BEAM-14104&lt;/a>).&lt;/li>
&lt;li>Upgrade to ZetaSQL 2022.04.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14348">BEAM-14348&lt;/a>).&lt;/li>
&lt;li>Fixed ReadFromBigQuery cannot be used with the interactive runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14112">BEAM-14112&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Unused functions &lt;code>ShallowCloneParDoPayload()&lt;/code>, &lt;code>ShallowCloneSideInput()&lt;/code>, and &lt;code>ShallowCloneFunctionSpec()&lt;/code> have been removed from the Go SDK&amp;rsquo;s pipelinex package (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13739">BEAM-13739&lt;/a>).&lt;/li>
&lt;li>JmsIO requires an explicit &lt;code>valueMapper&lt;/code> to be set (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>). You can use the &lt;code>TextMessageMapper&lt;/code> to convert &lt;code>String&lt;/code> inputs to JMS &lt;code>TestMessage&lt;/code>s:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="n">JmsIO&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">withConnectionFactory&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">jmsConnectionFactory&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">withValueMapper&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">TextMessageMapper&lt;/span>&lt;span class="o">());&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Coders in Python are expected to inherit from Coder. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14351">BEAM-14351&lt;/a>).&lt;/li>
&lt;li>New abstract method &lt;code>metadata()&lt;/code> added to io.filesystem.FileSystem in the
Python SDK. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14314">BEAM-14314&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Flink 1.11 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14139">BEAM-14139&lt;/a>).&lt;/li>
&lt;li>Python 3.6 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13657">BEAM-13657&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java Spanner IO NPE when ProjectID not specified in template executions (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14405">BEAM-14405&lt;/a>).&lt;/li>
&lt;li>Fixed potential NPE in BigQueryServicesImpl.getErrorInfo (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14133">BEAM-14133&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/browse/BEAM-14412?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.39.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.39.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alexander Zhuravlev,
Alexey Romanenko,
Anand Inguva,
Andrei Gurau,
Andrew Pilloud,
Andy Ye,
Arun Pandian,
Arwin Tio,
Aydar Farrakhov,
Aydar Zainutdinov,
AydarZaynutdinov,
BalÃ¡zs NÃ©meth,
Benjamin Gonzalez,
Brian Hulette,
Buqian Zheng,
Chamikara Jayalath,
Chun Yang,
Daniel Oliveira,
Daniela MartÃ­n,
Danny McCormick,
David Huntsperger,
Deepak Nagaraj,
Denise Case,
Esun Kim,
Etienne Chauchot,
Evan Galpin,
Hector Miuler Malpica Gallegos,
Heejong Lee,
Hengfeng Li,
Ilango Rajagopal,
Ilion Beyst,
Israel Herraiz,
Jack McCluskey,
Kamil Bregula,
Kamil BreguÅ‚a,
Ke Wu,
Kenneth Knowles,
KevinGG,
Kiley,
Kiley Sok,
Kyle Weaver,
Liam Miller-Cushon,
Luke Cwik,
Marco Robles,
Matt Casters,
Michael Li,
MiguelAnzoWizeline,
Milan Patel,
Minbo Bae,
Moritz Mack,
Nick Caballero,
Niel Markwick,
Ning Kang,
Oskar Firlej,
Pablo Estrada,
Pavel Avilov,
Reuven Lax,
Reza Rokni,
Ritesh Ghorse,
Robert Bradshaw,
Robert Burke,
Ryan Thompson,
Sam Whittle,
Steven Niemitz,
Thiago Nunes,
Tomo Suzuki,
Valentyn Tymofieiev,
Victor,
Yi Hu,
Yichi Zhang,
Yiru Tang,
ahmedabu98,
andoni-guzman,
brachipa,
bulat safiullin,
bullet03,
dannymartinm,
daria.malkova,
dpcollins-google,
egalpin,
emily,
fbeevikm,
johnjcasey,
kileys,
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>,
nguyennk92,
pablo rodriguez defino,
rszper,
rvballada,
sachinag,
tvalentyn,
vachan-shetty,
yirutang&lt;/p></description><link>/blog/beam-2.39.0/</link><pubDate>Wed, 25 May 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.39.0/</guid><category>blog</category><category>release</category></item><item><title>Running Beam SQL in notebooks</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>&lt;a href="https://beam.apache.org/documentation/dsls/sql/overview/">Beam SQL&lt;/a> allows a
Beam user to query PCollections with SQL statements.
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/runners/interactive#interactive-beam">Interactive Beam&lt;/a>
provides an integration between Apache Beam and
&lt;a href="https://docs.jupyter.org/en/latest/">Jupyter Notebooks&lt;/a> (formerly known as
IPython Notebooks) to make pipeline prototyping and data exploration much faster
and easier.
You can set up your own notebook user interface (for example,
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">JupyterLab&lt;/a>
or classic &lt;a href="https://docs.jupyter.org/en/latest/install.html">Jupyter Notebooks&lt;/a>)
on your own device following their documentations. Alternatively, you can
choose a hosted solution that does everything for you. You are free to select
whichever notebook user interface you prefer. For simplicity, this
post does not go through the notebook environment setup and uses
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development">Apache Beam Notebooks&lt;/a>
that provides a cloud-hosted
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab&lt;/a> environment and lets
a Beam user iteratively develop pipelines, inspect pipeline graphs, and parse
individual PCollections in a read-eval-print-loop (REPL) workflow.&lt;/p>
&lt;p>In this post, you will see how to use &lt;code>beam_sql&lt;/code>, a notebook
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">magic&lt;/a>, to
execute Beam SQL in notebooks and inspect the results.&lt;/p>
&lt;p>By the end of the post, it also demonstrates how to use the &lt;code>beam_sql&lt;/code> magic
with a production environment, such as running it as a one-shot job on
Dataflow. It&amp;rsquo;s optional. To follow those steps, you should have a project in
Google Cloud Platform with
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#before_you_begin">necessary APIs enabled&lt;/a>
, and you should have enough permissions to create a Google Cloud Storage bucket
(or to use an existing one), query a public Google Cloud BigQuery dataset, and
run Dataflow jobs.&lt;/p>
&lt;p>If you choose to use the cloud hosted notebook solution, once you have your
Google Cloud project ready, you will need to create an Apache Beam Notebooks
instance and open the JupyterLab web interface. Please follow the instructions
given at:
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance">https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance&lt;/a>&lt;/p>
&lt;h2 id="getting-familiar-with-the-environment">Getting familiar with the environment&lt;/h2>
&lt;h3 id="landing-page">Landing page&lt;/h3>
&lt;p>After starting your own notebook user interface: for example, if using Apche
Beam Notebooks, after clicking the &lt;code>OPEN JUPYTERLAB&lt;/code> link, you will land on
the default launcher page of the notebook environment.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image1.png"
alt="Beam SQL in Notebooks: landing page">&lt;/p>
&lt;p>On the left side, there is a file explorer to view examples, tutorials and
assets on the notebook instance. To easily navigate the files, you may
double-click the &lt;code>00-Start_Here.md&lt;/code> (#1 in the screenshot) file to view detailed
information about the files.&lt;/p>
&lt;p>On the right side, it displays the default launcher page of JupyterLab. To
create and open a completely new notebook file and code with a selected version
of Apache Beam, click one of (#2) the items with Apache Beam &amp;gt;=2.34.0 (because
&lt;code>beam_sql&lt;/code> was introduced in 2.34.0) installed.&lt;/p>
&lt;h3 id="createopen-a-notebook">Create/open a notebook&lt;/h3>
&lt;p>For example, if you clicked the image button with Apache Beam 2.36.0, you would
see an &lt;code>Untitled.ipynb&lt;/code> file created and opened.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image2.png"
alt="Beam SQL in Notebooks: create/open a notebook ">&lt;/p>
&lt;p>In the file explorer, your new notebook file has been created as
&lt;code>Untitled.ipynb&lt;/code>.&lt;/p>
&lt;p>On the right side, in the opened notebook, there are 4 buttons on top that you
may interact most frequently with:&lt;/p>
&lt;ul>
&lt;li>#1: insert an empty code block after the selected / highlighted code block&lt;/li>
&lt;li>#2: execute the code in the block that is selected / highlighted&lt;/li>
&lt;li>#3: interrupt code execution if your code execution is stuck&lt;/li>
&lt;li>#4: â€œRestart the kernelâ€: clear all states from code executions and start
from fresh&lt;/li>
&lt;/ul>
&lt;p>There is a button on the top-right (#5) for you to choose a different Apache
Beam version if needed, so itâ€™s not set in stone.&lt;/p>
&lt;p>You can always double-click a file from the file explorer to open it without
creating a new one.&lt;/p>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;h3 id="beam_sql-magic">&lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>&lt;code>beam_sql&lt;/code> is an IPython
&lt;a href="https://ipython.readthedocs.io/en/stable/config/custommagics.html">custom magic&lt;/a>.
If you&amp;rsquo;re not familiar with magics, here are some
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">built-in examples&lt;/a>.
It&amp;rsquo;s a convenient way to validate your queries locally against known/test data
sources when prototyping a Beam pipeline with SQL, before productionizing it on
remote cluster/services.&lt;/p>
&lt;p>The Apache Beam Notebooks environment has preloaded the &lt;code>beam_sql&lt;/code> magic and
basic &lt;code>apache-beam&lt;/code> modules so you can directly use them without additional
imports. You can also explicitly load the magic via
&lt;code>%load_ext apache_beam.runners.interactive.sql.beam_sql_magics&lt;/code> and
&lt;code>apache-beam&lt;/code> modules if you set up your own notebook elsewhere.&lt;/p>
&lt;p>You can type:&lt;/p>
&lt;pre>&lt;code>%beam_sql -h
&lt;/code>&lt;/pre>&lt;p>and then execute the code to learn how to use the magic:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image3.png"
alt="Beam SQL in Notebooks: beam_sql magic help message ">&lt;/p>
&lt;p>The selected/highlighted block is called a notebook cell. It mainly has 3
components:&lt;/p>
&lt;ul>
&lt;li>#1: The execution count. &lt;code>[1]&lt;/code> indicates this block is the first executed
code. It increases by 1 for each piece of code you execute even if you
re-execute the same piece of code. &lt;code>[ ]&lt;/code> indicates this block is not
executed.&lt;/li>
&lt;li>#2: The cell input: the code gets executed.&lt;/li>
&lt;li>#3: The cell output: the output of the code execution. Here it contains the
help documentation of the &lt;code>beam_sql&lt;/code> magic.&lt;/li>
&lt;/ul>
&lt;h3 id="create-a-pcollection">Create a PCollection&lt;/h3>
&lt;p>There are 3 scenarios for Beam SQL when creating a PCollection:&lt;/p>
&lt;ol>
&lt;li>Use Beam SQL to create a PCollection from constant values&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>%%beam_sql -o pcoll
SELECT CAST(1 AS INT) AS id, CAST('foo' AS VARCHAR) AS str, CAST(3.14 AS DOUBLE) AS flt
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image4.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from raw values.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic creates and outputs a PCollection named &lt;code>pcoll&lt;/code> with
element_type like &lt;code>BeamSchema_...(id: int32, str: str, flt: float64)&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> that you have &lt;strong>not&lt;/strong> explicitly created a Beam pipeline. You get a
PCollection because the &lt;code>beam_sql&lt;/code> magic always implicitly creates a pipeline to
execute your SQL query. To hold the elements with each field&amp;rsquo;s type info, Beam
automatically creates a
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>
as the &lt;code>element_type&lt;/code> for the created PCollection. You will learn more about
schema-aware PCollections later.&lt;/p>
&lt;ol start="2">
&lt;li>Use Beam SQL to query a PCollection&lt;/li>
&lt;/ol>
&lt;p>You can chain another SQL using the output from a previous SQL (or any
schema-aware PCollection produced by any normal Beam PTransforms) as the input
to produce a new PCollection.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: if you name the output PCollection, make sure that itâ€™s unique in your
notebook to avoid overwriting a different PCollection.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o id_pcoll
SELECT id FROM pcoll
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image5.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from another.">&lt;/p>
&lt;ol start="3">
&lt;li>Use Beam SQL to join multiple PCollections&lt;/li>
&lt;/ol>
&lt;p>You can query multiple PCollections from a single query.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o str_with_same_id
SELECT id, str FROM pcoll JOIN id_pcoll USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image6.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from multiple PCollections.">&lt;/p>
&lt;p>Now you have learned how to use the &lt;code>beam_sql&lt;/code> magic to create PCollections and
inspect their results.&lt;/p>
&lt;p>&lt;strong>Tip&lt;/strong>: if you accidentally delete some of the notebook cell outputs, you can
always check the content of a PCollection by invoking &lt;code>ib.show(pcoll_name)&lt;/code> or
&lt;code>ib.collect(pcoll_name)&lt;/code> where &lt;code>ib&lt;/code> stands for â€œInteractive Beamâ€
(&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#reading_and_visualizing_the_data">learn more&lt;/a>).&lt;/p>
&lt;h3 id="schema-aware-pcollections">Schema-aware PCollections&lt;/h3>
&lt;p>The &lt;code>beam_sql&lt;/code> magic provides the flexibility to seamlessly mix SQL and non-SQL
Beam statements to build pipelines and even run them on Dataflow. However, each
PCollection queried by Beam SQL needs to have a
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>.
For the &lt;code>beam_sql&lt;/code> magic, itâ€™s recommended to use &lt;code>typing.NamedTuple&lt;/code> when a
schema is desired. You can go through the below example to learn more details
about schema-aware PCollections.&lt;/p>
&lt;h4 id="setup">Setup&lt;/h4>
&lt;p>In the setup of this example, you will:&lt;/p>
&lt;ul>
&lt;li>Install PyPI package &lt;code>names&lt;/code> using the built-in &lt;code>%pip&lt;/code> magic: you will use
the module to generate some random English names as the raw data input.&lt;/li>
&lt;li>Define a schema with &lt;code>NamedTuple&lt;/code> that has 2 attributes: &lt;code>id&lt;/code> - an unique
numeric identifier of a person; &lt;code>name&lt;/code> - a string name of a person.&lt;/li>
&lt;li>Define a pipeline with an &lt;code>InteractiveRunner&lt;/code> to utilize notebook related
features of Apache Beam.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="o">%&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">names&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">names&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="nb">id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;span class="n">name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is no visible output for the code execution.&lt;/p>
&lt;h4 id="create-schema-aware-pcollections-without-using-sql">Create schema-aware PCollections without using SQL&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">persons&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image7.png"
alt="Beam SQL in Notebooks: create a schema-aware PCollection without SQL.">&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">persons_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">15&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons_2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image8.png"
alt="Beam SQL in Notebooks: create another schema-aware PCollection without SQL.">&lt;/p>
&lt;p>Now you have 2 PCollections both with the same schema defined by the &lt;code>Person&lt;/code>
class:&lt;/p>
&lt;ul>
&lt;li>&lt;code>persons&lt;/code> contains 10 records for 10 persons with ids ranging from 0 to 9,&lt;/li>
&lt;li>&lt;code>persons_2&lt;/code> contains another 10 records for 10 persons with ids ranging from
5 to 14.&lt;/li>
&lt;/ul>
&lt;h4 id="encode-and-decode-of-schema-aware-pcollections">Encode and Decode of schema-aware PCollections&lt;/h4>
&lt;p>For this example, you still need one more piece of data from the first &lt;code>pcoll&lt;/code>
that you have created with instructions in this post.&lt;/p>
&lt;p>You can use the original &lt;code>pcoll&lt;/code>. Optionally, if you want to exercise using
coders explicitly with schema-aware PCollections, you can add a Text I/O into
the mix: write the content of &lt;code>pcoll&lt;/code> into a text file retaining its schema
information, then read the file back into a new schema-aware PCollection called
&lt;code>pcoll_in_file&lt;/code>, and use the new PCollection to join &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>
to find names with the common id in all three of them.&lt;/p>
&lt;p>To encode &lt;code>pcoll&lt;/code> into a file, execute:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pcoll&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;/tmp/pcoll&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pipeline&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait_until_finish&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="err">!&lt;/span>&lt;span class="n">cat&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">tmp&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">*&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image9.png"
alt="Beam SQL in Notebooks: write a schema-aware PCollection into a text file.">&lt;/p>
&lt;p>The above code execution writes the PCollection &lt;code>pcoll&lt;/code> (basically
&lt;code>{id: 1, str: foo, flt: 3.14}&lt;/code>) into a text file using the coder assigned by
Beam. As you can see, the file content is recorded in a binary non
human-readable format, and thatâ€™s normal.&lt;/p>
&lt;p>To decode the file content into a new PCollection, execute:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">pcoll_in_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s1">&amp;#39;/tmp/pcoll*&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll_in_file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image10.png"
alt="Beam SQL in Notebooks: read a schema-aware PCollection from a text file.">&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> you have to use the same coder during encoding and decoding, and
furthermore you may assign the schema explicitly to the new PCollection through
&lt;code>with_output_types()&lt;/code>.&lt;/p>
&lt;p>Reading out the encoded binary content from the text file and decoding it with
the correct coder, the content of &lt;code>pcoll&lt;/code> is recovered into &lt;code>pcoll_in_file&lt;/code>. You
can use this technique to save and share your data through any Beam I/O (not
necessarily a text file) with collaborators who work on their own pipelines (not
just in your notebook session or pipelines).&lt;/p>
&lt;h4 id="schema-in-beam_sql-magic">Schema in &lt;code>beam_sql&lt;/code> magic&lt;/h4>
&lt;p>The &lt;code>beam_sql&lt;/code> magic automatically registers a &lt;code>RowCoder&lt;/code> for your &lt;code>NamedTuple&lt;/code>
schema so that you only need to focus on preparing your data for query without
worrying about coders. To see more verbose details of what the &lt;code>beam_sql&lt;/code> magic
does behind the scenes, you can use the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>For example, you can look for all elements with &lt;code>id &amp;lt; 5&lt;/code> in &lt;code>persons&lt;/code> with the
below query and assign the output to &lt;code>persons_id_lt_5&lt;/code>.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o persons_id_lt_5 -v
SELECT * FROM persons WHERE id &amp;lt; 5
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image11.png"
alt="Beam SQL in Notebooks: beam_sql registers a schema for a PCollection.">&lt;/p>
&lt;p>Since this is the first time running this query, you might see a warning message
about:&lt;/p>
&lt;blockquote>
&lt;p>Schema Person has not been registered to use a RowCoder. Automatically
registering it by running:
beam.coders.registry.register_coder(Person, beam.coders.RowCoder)&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>beam_sql&lt;/code> magic helps registering a &lt;code>RowCoder&lt;/code> for each schema you define
and use whenever it finds one. You can also explicitly run the same code to do
so.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> the output element type is &lt;code>Person(id: int, name: str)&lt;/code> instead of
&lt;code>BeamSchema_â€¦&lt;/code> because you have selected all the fields from a single
PCollection of the known type &lt;code>Person(id: int, name: str)&lt;/code>.&lt;/p>
&lt;p>Another example, you can query for all names from &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code> with
the same ids and assign the output to &lt;code>persons_with_common_id&lt;/code>:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o persons_with_common_id -v
SELECT * FROM persons JOIN persons_2 USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image12.png"
alt="Beam SQL in Notebooks: beam_sql creates a schema for a query.">&lt;/p>
&lt;p>Note the output element type is now some
&lt;code>BeamSchema_...(id: int64, name: str, name0: str)&lt;/code>. Because you have selected
columns from both PCollections, there is no known schema to hold the result.
Beam automatically creates a schema and differentiates the conflicted field
&lt;code>name&lt;/code> by suffixing 0 to one of them.&lt;/p>
&lt;p>And since &lt;code>Person&lt;/code> is already previously registered with a &lt;code>RowCoder&lt;/code>, there is
no more warning about registering it even with the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>Additionally, you can do a join with &lt;code>pcoll_in_file&lt;/code>, &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o entry_with_common_id
SELECT pcoll_in_file.id, persons.name AS name_1, persons_2.name AS name_2
FROM pcoll_in_file JOIN persons ON pcoll_in_file.id = persons.id
JOIN persons_2 ON pcoll_in_file.id = persons_2.id
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image13.png"
alt="Beam SQL in Notebooks: rename fields in a query.">&lt;/p>
&lt;p>The schema generated reflects the column renaming you have done in the SQL.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>You will go through an example to find out the US state with the most COVID
positive cases on a specific day with data provided by the
&lt;a href="https://covidtracking.com/">covid tracking project&lt;/a>.&lt;/p>
&lt;h3 id="get-the-data">Get the data&lt;/h3>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">requests&lt;/span>
&lt;span class="c1"># The covidtracking project has stopped collecting new data, current data ends on 2021-03-07&lt;/span>
&lt;span class="n">json_current&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;https://covidtracking.com/api/v1/states/current.json&amp;#39;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">with&lt;/span> &lt;span class="n">requests&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">loads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">data&lt;/span>
&lt;span class="n">current_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">json_current&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">current_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image14.png"
alt="Beam SQL in Notebooks: preview example data.">&lt;/p>
&lt;p>The data is dated as 2021-03-07. It contains many details about COVID cases for
different states in the US. &lt;code>current_data[0]&lt;/code> is just one of the data points.&lt;/p>
&lt;p>You can get rid of most of the columns of the data. For example, just focus on
â€œdateâ€, â€œstateâ€, â€œpositiveâ€ and â€œnegativeâ€, and then define a schema
&lt;code>UsCovidData&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="c1"># Remember to str(e[&amp;#39;date&amp;#39;]).&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">positive&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;span class="n">negative&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>date&lt;/code> is a keyword in (Calcite)SQL, use a different field name such as
&lt;code>partition_date&lt;/code>;&lt;/li>
&lt;li>&lt;code>date&lt;/code> from the data is an &lt;code>int&lt;/code> type, not &lt;code>str&lt;/code>. Make sure you convert the
data using &lt;code>str()&lt;/code> or use &lt;code>date: int&lt;/code>.&lt;/li>
&lt;li>&lt;code>negative&lt;/code> has missing values and the default is &lt;code>None&lt;/code>. So instead of
&lt;code>negative: int&lt;/code>, it should be &lt;code>negative: Optional[int]&lt;/code>. Or you can convert
&lt;code>None&lt;/code> into 0 when using the schema.&lt;/li>
&lt;/ul>
&lt;p>Then parse the json data into a PCollection with the schema:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">p_sql&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_sql&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Create PCollection from json&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">positive&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;positive&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">negative&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;negative&amp;#39;&lt;/span>&lt;span class="p">]))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">covid_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image15.png"
alt="Beam SQL in Notebooks: parse example data with a schema.">&lt;/p>
&lt;h3 id="query">Query&lt;/h3>
&lt;p>You can now find the biggest positive on the â€œcurrent dayâ€ (2021-03-07).&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o max_positive
SELECT partition_date, MAX(positive) AS positive
FROM covid_data
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image16.png"
alt="Beam SQL in Notebooks: find the biggest positive from the data.">&lt;/p>
&lt;p>However, this is just the positive number. You cannot observe the state that has
this maximum number nor the negative case number for the state.&lt;/p>
&lt;p>To enrich your result, you have to join this data back to the original data set
you have parsed.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o entry_with_max_positive
SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS negative
FROM covid_data JOIN max_positive
USING (partition_date, positive)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image17.png"
alt="Beam SQL in Notebooks: enriched data with biggest positive.">&lt;/p>
&lt;p>Now you can see all columns of the data with the maximum positive case on
2021-03-07.
&lt;strong>Note&lt;/strong>: to handle missing values of the negative column in the original data,
you can use &lt;code>{fn IFNULL(covid_data.negative, 0)}&lt;/code> to set null values to 0.&lt;/p>
&lt;p>When you&amp;rsquo;re ready to scale up, you can translate the SQLs into a pipeline with
&lt;code>SqlTransform&lt;/code>s and run your pipeline on a distributed runner like Flink or
Spark. This post demonstrates it by launching a one-shot job on Dataflow from
the notebook with the help of &lt;code>beam_sql&lt;/code> magic.&lt;/p>
&lt;h3 id="run-on-dataflow">Run on Dataflow&lt;/h3>
&lt;p>Now that you have a pipeline that parses US COVID data from json to find
positive/negative/state information for the state with the most positive cases
on each day, you can try applying it to all historical daily data and running it
on Dataflow.&lt;/p>
&lt;p>The new data source you will use is a public dataset from USAFacts US
Coronavirus Database that contains all historical daily summary of COVID cases
in the US.&lt;/p>
&lt;p>The schema of data is very similar to what the covid tracking project website
provides. The fields you will query are: &lt;code>date&lt;/code>, &lt;code>state&lt;/code>, &lt;code>confirmed_cases&lt;/code>, and
&lt;code>deaths&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image18.png"
alt="Beam SQL in Notebooks: schema of cloud data.">&lt;/p>
&lt;p>A preview of the data looks like below (you may skip the inspection in BigQuery
and just take a look at the screenshot):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image19.png"
alt="Beam SQL in Notebooks: preview of cloud data.">&lt;/p>
&lt;p>The format of the data is &lt;strong>slightly different&lt;/strong> from the json data you parsed
in the previous pipeline because the numbers are grouped by counties instead of
states, thus some additional aggregations need to be done in the SQLs.&lt;/p>
&lt;p>If you need a fresh execution, you may click the â€œRestart the kernelâ€ button on
the top menu.&lt;/p>
&lt;p>Full code is as below, on-top of the original pipeline and queries:&lt;/p>
&lt;ul>
&lt;li>It changes the source from a single-day data to a more complete historical
data;&lt;/li>
&lt;li>It changes the I/O and schema to accommodate the new dataset;&lt;/li>
&lt;li>It changes the SQLs to include more aggregations to accommodate the new
format of the dataset.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prepare the data with schema&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;span class="c1"># Public BQ dataset.&lt;/span>
&lt;span class="n">table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;bigquery-public-data:covid19_usafacts.summary&amp;#39;&lt;/span>
&lt;span class="c1"># Replace with your project.&lt;/span>
&lt;span class="n">project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;YOUR-PROJECT-NAME-HERE&amp;#39;&lt;/span>
&lt;span class="c1"># Replace with your GCS bucket.&lt;/span>
&lt;span class="n">gcs_location&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;gs://YOUR_GCS_BUCKET_HERE&amp;#39;&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">deaths&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">p_on_dataflow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_on_dataflow&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Read dataset&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">project&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gcs_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">deaths&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">])))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Run on Dataflow&lt;/strong>&lt;/p>
&lt;p>To run SQL on Dataflow is very simple, you just need to add the option
&lt;code>-r DataflowRunner&lt;/code>.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o data_by_state -r DataflowRunner
SELECT partition_date, state, SUM(confirmed_cases) as confirmed_cases, SUM(deaths) as deaths
FROM covid_data
GROUP BY partition_date, state
&lt;/code>&lt;/pre>&lt;p>Different from previous &lt;code>beam_sql&lt;/code> magic executions, you wonâ€™t see the result
immediately. Instead, a form like below is printed in the notebook cell output:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image20.png"
alt="Beam SQL in Notebooks: empty run-on-dataflow form.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic tries its best to guess your project id and preferred cloud
region. You still have to input additional information necessary to submit a
Dataflow job, such as a GCS bucket to stage the Dataflow job and any additional
Python dependencies the job needs.&lt;/p>
&lt;p>For now, ignore the form in the cell output, because you still need 2 more SQLs
to: 1) find the maximum confirmed cases on each day; 2) join the maximum case
data with the full data_by_state. The &lt;code>beam_sql&lt;/code> magic allows you to chain SQLs,
so chain 2 more by executing:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o max_cases -r DataflowRunner
SELECT partition_date, MAX(confirmed_cases) as confirmed_cases
FROM data_by_state
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>And&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o data_with_max_cases -r DataflowRunner
SELECT data_by_state.partition_date, data_by_state.state, data_by_state.confirmed_cases, data_by_state.deaths
FROM data_by_state JOIN max_cases
USING (partition_date, confirmed_cases)
&lt;/code>&lt;/pre>&lt;p>By default, when running &lt;code>beam_sql&lt;/code> on Dataflow, the output PCollection will be
written to a text file on GCS. The â€œwriteâ€ is automatically provided by
&lt;code>beam_sql&lt;/code> and mainly for your inspection of the output data for this one-shot
Dataflow job. Itâ€™s lightweight and does not encode elements for further
development. To save the output and share it with others, you can add more Beam
I/Os into the mix.&lt;/p>
&lt;p>For example, you can appropriately encode elements into text files using the
technique described in the above schema-aware PCollections example.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">GoogleCloudOptions&lt;/span>
&lt;span class="n">coder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">max_data_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcs_location&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s1">&amp;#39;/encoded_max_data&amp;#39;&lt;/span>
&lt;span class="n">data_with_max_cases&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_data_file&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, you can create a new BQ dataset in your own project to store the
processed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image21.png"
alt="Beam SQL in Notebooks: create a new BQ dataset.">&lt;/p>
&lt;p>You have to select the same data location as the public BigQuery data you are
reading. In this case, â€œus (multiple regions in United States)â€.&lt;/p>
&lt;p>Once you finish creating an empty dataset, you can execute below:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">output_table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s1">&amp;#39;{project}:covid_data.max_analysis&amp;#39;&lt;/span>
&lt;span class="n">bq_schema&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s1">&amp;#39;fields&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">}]}&lt;/span>
&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;To json-like&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">partition_date&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deaths&lt;/span>&lt;span class="p">})&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">output_table&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">schema&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bq_schema&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;STREAMING_INSERTS&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">custom_gcs_temp_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now back in the form of the last SQL cell output, you may fill in necessary
information to run the pipeline on Dataflow. An example input looks like below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image22.png"
alt="Beam SQL in Notebooks: fill in the run-on-Dataflow form.">&lt;/p>
&lt;p>Because this pipeline doesnâ€™t use any additional Python dependency, â€œAdditional
Packagesâ€ is left empty. In the previous example where you have installed a
package called &lt;code>names&lt;/code>, to run that pipeline on Dataflow, you have to put
&lt;code>names&lt;/code> in this field.&lt;/p>
&lt;p>Once you finish updating your inputs, you can click the &lt;code>Show Options&lt;/code> button to
view what pipeline options have been configured based on your inputs. A variable
&lt;code>options_[YOUR_OUTPUT_PCOLL_NAME]&lt;/code> is generated, and you can supply more
pipeline options to it if the form is not enough for your execution.&lt;/p>
&lt;p>Once you are ready to submit the Dataflow job, click the &lt;code>Run on Dataflow&lt;/code>
button. It tells you where the default output would be written, and after a
while, a line with:&lt;/p>
&lt;blockquote>
&lt;p>Click here for the details of your Dataflow job.&lt;/p>
&lt;/blockquote>
&lt;p>would be displayed. You can click on the hyperlink to go to your Dataflow job
page. (Optionally, you can ignore the form and continue development to extend
your pipeline. Once you are satisfied with the state of your pipeline, you can
come back to the form and submit the job to Dataflow.)&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image23.png"
alt="Beam SQL in Notebooks: a Dataflow job graph.">&lt;/p>
&lt;p>As you can see, each transform name of the generated Dataflow job is prefixed
with a string &lt;code>[number]: &lt;/code>. This is to distinguish re-executed codes in
notebooks because Beam requires each transform to have a distinct name. Under
the hood, the &lt;code>beam_sql&lt;/code> magic also stages your schema information to Dataflow,
so you might see transforms named as &lt;code>schema_loaded_beam_sql_â€¦&lt;/code>. This is because
the &lt;code>NamedTuple&lt;/code> defined in the notebook is likely in the &lt;code>__main__&lt;/code> scope and
Dataflow is not aware of them at all. To minimize user intervention and avoid
pickling the whole main session (and itâ€™s infeasible to pickle the main session
when it contains unpickle-able attributes), the &lt;code>beam_sql&lt;/code> magic optimizes the
staging process by serializing your schemas, staging them to Dataflow, and then
deserialize/load them for job execution.&lt;/p>
&lt;p>Once the job succeeds, the result of the output PCollection would be written to
places instructed by your I/O transforms. &lt;strong>Note&lt;/strong>: running &lt;code>beam_sql&lt;/code> on
Dataflow generates a one-shot job and itâ€™s not interactive.&lt;/p>
&lt;p>A simple inspection of the data from the default output location:&lt;/p>
&lt;pre>&lt;code>!gsutil cat 'gs://ningk-so-test/bq/staging/data_with_max_cases*'
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image24.png"
alt="Beam SQL in Notebooks: inspect the default output file.">&lt;/p>
&lt;p>The text file with encoded binary data written by your &lt;code>WriteToText&lt;/code>:&lt;/p>
&lt;pre>&lt;code>!gsutil cat 'gs://ningk-so-test/bq/encoded_max_data*'
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image25.png"
alt="Beam SQL in Notebooks: inspect the user-defined output file.">&lt;/p>
&lt;p>The table &lt;code>YOUR-PROJECT:covid_data.max_analysis&lt;/code> created by your
&lt;code>WriteToBigQuery&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image26.png"
alt="Beam SQL in Notebooks: inspect the output BQ dataset.">&lt;/p>
&lt;h3 id="run-on-other-oss-runners-directly-with-the-beam_sql-magic">Run on other OSS runners directly with the &lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>On the day this blog is posted, the &lt;code>beam_sql&lt;/code> magic only supports DirectRunner
(interactive) and DataflowRunner (one-shot). It&amp;rsquo;s a simple wrapper on top of
the &lt;code>SqlTransform&lt;/code> with interactive input widgets implemented by
&lt;a href="https://ipywidgets.readthedocs.io/en/stable/">ipywidgets&lt;/a>. You can implement
your own runner support or utilities by following the
&lt;a href="https://lists.apache.org/thread/psrx1xhbyjcqbhxx6trf5nvh66c6pk3y">instructions&lt;/a>.&lt;/p>
&lt;p>Additionally, support for other OSS runners are WIP, for example,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14373">support using FlinkRunner with the &lt;code>beam_sql&lt;/code> magic&lt;/a>.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>The &lt;code>beam_sql&lt;/code> magic and Apache Beam Notebooks combined is a convenient tool for
you to learn Beam SQL and mix Beam SQL into prototyping and productionizing (
e.g., to Dataflow) your Beam pipelines with minimum setups.&lt;/p>
&lt;p>For more details about the Beam SQL syntax, check out the Beam Calcite SQL
&lt;a href="https://beam.apache.org/documentation/dsls/sql/calcite/overview/">compatibility&lt;/a>
and the Apache Calcite SQL
&lt;a href="https://calcite.apache.org/docs/reference.html">syntax&lt;/a>.&lt;/p></description><link>/blog/beam-sql-with-notebooks/</link><pubDate>Thu, 28 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/beam-sql-with-notebooks/</guid><category>blog</category></item><item><title>Running Apache Hop visual pipelines with Google Cloud Dataflow</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Apache Hop (&lt;a href="https://hop.apache.org/">https://hop.apache.org/&lt;/a>) is a visual development environment for creating data pipelines using Apache Beam. You can run your Hop pipelines in Spark, Flink or Google Cloud Dataflow.&lt;/p>
&lt;p>In this post, we will see how to install Hop, and we will run a sample pipeline in the cloud with Dataflow. To follow the steps given in this post, you should have a project in Google Cloud Platform, and you should have enough permissions to create a Google Cloud Storage bucket (or to use an existing one), as well as to run Dataflow jobs.&lt;/p>
&lt;p>Once you have your Google Cloud project ready, you will need to &lt;a href="https://cloud.google.com/sdk/docs/install">install the Google Cloud SDK&lt;/a> to trigger the Dataflow pipeline.&lt;/p>
&lt;p>Also, don&amp;rsquo;t forget to configure the Google Cloud SDK to use your account and your project.&lt;/p>
&lt;h2 id="setup-and-local-execution">Setup and local execution&lt;/h2>
&lt;p>You can run Apache Hop as a local application, or use &lt;a href="https://hop.incubator.apache.org/manual/latest/hop-gui/hop-web.html">the Hop web version&lt;/a> from a Docker container. The instructions given in this post will work for the local application, as the authentication for Cloud Dataflow would be different if Hop is running in a container. All the rest of the instructions remain valid. The UI of Hop is exactly the same either running as a local app or in the web version.&lt;/p>
&lt;p>Now it&amp;rsquo;s time to download and install Apache Hop, following these &lt;a href="https://hop.apache.org/manual/latest/getting-started/hop-download-install.html">instructions&lt;/a>.&lt;/p>
&lt;p>For this post, I have used the binaries in the apache-hop-client package, version 1.2.0, released on March 7th, 2022.&lt;/p>
&lt;p>After installing Hop, we are ready to start.&lt;/p>
&lt;p>The Zip file contains a directory &lt;code>config&lt;/code> where you will find some sample projects and some pipeline run configuration for Dataflow and other runners.&lt;/p>
&lt;p>For this example, we are going to use the pipeline located in &lt;code>config/projects/samples/beam/pipelines/input-process-output.hpl.&lt;/code>&lt;/p>
&lt;p>Let&amp;rsquo;s start by opening Apache Hop. In the directory where you have unzipped the client, run&lt;/p>
&lt;p>&lt;code>./hop/hop-gui.sh&lt;/code>&lt;/p>
&lt;p>(or &lt;code>./hop/hop-gui.bat&lt;/code> if you are on Windows).&lt;/p>
&lt;p>Once we are in Hop, let&amp;rsquo;s open the pipeline.&lt;/p>
&lt;p>We first switch from the project &lt;code>default&lt;/code> to the project &lt;code>samples&lt;/code>. Locate the &lt;code>projects&lt;/code> box in the top left corner of the window, and select the project &lt;code>samples&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image18.png"
alt="Apache Hop projects">&lt;/p>
&lt;p>Now we click the open button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image4.png"
alt="Apache Hop open project">&lt;/p>
&lt;p>Select the pipeline &lt;code>input-process-output.hpl&lt;/code> in the &lt;code>beam/pipelines&lt;/code> subdirectory:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image12.png"
alt="Apache Hop select pipeline">&lt;/p>
&lt;p>You should see a graph like the following in the main window of Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image17.png"
alt="Apache Hop main window">&lt;/p>
&lt;p>This pipeline takes some customer data from a CSV file and filters out everything but the records with the column &lt;code>stateCode&lt;/code> equal to &lt;code>CA.&lt;/code>&lt;/p>
&lt;p>Then we select only some of the columns of the file, and the result is written to Google Cloud Storage.&lt;/p>
&lt;p>It is always a good idea to test the pipeline locally before submitting it to Dataflow. In Apache Hop, you can preview the output of each transform. Let&amp;rsquo;s have a look at the input &lt;code>Customers&lt;/code>.&lt;/p>
&lt;p>Click in the &lt;code>Customers&lt;/code> input transform and then in &lt;em>Preview Output&lt;/em> in the dialog box that opens after selecting the transform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image10.png"
alt="Apache Hop Customers preview">&lt;/p>
&lt;p>Now select the option &lt;em>Quick launch&lt;/em> and you will see some of the input data:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image24.png"
alt="Apache Hop input data">&lt;/p>
&lt;p>Click &lt;em>Stop&lt;/em> when you finish reviewing the data.&lt;/p>
&lt;p>If we repeat the process right after the &lt;code>Only CA&lt;/code> transform, we will see that all the rows have the &lt;code>stateCode&lt;/code> column equal to &lt;code>CA&lt;/code>.&lt;/p>
&lt;p>The next transform selects only some of the columns of the input data and reorders the columns. Let&amp;rsquo;s have a look. Click the transform and then &lt;em>Preview Output&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image15.png"
alt="Apache Hop preview output">&lt;/p>
&lt;p>Then click _Quick Launch _again, and you should see output like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image8.png"
alt="Apache Hop output">&lt;/p>
&lt;p>The column &lt;code>id&lt;/code> is now the first, and we see only a subset of the input columns. This is how the data will look once the pipeline finishes writing the full output.&lt;/p>
&lt;h2 id="using-the-beam-direct-runner">Using the Beam Direct Runner&lt;/h2>
&lt;p>Let&amp;rsquo;s run the pipeline. To run the pipeline, we need to specify a runner configuration. This is done through the Metadata tool of Apache Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image6.png"
alt="Apache Hop runner configuration">&lt;/p>
&lt;p>In the &lt;code>samples&lt;/code> project, there are already several configurations created:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image9.png"
alt="Apache Hop configurations">&lt;/p>
&lt;p>The &lt;code>local&lt;/code> configuration is the one used to run the pipeline using Hop. For instance, this is the configuration that we used when we examined the previews of the output of different steps.&lt;/p>
&lt;p>The &lt;code>Direct&lt;/code> configuration uses the direct runner of Apache Beam. Let&amp;rsquo;s examine what it looks like. There are two tabs in the Pipeline Run Configurations: main and variables.&lt;/p>
&lt;p>For the direct runner, the main tab has the following options:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image28.png"
alt="Apache Hop direct runner">&lt;/p>
&lt;p>We can change the number of workers settings to match our number of CPUs, or even limit it just to 1 so the pipeline does not consume a lot of resources.&lt;/p>
&lt;p>In the variables tab, we find the configuration parameters for the pipeline itself (not for the runner): \&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image14.png"
alt="Apache Hop variables tab">&lt;/p>
&lt;p>For this pipeline, only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables are used. The &lt;code>STATE_INPUT&lt;/code> is used in a different example.&lt;/p>
&lt;p>If you go to the Beam transforms in the input and output nodes of the pipeline, you will see how these variables are used there:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image29.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image11.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>Since those variables are correctly set up to point to the location of data in the samples project folder, let&amp;rsquo;s try to run the pipeline using the Beam Direct Runner.&lt;/p>
&lt;p>For that, we need to go back to the pipeline view (arrow button just above the Metadata tool), and click the run button (the small &amp;ldquo;play&amp;rdquo; button in the toolbar). Then choose the Direct pipeline run configuration, and click the &lt;em>Launch&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image20.png"
alt="Apache Hop launch">&lt;/p>
&lt;p>How do you know if the job has finished or not? You can check the logs at the bottom of the main window for that. You should see something like this:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image19.png"
alt="Apache Hop completed job">&lt;/p>
&lt;p>If we go to the location set by &lt;code>DATA_OUTPUT&lt;/code>, in our case &lt;code>config/projects/samples/beam/output&lt;/code>, we should see some output files there. In my case, I see these files:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image26.png"
alt="Apache Hop output files">&lt;/p>
&lt;p>The number of files depends on the number of workers that you have set in the run configuration.&lt;/p>
&lt;p>Great, so the pipeline works locally. It is time to run it in the cloud!&lt;/p>
&lt;h2 id="running-at-cloud-scale-with-dataflow">Running at cloud scale with Dataflow&lt;/h2>
&lt;p>Let&amp;rsquo;s have a look at the Dataflow Pipeline Run Configuration. Go to the metadata tool, then to Pipeline Run Configuration and select Dataflow:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image30.png"
alt="Apache Hop Pipeline Run Configuration">&lt;/p>
&lt;p>We have again the Main and the Variables tab. We will need to change some values in both. Let&amp;rsquo;s start with the Variables. Click the Variables tab, and you should see the following values:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image3.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Those are Google Cloud Storage (GCS) locations that belong to the author of that sample project. We need to change them to point to our own GCS bucket.&lt;/p>
&lt;h2 id="project-setup-in-google-cloud">Project setup in Google Cloud&lt;/h2>
&lt;p>But for that, we will have to create a bucket. For the next step, you need to make sure that you have configured gcloud (the Google Cloud SDK), and that you have managed to authenticate.&lt;/p>
&lt;p>To double check, run the command &lt;code>gcloud config list&lt;/code> and check if the account and the project look correct. If they do, let&amp;rsquo;s triple check and run &lt;code>gcloud auth login&lt;/code>. That should open a tab in your web browser, to do the authentication process. Once you have done that, you can interact with your project using the SDK.&lt;/p>
&lt;p>For this example, I will use the region europe-west1 of GCP. Let&amp;rsquo;s create a regional bucket there. In my case, I am using the name &lt;code>ihr-apache-hop-blog&lt;/code> for the bucket name. Choose a different name for your bucket!&lt;/p>
&lt;pre>&lt;code>gsutil mb -c regional -l europe-west1 gs://ihr-apache-hop-blog
&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s upload the sample data to the GCS bucket, to test how the pipeline would run in Dataflow. Go to the same directory where you have all the hop files (the same directory that &lt;code>hop-gui.sh&lt;/code> is in), and let&amp;rsquo;s copy the data to GCS:&lt;/p>
&lt;pre>&lt;code> gsutil cp config/projects/samples/beam/input/customers-noheader-1k.txt gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>Notice the final slash &lt;code>/&lt;/code> in the path, indicating that you want to create a directory of name &lt;code>data&lt;/code>, with all the contents.&lt;/p>
&lt;p>To make sure that you have uploaded the data correctly, check the contents of that location:&lt;/p>
&lt;pre>&lt;code>gsutil ls gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>You should see the file &lt;code>customer-noheader-1k.txt&lt;/code> in that location.&lt;/p>
&lt;p>Before we continue, make sure that Dataflow is enabled in your project, and that you have a service account ready to be used with Hop. Please check the instructions given at the documentation of Dataflow, in the &lt;em>&lt;a href="https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-java#before-you-begin">Before you begin section&lt;/a>&lt;/em> to see how to enable the API for Dataflow.&lt;/p>
&lt;p>Now we need to make sure that Hop can use the necessary credentials for accessing Dataflow. In the Hop documentation, you will find that it recommends creating a service account, exporting a key for that service account, and setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. This is also the method given in the above link.&lt;/p>
&lt;p>Exporting the key of a service account is potentially dangerous, so we are going to use a different method, by leveraging the Google Cloud SDK. Run the following command:&lt;/p>
&lt;pre>&lt;code>gcloud auth application-default login
&lt;/code>&lt;/pre>&lt;p>That will open a tab in your web browser asking to confirm the authentication. Once you have confirmed, any application in your system that needs to access Google Cloud Platform will use those credentials for that access.&lt;/p>
&lt;p>We need also to create a service account for the Dataflow job, with certain permissions. Create the service account with&lt;/p>
&lt;pre>&lt;code>â€‹â€‹gcloud iam service-accounts create dataflow-hop-sa
&lt;/code>&lt;/pre>&lt;p>And now we give permissions to this service account for Dataflow:&lt;/p>
&lt;pre>&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;quot;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;quot;\
--role=&amp;quot;roles/dataflow.worker&amp;quot;
&lt;/code>&lt;/pre>&lt;p>We also need to give additional permissions for Google Cloud Storage:&lt;/p>
&lt;pre>&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;quot;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;quot;\
--role=&amp;quot;roles/storage.admin&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Make sure that you change the project id &lt;code>ihr-hop-playground&lt;/code> to your own project id.&lt;/p>
&lt;p>Now let&amp;rsquo;s give permissions to our user to impersonate that service account. For that, go to &lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">Service Accounts in the Google Cloud Console&lt;/a> in your project, and click on the service account we have just created.&lt;/p>
&lt;p>Click on the &lt;em>Permissions&lt;/em> tab and then in the &lt;em>Grant Access&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image21.png"
alt="Apache Hop Permissions">&lt;/p>
&lt;p>Give your user the role &lt;em>Service Account User&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image13.png"
alt="Apache Hop Service Account User">&lt;/p>
&lt;p>You are now all set to be able to run Dataflow with that service account and your user.&lt;/p>
&lt;h2 id="updating-the-pipeline-run-configuration">Updating the Pipeline Run Configuration&lt;/h2>
&lt;p>Before we can run a pipeline in Dataflow, we need to generate the JAR package for the pipeline code. For that, you have to go to the &lt;em>Tools&lt;/em> menu (in the menu bar), and choose the option &lt;em>Generate a Hop fat jar&lt;/em>. Click ok in the dialog, and then select a location and filename for the jar, and click &lt;em>Save&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image5.png"
alt="Apache Hop Tools menu">&lt;/p>
&lt;p>It will take some minutes to generate the file:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image22.png"
alt="Apache Hop generate file">&lt;/p>
&lt;p>We are ready to run the pipeline in Dataflow. Or almost :).&lt;/p>
&lt;p>Go the pipeline editor, click the play button, and select &lt;em>DataFlow&lt;/em> as Pipeline run configuration, and then click the play button on the right side:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image7.png"
alt="Apache Hop pipeline editor">&lt;/p>
&lt;p>That will open the Dataflow Pipeline Run Configuration, where we can change the input variables, and other Dataflow settings.&lt;/p>
&lt;p>Click on the &lt;em>Variables&lt;/em> tab and modify only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image2.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Notice that we also need to change the filename.&lt;/p>
&lt;p>Let&amp;rsquo;s go now to the &lt;em>Main&lt;/em> tab, because there are some other options that we need to change there. We need to update:&lt;/p>
&lt;ul>
&lt;li>Project id&lt;/li>
&lt;li>Service account&lt;/li>
&lt;li>Staging location&lt;/li>
&lt;li>Region&lt;/li>
&lt;li>Temp location&lt;/li>
&lt;li>Fat jar file location&lt;/li>
&lt;/ul>
&lt;p>For project id, set your project id (the same one you see when you run &lt;code>gcloud config list&lt;/code>).&lt;/p>
&lt;p>For service account, use the address of the Service Account we have created. If you don&amp;rsquo;t remember, you can find it under S&lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">ervice Accounts in the Google Cloud Console&lt;/a>.&lt;/p>
&lt;p>For staging and temp locations, use the same bucket that we have just created. Change the bucket address in the paths, and leave the same &amp;ldquo;binaries&amp;rdquo; and &amp;ldquo;tmp&amp;rdquo; locations that are already set in the configuration.&lt;/p>
&lt;p>For region, in this example we are using &lt;code>europe-west1&lt;/code>.&lt;/p>
&lt;p>Also, depending on your network configuration, you may want to check the box of &amp;ldquo;Use Public IPs?&amp;quot;, or alternatively leave it unchecked but enable Google Private Access in the regional subnetwork for europe-west1 in your project (for more details, please see &lt;a href="https://cloud.google.com/vpc/docs/configure-private-google-access#enabling-pga">Configuring Private Google Access | VPC&lt;/a>). In this example, I will check the box for simplicity.&lt;/p>
&lt;p>For the fat jar location, use the _Browse _button on the right side, and locate the JAR that we generated above. In summary, my &lt;em>Main&lt;/em> options look like these (your project id and locations will be different):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image27.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>You may, of course, change any other option, depending on the specific settings that might be required for your project.&lt;/p>
&lt;p>When you are ready, click on the _Ok _button and then &lt;em>Launch&lt;/em> to trigger the pipeline.&lt;/p>
&lt;p>In the logging window, you should see a line like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image16.png"
alt="Apache Hop logging window">&lt;/p>
&lt;h2 id="checking-the-job-in-dataflow">Checking the job in Dataflow&lt;/h2>
&lt;p>If everything has gone well, you should now see a job running at &lt;a href="https://console.cloud.google.com/dataflow/jobs">https://console.cloud.google.com/dataflow/jobs&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image1.png"
alt="Dataflow job list">&lt;/p>
&lt;p>If for some reason the job has failed, open the failed job page, check the _Logs _at the bottom, and click the error icon to find why the pipeline has failed. It is normally because we have set some wrong option in your configuration:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image25.png"
alt="Dataflow Logs">&lt;/p>
&lt;p>When the pipeline starts running, you should see the graph of the pipeline in the job page:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image23.png"
alt="Dataflow pipeline graph">&lt;/p>
&lt;p>When the job finishes, there should be a file in the output location. You can check it out with &lt;code>gsutil&lt;/code>&lt;/p>
&lt;pre>&lt;code>% gsutil ls gs://ihr-apache-hop-blog/output
gs://ihr-apache-hop-blog/output/input-process-output-00000-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00001-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00002-of-00003.csv
&lt;/code>&lt;/pre>&lt;p>In my case, the job has generated three files, but the actual number will vary from run to run.&lt;/p>
&lt;p>Let&amp;rsquo;s explore the first lines of those files:&lt;/p>
&lt;pre>&lt;code>gsutil cat &amp;quot;gs://ihr-apache-hop-blog/output/*csv&amp;quot;| head
12,wha-firstname,vnaov-name,egm-city,CALIFORNIA
25,ayl-firstname,bwkoe-name,rtw-city,CALIFORNIA
26,zio-firstname,rezku-name,nvt-city,CALIFORNIA
44,rgh-firstname,wzkjq-name,hkm-city,CALIFORNIA
135,ttv-firstname,eqley-name,trs-city,CALIFORNIA
177,ahc-firstname,nltvw-name,uxf-city,CALIFORNIA
181,kxv-firstname,bxerk-name,sek-city,CALIFORNIA
272,wpy-firstname,qxjcn-name,rew-city,CALIFORNIA
304,skq-firstname,cqapx-name,akw-city,CALIFORNIA
308,sfu-firstname,ibfdt-name,kqf-city,CALIFORNIA
&lt;/code>&lt;/pre>&lt;p>We can see that all the rows have CALIFORNIA as the state, that the output contains only the columns that we selected, and that the user id is the first column. The actual output you get will probably be different, as the order in which data is processed will not be the same in each run.&lt;/p>
&lt;p>We have run this job with a small data sample, but we could have run the same job with an arbitrarily large input CSV. Dataflow would parallelize and process the data in chunks.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>Apache Hop is a visual development environment for Beam pipelines, that allows us to run the pipelines locally, inspect the data, debug, unit test and many other capabilities. Once we are happy with a pipeline that has run locally, we can deploy the same visual pipeline in the cloud by just setting the necessary parameters for using Dataflow.&lt;/p>
&lt;p>If you want to know more about Apache Hop, don&amp;rsquo;t miss &lt;a href="https://www.youtube.com/watch?v=sZSIbcPtebI">the Beam Summit talk delivered by the author of Hop&lt;/a>, and don&amp;rsquo;t forget to check out the &lt;a href="https://hop.apache.org/manual/latest/getting-started/index.html">getting started guide&lt;/a>.&lt;/p></description><link>/blog/apache-hop-with-dataflow/</link><pubDate>Fri, 22 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/apache-hop-with-dataflow/</guid><category>blog</category></item><item><title>Apache Beam 2.38.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.38.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2380-2022-04-20">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.38.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351169">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Introduce projection pushdown optimizer to the Java SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12976">BEAM-12976&lt;/a>). The optimizer currently only works on the &lt;a href="https://beam.apache.org/documentation/io/built-in/google-bigquery/#storage-api">BigQuery Storage API&lt;/a>, but more I/Os will be added in future releases. If you encounter a bug with the optimizer, please file a JIRA and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;li>A new IO for Neo4j graph databases was added. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1857">BEAM-1857&lt;/a>) It has the ability to update nodes and relationships using UNWIND statements and to read data using cypher statements with parameters.&lt;/li>
&lt;li>&lt;code>amazon-web-services2&lt;/code> has reached feature parity and is finally recommended over the earlier &lt;code>amazon-web-services&lt;/code> and &lt;code>kinesis&lt;/code> modules (Java). These will be deprecated in one of the next releases (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13174">BEAM-13174&lt;/a>).
&lt;ul>
&lt;li>Long outstanding write support for &lt;code>Kinesis&lt;/code> was added (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13175">BEAM-13175&lt;/a>).&lt;/li>
&lt;li>Configuration was simplified and made consistent across all IOs, including the usage of &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13663">BEAM-13663&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13587">BEAM-13587&lt;/a>).&lt;/li>
&lt;li>Additionally, there&amp;rsquo;s a long list of recent improvements and fixes to
&lt;code>S3&lt;/code> Filesystem (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13245">BEAM-13245&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13246">BEAM-13246&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13441">BEAM-13441&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13445">BEAM-13445&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>),
&lt;code>DynamoDB&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13209&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13209">BEAM-13209&lt;/a>),
&lt;code>SQS&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13631">BEAM-13631&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13510">BEAM-13510&lt;/a>) and others.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Pipeline dependencies supplied through &lt;code>--requirements_file&lt;/code> will now be staged to the runner using binary distributions (wheels) of the PyPI packages for linux_x86_64 platform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-4032">BEAM-4032&lt;/a>). To restore the behavior to use source distributions, set pipeline option &lt;code>--requirements_cache_only_sources&lt;/code>. To skip staging the packages at submission time, set pipeline option &lt;code>--requirements_cache=skip&lt;/code> (Python).&lt;/li>
&lt;li>The Flink runner now supports Flink 1.14.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13106">BEAM-13106&lt;/a>).&lt;/li>
&lt;li>Interactive Beam now supports remotely executing Flink pipelines on Dataproc (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14071">BEAM-14071&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>(Python) Previously &lt;code>DoFn.infer_output_types&lt;/code> was expected to return &lt;code>Iterable[element_type]&lt;/code> where &lt;code>element_type&lt;/code> is the PCollection elemnt type. It is now expected to return &lt;code>element_type&lt;/code>. Take care if you have overriden &lt;code>infer_output_type&lt;/code> in a &lt;code>DoFn&lt;/code> (this is not common). See &lt;a href="https://issues.apache.org/jira/browse/BEAM-13860">BEAM-13860&lt;/a>.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) The types of &lt;code>awsRegion&lt;/code> / &lt;code>endpoint&lt;/code> in &lt;code>AwsOptions&lt;/code> changed from String to &lt;code>Region&lt;/code> / &lt;code>URI&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Beam 2.38.0 will be the last minor release to support Flink 1.11.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) Client providers (&lt;code>withXYZClientProvider()&lt;/code>) as well as IO specific &lt;code>RetryConfiguration&lt;/code>s are deprecated, instead use &lt;code>withClientConfiguration()&lt;/code> or &lt;code>AwsOptions&lt;/code> to configure AWS IOs / clients.
Custom implementations of client providers shall be replaced with a respective &lt;code>ClientBuilderFactory&lt;/code> and configured through &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fix S3 copy for large objects (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>)&lt;/li>
&lt;li>Fix quadratic behavior of pipeline canonicalization (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14128">BEAM-14128&lt;/a>)
&lt;ul>
&lt;li>This caused unnecessarily long pre-processing times before job submission for large complex pipelines.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Fix &lt;code>pyarrow&lt;/code> version parsing (Python)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-14235">BEAM-14235&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.38.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.38.0 release. Thank you to all contributors!&lt;/p>
&lt;p>abhijeet-lele
Ahmet Altay
akustov
Alexander
Alexander Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Ankur Goenka
ansh0l
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
Brian Hulette
brucearctor
bulat safiullin
bullet03
Carl Mastrangelo
Chamikara Jayalath
Chun Yang
Daniela MartÃ­n
Daniel Oliveira
Danny McCormick
daria.malkova
David Cavazos
David Huntsperger
dmitryor
Dmytro Sadovnychyi
dpcollins-google
egalpin
Elias Segundo Antonio
emily
Etienne Chauchot
Hengfeng Li
IsmaÃ«l MejÃ­a
Israel Herraiz
Jack McCluskey
Jakub Kukul
Janek Bevendorff
Jeff Klukas
Johan Sternby
Kamil BreguÅ‚a
Kenneth Knowles
Ke Wu
Kiley
Kyle Weaver
laraschmidt
Lara Schmidt
LE QUELLEC Olivier
Luka Kalinovcic
Luke Cwik
Marcin Kuthan
masahitojp
Masato Nakamura
Matt Casters
Melissa Pashniak
Michael Li
Miguel Hernandez
Moritz Mack
mosche
nancyxu123
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
paul-tlh
Pavel Avilov
Rahul Iyer
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Skraba
Ryan Thompson
Sam Whittle
Seth Vargo
sp029619
Steven Niemitz
Thiago Nunes
Udi Meiri
Valentyn Tymofieiev
Victor
vitaly.terentyev
Yichi Zhang
Yi Hu
yirutang
Zachary Houfek
Zoe&lt;/p></description><link>/blog/beam-2.38.0/</link><pubDate>Wed, 20 Apr 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.38.0/</guid><category>blog</category><category>release</category></item></channel></rss>