<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Apache Beam 2.69.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.69.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2690-2025-10-28">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.69.0, check out the &lt;a href="https://github.com/apache/beam/milestone/37?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>(Python) Add YAML Editor and Visualization Panel (&lt;a href="https://github.com/apache/beam/issues/35772">#35772&lt;/a>).&lt;/li>
&lt;li>(Java) Java 25 Support (&lt;a href="https://github.com/apache/beam/issues/35627">#35627&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Upgraded Iceberg dependency to 1.10.0 (&lt;a href="https://github.com/apache/beam/issues/36123">#36123&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Enhance JAXBCoder with XMLInputFactory support (Java) (&lt;a href="https://github.com/apache/beam/issues/36446">#36446&lt;/a>).&lt;/li>
&lt;li>Python examples added for CloudSQL enrichment handler on &lt;a href="https://beam.apache.org/documentation/transforms/python/elementwise/enrichment-cloudsql/">Beam website&lt;/a> (Python) (&lt;a href="https://github.com/apache/beam/issues/36095">#35473&lt;/a>).&lt;/li>
&lt;li>Support for batch mode execution in WriteToPubSub transform added (Python) (&lt;a href="https://github.com/apache/beam/issues/35990">#35990&lt;/a>).&lt;/li>
&lt;li>Added official support for Python 3.13 (&lt;a href="https://github.com/apache/beam/issues/34869">#34869&lt;/a>).&lt;/li>
&lt;li>Added an optional output_schema verification to all YAML transforms (&lt;a href="https://github.com/apache/beam/issues/35952">#35952&lt;/a>).&lt;/li>
&lt;li>Support for encryption when using GroupByKey added, along with &lt;code>--gbek&lt;/code> pipeline option to automatically replace all GroupByKey transforms (Java/Python) (&lt;a href="https://github.com/apache/beam/issues/36214">#36214&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>(Python) &lt;code>dill&lt;/code> is no longer a required, default dependency for Apache Beam (&lt;a href="https://github.com/apache/beam/issues/21298">#21298&lt;/a>).
&lt;ul>
&lt;li>This change only affects pipelines that explicitly use the &lt;code>pickle_library=dill&lt;/code> pipeline option.&lt;/li>
&lt;li>While &lt;code>dill==0.3.1.1&lt;/code> is still pre-installed on the official Beam SDK base images, it is no longer a direct dependency of the apache-beam Python package. This means it can be overridden by other dependencies in your environment.&lt;/li>
&lt;li>If your pipeline uses &lt;code>pickle_library=dill&lt;/code>, you must manually ensure &lt;code>dill==0.3.1.1&lt;/code> is installed in both your submission and runtime environments.
&lt;ul>
&lt;li>Submission environment: Install the dill extra in your local environment &lt;code>pip install apache-beam[gcpdill]&lt;/code>.&lt;/li>
&lt;li>Runtime (worker) environment: Your action depends on how you manage your worker&amp;rsquo;s environment.
&lt;ul>
&lt;li>If using default containers or custom containers with the official Beam base image e.g. &lt;code>FROM apache/beam_python3.10_sdk:2.69.0&lt;/code>
&lt;ul>
&lt;li>Add &lt;code>dill==0.3.1.1&lt;/code> to your worker&amp;rsquo;s requirements file (e.g., requirements.txt)&lt;/li>
&lt;li>Pass this file to your pipeline using the &amp;ndash;requirements_file requirements.txt pipeline option (For more details see &lt;a href="https://cloud.google.com/dataflow/docs/guides/manage-dependencies#py-custom-containers">managing Dataflow dependencies&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If custom containers with a non-Beam base image e.g. &lt;code>FROM python:3.9-slim&lt;/code>
&lt;ul>
&lt;li>Install apache-beam with the dill extra in your docker file e.g. &lt;code>RUN pip install --no-cache-dir apache-beam[gcp,dill]&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If there is a dill version mismatch between submission and runtime environments you might encounter unpickling errors like &lt;code>Can't get attribute '_create_code' on &amp;lt;module 'dill._dill' from...&lt;/code>.&lt;/li>
&lt;li>If dill is not installed in the runtime environment you will see the error &lt;code>ImportError: Pipeline option pickle_library=dill is set, but dill is not installed...&lt;/code>&lt;/li>
&lt;li>Report any issues you encounter when using &lt;code>pickle_library=dill&lt;/code> to the GitHub issue (&lt;a href="https://github.com/apache/beam/issues/21298">#21298&lt;/a>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>(Python) Added a &lt;code>pickle_library=dill_unsafe&lt;/code> pipeline option. This allows overriding &lt;code>dill==0.3.1.1&lt;/code> using dill as the pickle_library. Use with extreme caution. Other versions of dill has not been tested with Apache Beam (&lt;a href="https://github.com/apache/beam/issues/21298">#21298&lt;/a>).&lt;/li>
&lt;li>(Python) The deterministic fallback coder for complex types like NamedTuple, Enum, and dataclasses now normalizes filepaths for better determinism guarantees. This affects streaming pipelines updating from 2.68 to 2.69 that utilize this fallback coder. If your pipeline is affected, you may see a warning like: &amp;ldquo;Using fallback deterministic coder for type X&amp;hellip;&amp;rdquo;. To update safely sepcify the pipeline option &lt;code>--update_compatibility_version=2.68.0&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/36345">#36345&lt;/a>).&lt;/li>
&lt;li>(Python) Fixed transform naming conflict when executing DataTransform on a dictionary of PColls (&lt;a href="https://github.com/apache/beam/issues/30445">#30445&lt;/a>).
This may break update compatibility if you don&amp;rsquo;t provide a &lt;code>--transform_name_mapping&lt;/code>.&lt;/li>
&lt;li>Removed deprecated Hadoop versions (2.10.2 and 3.2.4) that are no longer supported for &lt;a href="https://github.com/apache/iceberg/issues/10940">Iceberg&lt;/a> from IcebergIO (&lt;a href="https://github.com/apache/beam/issues/36282">#36282&lt;/a>).&lt;/li>
&lt;li>(Go) Coder construction on SDK side is more faithful to the specs from runners without stripping length-prefix. This may break streaming pipeline update as the underlying coder could be changed (&lt;a href="https://github.com/apache/beam/issues/36387">#36387&lt;/a>).&lt;/li>
&lt;li>Minimum Go version for Beam Go updated to 1.25.2 (&lt;a href="https://github.com/apache/beam/issues/36461">#36461&lt;/a>).&lt;/li>
&lt;li>(Java) DoFn OutputReceiver now requires implementing a builder method as part of extended metadata support for elements (&lt;a href="https://github.com/apache/beam/issues/34902">#34902&lt;/a>).&lt;/li>
&lt;li>(Java) Removed ProcessContext outputWindowedValue introduced in 2.68 that allowed setting offset and record Id. Use OutputReceiver&amp;rsquo;s builder to set those field (&lt;a href="https://github.com/apache/beam/pull/36523">#36523&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Fixed passing of pipeline options to x-lang transforms when called from the Java SDK (Java) (&lt;a href="https://github.com/apache/beam/issues/36443">#36443&lt;/a>).&lt;/li>
&lt;li>PulsarIO has now changed support status from incomplete to experimental. Both read and writes should now minimally
function (un-partitioned topics, without schema support, timestamp ordered messages for read) (Java)
(&lt;a href="https://github.com/apache/beam/issues/36141">#36141&lt;/a>).&lt;/li>
&lt;li>Fixed Spanner Change Stream reading stuck issue due to watermark of partition moving backwards (&lt;a href="https://github.com/apache/beam/issues/36470">#36470&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.69.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abdelrahman Ibrahim, Ahmed Abualsaud, Andrew Crites, Arun Pandian, Bryan Dang, Chamikara Jayalath, Charles Nguyen, Chenzo, Clay Johnson, Danny McCormick, David A, Derrick Williams, Enrique Calderon, Hai Joey Tran, Ian Liao, Ian Mburu, Jack McCluskey, Jiang Zhu, Joey Tran, Kenneth Knowles, Kyle Stanley, Maciej Szwaja, Minbo Bae, Mohamed Awnallah, Radek Stankiewicz, Radosław Stankiewicz, Razvan Culea, Reuven Lax, Sagnik Ghosh, Sam Whittle, Shunping Huang, Steven van Rossum, Talat UYARER, Tanu Sharma, Tarun Annapareddy, Tom Stepp, Valentyn Tymofieiev, Vitaly Terentyev, XQ Hu, Yi Hu, Yilei, claudevdm, flpablo, fozzie15, johnjcasey, lim1t, parveensania, yashu&lt;/p></description><link>/blog/beam-2.69.0/</link><pubDate>Tue, 28 Oct 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.69.0/</guid><category>blog</category><category>release</category></item><item><title>Google Summer of Code 2025 - Enhanced Interactive Pipeline Development Environment for JupyterLab</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="gsoc-2025-basic-information">GSoC 2025 Basic Information&lt;/h1>
&lt;p>&lt;strong>Student:&lt;/strong> [Canyu Chen] (&lt;a href="https://github.com/Chenzo1001">@Chenzo1001&lt;/a>)
&lt;strong>Mentors:&lt;/strong> [XQ Hu] (&lt;a href="https://github.com/liferoad">@liferoad&lt;/a>)
&lt;strong>Organization:&lt;/strong> [Apache Beam]
&lt;strong>Proposal Link:&lt;/strong> &lt;a href="https://drive.google.com/file/d/1gmrSUGpXMXujVnFffuj0UWQjbghWI8Oy/view?usp=sharing">Here&lt;/a>&lt;/p>
&lt;h1 id="project-overview">Project Overview&lt;/h1>
&lt;p>BeamVision significantly enhances the Apache Beam development experience within JupyterLab by providing a unified, visual interface for pipeline inspection and analysis. This project successfully delivered a production-ready JupyterLab extension that replaces fragmented workflows with an integrated workspace, featuring a dynamic side panel for pipeline visualization and a multi-tab interface for comparative workflow analysis.&lt;/p>
&lt;p>Core Achievements:&lt;/p>
&lt;p>Modernized Extension: Upgraded the JupyterLab Sidepanel to v4.x, ensuring compatibility with the latest ecosystem and releasing the package on both &lt;a href="https://www.npmjs.com/package/apache-beam-jupyterlab-sidepanel">NPM&lt;/a> and &lt;a href="https://pypi.org/project/apache-beam-jupyterlab-sidepanel/">PyPI&lt;/a>.&lt;/p>
&lt;p>YAML Visualization Suite: Implemented a powerful visual editor for Beam YAML, combining a code editor, an interactive flow chart (built with @xyflow/react-flow), and a collapsible key-value panel for intuitive pipeline design.&lt;/p>
&lt;p>Enhanced Accessibility &amp;amp; Stability: Added pip installation support and fixed critical bugs in Interactive Beam, improving stability and user onboarding.&lt;/p>
&lt;p>Community Engagement: Active participation in the Beam community, including contributing to a hackathon project and successfully integrating all work into the Apache Beam codebase via merged Pull Requests.&lt;/p>
&lt;h1 id="development-workflow">Development Workflow&lt;/h1>
&lt;p>As early as the beginning of March, I saw Apache&amp;rsquo;s project information on the official GSoC website and came across Beam among the projects released by Apache. Since I have some interest in front-end development and wanted to truly integrate into the open-source community for development work, I contacted mentor XQ Hu via email and received positive feedback from him. In April, XQ Hu posted notes for all GSoC students on the Beam Mailing List. It was essential to keep an eye on the Mailing List promptly. Between March and May, besides completing the project proposal and preparation work, I also used my spare time to partially migrate the Beam JupyterLab Extension to version 4.0. This helped me get into the development state more quickly.&lt;/p>
&lt;p>I also participated in the Beam Hackathon held in May. There were several topics to choose from, and I opted for the free topic. This allowed me to implement any innovative work on Beam. I combined Beam and GCP to create an &lt;a href="https://github.com/Chenzo1001/Beam_auto_emotion_analysis">Automatic Emotion Analysis Tool for comments&lt;/a>. This tool integrates Beam Pipeline, Flink, Docker, and GCP to collect and perform sentiment analysis on real-time comment stream data, storing the results in GCP&amp;rsquo;s BigQuery. This is a highly meaningful task because sentiment analysis of comments can help businesses better understand users&amp;rsquo; opinions about their products, thereby improving the products more effectively. However, the time during the Hackathon was too tight, so I haven&amp;rsquo;t fully completed this project yet, and it can be further improved later. This Hackathon gave me a deeper understanding of Beam and GCP, and also enhanced my knowledge of the development of the Beam JupyterLab Extension.&lt;/p>
&lt;p>In June, I officially started the project development and maintained close communication with my mentor to ensure the project progressed smoothly. XQ Hu and I held a half-hour weekly meeting every Monday on Google Meet, primarily to address issues encountered during the previous week&amp;rsquo;s development and to discuss the tasks for the upcoming week. XQ Hu is an excellent mentor, and I had no communication barriers with him whatsoever. He is also very understanding; sometimes, when I needed to postpone some development tasks due to personal reasons, he was always supportive and gave me ample freedom. During this month, I improved the plugin to make it fully compatible with JupyterLab 4.0.&lt;/p>
&lt;p>In July and August, I made some modifications to the plugin&amp;rsquo;s source code structure and published it on PyPI to facilitate user installation and promote the plugin. During this period, I also fixed several bugs. Afterwards, I began developing a new feature: the YAML visual editor (design doc &lt;a href="https://s.apache.org/beam-yaml-jupyterlab">HERE&lt;/a>). This feature is particularly meaningful because Beam&amp;rsquo;s Pipeline is described through YAML files, and a visual editor for YAML files can significantly improve developers&amp;rsquo; efficiency. In July, I published the proposal for the YAML visual editor and, after gathering feedback from the community for some time, started working on its development. Initially, I planned to use native Cytoscape to build the plugin from scratch, but the workload was too heavy, and there were many mature flow chart plugins in the open-source community that could be referenced. Therefore, I chose XYFlow as the component for flow visualization and integrated it into the plugin. In August, I further optimized the YAML visual editor and fixed some bugs.&lt;/p>
&lt;img src="/images/blog/gsoc-25-jupyterlab-extensions/Yaml_main.png" alt="Main page of the YAML visual editor" width="100%">
&lt;p>In September, I completed the project submission, passed Google&amp;rsquo;s review, and successfully concluded the project.&lt;/p>
&lt;h1 id="development-conclusion">Development Conclusion&lt;/h1>
&lt;p>Overall, collaborating with Apache Beam&amp;rsquo;s developers was a very enjoyable process. I learned a lot about Beam, and since I am a student engaged in high-performance geographic computing research, Beam may play a significant role in my future studies and work.&lt;/p>
&lt;p>I am excited to remain an active member of the Beam community. I hope to continue contributing to its development, applying what I have learned to both my academic pursuits and future collaborative projects. The experience has strengthened my commitment to open-source innovation and has set a strong foundation for ongoing participation in Apache Beam and related technologies.&lt;/p>
&lt;h1 id="special-thanks">Special Thanks&lt;/h1>
&lt;p>I would like to express my sincere gratitude to my mentor XQ Hu for his guidance and support throughout the project. Without his help, I would not have been able to complete this project successfully. His professionalism, patience, and passion have been truly inspiring. As a Google employee, he consistently dedicated time each week to the open-source community and willingly assisted students like me. His selfless dedication to open source is something I deeply admire and strive to emulate. He is also an exceptionally devoted teacher who not only imparted technical knowledge but also taught me how to communicate more effectively, handle interpersonal relationships, and collaborate better in a team setting. He always patiently addressed my questions and provided invaluable advice. I am immensely grateful to him and hope to have the opportunity to work with him again in the future.&lt;/p>
&lt;p>I also want to thank the Apache Beam community for their valuable feedback and suggestions, which have greatly contributed to the improvement of the plugin. I feel incredibly fortunate that we, as a society, have open-source communities where individuals contribute their intellect and time to drive collective technological progress and innovation. These communities provide students like me with invaluable opportunities to grow and develop rapidly.&lt;/p>
&lt;p>Finally, I would like to thank the Google Summer of Code program for providing me with this opportunity to contribute to open-source projects and gain valuable experience. Without Google Summer of Code, I might never have had the chance to engage with so many open-source projects, take that first step into the open-source community, or experience such substantial personal and professional growth.&lt;/p></description><link>/blog/gsoc-25-jupyterlab-extensions/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0800</pubDate><guid>/blog/gsoc-25-jupyterlab-extensions/</guid><category>blog</category><category>gsoc</category></item><item><title>Google Summer of Code 2025 - Beam ML Vector DB/Feature Store integrations</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="what-will-i-cover-in-this-blog-post">What Will I Cover In This Blog Post?&lt;/h2>
&lt;p>I have three objectives in mind when writing this blog post:&lt;/p>
&lt;ul>
&lt;li>Documenting the work I&amp;rsquo;ve been doing during this GSoC period in collaboration
with the Apache Beam community&lt;/li>
&lt;li>A thoughtful and cumulative thank you to my mentor and the Beam Community&lt;/li>
&lt;li>Writing to an older version of myself before making my first ever contribution
to Beam. This can be helpful for future contributors&lt;/li>
&lt;/ul>
&lt;h2 id="what-was-this-gsoc-project-about">What Was This GSoC Project About?&lt;/h2>
&lt;p>The goal of this project is to enhance Beam&amp;rsquo;s Python SDK by developing
connectors for vector databases like Milvus and feature stores like Tecton. These
integrations will improve support for ML use cases such as Retrieval-Augmented
Generation (RAG) and feature engineering. By bridging Beam with these systems,
this project will attract more users, particularly in the ML community.&lt;/p>
&lt;h2 id="why-was-this-project-important">Why Was This Project Important?&lt;/h2>
&lt;p>While Beam&amp;rsquo;s Python SDK supports some vector databases, feature stores and
embedding generators, the current integrations are limited to a few systems as
mentioned in the tables down below. Expanding this ecosystem will provide more
flexibility and richness for ML workflows particularly in feature engineering
and RAG applications, potentially attracting more users, particularly in the ML
community.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Vector Database&lt;/th>
&lt;th>Feature Store&lt;/th>
&lt;th>Embedding Generator&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BigQuery&lt;/td>
&lt;td>Vertex AI&lt;/td>
&lt;td>Vertex AI&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AlloyDB&lt;/td>
&lt;td>Feast&lt;/td>
&lt;td>Hugging Face&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="why-did-i-choose-beam-as-part-of-gsoc-among-180-orgs">Why Did I Choose Beam As Part of GSoC Among 180+ Orgs?&lt;/h2>
&lt;p>I chose to apply to Beam from among 180+ GSoC organizations because it aligns
well with my passion for data processing systems that serve information
retrieval systems and my core career values:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Freedom:&lt;/strong> Working on Beam supports open-source development, liberating
developers from vendor lock-in through its unified programming model while
enabling services like &lt;a href="https://projectshield.withgoogle.com/landing">Project Shield&lt;/a> to protect free
speech globally&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Innovation:&lt;/strong> Working on Beam allows engagement with cutting-edge data
processing techniques and distributed computing paradigms&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Accessibility:&lt;/strong> Working on Beam helps build open-source technology that
makes powerful data processing capabilities available to all organizations
regardless of size or resources. This accessibility enables projects like
Project Shield to provide free protection to media, elections, and human rights
websites worldwide&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-did-i-work-on-during-the-gsoc-program">What Did I Work On During the GSoC Program?&lt;/h2>
&lt;p>During my GSoC program, I focused on developing connectors for vector databases,
feature stores, and embedding generators to enhance Beam&amp;rsquo;s ML capabilities.
Here are the artifacts I worked on and what remains to be done:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>System&lt;/th>
&lt;th>Artifact&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Enrichment Handler&lt;/td>
&lt;td>Milvus&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35216">PR #35216&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35577">PR #35577&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35467">PR #35467&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sink I/O&lt;/td>
&lt;td>Milvus&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35708">PR #35708&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35944">PR #35944&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Enrichment Handler&lt;/td>
&lt;td>Tecton&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/36062">PR #36062&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sink I/O&lt;/td>
&lt;td>Tecton&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/36078">PR #36078&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Gen&lt;/td>
&lt;td>OpenAI&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/36081">PR #36081&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Gen&lt;/td>
&lt;td>Anthropic&lt;/td>
&lt;td>To Be Added&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Here are side-artifacts that are not directly linked to my project:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>System&lt;/th>
&lt;th>Artifact&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AI Code Review&lt;/td>
&lt;td>Gemini Code Assist&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35532">PR #35532&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Enrichment Handler&lt;/td>
&lt;td>CloudSQL&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/34398">PR #34398&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35473">PR #35473&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pytest Markers&lt;/td>
&lt;td>GitHub CI&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/beam/pull/35655">PR #35655&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35740">PR #35740&lt;/a> &lt;br> &lt;a href="https://github.com/apache/beam/pull/35816">PR #35816&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For more granular contributions, checking out my
&lt;a href="https://github.com/apache/beam/pulls?q=is%3Apr+author%3Amohamedawnallah">ongoing Beam contributions&lt;/a>.&lt;/p>
&lt;h2 id="how-did-i-approach-this-project">How Did I Approach This Project?&lt;/h2>
&lt;p>My approach centered on community-driven design and iterative implementation,
Originally inspired by my mentor&amp;rsquo;s work. Here&amp;rsquo;s how it looked:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Design Document&lt;/strong>: Created a comprehensive design document outlining the
proposed ML connector architecture&lt;/li>
&lt;li>&lt;strong>Community Feedback&lt;/strong>: Shared the design with the Beam developer community
mailing list for review&lt;/li>
&lt;li>&lt;strong>Iterative Implementation&lt;/strong>: Incorporated community feedback and applied
learnings in subsequent pull requests&lt;/li>
&lt;li>&lt;strong>Continuous Improvement&lt;/strong>: Refined the approach based on real-world usage
patterns and maintainer guidance&lt;/li>
&lt;/ol>
&lt;p>Here are some samples of those design docs:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Design Document&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Milvus&lt;/td>
&lt;td>Vector Enrichment Handler&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/4c6l20tjopd94cqg6vsgj20xl2qgywtx">[Proposal][GSoC 2025] Milvus Vector Enrichment Handler for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Milvus&lt;/td>
&lt;td>Vector Sink I/O Connector&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/cwlbwnhnf1kl7m0dn40jrqfsf4ho98tf">[Proposal][GSoC 2025] Milvus Vector Sink I/O Connector for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tecton&lt;/td>
&lt;td>Feature Store Enrichment Handler&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/7ynn4r8b8b1c47ojxlk39fhsn3t0jrd1">[Proposal][GSoC 2025] Tecton Feature Store Enrichment Handler for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tecton&lt;/td>
&lt;td>Feature Store Sink I/O Connector&lt;/td>
&lt;td>&lt;a href="https://lists.apache.org/thread/dthd3t6md9881ksvbf4v05rxnlj1fgvn">[Proposal][GSoC 2025] Tecton Feature Store Sink I/O Connector for Beam&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="where-did-challenges-arise-during-the-project">Where Did Challenges Arise During The Project?&lt;/h2>
&lt;p>There were 2 places where challenges arose:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Running Docker TestContainers in Beam Self-Hosted CI Environment:&lt;/strong> The main
challenge was that Beam runs in CI on Ubuntu 20.04, which caused compatibility
and connectivity issues with Milvus TestContainers due to the Docker-in-Docker
environment. After several experiments with trial and error, I eventually tested
with Ubuntu latest (which at the time of writing this blog post is Ubuntu 25.04),
and no issues arose. This version compatibility problem led to the container
startup failures and network connectivity issues&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Triggering and Modifying the PostCommit Python Workflows:&lt;/strong> This challenge
magnified the above issue since for every experiment update to the given
workflow, I had to do a round trip to my mentor to include those changes in the
relevant workflow files and evaluate the results. I also wasn&amp;rsquo;t aware that
someone can trigger post-commit Python workflows by updating the trigger files
in &lt;code>.github/trigger_files&lt;/code> until near the middle of GSoC. I discovered there is
actually a workflows README document in &lt;code>.github/workflows/README.md&lt;/code> that was
not referenced in the &lt;code>CONTRIBUTING.md&lt;/code> file at the time of writing this post&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-did-this-project-start-to-attract-users-in-the-ml-community">How Did This Project Start To Attract Users in the ML Community?&lt;/h2>
&lt;p>It is observed that after we had a Milvus Enrichment Handler PR before even
merging, we started to see community-driven contributions like
&lt;a href="https://github.com/apache/beam/pull/35686">this one that adds Qdrant&lt;/a>. Qdrant
is a competitor to Milvus in the vector space. This demonstrates how
the project&amp;rsquo;s momentum and visibility in the ML community space attracted
contributors who wanted to expand the Beam ML ecosystem with additional vector
database integrations.&lt;/p>
&lt;h2 id="how-did-this-gsoc-experience-working-with-beam-community-shape-me">How Did This GSoC Experience Working With Beam Community Shape Me?&lt;/h2>
&lt;p>If I have to boil it down across three dimensions, they would be:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mindset:&lt;/strong> Before I was probably working in solitude making PRs about new
integrations with mental chatter in the form of fingers crossed, hoping that
there will be no divergence on the design. Now I can engage people I am working
with through design docs, making sure my work aligns with their vision, which
potentially leads to faster PR merges&lt;/li>
&lt;li>&lt;strong>Skillset:&lt;/strong> It was one year before contributing to Beam where I wrote
professionally in Python, so it was a great opprtunity to brush up on my Python
skills and seeing how some design patterns are used in practice, like the query
builder pattern seen in CloudSQL Vector Ingestion in the RAG package. I also
learned about vector databases and feature stores, and also some AI
integrations. I also think I got a bit better than before in root cause analysis
and filtering signals from noise in long log files like PostCommit Python
workflows&lt;/li>
&lt;li>&lt;strong>Toolset:&lt;/strong> Learning about Beam Python SDK, Milvus, Tecton, Google CloudSQL,
OpenAI and Anthropic text embedding generators, and lnav for effective log file
navigation, including their capabilities and limitations&lt;/li>
&lt;/ul>
&lt;h2 id="tips-for-future-contributors">Tips for Future Contributors&lt;/h2>
&lt;p>If I have to boil them down to three, they would be:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Observing:&lt;/strong> Observing how experienced developers in the Beam dev team
work—how their PRs look, how they write design docs, what kind of feedback they
get on their design docs and PRs, and how you can apply it (if feasible) to
avoid getting the same feedback again. What kind of follow-up PRs do they create
after their initial ones? How do they document and illustrate their work? What
kind of comments do they post when reviewing other people&amp;rsquo;s related work? Over
time, you build your own mental model and knowledge base on how the ideal
contribution looks in this area. There is a lot to learn and explore in an
exciting, not intimidating way&lt;/li>
&lt;li>&lt;strong>Orienting:&lt;/strong> Understanding your place in the ecosystem and aligning your
work with the project&amp;rsquo;s context. This means grasping how your contribution fits
into Beam&amp;rsquo;s architecture and roadmap, identifying your role in addressing
current gaps, and mapping stakeholders who will review, use, and maintain your
work. Most importantly, align with both your mentor&amp;rsquo;s vision and the community&amp;rsquo;s
vision to ensure your work serves the broader goals&lt;/li>
&lt;li>&lt;strong>Acting:&lt;/strong> Acting on feedback from code reviews, design document discussions,
and community input. This means thoughtfully addressing suggested changes in a
way that moves the discussion forward, addressing concerns raised by
maintainers, and iterating on your work based on community guidance. Being
responsive to feedback, asking clarifying questions when needed, and
demonstrating that you&amp;rsquo;re incorporating the community&amp;rsquo;s input into your
contributions given that it is aligned with the project direction&lt;/li>
&lt;/ul>
&lt;h2 id="who-do-i-want-to-thank-for-making-this-journey-possible">Who Do I Want To Thank for Making This Journey Possible?&lt;/h2>
&lt;p>If I have to boil them down to three, they would be:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>My Mentor, Danny McCormick:&lt;/strong> I wouldn&amp;rsquo;t hesitate to say that Danny is the
best mentor I have worked with so far, given that I have worked with several
mentors. What makes me say that:
&lt;ul>
&lt;li>&lt;strong>Generosity:&lt;/strong> Danny is very generous with his time, feedback, and
genuinely committed to reviewing my work on a regular basis. We have weekly
30-minute sync calls over almost 21 weeks (5 months) since the official
community bonding period, where he shares with me his contextual expertise and
addresses any questions I may have with openness to extend time if needed and
flexible about skipping calls when there was no agenda&lt;/li>
&lt;li>&lt;strong>Flexibility:&lt;/strong> When I got accepted to GSoC, after a few days I also got
accepted to a part-time internship that I had applied to before GSoC, while
also managing my last semester in my Bachelor of Computer Science, which was
probably the hardest semester. During our discussion about working capacity,
Danny was very flexible regarding that, with more emphasis on making progress,
which encouraged me to make even more progress. I have also never felt there
are very hard boundaries around my project scope—I felt there was an area to
explore that motivated me to think of and add some side-artifacts to Beam,
e.g., adding Gemini Code Assist for AI code review&lt;/li>
&lt;li>&lt;strong>Proactivity&lt;/strong>: Danny was very proactive in offering support and help
without originally asking, e.g., making Beam Infra tickets that add API keys
to unblock my work&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Beam Community:&lt;/strong> From my first ever contribution to Beam &lt;a href="https://github.com/apache/beam/issues/32840#issuecomment-2424055627">adding FlattenWith and Tee examples to the playground&lt;/a>,
I was welcomed with open arms and felt encouraged to make more contributions.
Also, for their valuable comments on my design documents on the dev mailing list
as well as the PRs&lt;/li>
&lt;li>&lt;strong>Google:&lt;/strong> I would like to genuinely thank Google for introducing me to open
source in &lt;a href="https://summerofcode.withgoogle.com/archive/2023/projects/u7Y9S6sc">GSoC 2023&lt;/a>
and giving me a second chance to interact with Apache Beam through GSoC 2025.
Without it, I probably wouldn&amp;rsquo;t be here writing this blog post, nor would I have
this fruitful experience&lt;/li>
&lt;/ul>
&lt;h2 id="whats-next">What&amp;rsquo;s Next?&lt;/h2>
&lt;p>I am now focusing on helping move the remaining artifacts in this project scope
from the in-progress state to the merging state. After this, I would love to
keep my contributions alive in Beam Python and Go SDK, to name a few. I would
also love to connect with you all on my
&lt;a href="https://www.linkedin.com/in/mohamedawnallah/">LinkedIn&lt;/a> and
&lt;a href="https://github.com/mohamedawnallah">GitHub&lt;/a>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://summerofcode.withgoogle.com/programs/2025/projects/X32yGjqz">Google Summer of Code Project Listing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1YOeK3jb94kSOUxucfqeZL0pkRI08dYljV_4v5SH5i5U/edit?usp=sharing">Original GSoC Proposal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/issues/35046">GSoC 2025 Tracking Issue&lt;/a>&lt;/li>
&lt;/ul></description><link>/blog/gsoc-25-ml-connectors/</link><pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate><guid>/blog/gsoc-25-ml-connectors/</guid><category>blog</category><category>gsoc</category></item><item><title>Google Summer of Code 2025 - Beam YAML, Kafka and Iceberg User Accessibility</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The relatively new Beam YAML SDK was introduced in the spirit of making data processing easy,
but it has gained little adoption for complex ML tasks and hasn’t been widely used with
&lt;a href="https://beam.apache.org/documentation/io/managed-io/">Managed I/O&lt;/a> such as Kafka and Iceberg.
As part of Google Summer of Code 2025, new illustrative, production-ready pipeline examples
of ML use cases with Kafka and Iceberg data sources using the YAML SDK have been developed
to address this adoption gap.&lt;/p>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>The YAML SDK was introduced in Spring 2024 as Beam’s first no-code SDK. It follows a declarative approach
of defining a data processing pipeline using a YAML DSL, as opposed to other programming language specific SDKs.
At the time, it had few meaningful examples and documentation to go along with it. Key missing examples
were ML workflows and integration with the Kafka and Iceberg Managed I/O. Foundational work had already been done
to add support for ML capabilities as well as Kafka and Iceberg IO connectors in the YAML SDK, but there were no
end-to-end examples demonstrating their usage.&lt;/p>
&lt;p>Beam, as well as Kafka and Iceberg, are mainstream big data technologies but they also have a learning curve.
The overall theme of the project is to help democratize data processing for scientists and analysts who traditionally
don’t have a strong background in software engineering. They can now refer to these meaningful examples as the starting point,
helping them onboard faster and be more productive when authoring ML/data pipelines to their use cases with Beam and its YAML DSL.&lt;/p>
&lt;h2 id="contributions">Contributions&lt;/h2>
&lt;p>The data pipelines/workflows developed are production-ready: Kafka and Iceberg data sources are set up on GCP,
and the data used are raw public datasets. The pipelines are tested end-to-end on Google Cloud Dataflow and
are also unit tested to ensure correct transformation logic.&lt;/p>
&lt;p>Delivered pipelines/workflows, each with documentation as README.md, address 4 main ML use cases below:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Streaming Classification Inference&lt;/strong>: A streaming ML pipeline that demonstrates Beam YAML capability to perform
classification inference on a stream of incoming data from Kafka. The overall workflow also includes
DistilBERT model deployment and serving on Google Cloud Vertex AI where the pipeline can access for remote inferences.
The pipeline is applied to a sentiment analysis task on a stream of YouTube comments, preprocessing data and classifying
whether a comment is positive or negative. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/sentiment_analysis/streaming_sentiment_analysis.yaml">pipeline&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/sentiment_analysis">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Streaming Regression Inference&lt;/strong>: A streaming ML pipeline that demonstrates Beam YAML capability to perform
regression inference on a stream of incoming data from Kafka. The overall workflow also includes
custom model training, deployment and serving on Google Cloud Vertex AI where the pipeline can access for remote inferences.
The pipeline is applied to a regression task on a stream of taxi rides, preprocessing data and predicting the fare amount
for every ride. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/taxi_fare/streaming_taxifare_prediction.yaml">pipeline&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/taxi_fare">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Batch Anomaly Detection&lt;/strong>: A ML workflow that demonstrates ML-specific transformations
and reading from/writing to Iceberg IO. The workflow contains unsupervised model training and several pipelines that leverage
Iceberg for storing results, BigQuery for storing vector embeddings and MLTransform for computing embeddings to demonstrate
an end-to-end anomaly detection workflow on a dataset of system logs. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/log_analysis/batch_log_analysis.sh">workflow&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/log_analysis">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Feature Engineering &amp;amp; Model Evaluation&lt;/strong>: A ML workflow that demonstrates Beam YAML capability to do feature engineering
which is subsequently used for model evaluation, and its integration with Iceberg IO. The workflow contains model training
and several pipelines, showcasing an end-to-end Fraud Detection MLOps solution that generates features and evaluates models
to detect credit card transaction frauds. See &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/examples/transforms/ml/fraud_detection/fraud_detection_mlops_beam_yaml_sdk.ipynb">workflow&lt;/a> and &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/yaml/examples/transforms/ml/fraud_detection">documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>The main challenge of the project was a lack of previous YAML pipeline examples and good documentation to rely on.
Unlike the Python or Java SDKs where there are already many notebooks and end-to-end examples demonstrating various use cases,
the examples for YAML SDK only involved simple transformations such as filter, group by, etc. More complex transforms like
&lt;code>MLTransform&lt;/code> and &lt;code>ReadFromIceberg&lt;/code> had no examples and requires configurations that didn&amp;rsquo;t have clear API reference at the time.
As a result, there were a lot of deep dives into the actual implementation of the PTransforms across YAML, Python and Java SDKs to
understand the error messages and how to correctly use the transforms.&lt;/p>
&lt;p>Another challenge was writing unit tests for the pipeline to ensure that the pipeline’s logic is correct.
It was a learning curve to understand how the existing test suite is set up and how it can be used to write unit tests for
the data pipelines. A lot of time was spent on properly writing mocks for the pipeline&amp;rsquo;s sources and sinks, as well as for the
transforms that require external services such as Vertex AI.&lt;/p>
&lt;h2 id="conclusion--personal-thoughts">Conclusion &amp;amp; Personal Thoughts&lt;/h2>
&lt;p>These production-ready pipelines demonstrate the potential of Beam YAML SDK to author complex ML workflows
that interact with Iceberg and Kafka. The examples are a nice addition to Beam, especially with Beam 3.0.0 milestones
coming up where low-code/no-code, ML capabilities and Managed I/O are focused on.&lt;/p>
&lt;p>I had an amazing time working with the big data technologies Beam, Iceberg, and Kafka as well as many Google Cloud services
(Dataflow, Vertex AI and Google Kubernetes Engine, to name a few). I’ve always wanted to work more in the ML space, and this
experience has been a great growth opportunity for me. Google Summer of Code this year has been selective, and the project&amp;rsquo;s success
would not have been possible without the support of my mentor, Chamikara Jayalath. It&amp;rsquo;s been a pleasure working closely
with him and the broader Beam community to contribute to this open-source project that has a meaningful impact on the
data engineering community.&lt;/p>
&lt;p>My advice for future Google Summer of Code participants is to first and foremost research and choose a project that aligns closely
with your interest. Most importantly, spend a lot of time making yourself visible and writing a good proposal when the program
is opened for applications. Being visible (e.g. by sharing your proposal, or generally any ideas and questions on the project&amp;rsquo;s
communication channel early on) makes it more likely for you to be selected; and a good proposal not only will make you even
more likely to be in the program, but also give you a lot of confidence when contributing to and completing the project.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://summerofcode.withgoogle.com/programs/2025/projects/f4kiDdus">Google Summer of Code Project Listing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1MSAVF6X9ggtVZbqz8YJGmMgkolR_dve0Lr930cByyac/edit?usp=sharing">Google Summer of Code Final Report&lt;/a>&lt;/li>
&lt;/ul></description><link>/blog/gsoc-25-yaml-user-accessibility/</link><pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate><guid>/blog/gsoc-25-yaml-user-accessibility/</guid><category>blog</category><category>gsoc</category></item><item><title>Apache Beam 2.68.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.68.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2680-2025-09-??">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.68.0, check out the &lt;a href="https://github.com/apache/beam/milestone/36?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>[Python] Prism runner now enabled by default for most Python pipelines using the direct runner (&lt;a href="https://github.com/apache/beam/pull/34612">#34612&lt;/a>). This may break some tests, see &lt;a href="https://github.com/apache/beam/pull/34612">https://github.com/apache/beam/pull/34612&lt;/a> for details on how to handle issues.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Upgraded Iceberg dependency to 1.9.2 (&lt;a href="https://github.com/apache/beam/pull/35981">#35981&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>BigtableRead Connector for BeamYaml added with new Config Param (&lt;a href="https://github.com/apache/beam/pull/35696">#35696&lt;/a>)&lt;/li>
&lt;li>MongoDB Java driver upgraded from 3.12.11 to 5.5.0 with API refactoring and GridFS implementation updates (Java) (&lt;a href="https://github.com/apache/beam/pull/35946">#35946&lt;/a>).&lt;/li>
&lt;li>Introduced a dedicated module for JUnit-based testing support: &lt;code>sdks/java/testing/junit&lt;/code>, which provides &lt;code>TestPipelineExtension&lt;/code> for JUnit 5 while maintaining backward compatibility with existing JUnit 4 &lt;code>TestRule&lt;/code>-based tests (Java) (&lt;a href="https://github.com/apache/beam/issues/18733">#18733&lt;/a>, &lt;a href="https://github.com/apache/beam/pull/35688">#35688&lt;/a>).
&lt;ul>
&lt;li>To use JUnit 5 with Beam tests, add a test-scoped dependency on &lt;code>org.apache.beam:beam-sdks-java-testing-junit&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Google CloudSQL enrichment handler added (Python) (&lt;a href="https://github.com/apache/beam/pull/34398">#34398&lt;/a>).
Beam now supports data enrichment capabilities using SQL databases, with built-in support for:
&lt;ul>
&lt;li>Managed PostgreSQL, MySQL, and Microsoft SQL Server instances on CloudSQL&lt;/li>
&lt;li>Unmanaged SQL database instances not hosted on CloudSQL (e.g., self-hosted or on-premises databases)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>[Python] Added the &lt;code>ReactiveThrottler&lt;/code> and &lt;code>ThrottlingSignaler&lt;/code> classes to streamline throttling behavior in DoFns, expose throttling mechanisms for users (&lt;a href="https://github.com/apache/beam/pull/35984">#35984&lt;/a>)&lt;/li>
&lt;li>Added a pipeline option to specify the processing timeout for a single element by any PTransform (Java/Python/Go) (&lt;a href="https://github.com/apache/beam/issues/35174">#35174&lt;/a>).
&lt;ul>
&lt;li>When specified, the SDK harness automatically restarts if an element takes too long to process. Beam runner may then retry processing of the same work item.&lt;/li>
&lt;li>Use the &lt;code>--element_processing_timeout_minutes&lt;/code> option to reduce the chance of having stalled pipelines due to unexpected cases of slow processing, where slowness might not happen again if processing of the same element is retried.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>(Python) Adding GCP Spanner Change Stream support for Python (apache_beam.io.gcp.spanner) (&lt;a href="https://github.com/apache/beam/issues/24103">#24103&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Previously deprecated Beam ZetaSQL component has been removed (&lt;a href="https://github.com/apache/beam/issues/34423">#34423&lt;/a>).
ZetaSQL users could migrate to Calcite SQL with BigQuery dialect enabled.&lt;/li>
&lt;li>Upgraded Beam vendored Calcite to 1.40.0 for Beam SQL (&lt;a href="https://github.com/apache/beam/issues/35483">#35483&lt;/a>), which
improves support for BigQuery and other SQL dialects. Note: Minor behavior changes are observed such as output
significant digits related to casting.&lt;/li>
&lt;li>(Python) The deterministic fallback coder for complex types like NamedTuple, Enum, and dataclasses now uses cloudpickle instead of dill. If your pipeline is affected, you may see a warning like: &amp;ldquo;Using fallback deterministic coder for type X&amp;hellip;&amp;rdquo;. You can revert to the previous behavior by using the pipeline option &lt;code>--update_compatibility_version=2.67.0&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/35725">35725&lt;/a>). Report any pickling related issues to &lt;a href="https://github.com/apache/beam/issues/34903">#34903&lt;/a>&lt;/li>
&lt;li>(Python) Prism runner now enabled by default for most Python pipelines using the direct runner (&lt;a href="https://github.com/apache/beam/pull/34612">#34612&lt;/a>). This may break some tests, see &lt;a href="https://github.com/apache/beam/pull/34612">https://github.com/apache/beam/pull/34612&lt;/a> for details on how to handle issues.&lt;/li>
&lt;li>Dropped Java 8 support for &lt;a href="https://central.sonatype.com/artifact/org.apache.beam/beam-sdks-java-io-expansion-service">IO expansion-service&lt;/a>. Cross-language pipelines using this expansion service will need a Java11+ runtime (&lt;a href="https://github.com/apache/beam/pull/35981">#35981&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Python SDK native SpannerIO (apache_beam/io/gcp/experimental/spannerio) is deprecated. Use cross-language wrapper
(apache_beam/io/gcp/spanner) instead (Python) (&lt;a href="https://github.com/apache/beam/issues/35860">#35860&lt;/a>).&lt;/li>
&lt;li>Samza runner is deprecated and scheduled for removal in Beam 3.0 (&lt;a href="https://github.com/apache/beam/issues/35448">#35448&lt;/a>).&lt;/li>
&lt;li>Twister2 runner is deprecated and scheduled for removal in Beam 3.0 (&lt;a href="https://github.com/apache/beam/issues/35905">#35905&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>(Python) Fixed Java YAML provider fails on Windows (&lt;a href="https://github.com/apache/beam/issues/35617">#35617&lt;/a>).&lt;/li>
&lt;li>Fixed BigQueryIO creating temporary datasets in wrong project when temp_dataset is specified with a different project than the pipeline project. For some jobs, temporary datasets will now be created in the correct project (Python) (&lt;a href="https://github.com/apache/beam/issues/35813">#35813&lt;/a>).&lt;/li>
&lt;li>(Go) Fix duplicates due to reads after blind writes to Bag State (&lt;a href="https://github.com/apache/beam/issues/35869">#35869&lt;/a>).
&lt;ul>
&lt;li>Earlier Go SDK versions can avoid the issue by not reading in the same call after a blind write.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.68.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud, Andrew Crites, Ashok Devireddy, Chamikara Jayalath, Charles Nguyen, Danny McCormick, Davda James, Derrick Williams, Diego Hernandez, Dip Patel, Dustin Rhodes, Enrique Calderon, Hai Joey Tran, Jack McCluskey, Kenneth Knowles, Keshav, Khorbaladze A., LEEKYE, Lanny Boarts, Mattie Fu, Minbo Bae, Mohamed Awnallah, Naireen Hussain, Nathaniel Young, Radosław Stankiewicz, Razvan Culea, Robert Bradshaw, Robert Burke, Sam Whittle, Shehab, Shingo Furuyama, Shunping Huang, Steven van Rossum, Suvrat Acharya, Svetak Sundhar, Tarun Annapareddy, Tom Stepp, Valentyn Tymofieiev, Vitaly Terentyev, XQ Hu, Yi Hu, apanich, arnavarora2004, claudevdm, flpablo, kristynsmith, shreyakhajanchi&lt;/p></description><link>/blog/beam-2.68.0/</link><pubDate>Mon, 22 Sep 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.68.0/</guid><category>blog</category><category>release</category></item><item><title>Google Summer of Code 25 - Improving Apache Beam's Infrastructure</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>I loved contributing to Apache Beam during Google Summer of Code 2025. I worked on improving the infrastructure of Apache Beam, which included enhancing the CI/CD pipelines, automating various tasks, and improving the overall developer experience.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Since I was in high school, I have been fascinated by computers, but when I discovered Open Source, I was amazed by the idea of people from all around the world collaborating to build software that anyone can use, just for the love of it. I started participating in open source communities, and I found it to be a great way to learn and grow as a developer.&lt;/p>
&lt;p>When I heard about Google Summer of Code, I saw it as an opportunity to take my open source contributions to the next level. The idea of working on a real-world project while being mentored by experienced developers sounded like an amazing opportunity. I heard about Apache Beam from another contributor and ex-GSoC participant, and I was immediately drawn to the project, specifically on the infrastructure side of things, as I have a strong interest in DevOps and automation.&lt;/p>
&lt;h2 id="the-challenge">The Challenge&lt;/h2>
&lt;p>When searching for a project, I was told that Apache Beam&amp;rsquo;s infrastructure had several areas that could be improved. I was excited because the ideas were focused on improving the developer experience, and creating tools that could benefit not only Beam&amp;rsquo;s developers but also the wider open source community.&lt;/p>
&lt;p>There were four main challenges:&lt;/p>
&lt;ol>
&lt;li>Automating the cleanup of unused cloud resources to reduce costs and improve resource management.&lt;/li>
&lt;li>Implementing a system for managing permissions through Git, allowing for better tracking and auditing of changes.&lt;/li>
&lt;li>Creating a tool for rotating service account keys to enhance security.&lt;/li>
&lt;li>Developing a security monitoring system to detect and respond to potential threats.&lt;/li>
&lt;/ol>
&lt;h2 id="the-solution">The Solution&lt;/h2>
&lt;p>I worked closely with my mentor to break down and define each challenge into manageable tasks, creating a plan for the summer. I started by taking a look at the current state of the infrastructure, after which I began working on each challenge one by one.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Automating the cleanup of unused cloud resources:&lt;/strong> We noticed that some resources in the GCP project, especially Pub/Sub topics created for testing, were often forgotten, leading to unnecessary costs. Since the infrastructure is primarily for testing and development, there&amp;rsquo;s no need to keep unused resources. I developed a Python script that identifies and removes stale Pub/Sub topics that have existed for too long. This tool is now scheduled to run periodically via a GitHub Actions workflow to keep the project tidy and cost-effective.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Implementing a system for managing permissions through Git:&lt;/strong> This was more challenging, as it required a good understanding of both GCP IAM and the existing workflow. After some investigation, I learned that the current process was mostly manual and error-prone. The task involved creating a more automated and reliable system. This was achieved by using Terraform to define the desired state of IAM roles and permissions in code, which allows for better tracking and auditing of changes. This also included some custom roles, but that is still a work in progress.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Creating a tool for rotating service account keys:&lt;/strong> Key rotation is a security practice that we don&amp;rsquo;t always follow, but it is essential to ensure that service account keys are not compromised. I noticed that GCP had some APIs that could help with this, but the rotation process itself was not automated. So I wrote a Python script that automates the rotation of GCP service account keys, enhancing the security of service account credentials.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Developing a security monitoring system:&lt;/strong> To keep track of incorrect usage and potential threats, I built a log analysis tool that monitors GCP audit logs for suspicious activity, collecting and parsing logs to identify potential security threats, delivering email alerts when something unusual is detected.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>As an extra, and after noticing that some of these tools and policies could be ignored by developers, we also came up with the idea of an enforcement module to ensure the usage of these new tools and policies. This module would be integrated into the CI/CD pipeline, checking for compliance with the new infrastructure policies and notifying developers of any violations.&lt;/p>
&lt;h2 id="the-impact">The Impact&lt;/h2>
&lt;p>The tools developed during this project will have an impact on the Apache Beam community and the wider open source community. The automation of resource cleanup will help reduce costs and improve resource management, while the permission management system will provide better tracking and auditing of changes. The service account key rotation tool will enhance security, and the security monitoring system will help detect and respond to potential threats.&lt;/p>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>This project has been an incredible learning experience for me. I have gained a better understanding of how GCP works, as well as how to use Terraform and GitHub Actions. I have also learned a lot about security best practices and how to implement them in a real-world project.&lt;/p>
&lt;p>I also learned a lot about working in an open source community, having direct communication with such experienced developers, and the importance of collaboration and communication in a distributed team. I am grateful for the opportunity to work on such an important project and to contribute to the Apache Beam community.&lt;/p>
&lt;p>Finally, a special thanks to my mentor, Pablo Estrada, for his guidance and support throughout the summer. I am grateful not only for his amazing technical skills but especially for his patience and encouragement on my journey contributing to open source.&lt;/p>
&lt;p>You can find my final report &lt;a href="https://gist.github.com/ksobrenat32/b028b8303393afbe73a8fc5e17daff90">here&lt;/a> if you want to take a look at the details of my work.&lt;/p>
&lt;h2 id="advice-for-future-participants">Advice for Future Participants&lt;/h2>
&lt;p>If you are considering participating in Google Summer of Code, my advice would be to choose an area you are passionate about; this will make any coding challenge easier to overcome. Also, don&amp;rsquo;t be afraid to ask questions and seek help from your mentors and the community. At the start, I made that mistake, and I learned that asking for help is a sign of strength, not weakness.&lt;/p>
&lt;p>Finally, make sure to manage your time effectively and stay organized (keeping a progress journal is a great idea). GSoC is a great opportunity to learn and grow as a developer, but it can also be time-consuming, so it&amp;rsquo;s important to stay focused and on track.&lt;/p></description><link>/blog/gsoc-25-infra/</link><pubDate>Mon, 15 Sep 2025 00:00:00 -0600</pubDate><guid>/blog/gsoc-25-infra/</guid><category>blog</category><category>gsoc</category></item><item><title>Apache Beam 2.67.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.67.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2670-2025-08-12">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.67.0, check out the &lt;a href="https://github.com/apache/beam/milestone/35?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Debezium IO upgraded to 3.1.1 requires Java 17 (Java) (&lt;a href="https://github.com/apache/beam/issues/34747">#34747&lt;/a>).&lt;/li>
&lt;li>Add support for streaming writes in IOBase (Python)&lt;/li>
&lt;li>Implement support for streaming writes in FileBasedSink (Python)&lt;/li>
&lt;li>Expose support for streaming writes in TextIO (Python)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Added support for Processing time Timer in the Spark Classic runner (&lt;a href="https://github.com/apache/beam/issues/33633">#33633&lt;/a>).&lt;/li>
&lt;li>Add pip-based install support for JupyterLab Sidepanel extension (&lt;a href="https://github.com/apache/beam/issues/35397">#35397&lt;/a>).&lt;/li>
&lt;li>[IcebergIO] Create tables with a specified table properties (&lt;a href="https://github.com/apache/beam/pull/35496">#35496&lt;/a>)&lt;/li>
&lt;li>Add support for comma-separated options in Python SDK (Python) (&lt;a href="https://github.com/apache/beam/pull/35580">#35580&lt;/a>).
Python SDK now supports comma-separated values for experiments and dataflow_service_options,
matching Java SDK behavior while maintaining backward compatibility.&lt;/li>
&lt;li>Milvus enrichment handler added (Python) (&lt;a href="https://github.com/apache/beam/pull/35216">#35216&lt;/a>).
Beam now supports Milvus enrichment handler capabilities for vector, keyword,
and hybrid search operations.&lt;/li>
&lt;li>[Beam SQL] Add support for DATABASEs, with an implementation for Iceberg (&lt;a href="https://github.com/apache/beam/issues/35637">#35637&lt;/a>)&lt;/li>
&lt;li>Respect BatchSize and MaxBufferingDuration when using &lt;code>JdbcIO.WriteWithResults&lt;/code>. Previously, these settings were ignored (&lt;a href="https://github.com/apache/beam/pull/35669">#35669&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Go: The pubsubio.Read transform now accepts ReadOptions as a value type instead of a pointer, and requires exactly one of Topic or Subscription to be set (they are mutually exclusive). Additionally, the ReadOptions struct now includes a Topic field for specifying the topic directly, replacing the previous topic parameter in the Read function signature (&lt;a href="https://github.com/apache/beam/pull/35369">#35369&lt;/a>).&lt;/li>
&lt;li>SQL: The &lt;code>ParquetTable&lt;/code> external table provider has changed its handling of the &lt;code>LOCATION&lt;/code> property. To read from a directory, the path must now end with a trailing slash (e.g., &lt;code>LOCATION '/path/to/data/'&lt;/code>). Previously, a trailing slash was not required. This change was made to enable support for glob patterns and single-file paths (&lt;a href="https://github.com/apache/beam/pull/35582">#35582&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>[YAML] Fixed handling of missing optional fields in JSON parsing (&lt;a href="https://github.com/apache/beam/issues/35179">#35179&lt;/a>).&lt;/li>
&lt;li>[Python] Fix WriteToBigQuery transform using CopyJob does not work with WRITE_TRUNCATE write disposition (&lt;a href="https://github.com/apache/beam/issues/34247">#34247&lt;/a>)&lt;/li>
&lt;li>[Python] Fixed dicomio tags mismatch in integration tests (&lt;a href="https://github.com/apache/beam/issues/30760">#30760&lt;/a>).&lt;/li>
&lt;li>[Java] Fixed spammy logging issues that affected versions 2.64.0 to 2.66.0.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>(&lt;a href="https://github.com/apache/beam/issues/35666">#35666&lt;/a>). YAML Flatten incorrectly drops fields when input PCollections&amp;rsquo; schema are different. This issue exists for all versions since 2.52.0.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.67.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aditya Shukla, Ahmed Abualsaud, Arun Pandian, Boris Li, Chamikara Jayalath, Charles Nguyen, Chenzo, Danny McCormick, David Adeniji, Derrick Williams, Dmytro Tsyliuryk, Dustin Rhodes, Enrique Calderon, Gottipati Gautam, Hai Joey Tran, Hunor Portik, Jack McCluskey, Kenneth Knowles, Khorbaladze A., Marcio Sugar, Minh Son Nguyen, Mohamed Awnallah, Nathaniel Young, Nhon Dinh, Quentin Sommer, Rafael Raposo, Rakesh Kumar, Razvan Culea, Reuven Lax, Robert Bradshaw, Sam Whittle, Shunping Huang, Steven van Rossum, Talat UYARER, Tanu Sharma, Tarun Annapareddy, Tobi Kaymak, Tobias Kaymak, Valentyn Tymofieiev, Veronica Wasson, Vitaly Terentyev, XQ Hu, Yi Hu, akashorabek, arnavarora2004, changliiu, claudevdm, fozzie15, mvhensbergen, twosom&lt;/p></description><link>/blog/beam-2.67.0/</link><pubDate>Tue, 12 Aug 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.67.0/</guid><category>blog</category><category>release</category></item><item><title>Our Experience at Beam College 2025: 1st Place Hackathon Winners</title><description>
&lt;!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements. See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="introduction-the-beam-of-an-idea">Introduction: The Beam of an Idea&lt;/h2>
&lt;p>In the world of machine learning for healthcare, preprocessing large pathology image datasets at scale remains a bottleneck. Whole Slide Images (WSIs) in medical imaging can reach massive sizes. Traditional Python tools (PIL, etc.) fail under memory pressure, especially when handling thousands of such high-resolution images. This becomes a bottleneck for ML modeling tasks using standard tools.&lt;/p>
&lt;p>Having previously worked on image processing for object detection in machine learning, we also understood how crucial it is to preprocess and structure image data correctly for downstream tasks. These challenges are non-trivial and even more critical in healthcare, making it a natural and high-impact use case for scalable data processing frameworks like Apache Beam.&lt;/p>
&lt;p>So, in the &lt;a href="https://beamcollege.dev/hackathon/">Beam Summit 2025 Hackathon&lt;/a>, we joined as team &amp;ldquo;PCollectors&amp;rdquo; with the goal to leverage Beam to process large image data and convert it to a format suitable for downstream ML tasks. We were amazed to know that we secured 1st place with the implemented solution!&lt;/p>
&lt;h2 id="the-project-scalable-wsi-preprocessing-beam-pipeline">The Project: Scalable WSI Preprocessing Beam Pipeline&lt;/h2>
&lt;p>&lt;a href="https://github.com/adityashukla8/medical_image_processing_beam">GitHub Repo&lt;/a>&lt;/p>
&lt;h3 id="the-goal">The Goal&lt;/h3>
&lt;p>The primary objective of the pipeline was to process patient data (CSV) &amp;amp; WSIs, extract embeddings, combine the metadata, and output the final dataset in TFRecord format, ready for large-scale ML training.&lt;/p>
&lt;h3 id="solution-overview">Solution Overview&lt;/h3>
&lt;p>Our pipeline processes:&lt;/p>
&lt;ul>
&lt;li>Patient metadata (CSV)&lt;/li>
&lt;li>WSI files (.tif)&lt;/li>
&lt;li>Split the images into “tiles”&lt;/li>
&lt;li>Extract filtered image tiles based on the background threshold&lt;/li>
&lt;li>Generate max &amp;amp; avg embeddings per patient using EfficientNet&lt;/li>
&lt;li>Merge metadata + embeddings into TFRecords&lt;/li>
&lt;/ul>
&lt;p>All in a scalable, memory-efficient, cloud-native pipeline using Apache Beam and Dataflow.&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>Source: Mayo Clinic STRIP AI Dataset (Kaggle)
Metadata: Each row = { image_id, center_id, patient_id, image_num, label }
Multiple images per patient
Labels exist only at the patient level
Images:
High-res .tif pathology slides&lt;/p>
&lt;h3 id="tech-stack">Tech Stack&lt;/h3>
&lt;ul>
&lt;li>Apache Beam: Orchestration engine&lt;/li>
&lt;li>Google Cloud Dataflow: Scalable runner&lt;/li>
&lt;li>Google Cloud Storage: Input TIFFs + output TFRecords&lt;/li>
&lt;li>TensorFlow: For embedding generation (EfficientNet) and TFRecord serialization&lt;/li>
&lt;/ul>
&lt;h2 id="the-hackathon-journey">The Hackathon Journey&lt;/h2>
&lt;p>Participating in the hackathon introduced us to multiple new things and allowed us to learn and implement simultaneously. Through the hackathon weekend, we:&lt;/p>
&lt;ul>
&lt;li>Designed the end-to-end pipeline&lt;/li>
&lt;li>Integrated pyvips + openslide for efficient image loading&lt;/li>
&lt;li>Used Beam&amp;rsquo;s RunInference API with TensorFlow&lt;/li>
&lt;li>Tiled and filtered images&lt;/li>
&lt;li>Wrote patient-level embeddings to TFRecords&lt;/li>
&lt;/ul>
&lt;h2 id="what-we-learnt">What we Learnt&lt;/h2>
&lt;p>Apache Beam is really powerful for parallel and cloud-native ML preprocessing.
Dataflow is the go-to tool when processing large data, like medical images&lt;/p>
&lt;h2 id="whats-next-for-the-project">What’s Next for The Project&lt;/h2>
&lt;p>Looking ahead, the pipeline can be extended beyond fixed-size tiling by incorporating image segmentation techniques to generate more meaningful patches based on tissue regions. This approach can improve ML model performance by focusing only on relevant areas. Moreover, the same preprocessing framework can be adapted for video data, where frames can be treated as time-indexed image slices, effectively enabling temporal modeling for time-series tasks such as motion analysis or progression tracking. Finally, we plan to adapt this pipeline to multiple downstream use cases for AI in healthcare by combining histology images with genomic data, clinical notes, or radiology scans, paving the way for more comprehensive and context-aware models in biomedical machine learning.&lt;/p>
&lt;p>&lt;strong>Project Submission Demo&lt;/strong>: &lt;a href="https://drive.google.com/file/d/1Os5SvgqHiqfMkoCWOuaVvEPXsnhqXlLx/view?usp=sharing">Beam Demo - PCollectors.mp4&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We are ML Engineers, working at &lt;a href="www.intuitive.cloud">Intuitive.Cloud&lt;/a>, where we play around with large-scale data to build scalable, efficient, dynamic data processing pipelines that prepare it for downstream ML tasks, with Apache Beam and Google Cloud DataFlow being the central pieces.&lt;/p>
&lt;p>Participating in the hackathon was a great learning opportunity, huge thanks to the organizers, mentors, and the Apache Beam community!&lt;/p>
&lt;p>- &lt;a href="https://www.linkedin.com/in/adityashukla8/">Aditya Shukla&lt;/a> &amp;amp; &lt;a href="https://in.linkedin.com/in/darshan-kanade-0797851b3">Darshan Kanade&lt;/a>&lt;/p></description><link>/blog/beam-summit-2025-hackathon-pcollectors-blog/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>/blog/beam-summit-2025-hackathon-pcollectors-blog/</guid><category>blog</category></item><item><title>Apache Beam 2.66.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.66.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2660-2025-07-01">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.66.0, check out the &lt;a href="https://github.com/apache/beam/milestone/30?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="beam-300-development-highlights">Beam 3.0.0 Development Highlights&lt;/h2>
&lt;ul>
&lt;li>[Java] Java 8 support is now deprecated. It is still supported until Beam 3.
From now, pipeline submitted by Java 8 client uses Java 11 SDK container for
remote pipeline execution (&lt;a href="https://github.com/apache/beam/pull/35064">35064&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>[Python] Several quality-of-life improvements to the vLLM model handler. If you use Beam RunInference with vLLM model handlers, we strongly recommend updating past this release.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>[IcebergIO] Now available with Beam SQL! (&lt;a href="https://github.com/apache/beam/pull/34799">#34799&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Support reading with column pruning (&lt;a href="https://github.com/apache/beam/pull/34856">#34856&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Support reading with pushdown filtering (&lt;a href="https://github.com/apache/beam/pull/34827">#34827&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Create tables with a specified partition spec (&lt;a href="https://github.com/apache/beam/pull/34966">#34966&lt;/a>, &lt;a href="https://github.com/apache/beam/pull/35268">#35268&lt;/a>)&lt;/li>
&lt;li>[IcebergIO] Dynamically create namespaces if needed (&lt;a href="https://github.com/apache/beam/pull/35228">#35228&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>[Beam SQL] Introducing Beam Catalogs (&lt;a href="https://github.com/apache/beam/pull/35223">#35223&lt;/a>)&lt;/li>
&lt;li>Adding Google Storage Requests Pays feature (Golang)(&lt;a href="https://github.com/apache/beam/issues/30747">#30747&lt;/a>).&lt;/li>
&lt;li>[Python] Prism runner now auto-enabled for some Python pipelines using the direct runner (&lt;a href="https://github.com/apache/beam/pull/34921">#34921&lt;/a>).&lt;/li>
&lt;li>[YAML] WriteToTFRecord and ReadFromTFRecord Beam YAML support&lt;/li>
&lt;li>Python: Added JupyterLab 4.x extension compatibility for enhanced notebook integration (&lt;a href="https://github.com/apache/beam/pull/34495">#34495&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Yapf version upgraded to 0.43.0 for formatting (Python) (&lt;a href="https://github.com/apache/beam/pull/34801/">#34801&lt;/a>).&lt;/li>
&lt;li>Python: Added JupyterLab 4.x extension compatibility for enhanced notebook integration (&lt;a href="https://github.com/apache/beam/pull/34495">#34495&lt;/a>).&lt;/li>
&lt;li>Python: Argument abbreviation is no longer enabled within Beam. If you previously abbreviated arguments (e.g. &lt;code>--r&lt;/code> for &lt;code>--runner&lt;/code>), you will now need to specify the whole argument (&lt;a href="https://github.com/apache/beam/pull/34934">#34934&lt;/a>).&lt;/li>
&lt;li>Java: Users of ReadFromKafkaViaSDF transform might encounter pipeline graph compatibility issues when updating the pipeline. To mitigate, set the &lt;code>updateCompatibilityVersion&lt;/code> option to the SDK version used for the original pipeline, example &lt;code>--updateCompatabilityVersion=2.64.0&lt;/code>&lt;/li>
&lt;li>Python: Updated &lt;code>AlloyDBVectorWriterConfig&lt;/code> API to align with new &lt;code>PostgresVectorWriter&lt;/code> transform. Heres a quick guide to update your code: (&lt;a href="https://github.com/apache/beam/issues/35225">#35225&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>(Java) Fixed CassandraIO ReadAll does not let a pipeline handle or retry exceptions (&lt;a href="https://github.com/apache/beam/pull/34191">#34191&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed vLLM model handlers breaking Beam logging. (&lt;a href="https://github.com/apache/beam/pull/35053">#35053&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed vLLM connection leaks that caused a throughput bottleneck and underutilization of GPU (&lt;a href="https://github.com/apache/beam/pull/35053">#35053&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed vLLM server recovery mechanism in the event of a process termination (&lt;a href="https://github.com/apache/beam/pull/35234">#35234&lt;/a>).&lt;/li>
&lt;li>(Python) Fixed cloudpickle overwriting class states every time loading a same object of dynamic class (&lt;a href="https://github.com/apache/beam/issues/35062">#35062&lt;/a>).&lt;/li>
&lt;li>[Python] Fixed pip install apache-beam[interactive] causes crash on google colab (&lt;a href="https://github.com/apache/beam/pull/35148">#35148&lt;/a>).&lt;/li>
&lt;li>[IcebergIO] Fixed Beam &amp;lt;-&amp;gt; Iceberg conversion logic for arrays of structs and maps of structs (&lt;a href="https://github.com/apache/beam/pull/35230">#35230&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;p>N/A&lt;/p>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.66.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aditya Yadav, Adrian Stoll, Ahmed Abualsaud, Bhargavkonidena, Chamikara Jayalath, Charles Nguyen, Chenzo, Damon, Danny McCormick, Derrick Williams, Enrique Calderon, Hai Joey Tran, Jack McCluskey, Kenneth Knowles, Leonardo Cesar Borges, Michael Gruschke, Minbo Bae, Minh Son Nguyen, Niel Markwick, Radosław Stankiewicz, Rakesh Kumar, Robert Bradshaw, S. Veyrié, Sam Whittle, Shubham Jaiswal, Shunping Huang, Steven van Rossum, Tanu Sharma, Vardhan Thigle, Vitaly Terentyev, XQ Hu, Yi Hu, akashorabek, atask-g, atognolag, bullet03, changliiu, claudevdm, fozzie15, ikarapanca, kristynsmith, Pablo Rodriguez Defino, tvalentyn, twosom, wollowizard&lt;/p></description><link>/blog/beam-2.66.0/</link><pubDate>Tue, 01 Jul 2025 15:00:00 -0500</pubDate><guid>/blog/beam-2.66.0/</guid><category>blog</category><category>release</category></item><item><title>My Experience at Beam College 2025: 3rd Place Hackathon Winner</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="introduction-the-spark-of-an-idea">Introduction: The Spark of an Idea&lt;/h2>
&lt;p>In 2025, I had the opportunity to participate in the &lt;a href="https://beamcollege.dev/hackathon/">Beam College Hackathon&lt;/a>, a fantastic event that brings together students and professionals to explore the power of Apache Beam.&lt;/p>
&lt;p>For my project, I built &lt;strong>&lt;a href="https://github.com/msugar/anomaflow">Anomaflow&lt;/a>&lt;/strong>, an anomaly detection pipeline using &lt;strong>Apache Beam&lt;/strong> and &lt;strong>Google Cloud Dataflow&lt;/strong>. It was my first public hackathon, and the experience was both rewarding and creatively energizing. I’m proud to share that Anomaflow earned &lt;strong>3rd place&lt;/strong> in the competition.&lt;/p>
&lt;h2 id="the-project-building-anomaflow">The Project: Building Anomaflow&lt;/h2>
&lt;h3 id="goal">Goal&lt;/h3>
&lt;p>The primary objective of Anomaflow was to process &lt;strong>host telemetry data&lt;/strong> from systems monitored by OpenTelemetry agents. The long-term goal is to detect anomalies in near real time, which has important applications in cybersecurity.&lt;/p>
&lt;h3 id="tech-stack">Tech Stack&lt;/h3>
&lt;p>Here’s a breakdown of the technologies used:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Apache Beam (Python SDK)&lt;/strong> for pipeline development&lt;/li>
&lt;li>&lt;strong>Bindplane Collector &amp;amp; Server&lt;/strong> for OpenTelemetry data collection and configuration&lt;/li>
&lt;li>&lt;strong>Google Compute Engine (GCE)&lt;/strong> instancess for hosting the Bindplane Server and Collector&lt;/li>
&lt;li>&lt;strong>Google Cloud Dataflow&lt;/strong> for running the Beam pipeline at scale&lt;/li>
&lt;li>&lt;strong>Google Cloud Storage (GCS)&lt;/strong> as the pipeline’s source and sink during the hackathon&lt;/li>
&lt;li>&lt;strong>Terraform&lt;/strong> to provision GCP infrastructure&lt;/li>
&lt;li>&lt;strong>Docker&lt;/strong> for packaging and deployment&lt;/li>
&lt;/ul>
&lt;h2 id="the-hackathon-journey-from-streaming-vision-to-batch-reality">The Hackathon Journey: From Streaming Vision to Batch Reality&lt;/h2>
&lt;h3 id="the-initial-vision">The Initial Vision&lt;/h3>
&lt;p>The original plan was to create a fully streaming pipeline: the &lt;strong>Bindplane Collector&lt;/strong> running on &lt;strong>GCE&lt;/strong> would upload telemetry files to &lt;strong>GCS&lt;/strong>, which would trigger &lt;strong>notifications via Pub/Sub&lt;/strong>. These notifications would then initiate processing in a &lt;strong>Beam pipeline&lt;/strong>, with enriched results written to &lt;strong>BigQuery&lt;/strong> for analysis and visualization.&lt;/p>
&lt;h3 id="the-pivot">The Pivot&lt;/h3>
&lt;p>However, working solo during a time-limited hackathon meant I had to be pragmatic. I decided to implement a &lt;strong>batch pipeline&lt;/strong> instead, reading from and writing to &lt;strong>GCS buckets&lt;/strong>. This allowed me to deliver a functional MVP while preserving a foundation that can evolve toward the original streaming vision.&lt;/p>
&lt;h3 id="key-learnings">Key Learnings&lt;/h3>
&lt;p>Although I already had some real-world experience with Apache Beam, the hackathon gave me the freedom to explore new patterns and tools in a low-risk environment. It was refreshing to iterate rapidly, test ideas, and push beyond my daily work scope.&lt;/p>
&lt;h2 id="whats-next-for-anomaflow">What’s Next for Anomaflow?&lt;/h2>
&lt;p>Anomaflow is just getting started.&lt;/p>
&lt;p>I plan to evolve the pipeline into a true &lt;strong>streaming system&lt;/strong> using &lt;strong>Pub/Sub&lt;/strong> and &lt;strong>BigQuery&lt;/strong>. I also want to explore &lt;strong>sliding windows&lt;/strong>, &lt;strong>custom anomaly detection models&lt;/strong>, and &lt;strong>alerting mechanisms&lt;/strong>. With its modular design and strong foundation in Beam, Anomaflow will serve as a base for several future cybersecurity analytics tools I have in mind.&lt;/p>
&lt;p>&lt;strong>Watch the demo&lt;/strong>: &lt;a href="https://www.youtube.com/watch?v=dpbOm5ekOTc">https://www.youtube.com/watch?v=dpbOm5ekOTc&lt;/a>&lt;/p>
&lt;h2 id="tips-for-future-participants">Tips for Future Participants&lt;/h2>
&lt;p>If you’re considering joining Beam College next year:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Use the mentors!&lt;/strong> The Beam community professionals volunteering their time are a valuable resource. Ask questions, get feedback.&lt;/li>
&lt;li>&lt;strong>Check out the &lt;a href="https://beam.apache.org/get-started/resources/learning-resources/">Beam learning resources&lt;/a>&lt;/strong>. They’re super helpful, especially for getting started with the Beam model and runners.&lt;/li>
&lt;/ul>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>I’m currently a &lt;strong>Software Architect and Data Engineer at TELUS Security&lt;/strong>, where I work on the &lt;strong>Cybersecurity Analytics and Software Engineering&lt;/strong> team. We design data pipelines to detect and respond to threats at scale.&lt;/p>
&lt;p>Participating in Beam College was a great way to stretch my skills, meet passionate Beam users, and contribute to a vibrant open source community. I’m excited to see what others will build in future editions!&lt;/p>
&lt;p>– &lt;a href="https://www.linkedin.com/in/marcio-sugar/">Marcio Sugar&lt;/a>&lt;/p></description><link>/blog/beam-college-2025-anomaflow/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>/blog/beam-college-2025-anomaflow/</guid><category>blog</category></item></channel></rss>