<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Build a scalable, self-managed streaming infrastructure with Beam and Flink</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog series, &lt;a href="https://www.linkedin.com/in/talatuyarer/">Talat Uyarer (Architect / Senior Principal Engineer)&lt;/a>, &lt;a href="https://www.linkedin.com/in/rishabhkedia/">Rishabh Kedia (Principal Engineer)&lt;/a>, and &lt;a href="https://www.linkedin.com/in/davidqhe/">David He (Engineering Director)&lt;/a> describe how we built a self-managed streaming platform by using Apache Beam and Flink. In this part of the series, we describe why and how we built a large-scale, self-managed streaming infrastructure and services based on Flink by migrating from a cloud managed streaming service. We also outline the learnings for operational scalability and observability, performance, and cost effectiveness. We summarize techniques that we found useful in our journey.&lt;/p>
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink---part-1">Build a scalable, self-managed streaming infrastructure with Flink - part 1&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Palo Alto Networks (PANW) is a leader in cybersecurity, providing products, services and solutions to our customers. Data is the center of our products and services. We stream and store exabytes of data in our data lake, with near real-time ingestion, data transformation, data insertion to data store, and forwarding data to our internal ML-based systems and external SIEM’s. We support multi-tenancy in each component so that we can isolate tenants and provide optimal performance and SLA. Streaming processing plays a critical role in the pipelines.&lt;/p>
&lt;p>In the second part of the series, we provide a more thorough description of the core building blocks of our streaming infrastructure, such as autoscaler. We also give more details about our customizations, which enabled us to build a high-performance, large-scale streaming system. Finally, we explain how we solved challenging problems.&lt;/p>
&lt;h2 id="the-importance-of-self-managed-streaming-infrastructure">The importance of self-managed streaming Infrastructure&lt;/h2>
&lt;p>We built a large-scale data platform on Google Cloud. We used Dataflow as a managed streaming service. With Dataflow, we used the streaming engine running our application using Apache Beam and observability tools such as Cloud Logging and Cloud Monitoring. For more details, see [1]. The system can handle 15 million of events per second and one trillion events daily, at four petabytes of data volume daily. We run about 30,000 Dataflow jobs. Each job can have one or hundreds of workers, depending on the customer’s event throughputs.&lt;/p>
&lt;p>We support various applications using different endpoints: BigQuery data store, HTTPS-based external SIEMs or internal endpoints, Syslog based SIEMs, and Google Cloud Storage endpoints. Our customers and products rely on this data platform to handle cybersecurity postures and reactions. Our streaming infrastructure is highly flexible to add, update, and delete use cases through a streaming job subscription. For example, a customer wants to ingest log events from a firewall device into the data lake buffered in Kafka topics. A streaming job is subscribed to extract and filter the data, transform the data format, and do a streaming insert to our BigQuery data warehouse endpoint in real-time. The customer can use our visualization and dashboard products to view traffic or threads captured by this firewall. The following diagram illustrates the event producer, the use case subscription workflow, and the key components of the streaming platform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/image1.png"
alt="Streaming service design">&lt;/p>
&lt;p>This managed, Dataflow-based streaming infrastructure runs fine, but with some caveats:&lt;/p>
&lt;ol>
&lt;li>Cost is high, because it is a managed service. For the same resources used in a Dataflow application, such as vCPU and memory, the cost is much more expensive than using an open source streaming engine such as Flink running the same Beam application code.&lt;/li>
&lt;li>It&amp;rsquo;s not easy to achieve our latency and SLA goals, because it&amp;rsquo;s difficult to extend features, such as autoscaling based on different applications, endpoints, or different parameters within one application.&lt;/li>
&lt;li>The pipeline only runs on Google Cloud.&lt;/li>
&lt;/ol>
&lt;p>The uniqueness of PANW’s streaming use cases is another reason that we use a self-managed service. We support multi-tenancy. A tenant (a customer) can ingest data at a very high rate (&amp;gt;100k requests per second), or at a very low rate (&amp;lt; 100 requests per second). A Dataflow job runs on VMs instead of Kubernetes, requiring a minimal one vCPU core. With a small tenant, this wastes resources. Our streaming infrastructure supports thousands of jobs, and the CPU utilization is more efficient if we do not have to use one core for a job. It is natural for us to use a streaming engine running on Kubernetes, so that we can allocate minimal resources for a small tenant, for example, using a Google Kubernetes Engine (GKE) pod with ½ or less vCPU core.&lt;/p>
&lt;h2 id="the-choice-of-apache-flink-and-kubernetes">The choice of Apache Flink and Kubernetes&lt;/h2>
&lt;p>In an effort to handle the problems already stated and to find the most efficient solution, we evaluated various streaming frameworks, including Apache Samza, Apache Flink, and Apache Spark, against Dataflow.&lt;/p>
&lt;h3 id="performance">Performance&lt;/h3>
&lt;ul>
&lt;li>One notable factor was Apache Flink’s native Kubernetes support. Unlike Samza, which lacked native Kubernetes support and required Apache Zookeeper for coordination, Flink seamlessly integrated with Kubernetes. This integration eliminated unnecessary complexities. In terms of performance, both Samza and Flink were close competitors.&lt;/li>
&lt;li>Apache Spark, while popular, proved to be significantly slower in our tests. A presentation at the Beam Summit revealed that Apache Beam’s Spark Runner was approximately ten times slower than Native Apache Spark [3]. We could not afford such a drastic performance hit. Rewriting our entire Beam codebase with native Spark was not a viable option, especially given the extensive codebase we had built over the past four years with Apache Beam.&lt;/li>
&lt;/ul>
&lt;h3 id="community">Community&lt;/h3>
&lt;p>The robustness of community support played a pivotal role in our decision making. Dataflow provided excellent support, but we needed assurance in our choice of an open-source framework. Apache Flink’s vibrant community and active contributions from multiple companies offered a level of confidence that was unmatched. This collaborative environment meant that bug identification and fixes were ongoing processes. In fact, in our journey, we have patched our system using many Flink fixes from the community:&lt;/p>
&lt;ul>
&lt;li>We fixed the Google Cloud Storage file reading exceptions by merging Flink 1.15 open source fix &lt;a href="https://issues.apache.org/jira/browse/FLINK-26063?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&amp;amp;focusedCommentId=17504555#comment-17504555">FLINK-26063&lt;/a> (we are using 1.13).&lt;/li>
&lt;li>We fixed an issue with workers restarting for stateful jobs from &lt;a href="https://issues.apache.org/jira/browse/FLINK-31963">FLINK-31963&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>We also contributed to the community during our journey by founding and fixing bugs in the open source code. For details, see &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> for Flink Kubernetes Operator. We also created a new GKE Auth support for Kubernetes clients and merged it to GitHub at [4].&lt;/p>
&lt;h3 id="integration">Integration&lt;/h3>
&lt;p>The seamless integration of Apache Flink with Kubernetes provided us with a flexible and scalable platform for orchestration. The synergy between Apache Flink and Kubernetes not only optimized our data processing workflows but also future-proofed our system.&lt;/p>
&lt;h2 id="architecture-and-deployment-workflow">Architecture and deployment workflow&lt;/h2>
&lt;p>In the realm of real-time data processing and analytics, Apache Flink distinguishes itself as a powerful and versatile framework. When combined with Kubernetes, the industry-standard container orchestration system, Flink applications can scale horizontally and have robust management capabilities. We explore a cutting-edge design where Apache Flink and Kubernetes synergize seamlessly, thanks to the Apache Flink Kubernetes Operator.&lt;/p>
&lt;p>At its core, the Flink Kubernetes Operator serves as a control plane, mirroring the knowledge and actions of a human operator managing Flink deployments. Unlike traditional methods, the Operator automates critical activities, from starting and stopping applications to handling upgrades and errors. Its versatile feature set includes fully-automated job lifecycle management, support for different Flink versions, and multiple deployment modes, such as application clusters and session jobs. Moreover, the Operator&amp;rsquo;s operational prowess extends to metrics, logging, and even dynamic scaling by using the Job Autoscaler.&lt;/p>
&lt;h3 id="build-a-seamless-deployment-workflow">Build a seamless deployment workflow&lt;/h3>
&lt;p>Imagine a robust system where Flink jobs are deployed effortlessly, monitored diligently, and managed proactively. Our team created this workflow by integrating Apache Flink, Apache Flink Kubernetes Operator, and Kubernetes. Central to this setup is our custom-built Apache Flink Kubernetes Operator Client Library. This library acts as a bridge, enabling atomic operations such as starting, stopping, updating, and canceling Flink jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/stream-service-changes.png"
alt="Streaming service changes">&lt;/p>
&lt;h3 id="the-deployment-process">The deployment process&lt;/h3>
&lt;p>In our code, the client provides Apache Beam pipeline options, which include essential information such as the Kubernetes cluster&amp;rsquo;s API endpoint, authentication details, the Google Cloud/S3 temporary location for uploading the JAR file, and worker type specifications. The Kubernetes Operator Library uses this information to orchestrate a seamless deployment process. The following sections explain the steps taken. Most of the core steps are automated in our code base.&lt;/p>
&lt;p>&lt;strong>Step 1:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>The client wants to start a job for a customer and a specific application.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 2:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Generate a unique job ID:&lt;/strong> The library generates a unique job ID, which is set as a Kubernetes label. This identifier helps track and manage the deployed Flink job.&lt;/li>
&lt;li>&lt;strong>Configuration and code upload:&lt;/strong> The library uploads all necessary configurations and user code to a designated location on Google Cloud Storage or Amazon S3. This step ensures that the Flink application&amp;rsquo;s resources are available for deployment.&lt;/li>
&lt;li>&lt;strong>YAML payload generation:&lt;/strong> After the upload process completes, the library constructs a YAML payload. This payload contains crucial deployment information, including resource settings based on the specified worker type.&lt;/li>
&lt;/ul>
&lt;p>We used a convention for naming our worker VM instance types. Our convention is similar to the naming convention that Google Cloud uses. The name &lt;code>n1-standard-1&lt;/code> refers to a specific, predefined VM machine type. Let’s break down what each component of the name means:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>n1&lt;/strong> indicates the CPU type of the instance. In this case, it refers to the Intel based on instances in the N1 series. Google Cloud has multiple generations of instances with varying hardware and performance characteristics.&lt;/li>
&lt;li>&lt;strong>standard&lt;/strong> signifies the machine type family. Standard machine types offer a balanced ratio of 1 virtual CPU (vCPUs) and 4 GB of memory for Task Manager, and 0.5 vCPU and 2 GB memory for Job Manager.&lt;/li>
&lt;li>&lt;strong>1&lt;/strong> represents the number of vCPUs available in the instance. In the case of n1-standard-1, it means the instance has 1 vCPU.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 3:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Calling the Kubernetes API with Fabric8&lt;/strong>: To initiate the deployment, the library interacts with the Kubernetes API using Fabric8. Fabric8 initially lacked support for authentication in Google Kubernetes Engine or Amazon Elastic Kubernetes Service (EKS). To address this limitation, our team implemented the necessary authentication support, which can be found in our merge request on GitHub PR [4].&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 4:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flink Operator deployment&lt;/strong>: When it receives the YAML payload, the Flink Operator takes charge of deploying the various components of the Flink job. Tasks include provisioning resources and managing the deployment of the Flink Job Manager, Task Manager, and Job Service.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 5:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Job submission and execution&lt;/strong>: When the Flink Job Manager is running, it fetches the JAR file and configurations from the designated Google Cloud Storage or S3 location. With all necessary resources in place, it submits the Flink job to the standalone Flink cluster for execution.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 6&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Continuous monitoring&lt;/strong>: Post-deployment, our operator continuously monitors the status of the running Flink job. This real-time feedback loop enables us to promptly address any issues that arise, ensuring the overall health and optimal performance of our Flink applications.&lt;/li>
&lt;/ul>
&lt;p>In summary, our deployment process leverages Apache Beam pipeline options, integrates seamlessly with Kubernetes and the Flink Operator, and employs custom logic to handle configuration uploads and authentication. This end-to-end workflow ensures a reliable and efficient deployment of Flink applications in Kubernetes clusters while maintaining vigilant monitoring for smooth operation. The following sequence diagram shows the steps.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-start-activity-diagram.png"
alt="Job Start Activity Diagram">&lt;/p>
&lt;h2 id="develope-an-autoscaler">Develope an autoscaler&lt;/h2>
&lt;p>Having an autoscaler is critical to having a self-managed streaming service. There are not enough resources available on the internet for us to learn to build our own autoscaler, which makes this part of the workflow difficult.&lt;/p>
&lt;p>The autoscaler scales up the number of task managers to drain the lag and to keep up with the throughput. It also scales down the minimum number of resources required to process the incoming traffic to reduce costs. We need to do this frequently while keeping the processing disruption to minimum.&lt;/p>
&lt;p>We extensively tuned the autoscaler to meet the SLA for latency. This tuning involved a cost trade off. We also made the autoscaler application-specific to meet specific needs for certain applications. Every decision has a hidden cost. The second part of this blog provides more details about the autoscaler.&lt;/p>
&lt;h2 id="create-a-client-library-for-steaming-job-development">Create a client library for steaming job development&lt;/h2>
&lt;p>To deploy the job using the Flink Kubernetes Operator, you need to know about how Kubernetes works. The following steps explain how to create a single Flink job.&lt;/p>
&lt;ol>
&lt;li>Define a YAML file with proper specifications. The following image provides an example.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink.apache.org/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FlinkDeployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">basic-reactive-example&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink:1.13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1_13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">scheduler-mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">REACTIVE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskmanager.numberOfTaskSlots&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.savepoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/savepoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.checkpoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/checkpoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability.storageDir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/ha&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">serviceAccount&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jobManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-main-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeMounts&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/flink-data&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hostPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># directory location on host&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/tmp/flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># this field is optional&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Directory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">job&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jarURI&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">local:///opt/flink/examples/streaming/StateMachineExample.jar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">parallelism&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">upgradeMode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">savepoint&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">running&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">savepointTriggerNonce&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">standalone&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>SSH into your Flink cluster and run the command following command:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl create -f job1.yaml
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Use the following command to check the status of the job:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl get flinkdeployment job1
&lt;/code>&lt;/pre>&lt;p>This process impacts our scalability. Because we frequently update our jobs, we can&amp;rsquo;t manually follow these steps for every running job. To do so would be highly error prone and time consuming. One wrong space in the YAML can fail the deployment. This approach also acts as a barrier to innovation, because you need to know Kubernetes to interact with Flink jobs.&lt;/p>
&lt;p>We built a library to provide an interface for any teams and applications that want to to start, delete, update, or get the status of their jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/fko-library.png"
alt="Flink Kubernetes Operator Library">&lt;/p>
&lt;p>This library extends the Fabric8 client and FlinkDeployment CRD. FlinkDeployment CRD is exposed by the Flink Kubernetes Operator. CRD lets you store and retrieve structured data. By extending the CRD, we get access to POJO, making it easier to manipulate the YAML file.&lt;/p>
&lt;p>The library supports the following tasks:&lt;/p>
&lt;ol>
&lt;li>Authentication to ensure that you are allowed to perform actions on the Flink cluster.&lt;/li>
&lt;li>Validation (fetches the template from AWS/Google Cloud Storage for validation) takes user variable input and validates it against the policy, rules, YAML format.&lt;/li>
&lt;li>Action execution converts the Java call to invoke the Kubernetes operation.&lt;/li>
&lt;/ol>
&lt;p>During this process, we learned the following lessons:&lt;/p>
&lt;ol>
&lt;li>App specific operator service: At our large scale, the operator was unable to handle such a large number of jobs. Kubernetes calls started to time out and fail. To solve this problem, we created multiple operators (about 4) in high-traffic regions to handle each application.&lt;/li>
&lt;li>Kube call caching: To prevent overloading, we cached the results of Kubernetes calls for thirty to sixty seconds.&lt;/li>
&lt;li>Label support: Providing label support to search jobs using client-specific variables reduced the load on Kube and improved the job search speed by 5x.&lt;/li>
&lt;/ol>
&lt;p>The following are some of the biggest wins we achieved by exposing the library:&lt;/p>
&lt;ol>
&lt;li>Standardized job management: Users can start, delete, and get status updates for their Flink jobs in a Kubernetes environment using a single library.&lt;/li>
&lt;li>Abstracted Kubernetes complexity: Teams no longer need to worry about the inner workings of Kubernetes or the formatting job deployment YAML files. The library handles these details internally.&lt;/li>
&lt;li>Simplified upgrades: With the underlying Kubernetes infrastructure, the library brings robustness and fault tolerance to Flink job management, ensuring minimal downtime and efficient recovery.&lt;/li>
&lt;/ol>
&lt;h2 id="observability-and-alerting">Observability and alerting&lt;/h2>
&lt;p>Observability is important when runing a production system at a large scale. We have about 30,000 streaming jobs in PANW. Each job serves a customer for a specific application. Each job also reads data from multiple topics in Kafka, performs transformations, and then writes the data to various sinks and endpoints.&lt;/p>
&lt;p>Constraints can occur anywhere in the pipeline or its endpoints, such as the customer API, BigQuery, and so on. We want to make sure the latency of streaming meets the SLA. Therefore, understanding if a job is healthy, meeting SLA, and alerting and intervening when needed is very challenging.&lt;/p>
&lt;p>To achieve our operational goals, we built a sophisticated observability and alerting capability. We provide three kinds of observability and debugging tools, described in the following sections.&lt;/p>
&lt;h3 id="flink-job-list-and-job-insights-from-prometheus-and-grafana">Flink job list and job insights from Prometheus and Grafana&lt;/h3>
&lt;p>Each Flink job sends various metrics to our Prometheus with cardinality details, such as application name, customer Id, and regions, so that we can look at each job. Critical metrics include the input traffic rate, output throughput, backlogs in Kafka, timestamp-based latency, task CPU usage, task numbers, OOM counts, and so on.&lt;/p>
&lt;p>The following charts provide a few examples. The charts provide details about the ingestion traffic rate to Kafka for a specific customer, the streaming job’s overall throughput, each vCPU’s throughput, backlogs in Kafka, and worker autoscaling based on the observed backlog.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-metrics.png"
alt="Flink Job Metrics">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/autoscaling-metrics.png"
alt="Flink Job Autoscaling Metrics">&lt;/p>
&lt;p>The following chart shows streaming latency based on the timestamp watermark. In addition to the numbers of events in Kafka as backlogs, it is important to know the time latency for end-to-end streaming so that we can define and monitor the SLA. The latency is defined as the time taken for the streaming processing, starting from ingestion timestamp, to the timestamp sending to the streaming endpoint. A watermark is the last processed event’s time. With the watermark, we are tracking P100 latency. We track each event’s stream latency, so that we can understand each Kafka topic and partition or Flink job pipeline issue. The following example shows each event stream and its latency:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/watermark-metrics.png"
alt="Apache Beam Watermark Metrics">&lt;/p>
&lt;h3 id="flink-open-source-ui">Flink open source UI&lt;/h3>
&lt;p>We use and extend the Apache Flink dashboard UI to monitor jobs and tasks, such as the checkpoint duration, size, and failure. One important extension we used is a job history page that lets us see a job&amp;rsquo;s start and update timeline and details, which helps us to debug issues.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/flink-checkpoint-ui.png"
alt="Flink Checkpoint UI">&lt;/p>
&lt;h3 id="dashboards-and-alerting-for-backlog-and-latency">Dashboards and alerting for backlog and latency&lt;/h3>
&lt;p>We have about 30,000 jobs, and we want to closely monitor the jobs and receive alerts for jobs in abnormal states so that we can intervene. We created dashboards for each application so that we can show the list of jobs with the highest latency and create thresholds for alerts. The following example shows the timestamp-based latency dashboard for one application. We can set the alerting if the latency is larger than a threshold, such as 10 minutes, for a certain time continuously:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/latency-graph.png"
alt="Latency Graph">&lt;/p>
&lt;p>The following example shows more backlog-based dashboards:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/backlog-graph.png"
alt="Backlog Graph">&lt;/p>
&lt;p>The alerts are based on thresholds, and we frequently check metrics. If a threshold is met and continues for a certain amount of times, we alert our internal Slack channels or PagerDuty for immediate attention. We tune the alerting so that the accuracy is high.&lt;/p>
&lt;h2 id="cost-optimization-strategies-and-tuning">Cost optimization strategies and tuning&lt;/h2>
&lt;p>We also moved to a self-managed streaming service to improve cost efficiency. Several minor tunings have allowed us to reduce costs by half, and we have more opportunities for improvement.&lt;/p>
&lt;p>The following list includes a few tips that have helped us:&lt;/p>
&lt;ul>
&lt;li>Use Google Cloud Storage as checkpointing storage.&lt;/li>
&lt;li>Reduce the write frequency to Google Cloud Storage.&lt;/li>
&lt;li>Use appropriate machine types. For example, in Google Cloud, N2D machines are 15% less expensive than N2 machines.&lt;/li>
&lt;li>Autoscale tasks to use optimal resources while maintaining the latency SLA.&lt;/li>
&lt;/ul>
&lt;p>The following sections provide more details about the first two tips.&lt;/p>
&lt;h3 id="google-cloud-storage-and-checkpointing">Google Cloud Storage and checkpointing&lt;/h3>
&lt;p>We use Google Cloud Storage as our checkpoint store because it is cost-effective, scalable, and durable. When working with Google Cloud Storage, the following design considerations and best practices can help you optimize scaling and performance:&lt;/p>
&lt;ul>
&lt;li>Use data partitioning methods like range partitioning, which divides data based on specific attributes, and hash partitioning, which distributes data evenly using hash functions.&lt;/li>
&lt;li>Avoid sequential key names, especially timestamps, to avoid hotspots and uneven data distribution. Instead, introduce random prefixes for object distribution.&lt;/li>
&lt;li>Use a hierarchical folder structure to improve data management and reduce the number of objects in a single directory.&lt;/li>
&lt;li>Combine small files into larger ones to improve read throughput. Minimizing the number of small files reduces inefficient storage use and metadata operations.&lt;/li>
&lt;/ul>
&lt;h3 id="tune-the-frequency-of-writing-to-google-cloud-storage">Tune the frequency of writing to Google Cloud Storage&lt;/h3>
&lt;p>Scaling jobs efficiently was one of our primary challenges. Stateless jobs, which are relatively simpler, still present hurdles, especially in scenarios where Flink needed to process an overwhelming number of workers. To overcome this challenge, We increased the &lt;code>state.storage.fs.memory-threshold&lt;/code> settings to 1 MB from 20KB (??). This configuration allowed us to combine small checkpoint files into larger ones at the Job Manager level and to reduce metadata calls.&lt;/p>
&lt;p>Optimizing the performance of Google Cloud operations was another challenge. Although Google Cloud Storage is excellent for streaming large amounts of data, it has limitations when it comes to handling high-frequency I/O requests. To mitigate this issue, we introduced random prefixes in key names, avoided sequential key names, and optimized our Google Cloud Storage sharding techniques. These methods significantly enhanced our Google Cloud Storage performance, enabling the smooth operation of our stateless jobs.&lt;/p>
&lt;p>The following chart shows the Google Cloud Storage writes reduction after changing the memory-threshold:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/gcs-write-graph.png"
alt="GCS write Graph">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Palo Alto Networks® Cortex Data Lake is fully migrated from Dataflow streaming engine to Flink self managed streaming engine infrastructure. We have achieved our goals to run the system more cost efficiently (more than half cost cut), and run the infrastructure on multiple clouds such as GCP and AWS. We have learned how to build a large scale reliable production system based on open sources. We see large potentials to customize the system based on our specific needs as we have a lot of freedom to customize the open source code and configuration. In the next Part 2 post we will give more details on autoscaling and performance tuning parts. We hope our experience will be helpful for readers who will explore similar solutions for their own organizations.&lt;/p>
&lt;h1 id="additional-resources">Additional Resources&lt;/h1>
&lt;p>We provide links here for related presentations as further reading for readers interested in implementing similar solutions. By adding this section, we hope you can find more details to build a fully managed streaming infrastructure, making it easier for readers to follow our stories and learnings.&lt;/p>
&lt;p>[1] Streaming framework at PANW published at Apache Beam: &lt;a href="https://beam.apache.org/case-studies/paloalto/">https://beam.apache.org/case-studies/paloalto/&lt;/a>&lt;/p>
&lt;p>[2] PANW presentation at Beam Summit 2023: &lt;a href="https://youtu.be/IsGW8IU3NfA?feature=shared">https://youtu.be/IsGW8IU3NfA?feature=shared&lt;/a>&lt;/p>
&lt;p>[3] Benchmark presented at Beam Summit 2021: &lt;a href="https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/">https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/&lt;/a>&lt;/p>
&lt;p>[4] PANW open source contribution to Flink for GKE Auth support: &lt;a href="https://github.com/fabric8io/kubernetes-client/pull/4185">https://github.com/fabric8io/kubernetes-client/pull/4185&lt;/a>&lt;/p>
&lt;h1 id="acknowledgements">Acknowledgements&lt;/h1>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed Flink based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitk Dani, and David He.&lt;/p></description><link>/blog/apache-beam-flink-and-kubernetes/</link><pubDate>Fri, 03 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes/</guid><category>blog</category></item><item><title>Apache Beam 2.51.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.51.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2510-2023-10-03">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.51.0, check out the &lt;a href="https://github.com/apache/beam/milestone/15">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>In Python, &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#why-use-the-runinference-api">RunInference&lt;/a> now supports loading many models in the same transform using a &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#use-a-keyed-modelhandler">KeyedModelHandler&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/27628">#27628&lt;/a>).&lt;/li>
&lt;li>In Python, the &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.vertex_ai_inference.html#apache_beam.ml.inference.vertex_ai_inference.VertexAIModelHandlerJSON">VertexAIModelHandlerJSON&lt;/a> now supports passing in inference_args. These will be passed through to the Vertex endpoint as parameters.&lt;/li>
&lt;li>Added support to run &lt;code>mypy&lt;/code> on user pipelines (&lt;a href="https://github.com/apache/beam/issues/27906">#27906&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Removed fastjson library dependency for Beam SQL. Table property is changed to be based on jackson ObjectNode (Java) (&lt;a href="https://github.com/apache/beam/issues/24154">#24154&lt;/a>).&lt;/li>
&lt;li>Removed TensorFlow from Beam Python container images &lt;a href="https://github.com/apache/beam/pull/28424">PR&lt;/a>. If you have been negatively affected by this change, please comment on &lt;a href="https://github.com/apache/beam/issues/20605">#20605&lt;/a>.&lt;/li>
&lt;li>Removed the parameter &lt;code>t reflect.Type&lt;/code> from &lt;code>parquetio.Write&lt;/code>. The element type is derived from the input PCollection (Go) (&lt;a href="https://github.com/apache/beam/issues/28490">#28490&lt;/a>)&lt;/li>
&lt;li>Refactor BeamSqlSeekableTable.setUp adding a parameter joinSubsetType. &lt;a href="https://github.com/apache/beam/issues/28283">#28283&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed exception chaining issue in GCS connector (Python) (&lt;a href="https://github.com/apache/beam/issues/26769#issuecomment-1700422615">#26769&lt;/a>).&lt;/li>
&lt;li>Fixed streaming inserts exception handling, GoogleAPICallErrors are now retried according to retry strategy and routed to failed rows where appropriate rather than causing a pipeline error (Python) (&lt;a href="https://github.com/apache/beam/issues/21080">#21080&lt;/a>).&lt;/li>
&lt;li>Fixed a bug in Python SDK&amp;rsquo;s cross-language Bigtable sink that mishandled records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Python containers updated, fixing &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>&lt;/li>
&lt;li>Used go 1.21.1 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39320">CVE-2023-39320&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines using BigQuery Storage Read API must pin &lt;code>fastavro&lt;/code> dependency to 1.8.3
or earlier: &lt;a href="https://github.com/apache/beam/issues/28811">#28811&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Whitmore&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hao Xu&lt;/p>
&lt;p>Haruka Abe&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Joey Tran&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Melissa Pashniak&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruwann&lt;/p>
&lt;p>Ryan Tam&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sereana Seim&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tim Grein&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zbynek Konecny&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>gDuperran&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kberezin-nshl&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lostluck&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>olalamichelle&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p>
&lt;p>Łukasz Spyra&lt;/p></description><link>/blog/beam-2.51.0/</link><pubDate>Wed, 11 Oct 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.51.0/</guid><category>blog</category><category>release</category></item><item><title>DIY GenAI Content Discovery Platform with Apache Beam</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="diy-genai-content-discovery-platform-with-apache-beam">DIY GenAI Content Discovery Platform with Apache Beam&lt;/h1>
&lt;p>Your digital assets, such as documents, PDFs, spreadsheets, and presentations, contain a wealth of valuable information, but sometimes it&amp;rsquo;s hard to find what you&amp;rsquo;re looking for. This blog post explains how to build a DIY starter architecture, based on near real-time ingestion processing and large language models (LLMs), to extract meaningful information from your assets. The model makes the information available and discoverable through a simple natural language query.&lt;/p>
&lt;p>Building a near real-time processing pipeline for content ingestion might seem like a complex task, and it can be. To make pipeline building easier, the Apache Beam framework exposes a set of powerful constructs. These constructs remove the following complexities: interacting with multiple types of content sources and destinations, error handling, and modularity. They also maintain resiliency and scalability with minimal effort. You can use an Apache Beam streaming pipeline to complete the following tasks:&lt;/p>
&lt;ul>
&lt;li>Connect to the many components of a solution.&lt;/li>
&lt;li>Quickly process content ingestion requests of documents.&lt;/li>
&lt;li>Make the information in the documents available a few seconds after ingestion.&lt;/li>
&lt;/ul>
&lt;p>LLMs are often used to extract content and summarize information stored in many different places. Organizations can use LLMs to quickly find relevant information disseminated in multiple documents written across the years. The information might be in different formats, or the documents might be too long and complex to read and understand quickly. Use LLMs to process this content to make it easier for people to find the information that they need.&lt;/p>
&lt;p>Follow the steps in this guide to create a custom scalable solution for data extraction, content ingestion, and storage. Learn how to kickstart the development of a LLM-based solution using Google Cloud products and generative AI offerings. Google Cloud is designed to be simple to use, scalable, and flexible, so you can use it as a starting point for further expansion or experimentation.&lt;/p>
&lt;h3 id="high-level-flow">High-level Flow&lt;/h3>
&lt;p>In this workflow, content uptake and query interactions are completely separated. An external content owner can send documents stored in Google Docs or in a binary text format and receive a tracking ID for the ingestion request. The ingestion process gets the content of the document and creates chunks that are configurable in size. Each document chunk is used to generate embeddings. These embeddings represent the content semantics, in the form of a vector of 768 dimensions. Given the document identifier and the chunk identifier, you can store the embeddings in a Vector database for semantic matching. This process is central to contextualizing user inquiries.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-highlevel.png"
alt="Content Discovery Platform Overview">&lt;/p>
&lt;p>The query resolution process doesn&amp;rsquo;t depend directly on information ingestion. The user receives relevant answers based on the content ingested until the moment of the query request. Even if the platform doesn&amp;rsquo;t have any relevant content stored, the platform returns an answer stating that it doesn&amp;rsquo;t have relevant content. Therefore, the query resolution process first generates embeddings from the query content and from the previously existing context, like previous exchanges with the platform, then matches these embeddings with the existing embedding vectors stored from the content. When the platform has positive matches, it retrieves the plain-text content represented by the content embeddings. Finally, by using the textual representation of the query and the textual representation of the matched content, the platform formulates a request to the LLM to provide a final answer to the original user inquiry.&lt;/p>
&lt;h2 id="components-of-the-solution">Components of the solution&lt;/h2>
&lt;p>Use the low-ops capabilities of the Google Cloud services to create a set of highly scalable features. You can separate the solution into two main components: the service layer and the content ingestion pipeline. The service layer acts as the entry point for document ingestion and user queries. It’s a simple set of REST resources exposed through Cloud Run and implemented by using &lt;a href="https://quarkus.io/">Quarkus&lt;/a> and the client libraries to access other services (Vertex AI models, Cloud Bigtable and Pub/Sub). The content ingestion pipeline includes the following components:&lt;/p>
&lt;ul>
&lt;li>A streaming pipeline that captures user content from wherever it resides.&lt;/li>
&lt;li>A process that extracts meaning from this content as a set of multi-dimensional vectors (text embeddings).&lt;/li>
&lt;li>A storage system that simplifies context matching between knowledge content and user inquiries (a Vector Database).&lt;/li>
&lt;li>Another storage system that maps knowledge representation with the actual content, forming the aggregated context of the inquiry.&lt;/li>
&lt;li>A model capable of understanding the aggregated context and, through prompt engineering, delivering meaningful answers.&lt;/li>
&lt;li>HTTP and gRPC-based services.&lt;/li>
&lt;/ul>
&lt;p>Together, these components provide a comprehensive and simple implementation for a content discovery platform.&lt;/p>
&lt;h2 id="workflow-architecture">Workflow Architecture&lt;/h2>
&lt;p>This section explains how the different components interact.&lt;/p>
&lt;h3 id="dependencies-of-the-components">Dependencies of the components&lt;/h3>
&lt;p>The following diagram shows all of the components that the platform integrates with. It also shows all of the dependencies that exist between the components of the solution and the Google Cloud services.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-arch.png"
alt="Content Discovery Platform Interactions">&lt;/p>
&lt;p>As seen in the diagram, the context-extraction component is the central aspect in charge of retrieving the document’s content, also their semantic meaning from the embedding’s model and storing the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. PubSub resources are the glue between the streaming pipeline and the asynchronous processing, capturing the user ingestion requests, retries from potential errors from the ingestion pipeline (like the cases on where documents have been sent for ingestion but the permission has not been granted yet, triggering a retry after some minutes) and content refresh events (periodically the pipeline will scan the ingested documents, review the latest editions and define if a content refresh should be triggered).&lt;/p>
&lt;p>The context-extraction component retrieves the content of the documents, diving it in chunks. It also computes embeddings, using the LLM interaction, from the extracted content. Then it stores the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. Pub/Sub resources connect the streaming pipeline and the asynchronous processing, capturing the following actions:&lt;/p>
&lt;ul>
&lt;li>user ingestion requests&lt;/li>
&lt;li>retries from errors from the ingestion pipeline, such as when documents are sent for ingestion but access permissions are missing&lt;/li>
&lt;li>content refresh events (periodically the pipeline scans the ingested documents, reviews the latest editions, and decides whether to trigger a content refresh)&lt;/li>
&lt;/ul>
&lt;p>Also, CloudRun plays an important role exposing the services, interacting with many Google Cloud services to resolve the user query or ingestion requests. For example, while resolving a query request the service will:&lt;/p>
&lt;ul>
&lt;li>Request the computation of embeddings from the user’s query by interacting with the embeddings model&lt;/li>
&lt;li>Find near neighbor matches from the Vertex AI Vector Search (formerly Matching Engine) using the query embeddings representation&lt;/li>
&lt;li>Retrieve the text content from BigTable for those matched vectors, using their identifier, in order contextualize a LLM prompt&lt;/li>
&lt;li>And finally create a request to the VertexAI Chat-Bison model, generating the response the system will delivery to the user’s query.&lt;/li>
&lt;/ul>
&lt;h3 id="google-cloud-products">Google Cloud products&lt;/h3>
&lt;p>This section describes the Google Cloud products and services used in the solution and what purpose they serve.&lt;/p>
&lt;p>&lt;strong>Cloud Build:&lt;/strong> All container images, including services and pipelines, are built directly from source code by using Cloud Build. Using Cloud Build simplifies code distribution during the deployment of the solution.&lt;/p>
&lt;p>&lt;strong>CloudRun:&lt;/strong> The solution&amp;rsquo;s service entry points are deployed and automatically scaled by CloudRun.&lt;/p>
&lt;p>&lt;strong>Pub/Sub:&lt;/strong> A Pub/Sub topic and subscription queue all of the ingestion requests for Google Drive or self-contained content and deliver the requests to the pipeline.&lt;/p>
&lt;p>&lt;strong>Dataflow:&lt;/strong> A multi-language, streaming Apache Beam pipeline processes the ingestion requests. These requests are sent to the pipeline from the Pub/Sub subscription. The pipeline extracts content from Google Docs, Google Drive URLs, and self-contained binary encoded text content. It then produces content chunks. These chunks are sent to one of the Vertex AI foundational models for the embedding representation. The embeddings and chunks from the documents are sent to Vertex AI Vector Search and to Cloud Bigtable for indexing and rapid access. Finally, the ingested documentation is stored in Google Cloud Storage in JSON-L format, which can be used to fine-tune the Vertex AI models. By using Dataflow to run the Apache Beam streaming pipeline, you minimize the ops needed to scale resources. If you have a burst on ingestion requests, Dataflow can keep the latency less than a minute.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Vector Search:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/matching-engine/overview">Vector Search&lt;/a> is a high-performance, low-latency vector database. These vector databases are often called vector similarity search or approximate nearest neighbor (ANN) services. We use a Vector Search Index to store all the ingested documents embeddings as a meaning representation. These embeddings are indexed by chunk and document id. Later on, these identifiers can be used to contextualize the user queries and enrich the requests made to a LLM by providing knowledge extracted directly from the document’s content mappings stored on BigTable (using the same chunk-document identifiers).&lt;/p>
&lt;p>&lt;strong>Cloud BigTable:&lt;/strong> This storage system provides a low latency search by identifier at a predictable scale. Is a perfect fit, given the low latency of the requests resolution, for online exchanges between user queries and the platform component interactions. It used to store the content extracted from the documents since it&amp;rsquo;s indexed by chunk and document identifier. Every time a user makes a request to the query service, and after the query text embeddings are resolved and matched with the existing context, the document and chunk ids are used to retrieve the document’s content that will be used as context to request an answer to the LLM in use. Also, BigTable is used to keep track of the conversational exchanges between users and the platform, furthermore enriching the context included on the requests sent to the LLMs (embeddings, summarization, chat Q&amp;amp;A).&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Embedding Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings">Text embeddings&lt;/a> are a condensed vector (numeric) representation of a piece of text. If two pieces of text are semantically similar, their corresponding embeddings will be located close together in the embedding vector space. For more details please see &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings">get text embeddings&lt;/a>. These embeddings are directly used by the ingestion pipeline when processing the document’s content and the query service as an input to match the users query semantic with existing content indexed in Vector Search.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Summarization Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text">Text-bison&lt;/a> is the name of the PaLM 2 LLM that understands, summarizes and generates text. The types of content that text-bison can create include document summaries, answers to questions, and labels that classify the provided input content. We used this LLM to summarize the previously maintained conversation with the goal of enriching the user’s queries and better embedding matching. In summary, the user does not have to include all the context of his question, we extract and summarize it from the conversation history.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Chat Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat">Chat-bison&lt;/a> is the PaLM 2 LLM that excels at language understanding, language generation, and conversations. This chat model is fine-tuned to conduct natural multi-turn conversations, and is ideal for text tasks about code that require back-and-forth interactions. We use this LLM to provide answers to the queries made by users of the solution, including the conversation history between both parties and enriching the model’s context with the content stored in the solution.&lt;/p>
&lt;h3 id="extraction-pipeline">Extraction Pipeline&lt;/h3>
&lt;p>The content extraction pipeline is the platform&amp;rsquo;s centerpiece. It takes care of handling content ingestion requests, extracting documents content and computing embeddings from that content, to finally store the data in specialized storage systems that will be used in the query service components for rapid access.&lt;/p>
&lt;h4 id="high-level-view">High Level View&lt;/h4>
&lt;p>As previously mentioned the pipeline is implemented using Apache Beam framework and runs in streaming fashion on GCP&amp;rsquo;s &lt;a href="https://cloud.google.com/dataflow">Dataflow&lt;/a> service.&lt;/p>
&lt;p>By using Apache Beam and Dataflow we can ensure minimal latency (sub minute processing times), low ops (no need to manually scale up or down the pipeline when traffic spikes occur with time, worker recycle, updates, etc.) and with high level of observability (clear and abundant performance metrics are available).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-1.png"
alt="Apache Beam Pipeline">&lt;/p>
&lt;p>On a high level, the pipeline separates the extraction, computing, error handling and storage responsibilities on different components or PTransforms. As seen in the diagram, the messages are read from a PubSub subscription and immediately afterwards are included in the window definition before the content extraction.&lt;/p>
&lt;p>Each of those PTransforms can be expanded to reveal more details regarding the underlying stages for the implementation. We will dive into each in the following sections.&lt;/p>
&lt;p>The pipeline was implemented using a multi-language approach, with the main components written in the Java language (JDK version 17) and those related with the embeddings computations implemented in Python (version 3.11) since the Vertex AI API clients are available for this language.&lt;/p>
&lt;h4 id="content-extraction">Content Extraction&lt;/h4>
&lt;p>The content extraction component is in charge of reviewing the ingestion request payload and deciding (given the event properties) if it will need to retrieve the content from the event itself (self-contained content, text based document binary encoded) or retrieve it from Google Drive.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-2-extractcontent.png"
alt="Pipeline's Content Extraction">&lt;/p>
&lt;p>In case of a self-contained document, the pipeline will extract the document id and format the document in paragraphs for later embedding processing.&lt;/p>
&lt;p>When in need of retrieval from Google Drive, the pipeline will inspect if the provided URL in the event refers to a Google Drive folder or a single file format (supported formats are Documents, Spreadsheets and Presentations). In the case of a folder, the pipeline will crawl the folder’s content recursively extracting all the files for the supported formats, in case of a single document will just return that one.&lt;/p>
&lt;p>Finally, with all the file references retrieved from the ingestion request, textual content is extracted from the files (no image support implemented for this PoC). That content will also be passed to the embedding processing stages including the document’s identifier and the content as paragraphs.&lt;/p>
&lt;h4 id="error-handling">Error Handling&lt;/h4>
&lt;p>On every stage of the content extraction process multiple errors can be encountered, malformed ingestion requests, non-conformant URLs, lack of permissions for Drive resources, lack of permissions for File data retrieval.&lt;/p>
&lt;p>In all those cases a dedicated component will capture those potential errors and define, given the nature of the error, if the event should be retried or sent to a dead letter GCS bucket for later inspection.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-3-errorhandling.png"
alt="Pipeline's Error Handling">&lt;/p>
&lt;p>The final errors, or those which won’t be retried, are those errors related with bad request formats (the event itself or the properties content, like malformed or wrong URLs, etc.).&lt;/p>
&lt;p>The retryable errors are those related with content access and lack of permissions. A request may have been resolved faster than the manual process of providing the right permissions to the Service Account that runs the pipeline to access the resources included in the ingestion request (Google Drive folders or files). In case of detecting a retryable error, the pipeline will hold the retry for 10 minutes before re-sending the message to the upstream PubSub topic; each error is retried at most 5 times before being sent to the dead letter GCS bucket.&lt;/p>
&lt;p>In all cases of events ending on the dead letter destination, the inspection and re-processing must be done in a manual process.&lt;/p>
&lt;h4 id="process-embeddings">Process Embeddings&lt;/h4>
&lt;p>Once the content has been extracted from the request, or captured from Google Drive files, the pipeline will trigger the embeddings computation process. As previously mentioned the interactions with the Vertex AI Foundational Models API is implemented in Python language. For this reason we need to format the extracted content in Java types that have a direct translation to those existing in the Python world. Those are key-values (in Python those are 2-element tuples), Strings (available in both languages), and iterables (also available in both languages). We could have implemented coders in both languages to support custom transport types, but we opted out of that in favor of clarity and simplicity.&lt;/p>
&lt;p>Before computing the content’s embeddings we decided to introduce a Reshuffle step, making the output consistent to downstream stages, with the idea of avoiding the content extraction step being repeated in case of errors. This should avoid putting pressure on existing access quotas on Google Drive related APIs.&lt;/p>
&lt;p>The pipeline will then chunk the content in configurable sizes and also configurable overlapping, good parameters are hard to get for generic effective data extraction, so we opted to use smaller chunks with small overlapping factor as the default settings to favor diversity on the document results (at least that’s what we see from the empirical results obtained).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings1.png"
alt="Embeddings Processing">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings2.png"
alt="Embeddings Processing">
&lt;/p>
&lt;p>Once the embeddings vectors are retrieved from the embeddings Vertex AI LLM, we will consolidate them again avoiding repetition of this step in case of downstream errors.&lt;/p>
&lt;p>Worth to notice that this pipeline is interacting directly with Vertex AI models using the client SDKs, Apache Beam already provides supports for this interactions through the RunInference PTransform (see an example &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/vertex_ai_llm_text_classification.py">here&lt;/a>).&lt;/p>
&lt;h4 id="content-storage">Content Storage&lt;/h4>
&lt;p>Once the embeddings are computed for the content chunks extracted from the ingested documents, we need to store the vectors in a searchable storage and also the textual content that correlates with those embeddings. We will be using the embeddings vectors as a semantic match later from the query service, and the textual content that corresponds to those embeddings for LLM context as a way to improve and guide the response expectations.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-5-storecontent.png"
alt="Content Storage">&lt;/p>
&lt;p>With that in mind is that in mind we split the consolidated embeddings into 3 paths, one that stores the vectors into Vertex AI Vector Search (using simple REST calls), another storing the textual content into BigTable (for low latency retrieval after semantic matching) and the final one as a potential clean up of content refresh or re ingestion (more on that later). The three paths are using the ingested document identifier as the correlating data on the actions, this key is formed by the document name (in case of available), the document identifier and the chunk sequence number. The reason for using identifiers for the chunk comes behind the idea of subsequent updates. An increase in the content will generate a larger number of chunks, and upserting all the chunks will enable always fresh data; on the contrary, a decrease in content will generate a smaller chunk count for the document’s content, this number difference can be used to delete the remaining orphan indexed chunks (from content no longer existing in the latest version of the document).&lt;/p>
&lt;h4 id="content-refresh">Content Refresh&lt;/h4>
&lt;p>The last pipeline component is the simplest, at least conceptually. After the documents from Google Drive gets ingested, an external user can produce updates in them, causing the indexed content to become out of date. We implemented a simple periodic process, inside the same streaming pipeline, that will take care of the review of already ingested documents and see if there are content updates needed. We use a GenerateSequence transform to produce a periodic impulse (every 6 hours by default), that will trigger a scan on BigTable retrieving all the ingested document identifiers. Given those identifiers we can then query Google Drive for the latest update timestamp of each document and use that marker to decide if an update is needed.&lt;/p>
&lt;p>In case of needing to update the document’s content, we can simply send an ingestion request to the upstream PubSub topic and let the pipeline run its course for this new event. Since we are taking care of upserting embeddings and cleaning up those that no longer exist, we should be capable of taking care of the majority of the additions (as long those are text updates, image based content is not being processed as of now).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh1.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh2.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh3.png"
alt="Content Refresh">
&lt;/p>
&lt;p>This task could be performed as a separate job, possibly one that is periodically scheduled in batch form. This would result in lower costs, a separate error domain, and more predictable auto scaling behavior. However, for the purposes of this demonstration, it is simpler to have a single job.&lt;/p>
&lt;p>Next, we will be focusing on how the solution interacts with external clients for ingestion and content discovery use cases.&lt;/p>
&lt;h2 id="interaction-design">Interaction Design&lt;/h2>
&lt;p>The solution aims to make the interactions for ingesting and querying the platform as simple as possible. Also, since the ingestion part may imply interacting with several services and imply retries or content refresh, we decided to make both separated and asynchronous, freeing the external users of blocking themselves while waiting for requests resolutions.&lt;/p>
&lt;h3 id="example-interactions">Example Interactions&lt;/h3>
&lt;p>Once the platform is deployed in a GCP project, a simple way to interact with the services is through the use of a web client, curl is a good example. Also, since the endpoints are authenticated, a client needs to include its credentials in the request header to have its access granted.&lt;/p>
&lt;p>Here is an example of an interaction for content ingestion:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST -H &amp;#34;Content-Type: application/json&amp;#34; -H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; https://&amp;lt;service-address&amp;gt;/ingest/content/gdrive -d $&amp;#39;{&amp;#34;url&amp;#34;:&amp;#34;https://drive.google.com/drive/folders/somefolderid&amp;#34;}&amp;#39; | jq .
# response from service
{
&amp;#34;status&amp;#34;: &amp;#34;Ingestion trace id: &amp;lt;some identifier&amp;gt;&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>In this case, after the ingestion request has been sent to the PubSub topic for processing, the service will return the tracking identifier, which maps with the PubSub message identifier. Note the provided URL can be one of a Google Doc or a Google Drive folder, in the later case the ingestion process will crawl the folder’s content recursively to retrieve all the contained documents and their contents.&lt;/p>
&lt;p>Next, an example of a content query interaction, very similar to the previous one:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundation Models are a set of pre-trained models that can be used to accelerate the development of machine learning applications. They are available for a variety of tasks, including natural language processing, computer vision, and recommendation systems.\n\nVertexAI Foundation Models can be used to improve the performance of Generative AI applications by providing a starting point for model development. They can also be used to reduce the amount of time and effort required to train a model.\n\nIn addition, VertexAI Foundation Models can be used to improve the accuracy and robustness of Generative AI applications. This is because they are trained on large datasets and are subject to rigorous quality control.\n\nOverall, VertexAI Foundation Models can be a valuable resource for developers who are building Generative AI applications. They can help to accelerate the development process, reduce the cost of development, and improve the performance and accuracy of applications.&amp;#34;,
&amp;#34;previousConversationSummary&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;sourceLinks&amp;#34;: [
{
&amp;#34;link&amp;#34;: &amp;#34;&amp;lt;possibly some ingested doc url/id&amp;gt;&amp;#34;,
&amp;#34;distance&amp;#34;: 0.7233397960662842
}
],
&amp;#34;citationMetadata&amp;#34;: [
{
&amp;#34;citations&amp;#34;: []
}
],
&amp;#34;safetyAttributes&amp;#34;: [
{
&amp;#34;categories&amp;#34;: [],
&amp;#34;scores&amp;#34;: [],
&amp;#34;blocked&amp;#34;: false
}
]
}
&lt;/code>&lt;/pre>&lt;p>The platform will answer the request with a textual response from the LLM and include as well information about the categorization, citation metadata and source links (if available) of the content used to generate the response (this are for example, Google Docs links of the documents previously ingested by the platform).&lt;/p>
&lt;p>When interacting with the services, a good query will generally return good results, the clearer the query the easier it will be to contextualize its meaning and more accurate information will be sent to the LLMs to retrieve answers. But having to include all the details of the query context in a phrase on every exchange with the service can be very cumbersome and difficult. For that case the platform can use a provided session identifier that will be used to store all the previous exchanges between a user and the platform. This should help the implementation to better contextualize the initial query embeddings matching and even provide more concise contextual information in the model requests. Here is an example of a contextual exchange:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundational Models are a suite of pre-trained models that can be used to accelerate the development of Generative AI applications. These models are available in a variety of languages and domains, and they can be used to generate text, images, audio, and other types of content.\n\nUsing VertexAI Foundational Models can help you to:\n\n* Reduce the time and effort required to develop Generative AI applications\n* Improve the accuracy and quality of your models\n* Access the latest research and development in Generative AI\n\nVertexAI Foundational Models are a powerful tool for developers who want to create innovative and engaging Generative AI applications.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;describe the available LLM models?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models suite includes a variety of LLM models, including:\n\n* Text-to-text LLMs: These models can generate text based on a given prompt. They can be used for tasks such as summarization, translation, and question answering.\n* Image-to-text LLMs: These models can generate text based on an image. They can be used for tasks such as image captioning and description generation.\n* Audio-to-text LLMs: These models can generate text based on an audio clip. They can be used for tasks such as speech recognition and transcription.\n\nThese models are available in a variety of languages, including English, Spanish, French, German, and Japanese. They can be used to create a wide range of Generative AI applications, such as chatbots, customer service applications, and creative writing tools.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;do rate limit apply for those LLMs?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;Yes, there are rate limits for the VertexAI Foundational Models. The rate limits are based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models documentation](https://cloud.google.com/vertex-ai/docs/foundational-models#rate-limits).&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;care to share the price?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models are priced based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models pricing page](https://cloud.google.com/vertex-ai/pricing#foundational-models).&amp;#34;,
…
}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Usage Tip:&lt;/strong> in case of abruptly changing topics, sometimes is better to use a new session identifier.&lt;/p>
&lt;h3 id="deployment">Deployment&lt;/h3>
&lt;p>As part of the platform solution, there are a set of scripts that help with the deployment of all the different components. By running the &lt;code>start.sh&lt;/code> and setting the right parameters (GCP project, terraform state bucket and name for the platform instance) the script will take care of building the code, deploying the needed containers (service endpoint container and Dataflow python custom container), deploying all the GCP resources using Terraform and finally deploying the pipeline. There is also the possibility of modifying the pipeline’s execution by passing an extra parameter to the startup script, for example: &lt;code>start.sh &amp;lt;gcp project&amp;gt; &amp;lt;state-bucket-name&amp;gt; &amp;lt;a run name&amp;gt; &amp;quot;--update&amp;quot;&lt;/code> will update the content extraction pipeline in-place.&lt;/p>
&lt;p>Also, in case of wanting to focus only on the deployment of specific components other scripts have been included to help with those specific tasks (build the solution, deploy the infrastructure, deploy the pipeline, deploy the services, etc.).&lt;/p>
&lt;h3 id="solutions-notes">Solution&amp;rsquo;s Notes&lt;/h3>
&lt;p>This solution is designed to serve as an example for learning purposes. Many of the configuration values for the extraction pipeline and security restrictions are provided only as examples. The solution doesn&amp;rsquo;t propagate the existing access control lists (ACLs) of the ingested content. As a result, all users that have access to the service endpoints have access to summarizations of the ingested content from those original documents.&lt;/p>
&lt;h3 id="notes-about-the-source-code">Notes about the source code&lt;/h3>
&lt;p>The source code for the content discovery platform is available in &lt;a href="https://github.com/prodriguezdefino/content-dicovery-platform-gcp">Github&lt;/a>. You can run it in any Google Cloud project. The repository includes the source code for the integration services, the multi-language ingestion pipeline, and the deployment automation through Terraform. If you deploy this example, it might take up to 90 minutes to create and configure all the needed resources. The README file contains additional documentation about the deployment prerequisites and example REST interactions.&lt;/p></description><link>/blog/dyi-content-discovery-platform-genai-beam/</link><pubDate>Mon, 02 Oct 2023 00:00:01 -0800</pubDate><guid>/blog/dyi-content-discovery-platform-genai-beam/</guid><category>blog</category></item><item><title>Apache Beam 2.50.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.50.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2500-2023-08-30">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.50.0, check out the &lt;a href="https://github.com/apache/beam/milestone/14">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark 3.2.2 is used as default version for Spark runner (&lt;a href="https://github.com/apache/beam/issues/23804">#23804&lt;/a>).&lt;/li>
&lt;li>The Go SDK has a new default local runner, called Prism (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java KafkaIO now supports picking up topics via topicPattern (&lt;a href="https://github.com/apache/beam/pull/26948">#26948&lt;/a>)&lt;/li>
&lt;li>Support for read from Cosmos DB Core SQL API (&lt;a href="https://github.com/apache/beam/issues/23604">#23604&lt;/a>)&lt;/li>
&lt;li>Upgraded to HBase 2.5.5 for HBaseIO. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">#27711&lt;/a>)&lt;/li>
&lt;li>Added support for GoogleAdsIO source (Java) (&lt;a href="https://github.com/apache/beam/pull/27681">#27681&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Go SDK now requires Go 1.20 to build. (&lt;a href="https://github.com/apache/beam/issues/27558">#27558&lt;/a>)&lt;/li>
&lt;li>The Go SDK has a new default local runner, Prism. (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).
&lt;ul>
&lt;li>Prism is a portable runner that executes each transform independantly, ensuring coders.&lt;/li>
&lt;li>At this point it supercedes the Go direct runner in functionality. The Go direct runner is now deprecated.&lt;/li>
&lt;li>See &lt;a href="https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md">https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md&lt;/a> for the goals and features of Prism.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hugging Face Model Handler for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/26632">#26632&lt;/a>)&lt;/li>
&lt;li>Hugging Face Pipelines support for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/27399">#27399&lt;/a>)&lt;/li>
&lt;li>Vertex AI Model Handler for RunInference now supports private endpoints (&lt;a href="https://github.com/apache/beam/pull/27696">#27696&lt;/a>)&lt;/li>
&lt;li>MLTransform transform added with support for common ML pre/postprocessing operations (&lt;a href="https://github.com/apache/beam/pull/26795">#26795&lt;/a>)&lt;/li>
&lt;li>Upgraded the Kryo extension for the Java SDK to Kryo 5.5.0. This brings in bug fixes, performance improvements, and serialization of Java 14 records. (&lt;a href="https://github.com/apache/beam/issues/27635">#27635&lt;/a>)&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures. (&lt;a href="https://github.com/apache/beam/issues/27674">#27674&lt;/a>). The multi-arch container images include:
&lt;ul>
&lt;li>All versions of Go, Python, Java and Typescript SDK containers.&lt;/li>
&lt;li>All versions of Flink job server containers.&lt;/li>
&lt;li>Java and Python expansion service containers.&lt;/li>
&lt;li>Transform service controller container.&lt;/li>
&lt;li>Spark3 job server container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for batched writes to AWS SQS for improved throughput (Java, AWS 2).(&lt;a href="https://github.com/apache/beam/issues/21429">#21429&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python SDK: Legacy runner support removed from Dataflow, all pipelines must use runner v2.&lt;/li>
&lt;li>Python SDK: Dataflow Runner will no longer stage Beam SDK from PyPI in the &lt;code>--staging_location&lt;/code> at pipeline submission. Custom container images that are not based on Beam&amp;rsquo;s default image must include Apache Beam installation.(&lt;a href="https://github.com/apache/beam/issues/26996">#26996&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>The Go Direct Runner is now Deprecated. It remains available to reduce migration churn.
&lt;ul>
&lt;li>Tests can be set back to the direct runner by overriding TestMain: &lt;code>func TestMain(m *testing.M) { ptest.MainWithDefault(m, &amp;quot;direct&amp;quot;) }&lt;/code>&lt;/li>
&lt;li>It&amp;rsquo;s recommended to fix issues seen in tests using Prism, as they can also happen on any portable runner.&lt;/li>
&lt;li>Use the generic register package for your pipeline DoFns to ensure pipelines function on portable runners, like prism.&lt;/li>
&lt;li>Do not rely on closures or using package globals for DoFn configuration. They don&amp;rsquo;t function on portable runners.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed DirectRunner bug in Python SDK where GroupByKey gets empty PCollection and fails when pipeline option &lt;code>direct_num_workers!=1&lt;/code>.(&lt;a href="https://github.com/apache/beam/pull/27373">#27373&lt;/a>)&lt;/li>
&lt;li>Fixed BigQuery I/O bug when estimating size on queries that utilize row-level security (&lt;a href="https://github.com/apache/beam/pull/27474">#27474&lt;/a>)&lt;/li>
&lt;li>Beam Python containers rely on a version of Debian/aom that has several security vulnerabilities: &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python Pipelines using BigQuery IO or &lt;code>orjson&lt;/code> dependency might experience segmentation faults or get stuck: &lt;a href="https://github.com/apache/beam/issues/28318">#28318&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>acejune&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>al97&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anton Shalkovich&lt;/p>
&lt;p>ArjunGHUB&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brett Morgan&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Connor Brett&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Hansen&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmytro Sadovnychyi&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Gabriel Lacroix&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hong Liang Teoh&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jon esperanza&lt;/p>
&lt;p>Josef Šimánek&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>Mahmud Ridwan&lt;/p>
&lt;p>Manav Garg&lt;/p>
&lt;p>Marco Vela&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>Peter Sobot&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Saba Sathya&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Steven Niemitz&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p></description><link>/blog/beam-2.50.0/</link><pubDate>Wed, 30 Aug 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.50.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Beam 2.49.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.49.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2490-2023-07-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.49.0, check out the &lt;a href="https://github.com/apache/beam/milestone/13">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable Change Streams added in Java &lt;code>BigtableIO.ReadChangeStream&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/27183">#27183&lt;/a>).&lt;/li>
&lt;li>Added Bigtable Read and Write cross-language transforms to Python SDK ((&lt;a href="https://github.com/apache/beam/issues/26593">#26593&lt;/a>), (&lt;a href="https://github.com/apache/beam/issues/27146">#27146&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow prebuilding large images when using &lt;code>--prebuild_sdk_container_engine=cloud_build&lt;/code>, like images depending on &lt;code>tensorflow&lt;/code> or &lt;code>torch&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/27023">#27023&lt;/a>).&lt;/li>
&lt;li>Disabled &lt;code>pip&lt;/code> cache when installing packages on the workers. This reduces the size of prebuilt Python container images (&lt;a href="https://github.com/apache/beam/pull/27035">#27035&lt;/a>).&lt;/li>
&lt;li>Select dedicated avro datum reader and writer (Java) (&lt;a href="https://github.com/apache/beam/issues/18874">#18874&lt;/a>).&lt;/li>
&lt;li>Timer API for the Go SDK (Go) (&lt;a href="https://github.com/apache/beam/issues/22737">#22737&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Python 3.7 support. (&lt;a href="https://github.com/apache/beam/issues/26447">#26447&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed KinesisIO &lt;code>NullPointerException&lt;/code> when a progress check is made before the reader is started (IO) (&lt;a href="https://github.com/apache/beam/issues/23868">#23868&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.49.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Dopierała&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Gavin McDonald&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>John Gill&lt;/p>
&lt;p>Joseph Crowley&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Galloway&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Masato Nakamura&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nathaniel Young&lt;/p>
&lt;p>Nelson Osacky&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Smeet nagda&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>WuA&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Xianhua Liu&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zachary Houfek&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>bigduu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>jubebo&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>sultanalieva-s&lt;/p>
&lt;p>vitaly.terentyev&lt;/p></description><link>/blog/beam-2.49.0/</link><pubDate>Mon, 17 Jul 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.49.0/</guid><category>blog</category><category>release</category></item><item><title>Managing Beam dependencies in Java</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Managing your Java dependencies can be challenging, and if not done correctly,
it may cause a variety of problems, as incompatibilities may arise when using
specific and previously untested combinations.&lt;/p>
&lt;p>To make that process easier, Beam now
provides &lt;a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#bill-of-materials-bom-poms">Bill of Materials (BOM)&lt;/a>
artifacts that will help dependency management tools to select compatible
combinations.&lt;/p>
&lt;p>We hope this will make it easier for you to use Apache Beam, and have a simpler
transition when upgrading to newer versions.&lt;/p>
&lt;p>When bringing incompatible classes and libraries, the code is susceptible to
errors such
as &lt;code>NoClassDefFoundError&lt;/code>, &lt;code>NoSuchMethodError&lt;/code>, &lt;code>NoSuchFieldError&lt;/code>, &lt;code>FATAL ERROR in native method&lt;/code>.&lt;/p>
&lt;p>When importing Apache Beam, the recommended way is to use Bill of Materials
(BOMs). The way BOMs work is by providing hints to the dependency management
resolution tool, so when a project imports unspecified or ambiguous dependencies,
it will know what version to use.&lt;/p>
&lt;p>There are currently two BOMs provided by Beam:&lt;/p>
&lt;ul>
&lt;li>&lt;code>beam-sdks-java-bom&lt;/code>, which manages what dependencies of Beam will be used, so
you can specify the version only once.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>, a more comprehensive list,
which manages Beam, along with GCP client and third-party dependencies.&lt;/li>
&lt;/ul>
&lt;p>Since errors are more likely to arise when using third-party dependencies,
that&amp;rsquo;s the one that is recommended to use to minimize any conflicts.&lt;/p>
&lt;p>In order to use BOM, the artifact has to be imported to your Maven or Gradle
dependency configurations. For example, to
use &lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>,
the following changes have to be done (and make sure that &lt;em>BEAM_VERSION&lt;/em> is
replaced by a valid version):&lt;/p>
&lt;p>&lt;strong>Maven&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-google-cloud-platform-bom&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;version&amp;gt;&lt;/span>BEAM_VERSION&lt;span class="nt">&amp;lt;/version&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;type&amp;gt;&lt;/span>pom&lt;span class="nt">&amp;lt;/type&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;scope&amp;gt;&lt;/span>import&lt;span class="nt">&amp;lt;/scope&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Gradle&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>dependencies {
implementation(platform(&amp;#34;org.apache.beam:beam-sdks-java-google-cloud-platform-bom:BEAM_VERSION&amp;#34;))
}
&lt;/code>&lt;/pre>&lt;p>After importing the BOM, specific version pinning of dependencies, for example,
anything for &lt;code>org.apache.beam&lt;/code>, &lt;code>io.grpc&lt;/code>, &lt;code>com.google.cloud&lt;/code> (
including &lt;code>libraries-bom&lt;/code>) may be removed.&lt;/p>
&lt;p>Do not entirely remove the dependencies, as they are not automatically imported
by the BOM. It is important to keep the dependency without specifying a version.
For example, in Maven:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-core&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or Gradle:&lt;/p>
&lt;pre tabindex="0">&lt;code>implementation(&amp;#34;org.apache.beam:beam-sdks-java-core&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>For a full list of dependency versions that are managed by a specific BOM, the
Maven tool &lt;code>help:effective-pom&lt;/code> can be used. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">mvn help:effective-pom -f ~/.m2/repository/org/apache/beam/beam-sdks-java-google-cloud-platform-bom/BEAM_VERSION/beam-sdks-java-google-cloud-platform-bom-BEAM_VERSION.pom
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The third-party
website &lt;a href="https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-google-cloud-platform-bom/">mvnrepository.com&lt;/a>
can also be used to display such version information.&lt;/p>
&lt;p>We hope you find this
useful. &lt;a href="https://beam.apache.org/community/contact-us/">Feedback&lt;/a> and
contributions are always welcome! So feel free to create a GitHub issue, or open
a Pull Request if you encounter any problem when using those artifacts.&lt;/p></description><link>/blog/managing-beam-dependencies-in-java/</link><pubDate>Fri, 23 Jun 2023 09:00:00 -0700</pubDate><guid>/blog/managing-beam-dependencies-in-java/</guid><category>blog</category></item><item><title>Getting started with Apache Beam: An open source proficiency credential sponsored by Google Cloud</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-badge-image-scaled.png"
alt="Quest image">&lt;/p>
&lt;p>We’re excited to announce the release of the &lt;a href="https://www.cloudskillsboost.google/quests/310">“Getting Started with Apache Beam” quest&lt;/a>, a series of four online labs that venture into different Apache Beam concepts. When you complete all four labs, you’ll earn a Google Cloud badge that you can share on platforms like LinkedIn. Earning this badge should take less than seven hours total, and signing up for the quest costs $20 (there are often free specials for people who attend Beam events, such as &lt;a href="https://www.meetup.com/topics/apache-beam/">Meetups&lt;/a>, &lt;a href="https://beamsummit.org/">Beam Summit&lt;/a>, and &lt;a href="https://beamcollege.dev/">Beam College&lt;/a>).&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>&amp;ldquo;I really like the workshop to be honest. I learnt a lot from doing those labs. I would suggest we offer that for any new members to Beam&amp;rdquo;&lt;/em> &amp;ndash; Shunping Huang, Software Engineer&lt;/p>
&lt;/blockquote>
&lt;p>Beam is one of the largest big data open source projects actively in development. Over the past six years, the Apache Beam community has seen tremendous growth in the number of contributors, committers, and users. If you’re a long time Beam user, you can now earn a badge to show your skills to potential employers. If you’re new to Beam, you can begin your learning journey with this quest. To attempt this quest, you don’t need any prior knowledge of data processing or distributed systems. All you need is elementary knowledge about programming.&lt;/p>
&lt;p>Individuals aren’t the only ones who can benefit from completing this quest - organizations can too! Because earning this badge represents deep knowledge of an industry leading big data library, having the badge validates your organization’s understanding of Beam. In addition, you can run the Beam library on a wide variety of runners, including Google Cloud Dataflow, Flink, Spark, and more, making knowledge about this library highly transferable. Finally, your organization can use this quest as onboarding material for new hires on big data teams, allowing teams and organizations to get their newest employees up-to-date on the latest and greatest that Apache Beam has to offer.&lt;/p>
&lt;p>Data Processing is a key part of AI/ML workflows. Given the recent advancements in artificial intelligence, now’s the time to jump into the world of data processing! Get started on your journey &lt;a href="https://www.cloudskillsboost.google/quests/310">here&lt;/a>.&lt;/p>
&lt;p>We are currently offering this quest &lt;strong>FREE OF CHARGE&lt;/strong>. To obtain your badge for &lt;strong>FREE&lt;/strong>, use the &lt;a href="https://www.cloudskillsboost.google/catalog?qlcampaign=1h-swiss-19">Access Code&lt;/a>, create an account, and search &lt;a href="https://www.cloudskillsboost.google/quests/310">&amp;ldquo;Getting Started with Apache Beam&amp;rdquo;&lt;/a>. If the code does not work, please email &lt;a href="dev@beam.apache.org">dev@beam.apache.org&lt;/a> to obtain a free code.&lt;/p>
&lt;p>PS: Once you earn your badge, please &lt;a href="https://support.google.com/qwiklabs/answer/9222527?hl=en&amp;amp;sjid=14905615709060962899-NA">share it on social media&lt;/a>!&lt;/p></description><link>/blog/beamquest/</link><pubDate>Tue, 06 Jun 2023 00:00:01 -0800</pubDate><guid>/blog/beamquest/</guid><category>blog</category></item><item><title>Apache Beam 2.48.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.48.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2480-2023-05-31">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.48.0, check out the &lt;a href="https://github.com/apache/beam/milestone/12">detailed release notes&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Note: The release tag for Go SDK for this release is sdks/v2.48.2 instead of sdks/v2.48.0 because of incorrect commit attached to the release tag sdks/v2.48.0.&lt;/strong>&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Experimental&amp;rdquo; annotation cleanup: the annotation and concept have been removed from Beam to avoid
the misperception of code as &amp;ldquo;not ready&amp;rdquo;. Any proposed breaking changes will be subject to
case-by-case pro/con decision making (and generally avoided) rather than using the &amp;ldquo;Experimental&amp;rdquo;
to allow them.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added rename for GCS and copy for local filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/26064">#25779&lt;/a>).&lt;/li>
&lt;li>Added support for enhanced fan-out in KinesisIO.Read (Java) (&lt;a href="https://github.com/apache/beam/issues/19967">#19967&lt;/a>).
&lt;ul>
&lt;li>This change is not compatible with Flink savepoints created by Beam 2.46.0 applications which had KinesisIO sources.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added textio.ReadWithFilename transform (Go) (&lt;a href="https://github.com/apache/beam/issues/25812">#25812&lt;/a>).&lt;/li>
&lt;li>Added fileio.MatchContinuously transform (Go) (&lt;a href="https://github.com/apache/beam/issues/26186">#26186&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow passing service name for google-cloud-profiler (Python) (&lt;a href="https://github.com/apache/beam/issues/26280">#26280&lt;/a>).&lt;/li>
&lt;li>Dead letter queue support added to RunInference in Python (&lt;a href="https://github.com/apache/beam/issues/24209">#24209&lt;/a>).&lt;/li>
&lt;li>Support added for defining pre/postprocessing operations on the RunInference transform (&lt;a href="https://github.com/apache/beam/issues/26308">#26308&lt;/a>)&lt;/li>
&lt;li>Adds a Docker Compose based transform service that can be used to discover and use portable Beam transforms (&lt;a href="https://github.com/apache/beam/pull/26023">#26023&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Passing a tag into MultiProcessShared is now required in the Python SDK (&lt;a href="https://github.com/apache/beam/issues/26168">#26168&lt;/a>).&lt;/li>
&lt;li>CloudDebuggerOptions is removed (deprecated in Beam v2.47.0) for Dataflow runner as the Google Cloud Debugger service is &lt;a href="https://cloud.google.com/debugger/docs/deprecations">shutting down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;li>AWS 2 client providers (deprecated in Beam &lt;a href="#2380---2022-04-20">v2.38.0&lt;/a>) are finally removed (&lt;a href="https://github.com/apache/beam/issues/26681">#26681&lt;/a>).&lt;/li>
&lt;li>AWS 2 SnsIO.writeAsync (deprecated in Beam v2.37.0 due to risk of data loss) was finally removed (&lt;a href="https://github.com/apache/beam/issues/26710">#26710&lt;/a>).&lt;/li>
&lt;li>AWS 2 coders (deprecated in Beam v2.43.0 when adding Schema support for AWS Sdk Pojos) are finally removed (&lt;a href="https://github.com/apache/beam/issues/23315">#23315&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java bootloader failing with Too Long Args due to long classpaths, with a pathing jar. (Java) (&lt;a href="https://github.com/apache/beam/issues/25582">#25582&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>PubsubIO writes will throw &lt;em>SizeLimitExceededException&lt;/em> for any message above 100 bytes, when used in batch (bounded) mode. (Java) (&lt;a href="https://github.com/apache/beam/issues/27000">#27000&lt;/a>).&lt;/li>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.48.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bazyli Polednia&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Arn&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Novitskiy&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Kuba Rauch&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Pranjal Joshi&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>RuiLong J&lt;/p>
&lt;p>RyujiTamaki&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vishal Bhise&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>kellen&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>mokamoka03210120&lt;/p>
&lt;p>psolomin&lt;/p></description><link>/blog/beam-2.48.0/</link><pubDate>Wed, 31 May 2023 11:30:00 -0400</pubDate><guid>/blog/beam-2.48.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Beam 2.47.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.47.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2470-2023-05-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.47.0, check out the &lt;a href="https://github.com/apache/beam/milestone/10">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam adds Python 3.11 support (&lt;a href="https://github.com/apache/beam/issues/23848">#23848&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>BigQuery Storage Write API is now available in Python SDK via cross-language (&lt;a href="https://github.com/apache/beam/issues/21961">#21961&lt;/a>).&lt;/li>
&lt;li>Added HbaseIO support for writing RowMutations (ordered by rowkey) to Hbase (Java) (&lt;a href="https://github.com/apache/beam/issues/25830">#25830&lt;/a>).&lt;/li>
&lt;li>Added fileio transforms MatchFiles, MatchAll and ReadMatches (Go) (&lt;a href="https://github.com/apache/beam/issues/25779">#25779&lt;/a>).&lt;/li>
&lt;li>Add integration test for JmsIO + fix issue with multiple connections (Java) (&lt;a href="https://github.com/apache/beam/issues/25887">#25887&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Flink runner now supports Flink 1.16.x (&lt;a href="https://github.com/apache/beam/issues/25046">#25046&lt;/a>).&lt;/li>
&lt;li>Schema&amp;rsquo;d PTransforms can now be directly applied to Beam dataframes just like PCollections.
(Note that when doing multiple operations, it may be more efficient to explicitly chain the operations
like &lt;code>df | (Transform1 | Transform2 | ...)&lt;/code> to avoid excessive conversions.)&lt;/li>
&lt;li>The Go SDK adds new transforms periodic.Impulse and periodic.Sequence that extends support
for slowly updating side input patterns. (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>Several Google client libraries in Python SDK dependency chain were updated to latest available major versions. (&lt;a href="https://github.com/apache/beam/pull/24599">#24599&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>If a main session fails to load, the pipeline will now fail at worker startup. (&lt;a href="https://github.com/apache/beam/issues/25401">#25401&lt;/a>).&lt;/li>
&lt;li>Python pipeline options will now ignore unparsed command line flags prefixed with a single dash. (&lt;a href="https://github.com/apache/beam/issues/25943">#25943&lt;/a>).&lt;/li>
&lt;li>The SmallestPerKey combiner now requires keyword-only arguments for specifying optional parameters, such as &lt;code>key&lt;/code> and &lt;code>reverse&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/25888">#25888&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Cloud Debugger support and its pipeline options are deprecated and will be removed in the next Beam version,
in response to the Google Cloud Debugger service &lt;a href="https://cloud.google.com/debugger/docs/deprecations">turning down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>BigQuery sink in STORAGE_WRITE_API mode in batch pipelines might result in data consistency issues during the handling of other unrelated transient errors for Beam SDKs 2.35.0 - 2.46.0 (inclusive). For more details see: &lt;a href="https://github.com/apache/beam/issues/26521">https://github.com/apache/beam/issues/26521&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>BigQueryIO Storage API write with autoUpdateSchema may cause data corruption for Beam SDKs 2.45.0 - 2.47.0 (inclusive) (&lt;a href="https://github.com/apache/beam/issues/26789">#26789&lt;/a>)&lt;/li>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.47.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amir Fayazi&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Andrey Kot&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>ChangyuLi28&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Ma&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Jiangjie (Becket) Qin&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Weaver&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Reza Rokni&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Saifuddin53&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Thomas Gaddy&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yanan Hao&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yuvi Panda&lt;/p>
&lt;p>andres-vv&lt;/p>
&lt;p>bochap&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>hnnsgstfssn&lt;/p>
&lt;p>jrmccluskey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xianhualiu&lt;/p>
&lt;p>zhangskz&lt;/p></description><link>/blog/beam-2.47.0/</link><pubDate>Wed, 10 May 2023 12:00:00 -0500</pubDate><guid>/blog/beam-2.47.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Beam 2.46.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.46.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2460-2023-03-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.46.0, check out the &lt;a href="https://github.com/apache/beam/milestone/9?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java SDK containers migrated to &lt;a href="https://hub.docker.com/_/eclipse-temurin">Eclipse Temurin&lt;/a>
as a base. This change migrates away from the deprecated &lt;a href="https://hub.docker.com/_/openjdk">OpenJDK&lt;/a>
container. Eclipse Temurin is currently based upon Ubuntu 22.04 while the OpenJDK
container was based upon Debian 11.&lt;/li>
&lt;li>RunInference PTransform will accept model paths as SideInputs in Python SDK. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>RunInference supports ONNX runtime in Python SDK (&lt;a href="https://github.com/apache/beam/issues/22972">#22972&lt;/a>)&lt;/li>
&lt;li>Tensorflow Model Handler for RunInference in Python SDK (&lt;a href="https://github.com/apache/beam/issues/25366">#25366&lt;/a>)&lt;/li>
&lt;li>Java SDK modules migrated to use &lt;code>:sdks:java:extensions:avro&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/24748">#24748&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added in JmsIO a retry policy for failed publications (Java) (&lt;a href="https://github.com/apache/beam/issues/24971">#24971&lt;/a>).&lt;/li>
&lt;li>Support for &lt;code>LZMA&lt;/code> compression/decompression of text files added to the Python SDK (&lt;a href="https://github.com/apache/beam/issues/25316">#25316&lt;/a>)&lt;/li>
&lt;li>Added ReadFrom/WriteTo Csv/Json as top-level transforms to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;li>Option for SparkRunner to avoid the need of SDF output to fit in memory (&lt;a href="https://github.com/apache/beam/issues/23852">#23852&lt;/a>).
This helps e.g. with ParquetIO reads. Turn the feature on by adding experiment &lt;code>use_bounded_concurrent_output_for_sdf&lt;/code>.&lt;/li>
&lt;li>Add &lt;code>WatchFilePattern&lt;/code> transform, which can be used as a side input to the RunInference PTransfrom to watch for model updates using a file pattern. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>Add support for loading TorchScript models with &lt;code>PytorchModelHandler&lt;/code>. The TorchScript model path can be
passed to PytorchModelHandler using &lt;code>torch_script_model_path=&amp;lt;path_to_model&amp;gt;&lt;/code>. (&lt;a href="https://github.com/apache/beam/pull/25321">#25321&lt;/a>)&lt;/li>
&lt;li>The Go SDK now requires Go 1.19 to build. (&lt;a href="https://github.com/apache/beam/pull/25545">#25545&lt;/a>)&lt;/li>
&lt;li>The Go SDK now has an initial native Go implementation of a portable Beam Runner called Prism. (&lt;a href="https://github.com/apache/beam/pull/24789">#24789&lt;/a>)
&lt;ul>
&lt;li>For more details and current state see &lt;a href="https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism">https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The deprecated SparkRunner for Spark 2 (see &lt;a href="#2410---2022-08-23">2.41.0&lt;/a>) was removed (&lt;a href="https://github.com/apache/beam/pull/25263">#25263&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s BatchElements performs more aggressive batching in some cases,
capping at 10 second rather than 1 second batches by default and excluding
fixed cost in this computation to better handle cases where the fixed cost
is larger than a single second. To get the old behavior, one can pass
&lt;code>target_batch_duration_secs_including_fixed_cost=1&lt;/code> to BatchElements.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Avro related classes are deprecated in module &lt;code>beam-sdks-java-core&lt;/code> and will be eventually removed. Please, migrate to a new module &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead by importing the classes from &lt;code>org.apache.beam.sdk.extensions.avro&lt;/code> package.
For the sake of migration simplicity, the relative package path and the whole class hierarchy of Avro related classes in new module is preserved the same as it was before.
For example, import &lt;code>org.apache.beam.sdk.extensions.avro.coders.AvroCoder&lt;/code> class instead of&lt;code>org.apache.beam.sdk.coders.AvroCoder&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/24749">#24749&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.46.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Katz&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Egbert van der Wal&lt;/p>
&lt;p>Elizaveta Lomteva&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Herman Mak&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo E&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruslan Altynnikov&lt;/p>
&lt;p>Ryan Zhang&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sam sam&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shivam&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vachan&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vincent Devillers&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>William Ross Morrow&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>Ziqi Ma&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>aliftadvantage&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kamrankoupayi&lt;/p>
&lt;p>kileys&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>nickuncaged1201&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p></description><link>/blog/beam-2.46.0/</link><pubDate>Fri, 10 Mar 2023 13:00:00 -0500</pubDate><guid>/blog/beam-2.46.0/</guid><category>blog</category><category>release</category></item></channel></rss>