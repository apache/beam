<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Apache Snowflake I/O connector</title><meta name=description content="Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes."><link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400" rel=stylesheet><link rel=preload href=/scss/main.min.7bfa213b38fe814e9a5d5af502d4d2e0d4e9e7dfe8a528843e32a858c6c92bc2.css as=style><link href=/scss/main.min.7bfa213b38fe814e9a5d5af502d4d2e0d4e9e7dfe8a528843e32a858c6c92bc2.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-2.2.4.min.js></script><style>.body__contained img{max-width:100%}</style><script src=/js/bootstrap.min.js></script><script src=/js/language-switch.js></script><script src=/js/fix-menu.js></script><script src=/js/section-nav.js></script><script src=/js/page-nav.js></script><link rel=alternate type=application/rss+xml title="Apache Beam" href=/feed.xml><link rel=canonical href=/documentation/io/built-in/snowflake/ data-proofer-ignore><link rel="shortcut icon" type=image/x-icon href=/images/favicon.ico><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.4.1/css/all.css integrity=sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz crossorigin=anonymous><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-73650088-1','auto');ga('send','pageview');</script></head><body class=body data-spy=scroll data-target=.page-nav data-offset=0><nav class="header navbar navbar-fixed-top"><div class=navbar-header><button type=button class=navbar-toggle aria-expanded=false aria-controls=navbar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a href=/ class=navbar-brand><img alt=Brand style=height:25px src=/images/beam_logo_navbar.png></a></div><div class="navbar-mask closed"></div><div id=navbar class="navbar-container closed"><ul class="nav navbar-nav"><li><a href=/get-started/beam-overview/>Get Started</a></li><li><a href=/documentation/>Documentation</a></li><li><a href=/documentation/sdks/java/>Languages</a></li><li><a href=/documentation/runners/capability-matrix/>RUNNERS</a></li><li><a href=/roadmap/>Roadmap</a></li><li><a href=/contribute/>Contribute</a></li><li><a href=/community/contact-us/>Community</a></li><li><a href=/blog/>Blog</a></li></ul><ul class="nav navbar-nav navbar-right"><li><div style=width:300px><script>(function(){var cx='012923275103528129024:4emlchv9wzi';var gcse=document.createElement('script');gcse.type='text/javascript';gcse.async=true;gcse.src='https://cse.google.com/cse.js?cx='+cx;var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gcse,s);})();</script><gcse:search></gcse:search></div></li><li class=dropdown><a href=# class=dropdown-toggle data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false><img src=https://www.apache.org/foundation/press/kit/feather_small.png alt="Apache Logo" style=height:20px><span class=caret></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href=http://www.apache.org/>ASF Homepage</a></li><li><a href=http://www.apache.org/licenses/>License</a></li><li><a href=http://www.apache.org/security/>Security</a></li><li><a href=http://www.apache.org/foundation/thanks.html>Thanks</a></li><li><a href=http://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li><li><a href=https://www.apache.org/foundation/policies/conduct>Code of Conduct</a></li></ul></li><li><a href=https://github.com/apache/beam/edit/master/website/www/site/content/en/documentation/io/built-in/snowflake.md data-proofer-ignore><i class="far fa-edit fa-lg" alt="Edit on GitHub" title="Edit on GitHub"></i></a></li></ul></div></nav><div class="clearfix container-main-content"><div class="section-nav closed" data-offset-top=90 data-offset-bottom=500><span class="section-nav-back glyphicon glyphicon-menu-left"></span><nav><ul class=section-nav-list data-section-nav><li><span class=section-nav-list-main-title>Documentation</span></li><li><a href=/documentation>Using the Documentation</a></li><li><span class=section-nav-list-title>Pipeline development lifecycle</span><ul class=section-nav-list><li><a href=/documentation/pipelines/design-your-pipeline/>Design Your Pipeline</a></li><li><a href=/documentation/pipelines/create-your-pipeline/>Create Your Pipeline</a></li><li><a href=/documentation/pipelines/test-your-pipeline/>Test Your Pipeline</a></li></ul></li><li><span class=section-nav-list-title>Beam programming guide</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/>Overview</a></li><li><a href=/documentation/programming-guide/#creating-a-pipeline>Pipelines</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>PCollections</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#pcollections>Creating a PCollection</a></li><li><a href=/documentation/programming-guide/#pcollection-characteristics>PCollection characteristics</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Transforms</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#applying-transforms>Applying transforms</a></li><li><span class=section-nav-list-title>Core Beam transforms</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#pardo>ParDo</a></li><li><a href=/documentation/programming-guide/#groupbykey>GroupByKey</a></li><li><a href=/documentation/programming-guide/#cogroupbykey>CoGroupByKey</a></li><li><a href=/documentation/programming-guide/#combine>Combine</a></li><li><a href=/documentation/programming-guide/#flatten>Flatten</a></li><li><a href=/documentation/programming-guide/#partition>Partition</a></li></ul></li><li><a href=/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms>Requirements for user code</a></li><li><a href=/documentation/programming-guide/#side-inputs>Side inputs</a></li><li><a href=/documentation/programming-guide/#additional-outputs>Additional outputs</a></li><li><a href=/documentation/programming-guide/#composite-transforms>Composite transforms</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Pipeline I/O</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#pipeline-io>Using I/O transforms</a></li><li><a href=/documentation/io/built-in/>Built-in I/O connectors</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Built-in I/O connector guides</span><ul class=section-nav-list><li><a href=/documentation/io/built-in/parquet/>Apache Parquet I/O connector</a></li><li><a href=/documentation/io/built-in/hadoop/>Hadoop Input/Output Format IO</a></li><li><a href=/documentation/io/built-in/hcatalog/>HCatalog IO</a></li><li><a href=/documentation/io/built-in/google-bigquery/>Google BigQuery I/O connector</a></li><li><a href=/documentation/io/built-in/snowflake/>Snowflake I/O connector</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Developing new I/O connectors</span><ul class=section-nav-list><li><a href=/documentation/io/developing-io-overview/>Overview: Developing connectors</a></li><li><a href=/documentation/io/developing-io-java/>Developing connectors (Java)</a></li><li><a href=/documentation/io/developing-io-python/>Developing connectors (Python)</a></li></ul></li><li><a href=/documentation/io/testing/>Testing I/O transforms</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Schemas</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#what-is-a-schema>What is a schema</a></li><li><a href=/documentation/programming-guide/#schemas-for-pl-types>Schemas for programming language types</a></li><li><a href=/documentation/programming-guide/#schema-definition>Schema definition</a></li><li><a href=/documentation/programming-guide/#logical-types>Logical types</a></li><li><a href=/documentation/programming-guide/#creating-schemas>Creating schemas</a></li><li><a href=/documentation/programming-guide/#using-schemas>Using schemas</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Data encoding and type safety</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#data-encoding-and-type-safety>Data encoding basics</a></li><li><a href=/documentation/programming-guide/#specifying-coders>Specifying coders</a></li><li><a href=/documentation/programming-guide/#default-coders-and-the-coderregistry>Default coders and the CoderRegistry</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Windowing</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#windowing>Windowing basics</a></li><li><a href=/documentation/programming-guide/#provided-windowing-functions>Provided windowing functions</a></li><li><a href=/documentation/programming-guide/#setting-your-pcollections-windowing-function>Setting your PCollection’s windowing function</a></li><li><a href=/documentation/programming-guide/#watermarks-and-late-data>Watermarks and late data</a></li><li><a href=/documentation/programming-guide/#adding-timestamps-to-a-pcollections-elements>Adding timestamps to a PCollection’s elements</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Triggers</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#triggers>Trigger basics</a></li><li><a href=/documentation/programming-guide/#event-time-triggers>Event time triggers and the default trigger</a></li><li><a href=/documentation/programming-guide/#processing-time-triggers>Processing time triggers</a></li><li><a href=/documentation/programming-guide/#data-driven-triggers>Data-driven triggers</a></li><li><a href=/documentation/programming-guide/#setting-a-trigger>Setting a trigger</a></li><li><a href=/documentation/programming-guide/#composite-triggers>Composite triggers</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Metrics</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#metrics>Metrics basics</a></li><li><a href=/documentation/programming-guide/#types-of-metrics>Types of metrics</a></li><li><a href=/documentation/programming-guide/#querying-metrics>Querying metrics</a></li><li><a href=/documentation/programming-guide/#using-metrics>Using metrics in pipeline</a></li><li><a href=/documentation/programming-guide/#export-metrics>Export metrics</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>State and Timers</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#types-of-state>Types of state</a></li><li><a href=/documentation/programming-guide/#deferred-state-reads>Deferred state reads</a></li><li><a href=/documentation/programming-guide/#timers>Timers</a></li><li><a href=/documentation/programming-guide/#garbage-collecting-state>Garbage collecting state</a></li><li><a href=/documentation/programming-guide/#state-timers-examples>State and timers examples</a></li></ul></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Transform catalog</span><ul class=section-nav-list><li class=section-nav-item--collapsible><span class=section-nav-list-title>Python</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/overview/>Overview</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Element-wise</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/elementwise/filter/>Filter</a></li><li><a href=/documentation/transforms/python/elementwise/flatmap/>FlatMap</a></li><li><a href=/documentation/transforms/python/elementwise/keys/>Keys</a></li><li><a href=/documentation/transforms/python/elementwise/kvswap/>KvSwap</a></li><li><a href=/documentation/transforms/python/elementwise/map/>Map</a></li><li><a href=/documentation/transforms/python/elementwise/pardo/>ParDo</a></li><li><a href=/documentation/transforms/python/elementwise/partition/>Partition</a></li><li><a href=/documentation/transforms/python/elementwise/regex/>Regex</a></li><li><a href=/documentation/transforms/python/elementwise/reify/>Reify</a></li><li><a href=/documentation/transforms/python/elementwise/tostring/>ToString</a></li><li><a href=/documentation/transforms/python/elementwise/values/>Values</a></li><li><a href=/documentation/transforms/python/elementwise/withtimestamps/>WithTimestamps</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Aggregation</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/aggregation/cogroupbykey/>CoGroupByKey</a></li><li><a href=/documentation/transforms/python/aggregation/combineglobally/>CombineGlobally</a></li><li><a href=/documentation/transforms/python/aggregation/combineperkey/>CombinePerKey</a></li><li><a href=/documentation/transforms/python/aggregation/combinevalues/>CombineValues</a></li><li><a href=/documentation/transforms/python/aggregation/count/>Count</a></li><li><a href=/documentation/transforms/python/aggregation/distinct/>Distinct</a></li><li><a href=/documentation/transforms/python/aggregation/groupbykey/>GroupByKey</a></li><li><a href=/documentation/transforms/python/aggregation/groupby/>GroupBy</a></li><li><a href=/documentation/transforms/python/aggregation/groupintobatches/>GroupIntoBatches</a></li><li><a href=/documentation/transforms/python/aggregation/latest/>Latest</a></li><li><a href=/documentation/transforms/python/aggregation/max/>Max</a></li><li><a href=/documentation/transforms/python/aggregation/min/>Min</a></li><li><a href=/documentation/transforms/python/aggregation/mean/>Mean</a></li><li><a href=/documentation/transforms/python/aggregation/sample/>Sample</a></li><li><a href=/documentation/transforms/python/aggregation/sum/>Sum</a></li><li><a href=/documentation/transforms/python/aggregation/top/>Top</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Other</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/other/create/>Create</a></li><li><a href=/documentation/transforms/python/other/flatten/>Flatten</a></li><li><a href=/documentation/transforms/python/other/reshuffle/>Reshuffle</a></li><li><a href=/documentation/transforms/python/other/windowinto/>WindowInto</a></li></ul></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Java</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/overview/>Overview</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Element-wise</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/elementwise/filter/>Filter</a></li><li><a href=/documentation/transforms/java/elementwise/flatmapelements/>FlatMapElements</a></li><li><a href=/documentation/transforms/java/elementwise/keys/>Keys</a></li><li><a href=/documentation/transforms/java/elementwise/kvswap/>KvSwap</a></li><li><a href=/documentation/transforms/java/elementwise/mapelements/>MapElements</a></li><li><a href=/documentation/transforms/java/elementwise/pardo/>ParDo</a></li><li><a href=/documentation/transforms/java/elementwise/partition/>Partition</a></li><li><a href=/documentation/transforms/java/elementwise/regex/>Regex</a></li><li><a href=/documentation/transforms/java/elementwise/reify/>Reify</a></li><li><a href=/documentation/transforms/java/elementwise/values/>Values</a></li><li><a href=/documentation/transforms/java/elementwise/tostring/>ToString</a></li><li><a href=/documentation/transforms/java/elementwise/withkeys/>WithKeys</a></li><li><a href=/documentation/transforms/java/elementwise/withtimestamps/>WithTimestamps</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Aggregation</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/aggregation/approximatequantiles/>ApproximateQuantiles</a></li><li><a href=/documentation/transforms/java/aggregation/approximateunique/>ApproximateUnique</a></li><li><a href=/documentation/transforms/java/aggregation/cogroupbykey/>CoGroupByKey</a></li><li><a href=/documentation/transforms/java/aggregation/combine/>Combine</a></li><li><a href=/documentation/transforms/java/aggregation/combinewithcontext/>CombineWithContext</a></li><li><a href=/documentation/transforms/java/aggregation/count/>Count</a></li><li><a href=/documentation/transforms/java/aggregation/distinct/>Distinct</a></li><li><a href=/documentation/transforms/java/aggregation/groupbykey/>GroupByKey</a></li><li><a href=/documentation/transforms/java/aggregation/groupintobatches/>GroupIntoBatches</a></li><li><a href=/documentation/transforms/java/aggregation/hllcount/>HllCount</a></li><li><a href=/documentation/transforms/java/aggregation/latest/>Latest</a></li><li><a href=/documentation/transforms/java/aggregation/max/>Max</a></li><li><a href=/documentation/transforms/java/aggregation/mean/>Mean</a></li><li><a href=/documentation/transforms/java/aggregation/min/>Min</a></li><li><a href=/documentation/transforms/java/aggregation/sample/>Sample</a></li><li><a href=/documentation/transforms/java/aggregation/sum/>Sum</a></li><li><a href=/documentation/transforms/java/aggregation/top/>Top</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Other</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/other/create/>Create</a></li><li><a href=/documentation/transforms/java/other/flatten/>Flatten</a></li><li><a href=/documentation/transforms/java/other/passert/>PAssert</a></li><li><a href=/documentation/transforms/java/other/view/>View</a></li><li><a href=/documentation/transforms/java/other/window/>Window</a></li></ul></li></ul></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Common pipeline patterns</span><ul class=section-nav-list><li><a href=/documentation/patterns/overview/>Overview</a></li><li><a href=/documentation/patterns/file-processing/>File processing</a></li><li><a href=/documentation/patterns/side-inputs/>Side inputs</a></li><li><a href=/documentation/patterns/pipeline-options/>Pipeline options</a></li><li><a href=/documentation/patterns/custom-io/>Custom I/O</a></li><li><a href=/documentation/patterns/custom-windows/>Custom windows</a></li><li><a href=/documentation/patterns/bigqueryio/>BigQueryIO</a></li><li><a href=/documentation/patterns/ai-platform/>AI Platform</a></li><li><a href=/documentation/patterns/schema/>Schema</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Runtime systems</span><ul class=section-nav-list><li><a href=/documentation/runtime/model/>Execution model</a></li><li><a href=/documentation/runtime/environments/>Container environments</a></li><li><a href=/documentation/runtime/sdk-harness-config/>SDK Harness Configuration</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Learning resources</span><ul class=section-nav-list><li><a href=/documentation/resources/learning-resources/#getting-started>Getting Started</a></li><li><a href=/documentation/resources/learning-resources/#articles>Articles</a></li><li><a href=/documentation/resources/learning-resources/#interactive-labs>Interactive Labs</a></li><li><a href=/documentation/resources/learning-resources/#beam-katas>Beam Katas</a></li><li><a href=/documentation/resources/learning-resources/#code-examples>Code Examples</a></li><li><a href=/documentation/resources/learning-resources/#api-reference>API Reference</a></li><li><a href=/documentation/resources/learning-resources/#feedback-and-suggestions>Feedback and Suggestions</a></li><li><a href=/documentation/resources/learning-resources/#how-to-contribute>How to Contribute</a></li><li><a href=/documentation/resources/videos-and-podcasts>Videos and Podcasts</a></li></ul></li><li><a href=https://cwiki.apache.org/confluence/display/BEAM/Apache+Beam>Beam Wiki</a></li></ul></nav></div><nav class="page-nav clearfix" data-offset-top=90 data-offset-bottom=500><nav id=TableOfContents><ul><li><a href=#authentication>Authentication</a><ul><li><a href=#username-and-password>Username and password</a></li><li><a href=#key-pair>Key pair</a></li><li><a href=#oauth-token>OAuth token</a></li></ul></li><li><a href=#datasource-configuration>DataSource Configuration</a><ul><li><a href=#general-usage>General usage</a></li></ul></li><li><a href=#pipeline-options>Pipeline options</a><ul><li><a href=#snowflake-pipeline-options>Snowflake Pipeline options</a></li></ul></li><li><a href=#running-pipelines-on-dataflow>Running pipelines on Dataflow</a></li><li><a href=#writing-to-snowflake-tables>Writing to Snowflake tables</a><ul><li><a href=#batch-write-from-a-bounded-source>Batch write (from a bounded source)</a></li><li><a href=#userdatamapper-function>UserDataMapper function</a></li><li><a href=#additional-write-options>Additional write options</a><ul><li><a href=#transformation-query>Transformation query</a></li><li><a href=#write-disposition>Write disposition</a></li><li><a href=#create-disposition>Create disposition</a></li><li><a href=#table-schema-disposition>Table schema disposition</a></li></ul></li></ul></li><li><a href=#reading-from-snowflake>Reading from Snowflake</a><ul><li><a href=#general-usage-1>General usage</a></li><li><a href=#csvmapper>CSVMapper</a></li></ul></li></ul></nav></nav><div class="body__contained body__section-nav"><p><a href=/documentation/io/built-in/>Built-in I/O Transforms</a></p><h1 id=snowflake-io>Snowflake I/O</h1><p>Pipeline options and general information about using and running Snowflake IO.</p><h2 id=authentication>Authentication</h2><p>All authentication methods available for the Snowflake JDBC Driver are possible to use with the IO transforms:</p><ul><li>Username and password</li><li>Key pair</li><li>OAuth token</li></ul><p>Passing credentials is done via Pipeline options.</p><p>Passing credentials is done via Pipeline options used to instantiate <code>SnowflakeIO.DataSourceConfiguration</code>:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>SnowflakePipelineOptions</span> <span class=n>options</span> <span class=o>=</span> <span class=n>PipelineOptionsFactory</span>
        <span class=o>.</span><span class=na>fromArgs</span><span class=o>(</span><span class=n>args</span><span class=o>)</span>
        <span class=o>.</span><span class=na>withValidation</span><span class=o>()</span>
        <span class=o>.</span><span class=na>as</span><span class=o>(</span><span class=n>SnowflakePipelineOptions</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>SnowflakeCredentials</span> <span class=n>credentials</span> <span class=o>=</span> <span class=n>SnowflakeCredentialsFactory</span><span class=o>.</span><span class=na>of</span><span class=o>(</span><span class=n>options</span><span class=o>);</span>

<span class=n>SnowflakeIO</span><span class=o>.</span><span class=na>DataSourceConfiguration</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>credentials</span><span class=o>)</span>
        <span class=o>.(</span><span class=n>other</span> <span class=n>DataSourceConfiguration</span> <span class=n>options</span><span class=o>)</span></code></pre></div></div></p><h3 id=username-and-password>Username and password</h3><p>To use username/password authentication in SnowflakeIO, invoke your pipeline with the following Pipeline options:<pre><code>--username=&lt;USERNAME&gt; --password=&lt;PASSWORD&gt;</code></pre></p><h3 id=key-pair>Key pair</h3><p>To use this authentication method, you must first generate a key pair and associate the public key with the Snowflake user that will connect using the IO transform. For instructions, see the <a href=https://docs.snowflake.com/en/user-guide/jdbc-configure.html>Snowflake documentation</a>.</p><p>To use key pair authentication with SnowflakeIO, invoke your pipeline with following Pipeline options:<pre><code>--username=&lt;USERNAME&gt; --privateKeyPath=&lt;PATH_TO_P8_FILE&gt; --privateKeyPassphrase=&lt;PASSWORD_FOR_KEY&gt;</code></pre></p><h3 id=oauth-token>OAuth token</h3><p>SnowflakeIO also supports OAuth token.</p><p><strong>IMPORTANT</strong>: SnowflakeIO requires a valid OAuth access token. It will neither be able to refresh the token nor obtain it using a web-based flow. For information on configuring an OAuth integration and obtaining the token, see the <a href=https://docs.snowflake.com/en/user-guide/oauth-intro.html>Snowflake documentation</a>.</p><p>Once you have the token, invoke your pipeline with following Pipeline Options:<pre><code>--oauthToken=&lt;TOKEN&gt;</code></pre></p><h2 id=datasource-configuration>DataSource Configuration</h2><p>DataSource configuration is required in both read and write object for configuring Snowflake connection properties for IO purposes.</p><h3 id=general-usage>General usage</h3><p>Create the DataSource configuration:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java> <span class=n>SnowflakeIO</span><span class=o>.</span><span class=na>DataSourceConfiguration</span>
            <span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>SnowflakeCredentialsFactory</span><span class=o>.</span><span class=na>of</span><span class=o>(</span><span class=n>options</span><span class=o>))</span>
            <span class=o>.</span><span class=na>withUrl</span><span class=o>(</span><span class=n>options</span><span class=o>.</span><span class=na>getUrl</span><span class=o>())</span>
            <span class=o>.</span><span class=na>withServerName</span><span class=o>(</span><span class=n>options</span><span class=o>.</span><span class=na>getServerName</span><span class=o>())</span>
            <span class=o>.</span><span class=na>withDatabase</span><span class=o>(</span><span class=n>options</span><span class=o>.</span><span class=na>getDatabase</span><span class=o>())</span>
            <span class=o>.</span><span class=na>withWarehouse</span><span class=o>(</span><span class=n>options</span><span class=o>.</span><span class=na>getWarehouse</span><span class=o>())</span>
            <span class=o>.</span><span class=na>withSchema</span><span class=o>(</span><span class=n>options</span><span class=o>.</span><span class=na>getSchema</span><span class=o>());</span></code></pre></div></div>Where parameters can be:</p><ul><li><code>.withUrl(...)</code><ul><li>JDBC-like URL for your Snowflake account, including account name and region, without any parameters.</li><li>Example: <code>.withUrl("jdbc:snowflake://account.snowflakecomputing.com")</code></li></ul></li><li><code>.withServerName(...)</code><ul><li>Server Name - full server name with account, zone and domain.</li><li>Example: <code>.withServerName("account.snowflakecomputing.com")</code></li></ul></li><li><code>.withDatabase(...)</code><ul><li>Name of the Snowflake database to use.</li><li>Example: <code>.withDatabase("MY_DATABASE")</code></li></ul></li><li><code>.withWarehouse(...)</code><ul><li>Name of the Snowflake warehouse to use. This parameter is optional. If no warehouse name is specified, the default warehouse for the user is used.</li><li>Example: <code>.withWarehouse("MY_WAREHOUSE")</code></li></ul></li><li><code>.withSchema(...)</code><ul><li>Name of the schema in the database to use. This parameter is optional.</li><li>Example: <code>.withSchema("PUBLIC")</code></li></ul></li></ul><p><strong>Note</strong> - either <code>.withUrl(...)</code> or <code>.withServerName(...)</code> <strong>is required</strong>.</p><h2 id=pipeline-options>Pipeline options</h2><p>Use Beam’s <a href=https://beam.apache.org/releases/javadoc/2.17.0/org/apache/beam/sdk/options/PipelineOptions.html>Pipeline options</a> to set options via the command line.</p><h3 id=snowflake-pipeline-options>Snowflake Pipeline options</h3><p>Snowflake IO library supports following options that can be passed via the <a href=https://beam.apache.org/documentation/io/built-in/snowflake/#running-main-command-with-pipeline-options>command line</a> by default when a Pipeline uses them:</p><p><code>--url</code> Snowflake&rsquo;s JDBC-like url including account name and region without any parameters.</p><p><code>--serverName</code> Full server name with account, zone and domain.</p><p><code>--username</code> Required for username/password and Private Key authentication.</p><p><code>--oauthToken</code> Required for OAuth authentication only.</p><p><code>--password</code> Required for username/password authentication only.</p><p><code>--privateKeyPath</code> Path to Private Key file. Required for Private Key authentication only.</p><p><code>--privateKeyPassphrase</code> Private Key&rsquo;s passphrase. Required for Private Key authentication only.</p><p><code>--stagingBucketName</code> External bucket path ending with <code>/</code>. I.e. <code>gs://bucket/</code>. Sub-directories are allowed.</p><p><code>--storageIntegrationName</code> Storage integration name</p><p><code>--warehouse</code> Warehouse to use. Optional.</p><p><code>--database</code> Database name to connect to. Optional.</p><p><code>--schema</code> Schema to use. Optional.</p><p><code>--table</code> Table to use. Optional.</p><p><code>--query</code> Query to use. Optional.</p><p><code>--role</code> Role to use. Optional.</p><p><code>--authenticator</code> Authenticator to use. Optional.</p><p><code>--portNumber</code> Port number. Optional.</p><p><code>--loginTimeout</code> Login timeout. Optional.</p><h2 id=running-pipelines-on-dataflow>Running pipelines on Dataflow</h2><p>By default, pipelines are run on <a href=https://beam.apache.org/documentation/runners/direct/>Direct Runner</a> on your local machine. To run a pipeline on <a href=https://cloud.google.com/dataflow/>Google Dataflow</a>, you must provide the following Pipeline options:</p><ul><li><p><code>--runner=DataflowRunner</code></p><ul><li>The Dataflow’s specific runner.</li></ul></li><li><p><code>--project=&lt;GCS PROJECT></code></p><ul><li>Name of the Google Cloud Platform project.</li></ul></li><li><p><code>--stagingBucketName=&lt;GCS BUCKET NAME></code></p><ul><li>Google Cloud Services bucket where the Beam files will be staged.</li></ul></li><li><p><code>--maxNumWorkers=5</code></p><ul><li>(optional) Maximum number of workers.</li></ul></li><li><p><code>--appName=&lt;JOB NAME></code></p><ul><li>(optional) Prefix for the job name in the Dataflow Dashboard.</li></ul></li></ul><p>More pipeline options for Dataflow can be found <a href=https://beam.apache.org/releases/javadoc/2.17.0/org/apache/beam/runners/dataflow/options/DataflowPipelineOptions.html>here</a>.</p><p><strong>Note</strong>: To properly authenticate with Google Cloud, please use <a href=https://cloud.google.com/sdk/gcloud/>gcloud</a> or follow the <a href=https://cloud.google.com/docs/authentication/>Google Cloud documentation</a>.</p><p><strong>Important</strong>: Please acknowledge [Google Dataflow pricing](Important: Please acknowledge Google Dataflow pricing).</p><h2 id=writing-to-snowflake-tables>Writing to Snowflake tables</h2><p>One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user&rsquo;s <a href=https://beam.apache.org/releases/javadoc/2.17.0/org/apache/beam/sdk/values/PCollection.html>PCollection</a> to your Snowflake database.</p><h3 id=batch-write-from-a-bounded-source>Batch write (from a bounded source)</h3><p>The basic .<code>write()</code> operation usage is as follows:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>data</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
   <span class=n>SnowflakeIO</span><span class=o>.&lt;</span><span class=n>type</span><span class=o>&gt;</span><span class=n>write</span><span class=o>()</span>
       <span class=o>.</span><span class=na>withDataSourceConfiguration</span><span class=o>(</span><span class=n>dc</span><span class=o>)</span>
       <span class=o>.</span><span class=na>to</span><span class=o>(</span><span class=s>&#34;MY_TABLE&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStagingBucketName</span><span class=o>(</span><span class=s>&#34;BUCKET NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStorageIntegrationName</span><span class=o>(</span><span class=s>&#34;STORAGE INTEGRATION NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withUserDataMapper</span><span class=o>(</span><span class=n>mapper</span><span class=o>)</span>
<span class=o>)</span></code></pre></div></div>Replace type with the data type of the PCollection object to write; for example, SnowflakeIO.<string> for an input PCollection of Strings.</p><p>All the below parameters are required:</p><ul><li><p><code>.withDataSourceConfiguration()</code> Accepts a DatasourceConfiguration object.</p></li><li><p><code>.to()</code> Accepts the target Snowflake table name.</p></li><li><p><code>.withStagingBucketName()</code> Accepts a cloud bucket path ended with slash.
-Example: <code>.withStagingBucketName("gs://mybucket/my/dir/")</code></p></li><li><p><code>.withStorageIntegrationName()</code> Accepts a name of a Snowflake storage integration object created according to Snowflake documentationt. Example:<pre><code>CREATE OR REPLACE STORAGE INTEGRATION test_integration
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = GCS
ENABLED = TRUE
STORAGE_ALLOWED_LOCATIONS = (&#39;gcs://bucket/&#39;);</code></pre>Then:<pre><code>.withStorageIntegrationName(test_integration)</code></pre></p></li><li><p><code>.withUserDataMapper()</code> Accepts the UserDataMapper function that will map a user&rsquo;s PCollection to an array of String values <code>(String[])</code>.</p></li></ul><p><strong>Note</strong>:
SnowflakeIO uses COPY statements behind the scenes to write (using <a href=https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html>COPY to table</a>). StagingBucketName will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under the “stagingBucketName” path.</p><h3 id=userdatamapper-function>UserDataMapper function</h3><p>The UserDataMapper function is required to map data from a PCollection to an array of String values before the <code>write()</code> operation saves the data to temporary .csv files. For example:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=kd>public</span> <span class=kd>static</span> <span class=n>SnowflakeIO</span><span class=o>.</span><span class=na>UserDataMapper</span><span class=o>&lt;</span><span class=n>Long</span><span class=o>&gt;</span> <span class=nf>getCsvMapper</span><span class=o>()</span> <span class=o>{</span>
    <span class=k>return</span> <span class=o>(</span><span class=n>SnowflakeIO</span><span class=o>.</span><span class=na>UserDataMapper</span><span class=o>&lt;</span><span class=n>Long</span><span class=o>&gt;)</span> <span class=n>recordLine</span> <span class=o>-&gt;</span> <span class=k>new</span> <span class=n>String</span><span class=o>[]</span> <span class=o>{</span><span class=n>recordLine</span><span class=o>.</span><span class=na>toString</span><span class=o>()};</span>
<span class=o>}</span></code></pre></div></div></p><h3 id=additional-write-options>Additional write options</h3><h4 id=transformation-query>Transformation query</h4><p>The <code>.withQueryTransformation()</code> option for the <code>write()</code> operation accepts a SQL query as a String value, which will be performed while transfering data staged in CSV files directly to the target Snowflake table. For information about the transformation SQL syntax, see the <a href=https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html#transformation-parameters>Snowflake Documentation</a>.</p><p>Usage:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>String</span> <span class=n>query</span> <span class=o>=</span> <span class=s>&#34;SELECT t.$1 from YOUR_TABLE;&#34;</span><span class=o>;</span>
<span class=n>data</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
   <span class=n>SnowflakeIO</span><span class=o>.&lt;~&gt;</span><span class=n>write</span><span class=o>()</span>
       <span class=o>.</span><span class=na>withDataSourceConfiguration</span><span class=o>(</span><span class=n>dc</span><span class=o>)</span>
       <span class=o>.</span><span class=na>to</span><span class=o>(</span><span class=s>&#34;MY_TABLE&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStagingBucketName</span><span class=o>(</span><span class=s>&#34;BUCKET NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStorageIntegrationName</span><span class=o>(</span><span class=s>&#34;STORAGE INTEGRATION NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withUserDataMapper</span><span class=o>(</span><span class=n>mapper</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withQueryTransformation</span><span class=o>(</span><span class=n>query</span><span class=o>)</span>
<span class=o>)</span></code></pre></div></div></p><h4 id=write-disposition>Write disposition</h4><p>Define the write behaviour based on the table where data will be written to by specifying the <code>.withWriteDisposition(...)</code> option for the <code>write()</code> operation. The following values are supported:</p><ul><li><p>APPEND - Default behaviour. Written data is added to the existing rows in the table,</p></li><li><p>EMPTY - The target table must be empty; otherwise, the write operation fails,</p></li><li><p>TRUNCATE - The write operation deletes all rows from the target table before writing to it.</p></li></ul><p>Example of usage:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>data</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
   <span class=n>SnowflakeIO</span><span class=o>.&lt;~&gt;</span><span class=n>write</span><span class=o>()</span>
       <span class=o>.</span><span class=na>withDataSourceConfiguration</span><span class=o>(</span><span class=n>dc</span><span class=o>)</span>
       <span class=o>.</span><span class=na>to</span><span class=o>(</span><span class=s>&#34;MY_TABLE&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStagingBucketName</span><span class=o>(</span><span class=s>&#34;BUCKET NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStorageIntegrationName</span><span class=o>(</span><span class=s>&#34;STORAGE INTEGRATION NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withUserDataMapper</span><span class=o>(</span><span class=n>mapper</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withWriteDisposition</span><span class=o>(</span><span class=n>TRUNCATE</span><span class=o>)</span>
<span class=o>)</span></code></pre></div></div></p><h4 id=create-disposition>Create disposition</h4><p>The <code>.withCreateDisposition()</code> option defines the behavior of the write operation if the target table does not exist . The following values are supported:</p><ul><li><p>CREATE_IF_NEEDED - default behaviour. The write operation checks whether the specified target table exists; if it does not, the write operation attempts to create the table Specify the schema for the target table using the <code>.withTableSchema()</code> option.</p></li><li><p>CREATE_NEVER - The write operation fails if the target table does not exist.</p></li></ul><p>Usage:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>data</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
   <span class=n>SnowflakeIO</span><span class=o>.&lt;~&gt;</span><span class=n>write</span><span class=o>()</span>
       <span class=o>.</span><span class=na>withDataSourceConfiguration</span><span class=o>(</span><span class=n>dc</span><span class=o>)</span>
       <span class=o>.</span><span class=na>to</span><span class=o>(</span><span class=s>&#34;MY_TABLE&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStagingBucketName</span><span class=o>(</span><span class=s>&#34;BUCKET NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStorageIntegrationName</span><span class=o>(</span><span class=s>&#34;STORAGE INTEGRATION NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withUserDataMapper</span><span class=o>(</span><span class=n>mapper</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withCreateDisposition</span><span class=o>(</span><span class=n>CREATE_NEVER</span><span class=o>)</span>
<span class=o>)</span></code></pre></div></div></p><h4 id=table-schema-disposition>Table schema disposition</h4><p>When the <code>.withCreateDisposition()</code> .option is set to <code>CREATE_IF_NEEDED</code>, the <code>.withTableSchema()</code> option enables specifying the schema for the created target table.
A table schema is a list of <code>SFColumn</code> objects with name and type corresponding to column type for each column in the table.</p><p>Usage:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>SFTableSchema</span> <span class=n>tableSchema</span> <span class=o>=</span>
    <span class=k>new</span> <span class=n>SFTableSchema</span><span class=o>(</span>
        <span class=n>SFColumn</span><span class=o>.</span><span class=na>of</span><span class=o>(</span><span class=s>&#34;my_date&#34;</span><span class=o>,</span> <span class=k>new</span> <span class=n>SFDate</span><span class=o>(),</span> <span class=kc>true</span><span class=o>),</span>
        <span class=k>new</span> <span class=n>SFColumn</span><span class=o>(</span><span class=s>&#34;id&#34;</span><span class=o>,</span> <span class=k>new</span> <span class=n>SFNumber</span><span class=o>()),</span>
        <span class=n>SFColumn</span><span class=o>.</span><span class=na>of</span><span class=o>(</span><span class=s>&#34;name&#34;</span><span class=o>,</span> <span class=k>new</span> <span class=n>SFText</span><span class=o>(),</span> <span class=kc>true</span><span class=o>));</span>

<span class=n>data</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
   <span class=n>SnowflakeIO</span><span class=o>.&lt;~&gt;</span><span class=n>write</span><span class=o>()</span>
       <span class=o>.</span><span class=na>withDataSourceConfiguration</span><span class=o>(</span><span class=n>dc</span><span class=o>)</span>
       <span class=o>.</span><span class=na>to</span><span class=o>(</span><span class=s>&#34;MY_TABLE&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStagingBucketName</span><span class=o>(</span><span class=s>&#34;BUCKET NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStorageIntegrationName</span><span class=o>(</span><span class=s>&#34;STORAGE INTEGRATION NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withUserDataMapper</span><span class=o>(</span><span class=n>mapper</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withTableSchema</span><span class=o>(</span><span class=n>tableSchema</span><span class=o>)</span>
<span class=o>)</span></code></pre></div></div></p><h2 id=reading-from-snowflake>Reading from Snowflake</h2><p>One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a <a href=https://beam.apache.org/releases/javadoc/2.17.0/org/apache/beam/sdk/values/PCollection.html>PCollection</a> of user-defined data type.</p><h3 id=general-usage-1>General usage</h3><p>The basic <code>.read()</code> operation usage:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>USER_DATA_TYPE</span><span class=o>&gt;</span> <span class=n>items</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
   <span class=n>SnowflakeIO</span><span class=o>.&lt;</span><span class=n>USER_DATA_TYPE</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
       <span class=o>.</span><span class=na>withDataSourceConfiguration</span><span class=o>(</span><span class=n>dc</span><span class=o>)</span>
       <span class=o>.</span><span class=na>fromTable</span><span class=o>(</span><span class=s>&#34;MY_TABLE&#34;</span><span class=o>)</span> <span class=c1>// or .fromQuery(&#34;QUERY&#34;)
</span><span class=c1></span>       <span class=o>.</span><span class=na>withStagingBucketName</span><span class=o>(</span><span class=s>&#34;BUCKET NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withStorageIntegrationName</span><span class=o>(</span><span class=s>&#34;STORAGE INTEGRATION NAME&#34;</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withCsvMapper</span><span class=o>(</span><span class=n>mapper</span><span class=o>)</span>
       <span class=o>.</span><span class=na>withCoder</span><span class=o>(</span><span class=n>coder</span><span class=o>));</span>
<span class=o>)</span></code></pre></div></div>Where all below parameters are required:</p><ul><li><p><code>.withDataSourceConfiguration(...)</code></p><ul><li>Accepts a DataSourceConfiguration object.</li></ul></li><li><p><code>.fromTable(...) or .fromQuery(...)</code></p><ul><li>Specifies a Snowflake table name or custom SQL query.</li></ul></li><li><p><code>.withStagingBucketName()</code></p><ul><li>Accepts a cloud bucket name.</li></ul></li><li><p><code>.withStorageIntegrationName()</code></p></li><li><p>Accepts a name of a Snowflake storage integration object created according to Snowflake documentation. Example:<pre><code>CREATE OR REPLACE STORAGE INTEGRATION test_integration
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = GCS
ENABLED = TRUE
STORAGE_ALLOWED_LOCATIONS = (&#39;gcs://bucket/&#39;);</code></pre>Then:<pre><code>.withStorageIntegrationName(test_integration)</code></pre></p></li><li><p><code>.withCsvMapper(mapper)</code></p><ul><li>Accepts a <a href=https://beam.apache.org/documentation/io/built-in/snowflake/#csvmapper>CSVMapper</a> instance for mapping String[] to USER_DATA_TYPE.</li></ul></li><li><p><code>.withCoder(coder)</code></p><ul><li>Accepts the <a href=https://beam.apache.org/releases/javadoc/2.0.0/org/apache/beam/sdk/coders/Coder.html>Coder</a> for USER_DATA_TYPE.</li></ul></li></ul><p><strong>Note</strong>:
SnowflakeIO uses COPY statements behind the scenes to read (using <a href=https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html>COPY to location</a>) files staged in cloud storage.StagingBucketName will be used as a temporary location for storing CSV files. Those temporary directories will be named <code>sf_copy_csv_DATE_TIME_RANDOMSUFFIX</code> and they will be removed automatically once Read operation finishes.</p><h3 id=csvmapper>CSVMapper</h3><p>SnowflakeIO uses a <a href=https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html>COPY INTO <location></a>statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via <a href=https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html>FileIO</a> and processed line by line. Each line is split into an array of Strings using the <a href=http://opencsv.sourceforge.net/>OpenCSV</a> library.</p><p>The CSVMapper’s job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom POJO.</p><p>Example implementation of CsvMapper for GenericRecord:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=kd>static</span> <span class=n>SnowflakeIO</span><span class=o>.</span><span class=na>CsvMapper</span><span class=o>&lt;</span><span class=n>GenericRecord</span><span class=o>&gt;</span> <span class=nf>getCsvMapper</span><span class=o>()</span> <span class=o>{</span>
   <span class=k>return</span> <span class=o>(</span><span class=n>SnowflakeIO</span><span class=o>.</span><span class=na>CsvMapper</span><span class=o>&lt;</span><span class=n>GenericRecord</span><span class=o>&gt;)</span>
           <span class=n>parts</span> <span class=o>-&gt;</span> <span class=o>{</span>
               <span class=k>return</span> <span class=k>new</span> <span class=n>GenericRecordBuilder</span><span class=o>(</span><span class=n>PARQUET_SCHEMA</span><span class=o>)</span>
                       <span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;ID&#34;</span><span class=o>,</span> <span class=n>Long</span><span class=o>.</span><span class=na>valueOf</span><span class=o>(</span><span class=n>parts</span><span class=o>[</span><span class=n>0</span><span class=o>]))</span>
                       <span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;NAME&#34;</span><span class=o>,</span> <span class=n>parts</span><span class=o>[</span><span class=n>1</span><span class=o>])</span>
                       <span class=o>[...]</span>
                       <span class=o>.</span><span class=na>build</span><span class=o>();</span>
           <span class=o>};</span>
<span class=o>}</span></code></pre></div></div></p></div></div><footer class=footer><div class=footer__contained><div class=footer__cols><div class=footer__cols__col><div class=footer__cols__col__logo><img src=/images/beam_logo_circle.svg class=footer__logo alt="Beam logo"></div><div class=footer__cols__col__logo><img src=/images/apache_logo_circle.svg class=footer__logo alt="Apache logo"></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Start</div><div class=footer__cols__col__link><a href=/get-started/beam-overview/>Overview</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-java/>Quickstart (Java)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-py/>Quickstart (Python)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-go/>Quickstart (Go)</a></div><div class=footer__cols__col__link><a href=/get-started/downloads/>Downloads</a></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Docs</div><div class=footer__cols__col__link><a href=/documentation/programming-guide/>Concepts</a></div><div class=footer__cols__col__link><a href=/documentation/pipelines/design-your-pipeline/>Pipelines</a></div><div class=footer__cols__col__link><a href=/documentation/runners/capability-matrix/>Runners</a></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Community</div><div class=footer__cols__col__link><a href=/contribute/>Contribute</a></div><div class=footer__cols__col__link><a href=https://projects.apache.org/committee.html?beam target=_blank>Team<img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></div><div class=footer__cols__col__link><a href=/community/presentation-materials/>Media</a></div><div class=footer__cols__col__link><a href=/community/in-person/>Events/Meetups</a></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Resources</div><div class=footer__cols__col__link><a href=/blog/>Blog</a></div><div class=footer__cols__col__link><a href=/community/contact-us/>Contact Us</a></div><div class=footer__cols__col__link><a href=https://github.com/apache/beam>GitHub</a></div></div></div></div><div class=footer__bottom>&copy;
<a href=http://www.apache.org>The Apache Software Foundation</a>
| <a href=/privacy_policy>Privacy Policy</a>
| <a href=/feed.xml>RSS Feed</a><br><br>Apache Beam, Apache, Beam, the Beam logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation. All other products or name brands are trademarks of their respective holders, including The Apache Software Foundation.</div></footer></body></html>