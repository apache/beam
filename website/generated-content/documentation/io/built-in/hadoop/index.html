<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Apache Hadoop Input/Output Format IO</title><meta name=description content="Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes."><link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400" rel=stylesheet><link rel=preload href=/scss/main.min.7bfa213b38fe814e9a5d5af502d4d2e0d4e9e7dfe8a528843e32a858c6c92bc2.css as=style><link href=/scss/main.min.7bfa213b38fe814e9a5d5af502d4d2e0d4e9e7dfe8a528843e32a858c6c92bc2.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-2.2.4.min.js></script><style>.body__contained img{max-width:100%}</style><script src=/js/bootstrap.min.js></script><script src=/js/language-switch.js></script><script src=/js/fix-menu.js></script><script src=/js/section-nav.js></script><script src=/js/page-nav.js></script><link rel=alternate type=application/rss+xml title="Apache Beam" href=/feed.xml><link rel=canonical href=/documentation/io/built-in/hadoop/ data-proofer-ignore><link rel="shortcut icon" type=image/x-icon href=/images/favicon.ico><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.4.1/css/all.css integrity=sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz crossorigin=anonymous><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-73650088-1','auto');ga('send','pageview');</script></head><body class=body data-spy=scroll data-target=.page-nav data-offset=0><nav class="header navbar navbar-fixed-top"><div class=navbar-header><button type=button class=navbar-toggle aria-expanded=false aria-controls=navbar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a href=/ class=navbar-brand><img alt=Brand style=height:25px src=/images/beam_logo_navbar.png></a></div><div class="navbar-mask closed"></div><div id=navbar class="navbar-container closed"><ul class="nav navbar-nav"><li><a href=/get-started/beam-overview/>Get Started</a></li><li><a href=/documentation/>Documentation</a></li><li><a href=/documentation/sdks/java/>Languages</a></li><li><a href=/documentation/runners/capability-matrix/>RUNNERS</a></li><li><a href=/roadmap/>Roadmap</a></li><li><a href=/contribute/>Contribute</a></li><li><a href=/community/contact-us/>Community</a></li><li><a href=/blog/>Blog</a></li></ul><ul class="nav navbar-nav navbar-right"><li><div style=width:300px><script>(function(){var cx='012923275103528129024:4emlchv9wzi';var gcse=document.createElement('script');gcse.type='text/javascript';gcse.async=true;gcse.src='https://cse.google.com/cse.js?cx='+cx;var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gcse,s);})();</script><gcse:search></gcse:search></div></li><li class=dropdown><a href=# class=dropdown-toggle data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false><img src=https://www.apache.org/foundation/press/kit/feather_small.png alt="Apache Logo" style=height:20px><span class=caret></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href=http://www.apache.org/>ASF Homepage</a></li><li><a href=http://www.apache.org/licenses/>License</a></li><li><a href=http://www.apache.org/security/>Security</a></li><li><a href=http://www.apache.org/foundation/thanks.html>Thanks</a></li><li><a href=http://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li><li><a href=https://www.apache.org/foundation/policies/conduct>Code of Conduct</a></li></ul></li><li><a href=https://github.com/apache/beam/edit/master/website/www/site/content/en/documentation/io/built-in/hadoop.md data-proofer-ignore><i class="far fa-edit fa-lg" alt="Edit on GitHub" title="Edit on GitHub"></i></a></li></ul></div></nav><div class="clearfix container-main-content"><div class="section-nav closed" data-offset-top=90 data-offset-bottom=500><span class="section-nav-back glyphicon glyphicon-menu-left"></span><nav><ul class=section-nav-list data-section-nav><li><span class=section-nav-list-main-title>Documentation</span></li><li><a href=/documentation>Using the Documentation</a></li><li><span class=section-nav-list-title>Pipeline development lifecycle</span><ul class=section-nav-list><li><a href=/documentation/pipelines/design-your-pipeline/>Design Your Pipeline</a></li><li><a href=/documentation/pipelines/create-your-pipeline/>Create Your Pipeline</a></li><li><a href=/documentation/pipelines/test-your-pipeline/>Test Your Pipeline</a></li></ul></li><li><span class=section-nav-list-title>Beam programming guide</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/>Overview</a></li><li><a href=/documentation/programming-guide/#creating-a-pipeline>Pipelines</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>PCollections</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#pcollections>Creating a PCollection</a></li><li><a href=/documentation/programming-guide/#pcollection-characteristics>PCollection characteristics</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Transforms</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#applying-transforms>Applying transforms</a></li><li><span class=section-nav-list-title>Core Beam transforms</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#pardo>ParDo</a></li><li><a href=/documentation/programming-guide/#groupbykey>GroupByKey</a></li><li><a href=/documentation/programming-guide/#cogroupbykey>CoGroupByKey</a></li><li><a href=/documentation/programming-guide/#combine>Combine</a></li><li><a href=/documentation/programming-guide/#flatten>Flatten</a></li><li><a href=/documentation/programming-guide/#partition>Partition</a></li></ul></li><li><a href=/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms>Requirements for user code</a></li><li><a href=/documentation/programming-guide/#side-inputs>Side inputs</a></li><li><a href=/documentation/programming-guide/#additional-outputs>Additional outputs</a></li><li><a href=/documentation/programming-guide/#composite-transforms>Composite transforms</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Pipeline I/O</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#pipeline-io>Using I/O transforms</a></li><li><a href=/documentation/io/built-in/>Built-in I/O connectors</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Built-in I/O connector guides</span><ul class=section-nav-list><li><a href=/documentation/io/built-in/parquet/>Apache Parquet I/O connector</a></li><li><a href=/documentation/io/built-in/hadoop/>Hadoop Input/Output Format IO</a></li><li><a href=/documentation/io/built-in/hcatalog/>HCatalog IO</a></li><li><a href=/documentation/io/built-in/google-bigquery/>Google BigQuery I/O connector</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Developing new I/O connectors</span><ul class=section-nav-list><li><a href=/documentation/io/developing-io-overview/>Overview: Developing connectors</a></li><li><a href=/documentation/io/developing-io-java/>Developing connectors (Java)</a></li><li><a href=/documentation/io/developing-io-python/>Developing connectors (Python)</a></li></ul></li><li><a href=/documentation/io/testing/>Testing I/O transforms</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Schemas</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#what-is-a-schema>What is a schema</a></li><li><a href=/documentation/programming-guide/#schemas-for-pl-types>Schemas for programming language types</a></li><li><a href=/documentation/programming-guide/#schema-definition>Schema definition</a></li><li><a href=/documentation/programming-guide/#logical-types>Logical types</a></li><li><a href=/documentation/programming-guide/#creating-schemas>Creating schemas</a></li><li><a href=/documentation/programming-guide/#using-schemas>Using schemas</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Data encoding and type safety</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#data-encoding-and-type-safety>Data encoding basics</a></li><li><a href=/documentation/programming-guide/#specifying-coders>Specifying coders</a></li><li><a href=/documentation/programming-guide/#default-coders-and-the-coderregistry>Default coders and the CoderRegistry</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Windowing</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#windowing>Windowing basics</a></li><li><a href=/documentation/programming-guide/#provided-windowing-functions>Provided windowing functions</a></li><li><a href=/documentation/programming-guide/#setting-your-pcollections-windowing-function>Setting your PCollection’s windowing function</a></li><li><a href=/documentation/programming-guide/#watermarks-and-late-data>Watermarks and late data</a></li><li><a href=/documentation/programming-guide/#adding-timestamps-to-a-pcollections-elements>Adding timestamps to a PCollection’s elements</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Triggers</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#triggers>Trigger basics</a></li><li><a href=/documentation/programming-guide/#event-time-triggers>Event time triggers and the default trigger</a></li><li><a href=/documentation/programming-guide/#processing-time-triggers>Processing time triggers</a></li><li><a href=/documentation/programming-guide/#data-driven-triggers>Data-driven triggers</a></li><li><a href=/documentation/programming-guide/#setting-a-trigger>Setting a trigger</a></li><li><a href=/documentation/programming-guide/#composite-triggers>Composite triggers</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Metrics</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#metrics>Metrics basics</a></li><li><a href=/documentation/programming-guide/#types-of-metrics>Types of metrics</a></li><li><a href=/documentation/programming-guide/#querying-metrics>Querying metrics</a></li><li><a href=/documentation/programming-guide/#using-metrics>Using metrics in pipeline</a></li><li><a href=/documentation/programming-guide/#export-metrics>Export metrics</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>State and Timers</span><ul class=section-nav-list><li><a href=/documentation/programming-guide/#types-of-state>Types of state</a></li><li><a href=/documentation/programming-guide/#deferred-state-reads>Deferred state reads</a></li><li><a href=/documentation/programming-guide/#timers>Timers</a></li><li><a href=/documentation/programming-guide/#garbage-collecting-state>Garbage collecting state</a></li><li><a href=/documentation/programming-guide/#state-timers-examples>State and timers examples</a></li></ul></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Transform catalog</span><ul class=section-nav-list><li class=section-nav-item--collapsible><span class=section-nav-list-title>Python</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/overview/>Overview</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Element-wise</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/elementwise/filter/>Filter</a></li><li><a href=/documentation/transforms/python/elementwise/flatmap/>FlatMap</a></li><li><a href=/documentation/transforms/python/elementwise/keys/>Keys</a></li><li><a href=/documentation/transforms/python/elementwise/kvswap/>KvSwap</a></li><li><a href=/documentation/transforms/python/elementwise/map/>Map</a></li><li><a href=/documentation/transforms/python/elementwise/pardo/>ParDo</a></li><li><a href=/documentation/transforms/python/elementwise/partition/>Partition</a></li><li><a href=/documentation/transforms/python/elementwise/regex/>Regex</a></li><li><a href=/documentation/transforms/python/elementwise/reify/>Reify</a></li><li><a href=/documentation/transforms/python/elementwise/tostring/>ToString</a></li><li><a href=/documentation/transforms/python/elementwise/values/>Values</a></li><li><a href=/documentation/transforms/python/elementwise/withtimestamps/>WithTimestamps</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Aggregation</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/aggregation/cogroupbykey/>CoGroupByKey</a></li><li><a href=/documentation/transforms/python/aggregation/combineglobally/>CombineGlobally</a></li><li><a href=/documentation/transforms/python/aggregation/combineperkey/>CombinePerKey</a></li><li><a href=/documentation/transforms/python/aggregation/combinevalues/>CombineValues</a></li><li><a href=/documentation/transforms/python/aggregation/count/>Count</a></li><li><a href=/documentation/transforms/python/aggregation/distinct/>Distinct</a></li><li><a href=/documentation/transforms/python/aggregation/groupbykey/>GroupByKey</a></li><li><a href=/documentation/transforms/python/aggregation/mean/>Mean</a></li><li><a href=/documentation/transforms/python/aggregation/sample/>Sample</a></li><li><a href=/documentation/transforms/python/aggregation/top/>Top</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Other</span><ul class=section-nav-list><li><a href=/documentation/transforms/python/other/create/>Create</a></li><li><a href=/documentation/transforms/python/other/flatten/>Flatten</a></li><li><a href=/documentation/transforms/python/other/reshuffle/>Reshuffle</a></li><li><a href=/documentation/transforms/python/other/windowinto/>WindowInto</a></li></ul></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Java</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/overview/>Overview</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Element-wise</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/elementwise/filter/>Filter</a></li><li><a href=/documentation/transforms/java/elementwise/flatmapelements/>FlatMapElements</a></li><li><a href=/documentation/transforms/java/elementwise/keys/>Keys</a></li><li><a href=/documentation/transforms/java/elementwise/kvswap/>KvSwap</a></li><li><a href=/documentation/transforms/java/elementwise/mapelements/>MapElements</a></li><li><a href=/documentation/transforms/java/elementwise/pardo/>ParDo</a></li><li><a href=/documentation/transforms/java/elementwise/partition/>Partition</a></li><li><a href=/documentation/transforms/java/elementwise/regex/>Regex</a></li><li><a href=/documentation/transforms/java/elementwise/reify/>Reify</a></li><li><a href=/documentation/transforms/java/elementwise/values/>Values</a></li><li><a href=/documentation/transforms/java/elementwise/tostring/>ToString</a></li><li><a href=/documentation/transforms/java/elementwise/withkeys/>WithKeys</a></li><li><a href=/documentation/transforms/java/elementwise/withtimestamps/>WithTimestamps</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Aggregation</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/aggregation/approximatequantiles/>ApproximateQuantiles</a></li><li><a href=/documentation/transforms/java/aggregation/approximateunique/>ApproximateUnique</a></li><li><a href=/documentation/transforms/java/aggregation/cogroupbykey/>CoGroupByKey</a></li><li><a href=/documentation/transforms/java/aggregation/combine/>Combine</a></li><li><a href=/documentation/transforms/java/aggregation/combinewithcontext/>CombineWithContext</a></li><li><a href=/documentation/transforms/java/aggregation/count/>Count</a></li><li><a href=/documentation/transforms/java/aggregation/distinct/>Distinct</a></li><li><a href=/documentation/transforms/java/aggregation/groupbykey/>GroupByKey</a></li><li><a href=/documentation/transforms/java/aggregation/groupintobatches/>GroupIntoBatches</a></li><li><a href=/documentation/transforms/java/aggregation/hllcount/>HllCount</a></li><li><a href=/documentation/transforms/java/aggregation/latest/>Latest</a></li><li><a href=/documentation/transforms/java/aggregation/max/>Max</a></li><li><a href=/documentation/transforms/java/aggregation/mean/>Mean</a></li><li><a href=/documentation/transforms/java/aggregation/min/>Min</a></li><li><a href=/documentation/transforms/java/aggregation/sample/>Sample</a></li><li><a href=/documentation/transforms/java/aggregation/sum/>Sum</a></li><li><a href=/documentation/transforms/java/aggregation/top/>Top</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Other</span><ul class=section-nav-list><li><a href=/documentation/transforms/java/other/create/>Create</a></li><li><a href=/documentation/transforms/java/other/flatten/>Flatten</a></li><li><a href=/documentation/transforms/java/other/passert/>PAssert</a></li><li><a href=/documentation/transforms/java/other/view/>View</a></li><li><a href=/documentation/transforms/java/other/window/>Window</a></li></ul></li></ul></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Common pipeline patterns</span><ul class=section-nav-list><li><a href=/documentation/patterns/overview/>Overview</a></li><li><a href=/documentation/patterns/file-processing/>File processing</a></li><li><a href=/documentation/patterns/side-inputs/>Side inputs</a></li><li><a href=/documentation/patterns/pipeline-options/>Pipeline options</a></li><li><a href=/documentation/patterns/custom-io/>Custom I/O</a></li><li><a href=/documentation/patterns/custom-windows/>Custom windows</a></li><li><a href=/documentation/patterns/bigqueryio/>BigQueryIO</a></li><li><a href=/documentation/patterns/ai-platform/>AI Platform</a></li><li><a href=/documentation/patterns/schema/>Schema</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Runtime systems</span><ul class=section-nav-list><li><a href=/documentation/runtime/model/>Execution model</a></li><li><a href=/documentation/runtime/environments/>Container environments</a></li><li><a href=/documentation/runtime/sdk-harness-config/>SDK Harness Configuration</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Learning resources</span><ul class=section-nav-list><li><a href=/documentation/resources/learning-resources/#getting-started>Getting Started</a></li><li><a href=/documentation/resources/learning-resources/#articles>Articles</a></li><li><a href=/documentation/resources/learning-resources/#interactive-labs>Interactive Labs</a></li><li><a href=/documentation/resources/learning-resources/#beam-katas>Beam Katas</a></li><li><a href=/documentation/resources/learning-resources/#code-examples>Code Examples</a></li><li><a href=/documentation/resources/learning-resources/#api-reference>API Reference</a></li><li><a href=/documentation/resources/learning-resources/#feedback-and-suggestions>Feedback and Suggestions</a></li><li><a href=/documentation/resources/learning-resources/#how-to-contribute>How to Contribute</a></li><li><a href=/documentation/resources/videos-and-podcasts>Videos and Podcasts</a></li></ul></li><li><a href=https://cwiki.apache.org/confluence/display/BEAM/Apache+Beam>Beam Wiki</a></li></ul></nav></div><nav class="page-nav clearfix" data-offset-top=90 data-offset-bottom=500><nav id=TableOfContents><ul><li><ul><li><a href=#reading-using-hadoopformatio>Reading using HadoopFormatIO</a><ul><li><a href=#read-data-only-with-hadoop-configuration>Read data only with Hadoop configuration.</a></li><li><a href=#read-data-with-configuration-and-key-translation>Read data with configuration and key translation</a></li><li><a href=#read-data-with-configuration-and-value-translation>Read data with configuration and value translation</a></li><li><a href=#read-data-with-configuration-value-translation-and-key-translation>Read data with configuration, value translation and key translation</a></li></ul></li></ul></li></ul><ul><li><ul><li><a href=#cassandra---cqlinputformat>Cassandra - CqlInputFormat</a></li><li><a href=#elasticsearch---esinputformat>Elasticsearch - EsInputFormat</a></li><li><a href=#hcatalog---hcatinputformat>HCatalog - HCatInputFormat</a></li><li><a href=#amazon-dynamodb---dynamodbinputformat>Amazon DynamoDB - DynamoDBInputFormat</a></li><li><a href=#apache-hbase---tablesnapshotinputformat>Apache HBase - TableSnapshotInputFormat</a></li><li><a href=#writing-using-hadoopformatio>Writing using HadoopFormatIO</a><ul><li><a href=#batch-writing->Batch writing</a></li><li><a href=#stream-writing->Stream writing</a></li></ul></li></ul></li></ul></nav></nav><div class="body__contained body__section-nav"><h1 id=hadoop-inputoutput-format-io>Hadoop Input/Output Format IO</h1><blockquote><p><strong>IMPORTANT!</strong> Previous implementation of Hadoop Input Format IO, called <code>HadoopInputFormatIO</code>, is deprecated starting from <em>Apache Beam 2.10</em>. Please, use current <code>HadoopFormatIO</code> which supports both <code>InputFormat</code> and <code>OutputFormat</code>.</p></blockquote><p>A <code>HadoopFormatIO</code> is a transform for reading data from any source or writing data to any sink that implements Hadoop&rsquo;s <code>InputFormat</code> or <code>OurputFormat</code> accordingly. For example, Cassandra, Elasticsearch, HBase, Redis, Postgres, etc.</p><p><code>HadoopFormatIO</code> allows you to connect to many data sources/sinks that do not yet have a Beam IO transform. However, <code>HadoopFormatIO</code> has to make several performance trade-offs in connecting to <code>InputFormat</code> or <code>OutputFormat</code>. So, if there is another Beam IO transform for connecting specifically to your data source/sink of choice, we recommend you use that one.</p><h3 id=reading-using-hadoopformatio>Reading using HadoopFormatIO</h3><p>You will need to pass a Hadoop <code>Configuration</code> with parameters specifying how the read will occur. Many properties of the <code>Configuration</code> are optional and some are required for certain <code>InputFormat</code> classes, but the following properties must be set for all <code>InputFormat</code> classes:</p><ul><li><code>mapreduce.job.inputformat.class</code> - The <code>InputFormat</code> class used to connect to your data source of choice.</li><li><code>key.class</code> - The <code>Key</code> class returned by the <code>InputFormat</code> in <code>mapreduce.job.inputformat.class</code>.</li><li><code>value.class</code> - The <code>Value</code> class returned by the <code>InputFormat</code> in <code>mapreduce.job.inputformat.class</code>.</li></ul><p>For example:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>Configuration</span> <span class=n>myHadoopConfiguration</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>(</span><span class=kc>false</span><span class=o>);</span>
<span class=c1>// Set Hadoop InputFormat, key and value class in configuration
</span><span class=c1></span><span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.inputformat.class&#34;</span><span class=o>,</span> <span class=n>InputFormatClass</span><span class=o>,</span>
  <span class=n>InputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;key.class&#34;</span><span class=o>,</span> <span class=n>InputFormatKeyClass</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;value.class&#34;</span><span class=o>,</span> <span class=n>InputFormatValueClass</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span></code></pre></div></div></p><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>You will need to check if the <code>Key</code> and <code>Value</code> classes output by the <code>InputFormat</code> have a Beam <code>Coder</code> available. If not, you can use <code>withKeyTranslation</code> or <code>withValueTranslation</code> to specify a method transforming instances of those classes into another class that is supported by a Beam <code>Coder</code>. These settings are optional and you don&rsquo;t need to specify translation for both key and value.</p><p>For example:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>SimpleFunction</span><span class=o>&lt;</span><span class=n>InputFormatKeyClass</span><span class=o>,</span> <span class=n>MyKeyClass</span><span class=o>&gt;</span> <span class=n>myOutputKeyType</span> <span class=o>=</span>
<span class=k>new</span> <span class=n>SimpleFunction</span><span class=o>&lt;</span><span class=n>InputFormatKeyClass</span><span class=o>,</span> <span class=n>MyKeyClass</span><span class=o>&gt;()</span> <span class=o>{</span>
  <span class=kd>public</span> <span class=n>MyKeyClass</span> <span class=nf>apply</span><span class=o>(</span><span class=n>InputFormatKeyClass</span> <span class=n>input</span><span class=o>)</span> <span class=o>{</span>
  <span class=c1>// ...logic to transform InputFormatKeyClass to MyKeyClass
</span><span class=c1></span>  <span class=o>}</span>
<span class=o>};</span>
<span class=n>SimpleFunction</span><span class=o>&lt;</span><span class=n>InputFormatValueClass</span><span class=o>,</span> <span class=n>MyValueClass</span><span class=o>&gt;</span> <span class=n>myOutputValueType</span> <span class=o>=</span>
<span class=k>new</span> <span class=n>SimpleFunction</span><span class=o>&lt;</span><span class=n>InputFormatValueClass</span><span class=o>,</span> <span class=n>MyValueClass</span><span class=o>&gt;()</span> <span class=o>{</span>
  <span class=kd>public</span> <span class=n>MyValueClass</span> <span class=nf>apply</span><span class=o>(</span><span class=n>InputFormatValueClass</span> <span class=n>input</span><span class=o>)</span> <span class=o>{</span>
  <span class=c1>// ...logic to transform InputFormatValueClass to MyValueClass
</span><span class=c1></span>  <span class=o>}</span>
<span class=o>};</span></code></pre></div></div></p><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h4 id=read-data-only-with-hadoop-configuration>Read data only with Hadoop configuration.</h4><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>InputFormatKeyClass</span><span class=o>,</span> <span class=n>InputFormatKeyClass</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>myHadoopConfiguration</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h4 id=read-data-with-configuration-and-key-translation>Read data with configuration and key translation</h4><p>For example, a Beam <code>Coder</code> is not available for <code>Key</code> class, so key translation is required.</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>MyKeyClass</span><span class=o>,</span> <span class=n>InputFormatKeyClass</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>myHadoopConfiguration</span><span class=o>)</span>
  <span class=o>.</span><span class=na>withKeyTranslation</span><span class=o>(</span><span class=n>myOutputKeyType</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h4 id=read-data-with-configuration-and-value-translation>Read data with configuration and value translation</h4><p>For example, a Beam <code>Coder</code> is not available for <code>Value</code> class, so value translation is required.</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>InputFormatKeyClass</span><span class=o>,</span> <span class=n>MyValueClass</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>myHadoopConfiguration</span><span class=o>)</span>
  <span class=o>.</span><span class=na>withValueTranslation</span><span class=o>(</span><span class=n>myOutputValueType</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h4 id=read-data-with-configuration-value-translation-and-key-translation>Read data with configuration, value translation and key translation</h4><p>For example, Beam Coders are not available for both <code>Key</code> class and <code>Value</code> classes of <code>InputFormat</code>, so key and value translation are required.</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>MyKeyClass</span><span class=o>,</span> <span class=n>MyValueClass</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>myHadoopConfiguration</span><span class=o>)</span>
  <span class=o>.</span><span class=na>withKeyTranslation</span><span class=o>(</span><span class=n>myOutputKeyType</span><span class=o>)</span>
  <span class=o>.</span><span class=na>withValueTranslation</span><span class=o>(</span><span class=n>myOutputValueType</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h1 id=examples-for-specific-inputformats>Examples for specific InputFormats</h1><h3 id=cassandra---cqlinputformat>Cassandra - CqlInputFormat</h3><p>To read data from Cassandra, use <code>org.apache.cassandra.hadoop.cql3.CqlInputFormat</code>, which needs the following properties to be set:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>Configuration</span> <span class=n>cassandraConf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;cassandra.input.thrift.port&#34;</span><span class=o>,</span> <span class=s>&#34;9160&#34;</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;cassandra.input.thrift.address&#34;</span><span class=o>,</span> <span class=n>CassandraHostIp</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;cassandra.input.partitioner.class&#34;</span><span class=o>,</span> <span class=s>&#34;Murmur3Partitioner&#34;</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;cassandra.input.keyspace&#34;</span><span class=o>,</span> <span class=s>&#34;myKeySpace&#34;</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;cassandra.input.columnfamily&#34;</span><span class=o>,</span> <span class=s>&#34;myColumnFamily&#34;</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;key.class&#34;</span><span class=o>,</span> <span class=n>java</span><span class=o>.</span><span class=na>lang</span><span class=o>.</span><span class=na>Long</span> <span class=n>Long</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;value.class&#34;</span><span class=o>,</span> <span class=n>com</span><span class=o>.</span><span class=na>datastax</span><span class=o>.</span><span class=na>driver</span><span class=o>.</span><span class=na>core</span><span class=o>.</span><span class=na>Row</span> <span class=n>Row</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>cassandraConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.inputformat.class&#34;</span><span class=o>,</span> <span class=n>org</span><span class=o>.</span><span class=na>apache</span><span class=o>.</span><span class=na>cassandra</span><span class=o>.</span><span class=na>hadoop</span><span class=o>.</span><span class=na>cql3</span><span class=o>.</span><span class=na>CqlInputFormat</span> <span class=n>CqlInputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>InputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>Call Read transform as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>KV</span><span class=o>&lt;</span><span class=n>Long</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;&gt;</span> <span class=n>cassandraData</span> <span class=o>=</span>
  <span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>Long</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>cassandraConf</span><span class=o>)</span>
  <span class=o>.</span><span class=na>withValueTranslation</span><span class=o>(</span><span class=n>cassandraOutputValueType</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>The <code>CqlInputFormat</code> key class is <code>java.lang.Long</code> <code>Long</code>, which has a Beam <code>Coder</code>. The <code>CqlInputFormat</code> value class is <code>com.datastax.driver.core.Row</code> <code>Row</code>, which does not have a Beam <code>Coder</code>. Rather than write a new coder, you can provide your own translation method, as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>SimpleFunction</span><span class=o>&lt;</span><span class=n>Row</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;</span> <span class=n>cassandraOutputValueType</span> <span class=o>=</span> <span class=n>SimpleFunction</span><span class=o>&lt;</span><span class=n>Row</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;()</span>
<span class=o>{</span>
  <span class=kd>public</span> <span class=n>String</span> <span class=nf>apply</span><span class=o>(</span><span class=n>Row</span> <span class=n>row</span><span class=o>)</span> <span class=o>{</span>
    <span class=k>return</span> <span class=n>row</span><span class=o>.</span><span class=na>getString</span><span class=o>(</span><span class=err>&#39;</span><span class=n>myColName</span><span class=err>&#39;</span><span class=o>);</span>
  <span class=o>}</span>
<span class=o>};</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h3 id=elasticsearch---esinputformat>Elasticsearch - EsInputFormat</h3><p>To read data from Elasticsearch, use <code>EsInputFormat</code>, which needs following properties to be set:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>Configuration</span> <span class=n>elasticsearchConf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
<span class=n>elasticsearchConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;es.nodes&#34;</span><span class=o>,</span> <span class=n>ElasticsearchHostIp</span><span class=o>);</span>
<span class=n>elasticsearchConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;es.port&#34;</span><span class=o>,</span> <span class=s>&#34;9200&#34;</span><span class=o>);</span>
<span class=n>elasticsearchConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;es.resource&#34;</span><span class=o>,</span> <span class=s>&#34;ElasticIndexName/ElasticTypeName&#34;</span><span class=o>);</span>
<span class=n>elasticsearchConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;key.class&#34;</span><span class=o>,</span> <span class=n>org</span><span class=o>.</span><span class=na>apache</span><span class=o>.</span><span class=na>hadoop</span><span class=o>.</span><span class=na>io</span><span class=o>.</span><span class=na>Text</span> <span class=n>Text</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>elasticsearchConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;value.class&#34;</span><span class=o>,</span> <span class=n>org</span><span class=o>.</span><span class=na>elasticsearch</span><span class=o>.</span><span class=na>hadoop</span><span class=o>.</span><span class=na>mr</span><span class=o>.</span><span class=na>LinkedMapWritable</span> <span class=n>LinkedMapWritable</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>elasticsearchConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.inputformat.class&#34;</span><span class=o>,</span> <span class=n>org</span><span class=o>.</span><span class=na>elasticsearch</span><span class=o>.</span><span class=na>hadoop</span><span class=o>.</span><span class=na>mr</span><span class=o>.</span><span class=na>EsInputFormat</span> <span class=n>EsInputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>InputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>Call Read transform as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>KV</span><span class=o>&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LinkedMapWritable</span><span class=o>&gt;&gt;</span> <span class=n>elasticData</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LinkedMapWritable</span><span class=o>&gt;</span><span class=n>read</span><span class=o>().</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>elasticsearchConf</span><span class=o>));</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>The <code>org.elasticsearch.hadoop.mr.EsInputFormat</code>'s <code>EsInputFormat</code> key class is <code>org.apache.hadoop.io.Text</code> <code>Text</code>, and its value class is <code>org.elasticsearch.hadoop.mr.LinkedMapWritable</code> <code>LinkedMapWritable</code>. Both key and value classes have Beam Coders.</p><h3 id=hcatalog---hcatinputformat>HCatalog - HCatInputFormat</h3><p>To read data using HCatalog, use <code>org.apache.hive.hcatalog.mapreduce.HCatInputFormat</code>, which needs the following properties to be set:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>Configuration</span> <span class=n>hcatConf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
<span class=n>hcatConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.inputformat.class&#34;</span><span class=o>,</span> <span class=n>HCatInputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>InputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>hcatConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;key.class&#34;</span><span class=o>,</span> <span class=n>LongWritable</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>hcatConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;value.class&#34;</span><span class=o>,</span> <span class=n>HCatRecord</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>hcatConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;hive.metastore.uris&#34;</span><span class=o>,</span> <span class=s>&#34;thrift://metastore-host:port&#34;</span><span class=o>);</span>

<span class=n>org</span><span class=o>.</span><span class=na>apache</span><span class=o>.</span><span class=na>hive</span><span class=o>.</span><span class=na>hcatalog</span><span class=o>.</span><span class=na>mapreduce</span><span class=o>.</span><span class=na>HCatInputFormat</span><span class=o>.</span><span class=na>setInput</span><span class=o>(</span><span class=n>hcatConf</span><span class=o>,</span> <span class=s>&#34;my_database&#34;</span><span class=o>,</span> <span class=s>&#34;my_table&#34;</span><span class=o>,</span> <span class=s>&#34;my_filter&#34;</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>Call Read transform as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>KV</span><span class=o>&lt;</span><span class=n>Long</span><span class=o>,</span> <span class=n>HCatRecord</span><span class=o>&gt;&gt;</span> <span class=n>hcatData</span> <span class=o>=</span>
  <span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>Long</span><span class=o>,</span> <span class=n>HCatRecord</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>hcatConf</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h3 id=amazon-dynamodb---dynamodbinputformat>Amazon DynamoDB - DynamoDBInputFormat</h3><p>To read data from Amazon DynamoDB, use <code>org.apache.hadoop.dynamodb.read.DynamoDBInputFormat</code>.
DynamoDBInputFormat implements the older <code>org.apache.hadoop.mapred.InputFormat</code> interface and to make it compatible with HadoopFormatIO which uses the newer abstract class <code>org.apache.hadoop.mapreduce.InputFormat</code>,
a wrapper API is required which acts as an adapter between HadoopFormatIO and DynamoDBInputFormat (or in general any InputFormat implementing <code>org.apache.hadoop.mapred.InputFormat</code>)
The below example uses one such available wrapper API - <a href=https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/MapReduceInputFormatWrapper.java>https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/MapReduceInputFormatWrapper.java</a></p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>Configuration</span> <span class=n>dynamoDBConf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
<span class=n>Job</span> <span class=n>job</span> <span class=o>=</span> <span class=n>Job</span><span class=o>.</span><span class=na>getInstance</span><span class=o>(</span><span class=n>dynamoDBConf</span><span class=o>);</span>
<span class=n>com</span><span class=o>.</span><span class=na>twitter</span><span class=o>.</span><span class=na>elephantbird</span><span class=o>.</span><span class=na>mapreduce</span><span class=o>.</span><span class=na>input</span><span class=o>.</span><span class=na>MapReduceInputFormatWrapper</span><span class=o>.</span><span class=na>setInputFormat</span><span class=o>(</span><span class=n>org</span><span class=o>.</span><span class=na>apache</span><span class=o>.</span><span class=na>hadoop</span><span class=o>.</span><span class=na>dynamodb</span><span class=o>.</span><span class=na>read</span><span class=o>.</span><span class=na>DynamoDBInputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>job</span><span class=o>);</span>
<span class=n>dynamoDBConf</span> <span class=o>=</span> <span class=n>job</span><span class=o>.</span><span class=na>getConfiguration</span><span class=o>();</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;key.class&#34;</span><span class=o>,</span> <span class=n>Text</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>WritableComparable</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;value.class&#34;</span><span class=o>,</span> <span class=n>org</span><span class=o>.</span><span class=na>apache</span><span class=o>.</span><span class=na>hadoop</span><span class=o>.</span><span class=na>dynamodb</span><span class=o>.</span><span class=na>DynamoDBItemWritable</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Writable</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.servicename&#34;</span><span class=o>,</span> <span class=s>&#34;dynamodb&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.input.tableName&#34;</span><span class=o>,</span> <span class=s>&#34;table_name&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.endpoint&#34;</span><span class=o>,</span> <span class=s>&#34;dynamodb.us-west-1.amazonaws.com&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.regionid&#34;</span><span class=o>,</span> <span class=s>&#34;us-west-1&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.throughput.read&#34;</span><span class=o>,</span> <span class=s>&#34;1&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.throughput.read.percent&#34;</span><span class=o>,</span> <span class=s>&#34;1&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;dynamodb.version&#34;</span><span class=o>,</span> <span class=s>&#34;2011-12-05&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=n>DynamoDBConstants</span><span class=o>.</span><span class=na>DYNAMODB_ACCESS_KEY_CONF</span><span class=o>,</span> <span class=s>&#34;aws_access_key&#34;</span><span class=o>);</span>
<span class=n>dynamoDBConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=n>DynamoDBConstants</span><span class=o>.</span><span class=na>DYNAMODB_SECRET_KEY_CONF</span><span class=o>,</span> <span class=s>&#34;aws_secret_key&#34;</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>Call Read transform as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>DynamoDBItemWritable</span><span class=o>&gt;</span> <span class=n>dynamoDBData</span> <span class=o>=</span>
  <span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>DynamoDBItemWritable</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>dynamoDBConf</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h3 id=apache-hbase---tablesnapshotinputformat>Apache HBase - TableSnapshotInputFormat</h3><p>To read data from an HBase table snapshot, use <code>org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat</code>.
Reading from a table snapshot bypasses the HBase region servers, instead reading HBase data files directly from the filesystem.
This is useful for cases such as reading historical data or offloading of work from the HBase cluster.
There are scenarios when this may prove faster than accessing content through the region servers using the <code>HBaseIO</code>.</p><p>A table snapshot can be taken using the HBase shell or programmatically:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=k>try</span> <span class=o>(</span>
    <span class=n>Connection</span> <span class=n>connection</span> <span class=o>=</span> <span class=n>ConnectionFactory</span><span class=o>.</span><span class=na>createConnection</span><span class=o>(</span><span class=n>hbaseConf</span><span class=o>);</span>
    <span class=n>Admin</span> <span class=n>admin</span> <span class=o>=</span> <span class=n>connection</span><span class=o>.</span><span class=na>getAdmin</span><span class=o>()</span>
  <span class=o>)</span> <span class=o>{</span>
  <span class=n>admin</span><span class=o>.</span><span class=na>snapshot</span><span class=o>(</span>
    <span class=s>&#34;my_snaphshot&#34;</span><span class=o>,</span>
    <span class=n>TableName</span><span class=o>.</span><span class=na>valueOf</span><span class=o>(</span><span class=s>&#34;my_table&#34;</span><span class=o>),</span>
    <span class=n>HBaseProtos</span><span class=o>.</span><span class=na>SnapshotDescription</span><span class=o>.</span><span class=na>Type</span><span class=o>.</span><span class=na>FLUSH</span><span class=o>);</span>
<span class=o>}</span>  </code></pre></div></div></p><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>A <code>TableSnapshotInputFormat</code> is configured as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=c1>// Construct a typical HBase scan
</span><span class=c1></span><span class=n>Scan</span> <span class=n>scan</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Scan</span><span class=o>();</span>
<span class=n>scan</span><span class=o>.</span><span class=na>setCaching</span><span class=o>(</span><span class=n>1000</span><span class=o>);</span>
<span class=n>scan</span><span class=o>.</span><span class=na>setBatch</span><span class=o>(</span><span class=n>1000</span><span class=o>);</span>
<span class=n>scan</span><span class=o>.</span><span class=na>addColumn</span><span class=o>(</span><span class=n>Bytes</span><span class=o>.</span><span class=na>toBytes</span><span class=o>(</span><span class=s>&#34;CF&#34;</span><span class=o>),</span> <span class=n>Bytes</span><span class=o>.</span><span class=na>toBytes</span><span class=o>(</span><span class=s>&#34;col_1&#34;</span><span class=o>));</span>
<span class=n>scan</span><span class=o>.</span><span class=na>addColumn</span><span class=o>(</span><span class=n>Bytes</span><span class=o>.</span><span class=na>toBytes</span><span class=o>(</span><span class=s>&#34;CF&#34;</span><span class=o>),</span> <span class=n>Bytes</span><span class=o>.</span><span class=na>toBytes</span><span class=o>(</span><span class=s>&#34;col_2&#34;</span><span class=o>));</span>

<span class=n>Configuration</span> <span class=n>hbaseConf</span> <span class=o>=</span> <span class=n>HBaseConfiguration</span><span class=o>.</span><span class=na>create</span><span class=o>();</span>
<span class=n>hbaseConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=n>HConstants</span><span class=o>.</span><span class=na>ZOOKEEPER_QUORUM</span><span class=o>,</span> <span class=s>&#34;zk1:2181&#34;</span><span class=o>);</span>
<span class=n>hbaseConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=s>&#34;hbase.rootdir&#34;</span><span class=o>,</span> <span class=s>&#34;/hbase&#34;</span><span class=o>);</span>
<span class=n>hbaseConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span>
    <span class=s>&#34;mapreduce.job.inputformat.class&#34;</span><span class=o>,</span> <span class=n>TableSnapshotInputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>InputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>hbaseConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;key.class&#34;</span><span class=o>,</span> <span class=n>ImmutableBytesWritable</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Writable</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>hbaseConf</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;value.class&#34;</span><span class=o>,</span> <span class=n>Result</span><span class=o>.</span><span class=na>class</span><span class=o>,</span> <span class=n>Writable</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>ClientProtos</span><span class=o>.</span><span class=na>Scan</span> <span class=n>proto</span> <span class=o>=</span> <span class=n>ProtobufUtil</span><span class=o>.</span><span class=na>toScan</span><span class=o>(</span><span class=n>scan</span><span class=o>);</span>
<span class=n>hbaseConf</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=n>TableInputFormat</span><span class=o>.</span><span class=na>SCAN</span><span class=o>,</span> <span class=n>Base64</span><span class=o>.</span><span class=na>encodeBytes</span><span class=o>(</span><span class=n>proto</span><span class=o>.</span><span class=na>toByteArray</span><span class=o>()));</span>

<span class=c1>// Make use of existing utility methods
</span><span class=c1></span><span class=n>Job</span> <span class=n>job</span> <span class=o>=</span> <span class=n>Job</span><span class=o>.</span><span class=na>getInstance</span><span class=o>(</span><span class=n>hbaseConf</span><span class=o>);</span> <span class=c1>// creates internal clone of hbaseConf
</span><span class=c1></span><span class=n>TableSnapshotInputFormat</span><span class=o>.</span><span class=na>setInput</span><span class=o>(</span><span class=n>job</span><span class=o>,</span> <span class=s>&#34;my_snapshot&#34;</span><span class=o>,</span> <span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=s>&#34;/tmp/snapshot_restore&#34;</span><span class=o>));</span>
<span class=n>hbaseConf</span> <span class=o>=</span> <span class=n>job</span><span class=o>.</span><span class=na>getConfiguration</span><span class=o>();</span> <span class=o>//</span> <span class=n>extract</span> <span class=n>the</span> <span class=n>modified</span> <span class=n>clone</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>Call Read transform as follows:</p><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>ImmutableBytesWritable</span><span class=o>,</span> <span class=n>Result</span><span class=o>&gt;</span> <span class=n>hbaseSnapshotData</span> <span class=o>=</span>
  <span class=n>p</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span><span class=s>&#34;read&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>ImmutableBytesWritable</span><span class=o>,</span> <span class=n>Result</span><span class=o>&gt;</span><span class=n>read</span><span class=o>()</span>
  <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>hbaseConf</span><span class=o>);</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h3 id=writing-using-hadoopformatio>Writing using HadoopFormatIO</h3><p>You will need to pass a Hadoop <code>Configuration</code> with parameters specifying how the write will occur. Many properties of the <code>Configuration</code> are optional, and some are required for certain <code>OutputFormat</code> classes, but the following properties must be set for all <code>OutputFormat</code>s:</p><ul><li><code>mapreduce.job.id</code> - The identifier of the write job. E.g.: end timestamp of window.</li><li><code>mapreduce.job.outputformat.class</code> - The <code>OutputFormat</code> class used to connect to your data sink of choice.</li><li><code>mapreduce.job.output.key.class</code> - The key class passed to the <code>OutputFormat</code> in <code>mapreduce.job.outputformat.class</code>.</li><li><code>mapreduce.job.output.value.class</code> - The value class passed to the <code>OutputFormat</code> in <code>mapreduce.job.outputformat.class</code>.</li><li><code>mapreduce.job.reduces</code> - Number of reduce tasks. Value is equal to number of write tasks which will be genarated. This property is not required for <code>Write.PartitionedWriterBuilder#withoutPartitioning()</code> write.</li><li><code>mapreduce.job.partitioner.class</code> - Hadoop partitioner class which will be used for distributing of records among partitions. This property is not required for <code>Write.PartitionedWriterBuilder#withoutPartitioning()</code> write.</li></ul><p><em>Note</em>: All mentioned values have appropriate constants. E.g.: <code>HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR</code>.</p><p>For example:<div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=n>Configuration</span> <span class=n>myHadoopConfiguration</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>(</span><span class=kc>false</span><span class=o>);</span>
<span class=c1>// Set Hadoop OutputFormat, key and value class in configuration
</span><span class=c1></span><span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.outputformat.class&#34;</span><span class=o>,</span>
   <span class=n>MyDbOutputFormatClass</span><span class=o>,</span> <span class=n>OutputFormat</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.output.key.class&#34;</span><span class=o>,</span>
   <span class=n>MyDbOutputFormatKeyClass</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.output.value.class&#34;</span><span class=o>,</span>
   <span class=n>MyDbOutputFormatValueClass</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setClass</span><span class=o>(</span><span class=s>&#34;mapreduce.job.partitioner.class&#34;</span><span class=o>,</span>
   <span class=n>MyPartitionerClass</span><span class=o>,</span> <span class=n>Object</span><span class=o>.</span><span class=na>class</span><span class=o>);</span>
<span class=n>myHadoopConfiguration</span><span class=o>.</span><span class=na>setInt</span><span class=o>(</span><span class=s>&#34;mapreduce.job.reduces&#34;</span><span class=o>,</span> <span class=n>2</span><span class=o>);</span></code></pre></div></div></p><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><p>You will need to set <code>OutputFormat</code> key and value class (i.e. &ldquo;mapreduce.job.output.key.class&rdquo; and &ldquo;mapreduce.job.output.value.class&rdquo;) in Hadoop <code>Configuration</code> which are equal to <code>KeyT</code> and <code>ValueT</code>. If you set different <code>OutputFormat</code> key or value class than <code>OutputFormat</code>'s actual key or value class then, it will throw <code>IllegalArgumentException</code>.</p><h4 id=batch-writing->Batch writing</h4><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=c1>// Data which will we want to write
</span><span class=c1></span><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>KV</span><span class=o>&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LongWritable</span><span class=o>&gt;&gt;</span> <span class=n>boundedWordsCount</span> <span class=o>=</span> <span class=o>...</span>

<span class=c1>// Hadoop configuration for write
</span><span class=c1>// We have partitioned write, so Partitioner and reducers count have to be set - see withPartitioning() javadoc
</span><span class=c1></span><span class=n>Configuration</span> <span class=n>myHadoopConfiguration</span> <span class=o>=</span> <span class=o>...</span>
<span class=c1>// Path to directory with locks
</span><span class=c1></span><span class=n>String</span> <span class=n>locksDirPath</span> <span class=o>=</span> <span class=o>...;</span>

<span class=n>boundedWordsCount</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
    <span class=s>&#34;writeBatch&#34;</span><span class=o>,</span>
    <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LongWritable</span><span class=o>&gt;</span><span class=n>write</span><span class=o>()</span>
        <span class=o>.</span><span class=na>withConfiguration</span><span class=o>(</span><span class=n>myHadoopConfiguration</span><span class=o>)</span>
        <span class=o>.</span><span class=na>withPartitioning</span><span class=o>()</span>
        <span class=o>.</span><span class=na>withExternalSynchronization</span><span class=o>(</span><span class=k>new</span> <span class=n>HDFSSynchronization</span><span class=o>(</span><span class=n>locksDirPath</span><span class=o>)));</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div><h4 id=stream-writing->Stream writing</h4><div class=language-java><div class=highlight><pre class=chroma><code class=language-java data-lang=java><span class=c1>// Data which will we want to write
</span><span class=c1></span><span class=n>PCollection</span><span class=o>&lt;</span><span class=n>KV</span><span class=o>&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LongWritable</span><span class=o>&gt;&gt;</span> <span class=n>unboundedWordsCount</span> <span class=o>=</span> <span class=o>...;</span>

<span class=c1>// Transformation which transforms data of one window into one hadoop configuration
</span><span class=c1></span><span class=n>PTransform</span><span class=o>&lt;</span><span class=n>PCollection</span><span class=o>&lt;?</span> <span class=kd>extends</span> <span class=n>KV</span><span class=o>&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LongWritable</span><span class=o>&gt;&gt;,</span> <span class=n>PCollectionView</span><span class=o>&lt;</span><span class=n>Configuration</span><span class=o>&gt;&gt;</span>
  <span class=n>configTransform</span> <span class=o>=</span> <span class=o>...;</span>

<span class=n>unboundedWordsCount</span><span class=o>.</span><span class=na>apply</span><span class=o>(</span>
  <span class=s>&#34;writeStream&#34;</span><span class=o>,</span>
  <span class=n>HadoopFormatIO</span><span class=o>.&lt;</span><span class=n>Text</span><span class=o>,</span> <span class=n>LongWritable</span><span class=o>&gt;</span><span class=n>write</span><span class=o>()</span>
      <span class=o>.</span><span class=na>withConfigurationTransform</span><span class=o>(</span><span class=n>configTransform</span><span class=o>)</span>
      <span class=o>.</span><span class=na>withExternalSynchronization</span><span class=o>(</span><span class=k>new</span> <span class=n>HDFSSynchronization</span><span class=o>(</span><span class=n>locksDirPath</span><span class=o>)));</span></code></pre></div></div><div class=language-py><div class=highlight><pre class=chroma><code class=language-py data-lang=py>  <span class=c1># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span></code></pre></div></div></div></div><footer class=footer><div class=footer__contained><div class=footer__cols><div class=footer__cols__col><div class=footer__cols__col__logo><img src=/images/beam_logo_circle.svg class=footer__logo alt="Beam logo"></div><div class=footer__cols__col__logo><img src=/images/apache_logo_circle.svg class=footer__logo alt="Apache logo"></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Start</div><div class=footer__cols__col__link><a href=/get-started/beam-overview/>Overview</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-java/>Quickstart (Java)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-py/>Quickstart (Python)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-go/>Quickstart (Go)</a></div><div class=footer__cols__col__link><a href=/get-started/downloads/>Downloads</a></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Docs</div><div class=footer__cols__col__link><a href=/documentation/programming-guide/>Concepts</a></div><div class=footer__cols__col__link><a href=/documentation/pipelines/design-your-pipeline/>Pipelines</a></div><div class=footer__cols__col__link><a href=/documentation/runners/capability-matrix/>Runners</a></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Community</div><div class=footer__cols__col__link><a href=/contribute/>Contribute</a></div><div class=footer__cols__col__link><a href=https://projects.apache.org/committee.html?beam target=_blank>Team<img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></div><div class=footer__cols__col__link><a href=/community/presentation-materials/>Media</a></div><div class=footer__cols__col__link><a href=/community/in-person/>Events/Meetups</a></div></div><div class="footer__cols__col footer__cols__col--md"><div class=footer__cols__col__title>Resources</div><div class=footer__cols__col__link><a href=/blog/>Blog</a></div><div class=footer__cols__col__link><a href=/community/contact-us/>Contact Us</a></div><div class=footer__cols__col__link><a href=https://github.com/apache/beam>GitHub</a></div></div></div></div><div class=footer__bottom>&copy;
<a href=http://www.apache.org>The Apache Software Foundation</a>
| <a href=/privacy_policy>Privacy Policy</a>
| <a href=/feed.xml>RSS Feed</a><br><br>Apache Beam, Apache, Beam, the Beam logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation. All other products or name brands are trademarks of their respective holders, including The Apache Software Foundation.</div></footer></body></html>