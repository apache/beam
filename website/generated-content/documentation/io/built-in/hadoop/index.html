<!DOCTYPE html>
<!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<html lang="en">
  <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Apache Hadoop InputFormat IO</title>
  <meta name="description" content="Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.
">
  <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400" rel="stylesheet">
  <link rel="stylesheet" href="/css/site.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js"></script>
  <script src="/js/bootstrap.min.js"></script>
  <script src="/js/language-switch.js"></script>
  <script src="/js/fix-menu.js"></script>
  <script src="/js/section-nav.js"></script>
  <script src="/js/page-nav.js"></script>
  <link rel="canonical" href="https://beam.apache.org/documentation/io/built-in/hadoop/" data-proofer-ignore>
  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico">
  <link rel="alternate" type="application/rss+xml" title="Apache Beam" href="https://beam.apache.org/feed.xml">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-73650088-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

  <body class="body" data-spy="scroll" data-target=".page-nav" data-offset="0">
    <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<nav class="header navbar navbar-fixed-top">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" aria-expanded="false" aria-controls="navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a href="/" class="navbar-brand" >
        <img alt="Brand" style="height: 25px" src="/images/beam_logo_navbar.png">
      </a>
    </div>

    <div class="navbar-mask closed"></div>

    <div id="navbar" class="navbar-container closed">
      <ul class="nav navbar-nav">
        <li>
          <a href="/get-started/beam-overview/">Get Started</a>
        </li>
        <li>
          <a href="/documentation/">Documentation</a>
        </li>
        <li>
          <a href="/documentation/sdks/java/">SDKS</a>
        </li>
        <li>
          <a href="/documentation/runners/capability-matrix/">RUNNERS</a>
        </li>
        <li>
          <a href="/contribute/">Contribute</a>
        </li>
        <li>
          <a href="/community/contact-us/">Community</a>
        </li>
        <li><a href="/blog">Blog</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><img src="https://www.apache.org/foundation/press/kit/feather_small.png" alt="Apache Logo" style="height:20px;"><span class="caret"></span></a>
          <ul class="dropdown-menu dropdown-menu-right">
            <li><a href="http://www.apache.org/">ASF Homepage</a></li>
            <li><a href="http://www.apache.org/licenses/">License</a></li>
            <li><a href="http://www.apache.org/security/">Security</a></li>
            <li><a href="http://www.apache.org/foundation/thanks.html">Thanks</a></li>
            <li><a href="http://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
            <li><a href="https://www.apache.org/foundation/policies/conduct">Code of Conduct</a></li>
          </ul>
        </li>
      </ul>
    </div>
</nav>

    <div class="clearfix container-main-content">
      <div class="section-nav closed" data-offset-top="90" data-offset-bottom="500">
        <span class="section-nav-back glyphicon glyphicon-menu-left"></span>
        <nav>
          <ul class="section-nav-list" data-section-nav>
            <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<li><span class="section-nav-list-main-title">Documentation</span></li>
<li><a href="/documentation">Using the Documentation</a></li>
<li><a href="/documentation/execution-model">Beam Execution Model</a></li>
<li class="section-nav-item--collapsible">
  <span class="section-nav-list-title">Pipeline development lifecycle</span>

  <ul class="section-nav-list">
    <li><a href="/documentation/pipelines/design-your-pipeline/">Design Your Pipeline</a></li>
    <li><a href="/documentation/pipelines/create-your-pipeline/">Create Your Pipeline</a></li>
    <li><a href="/documentation/pipelines/test-your-pipeline/">Test Your Pipeline</a></li>
  </ul>
</li>
<li class="section-nav-item--collapsible">
  <span class="section-nav-list-title">Beam programming guide</span>

  <ul class="section-nav-list">
    <li><a href="/documentation/programming-guide/">Overview</a></li>
    <li><a href="/documentation/programming-guide/#creating-a-pipeline">Pipelines</a></li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">PCollections</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#pcollections">Creating a PCollection</a></li>
        <li><a href="/documentation/programming-guide/#pcollection-characteristics">PCollection characteristics</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Transforms</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#applying-transforms">Applying transforms</a></li>
        <li>
          <span class="section-nav-list-title">Core Beam transforms</span>

          <ul class="section-nav-list">
            <li><a href="/documentation/programming-guide/#pardo">ParDo</a></li>
            <li><a href="/documentation/programming-guide/#groupbykey">GroupByKey</a></li>
            <li><a href="/documentation/programming-guide/#cogroupbykey">CoGroupByKey</a></li>
            <li><a href="/documentation/programming-guide/#combine">Combine</a></li>
            <li><a href="/documentation/programming-guide/#flatten">Flatten</a></li>
            <li><a href="/documentation/programming-guide/#partition">Partition</a></li>
          </ul>
        </li>

        <li><a href="/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms">Requirements for user code</a></li>
        <li><a href="/documentation/programming-guide/#side-inputs">Side inputs</a></li>
        <li><a href="/documentation/programming-guide/#additional-outputs">Additional outputs</a></li>
        <li><a href="/documentation/programming-guide/#composite-transforms">Composite transforms</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Pipeline I/O</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#pipeline-io">Using I/O transforms</a></li>
        <li><a href="/documentation/io/built-in/">Built-in I/O transforms</a></li>
        <li><a href="/documentation/io/authoring-overview/">Authoring new I/O transforms</a></li>
        <li><a href="/documentation/io/testing/">Testing I/O transforms</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Data encoding and type safety</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#data-encoding-and-type-safety">Data encoding basics</a></li>
        <li><a href="/documentation/programming-guide/#specifying-coders">Specifying coders</a></li>
        <li><a href="/documentation/programming-guide/#default-coders-and-the-coderregistry">Default coders and the CoderRegistry</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Windowing</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#windowing">Windowing basics</a></li>
        <li><a href="/documentation/programming-guide/#provided-windowing-functions">Provided windowing functions</a></li>
        <li><a href="/documentation/programming-guide/#setting-your-pcollections-windowing-function">Setting your PCollection’s windowing function</a></li>
        <li><a href="/documentation/programming-guide/#watermarks-and-late-data">Watermarks and late data</a></li>
        <li><a href="/documentation/programming-guide/#adding-timestamps-to-a-pcollections-elements">Adding timestamps to a PCollection’s elements</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Triggers</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#triggers">Trigger basics</a></li>
        <li><a href="/documentation/programming-guide/#event-time-triggers">Event time triggers and the default trigger</a></li>
        <li><a href="/documentation/programming-guide/#processing-time-triggers">Processing time triggers</a></li>
        <li><a href="/documentation/programming-guide/#data-driven-triggers">Data-driven triggers</a></li>
        <li><a href="/documentation/programming-guide/#setting-a-trigger">Setting a trigger</a></li>
        <li><a href="/documentation/programming-guide/#composite-triggers">Composite triggers</a></li>
      </ul>
    </li>
    <li><a href="/documentation/resources/">Additional Resources</a></li>
  </ul>
</li>

          </ul>
        </nav>
      </div>

      <nav class="page-nav clearfix" data-offset-top="90" data-offset-bottom="500">
        <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->



<ul class="nav">
  <li><a href="#reading-using-hadoop-inputformat-io">Reading using Hadoop InputFormat IO</a></li>
  <li><a href="#cassandra---cqlinputformat">Cassandra - CqlInputFormat</a></li>
  <li><a href="#elasticsearch---esinputformat">Elasticsearch - EsInputFormat</a></li>
  <li><a href="#hcatalog---hcatinputformat">HCatalog - HCatInputFormat</a></li>
  <li><a href="#amazon-dynamodb---dynamodbinputformat">Amazon DynamoDB - DynamoDBInputFormat</a></li>
  <li><a href="#apache-hbase---tablesnapshotinputformat">Apache HBase - TableSnapshotInputFormat</a></li>
</ul>


      </nav>

      <div class="body__contained body__section-nav">
        <!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p><a href="/documentation/io/io-toc/">Pipeline I/O Table of Contents</a></p>

<h1 id="hadoop-inputformat-io">Hadoop InputFormat IO</h1>

<p>A <code class="highlighter-rouge">HadoopInputFormatIO</code> is a transform for reading data from any source that implements Hadoop’s <code class="highlighter-rouge">InputFormat</code>. For example, Cassandra, Elasticsearch, HBase, Redis, Postgres, etc.</p>

<p><code class="highlighter-rouge">HadoopInputFormatIO</code> allows you to connect to many data sources that do not yet have a Beam IO transform. However, <code class="highlighter-rouge">HadoopInputFormatIO</code> has to make several performance trade-offs in connecting to <code class="highlighter-rouge">InputFormat</code>. So, if there is another Beam IO transform for connecting specifically to your data source of choice, we recommend you use that one.</p>

<p>You will need to pass a Hadoop <code class="highlighter-rouge">Configuration</code> with parameters specifying how the read will occur. Many properties of the <code class="highlighter-rouge">Configuration</code> are optional and some are required for certain <code class="highlighter-rouge">InputFormat</code> classes, but the following properties must be set for all <code class="highlighter-rouge">InputFormat</code> classes:</p>

<ul>
  <li><code class="highlighter-rouge">mapreduce.job.inputformat.class</code> - The <code class="highlighter-rouge">InputFormat</code> class used to connect to your data source of choice.</li>
  <li><code class="highlighter-rouge">key.class</code> - The <code class="highlighter-rouge">Key</code> class returned by the <code class="highlighter-rouge">InputFormat</code> in <code class="highlighter-rouge">mapreduce.job.inputformat.class</code>.</li>
  <li><code class="highlighter-rouge">value.class</code> - The <code class="highlighter-rouge">Value</code> class returned by the <code class="highlighter-rouge">InputFormat</code> in <code class="highlighter-rouge">mapreduce.job.inputformat.class</code>.</li>
</ul>

<p>For example:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">myHadoopConfiguration</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="kc">false</span><span class="o">);</span>
<span class="c1">// Set Hadoop InputFormat, key and value class in configuration</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">InputFormatClass</span><span class="o">,</span>
  <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">InputFormatValueClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>You will need to check if the <code class="highlighter-rouge">Key</code> and <code class="highlighter-rouge">Value</code> classes output by the <code class="highlighter-rouge">InputFormat</code> have a Beam <code class="highlighter-rouge">Coder</code> available. If not, you can use <code class="highlighter-rouge">withKeyTranslation</code> or <code class="highlighter-rouge">withValueTranslation</code> to specify a method transforming instances of those classes into another class that is supported by a Beam <code class="highlighter-rouge">Coder</code>. These settings are optional and you don’t need to specify translation for both key and value.</p>

<p>For example:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">MyKeyClass</span><span class="o">&gt;</span> <span class="n">myOutputKeyType</span> <span class="o">=</span>
<span class="k">new</span> <span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">MyKeyClass</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">MyKeyClass</span> <span class="nf">apply</span><span class="o">(</span><span class="n">InputFormatKeyClass</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
  <span class="c1">// ...logic to transform InputFormatKeyClass to MyKeyClass</span>
  <span class="o">}</span>
<span class="o">};</span>
<span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatValueClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;</span> <span class="n">myOutputValueType</span> <span class="o">=</span>
<span class="k">new</span> <span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatValueClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">MyValueClass</span> <span class="nf">apply</span><span class="o">(</span><span class="n">InputFormatValueClass</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
  <span class="c1">// ...logic to transform InputFormatValueClass to MyValueClass</span>
  <span class="o">}</span>
<span class="o">};</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h3 id="reading-using-hadoop-inputformat-io">Reading using Hadoop InputFormat IO</h3>

<h4 id="read-data-only-with-hadoop-configuration">Read data only with Hadoop configuration.</h4>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">InputFormatKeyClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h4 id="read-data-with-configuration-and-key-translation">Read data with configuration and key translation</h4>

<p>For example, a Beam <code class="highlighter-rouge">Coder</code> is not available for <code class="highlighter-rouge">Key</code> class, so key translation is required.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">MyKeyClass</span><span class="o">,</span> <span class="n">InputFormatKeyClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withKeyTranslation</span><span class="o">(</span><span class="n">myOutputKeyType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h4 id="read-data-with-configuration-and-value-translation">Read data with configuration and value translation</h4>

<p>For example, a Beam <code class="highlighter-rouge">Coder</code> is not available for <code class="highlighter-rouge">Value</code> class, so value translation is required.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withValueTranslation</span><span class="o">(</span><span class="n">myOutputValueType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h4 id="read-data-with-configuration-value-translation-and-key-translation">Read data with configuration, value translation and key translation</h4>

<p>For example, Beam Coders are not available for both <code class="highlighter-rouge">Key</code> class and <code class="highlighter-rouge">Value</code> classes of <code class="highlighter-rouge">InputFormat</code>, so key and value translation are required.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">MyKeyClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withKeyTranslation</span><span class="o">(</span><span class="n">myOutputKeyType</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withValueTranslation</span><span class="o">(</span><span class="n">myOutputValueType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h1 id="examples-for-specific-inputformats">Examples for specific InputFormats</h1>

<h3 id="cassandra---cqlinputformat">Cassandra - CqlInputFormat</h3>

<p>To read data from Cassandra, use <code class="highlighter-rouge">org.apache.cassandra.hadoop.cql3.CqlInputFormat</code>, which needs the following properties to be set:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">cassandraConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.thrift.port"</span><span class="o">,</span> <span class="s">"9160"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.thrift.address"</span><span class="o">,</span> <span class="n">CassandraHostIp</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.partitioner.class"</span><span class="o">,</span> <span class="s">"Murmur3Partitioner"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.keyspace"</span><span class="o">,</span> <span class="s">"myKeySpace"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.columnfamily"</span><span class="o">,</span> <span class="s">"myColumnFamily"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">java</span><span class="o">.</span><span class="na">lang</span><span class="o">.</span><span class="na">Long</span> <span class="n">Long</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">com</span><span class="o">.</span><span class="na">datastax</span><span class="o">.</span><span class="na">driver</span><span class="o">.</span><span class="na">core</span><span class="o">.</span><span class="na">Row</span> <span class="n">Row</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">cassandra</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">cql3</span><span class="o">.</span><span class="na">CqlInputFormat</span> <span class="n">CqlInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">cassandraData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">cassandraConf</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withValueTranslation</span><span class="o">(</span><span class="n">cassandraOutputValueType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>The <code class="highlighter-rouge">CqlInputFormat</code> key class is <code class="highlighter-rouge">java.lang.Long</code> <code class="highlighter-rouge">Long</code>, which has a Beam <code class="highlighter-rouge">Coder</code>. The <code class="highlighter-rouge">CqlInputFormat</code> value class is <code class="highlighter-rouge">com.datastax.driver.core.Row</code> <code class="highlighter-rouge">Row</code>, which does not have a Beam <code class="highlighter-rouge">Coder</code>. Rather than write a new coder, you can provide your own translation method, as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">cassandraOutputValueType</span> <span class="o">=</span> <span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span>
<span class="o">{</span>
  <span class="kd">public</span> <span class="n">String</span> <span class="nf">apply</span><span class="o">(</span><span class="n">Row</span> <span class="n">row</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">row</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="err">'</span><span class="n">myColName</span><span class="err">'</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">};</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h3 id="elasticsearch---esinputformat">Elasticsearch - EsInputFormat</h3>

<p>To read data from Elasticsearch, use <code class="highlighter-rouge">EsInputFormat</code>, which needs following properties to be set:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">elasticSearchConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"es.nodes"</span><span class="o">,</span> <span class="n">ElasticsearchHostIp</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"es.port"</span><span class="o">,</span> <span class="s">"9200"</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"es.resource"</span><span class="o">,</span> <span class="s">"ElasticIndexName/ElasticTypeName"</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">io</span><span class="o">.</span><span class="na">Text</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">elasticsearch</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">mr</span><span class="o">.</span><span class="na">LinkedMapWritable</span> <span class="n">LinkedMapWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">elasticsearch</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">mr</span><span class="o">.</span><span class="na">EsInputFormat</span> <span class="n">EsInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LinkedMapWritable</span><span class="o">&gt;&gt;</span> <span class="n">elasticData</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LinkedMapWritable</span><span class="o">&gt;</span><span class="n">read</span><span class="o">().</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">elasticSearchConf</span><span class="o">));</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>The <code class="highlighter-rouge">org.elasticsearch.hadoop.mr.EsInputFormat</code>’s <code class="highlighter-rouge">EsInputFormat</code> key class is <code class="highlighter-rouge">org.apache.hadoop.io.Text</code> <code class="highlighter-rouge">Text</code>, and its value class is <code class="highlighter-rouge">org.elasticsearch.hadoop.mr.LinkedMapWritable</code> <code class="highlighter-rouge">LinkedMapWritable</code>. Both key and value classes have Beam Coders.</p>

<h3 id="hcatalog---hcatinputformat">HCatalog - HCatInputFormat</h3>

<p>To read data using HCatalog, use <code class="highlighter-rouge">org.apache.hive.hcatalog.mapreduce.HCatInputFormat</code>, which needs the following properties to be set:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">hcatConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">HCatInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">HCatRecord</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"hive.metastore.uris"</span><span class="o">,</span> <span class="s">"thrift://metastore-host:port"</span><span class="o">);</span>

<span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hive</span><span class="o">.</span><span class="na">hcatalog</span><span class="o">.</span><span class="na">mapreduce</span><span class="o">.</span><span class="na">HCatInputFormat</span><span class="o">.</span><span class="na">setInput</span><span class="o">(</span><span class="n">hcatConf</span><span class="o">,</span> <span class="s">"my_database"</span><span class="o">,</span> <span class="s">"my_table"</span><span class="o">,</span> <span class="s">"my_filter"</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">HCatRecord</span><span class="o">&gt;&gt;</span> <span class="n">hcatData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">HCatRecord</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">hcatConf</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h3 id="amazon-dynamodb---dynamodbinputformat">Amazon DynamoDB - DynamoDBInputFormat</h3>

<p>To read data from Amazon DynamoDB, use <code class="highlighter-rouge">org.apache.hadoop.dynamodb.read.DynamoDBInputFormat</code>.
DynamoDBInputFormat implements the older <code class="highlighter-rouge">org.apache.hadoop.mapred.InputFormat</code> interface and to make it compatible with HadoopInputFormatIO which uses the newer abstract class <code class="highlighter-rouge">org.apache.hadoop.mapreduce.InputFormat</code>,
a wrapper API is required which acts as an adapter between HadoopInputFormatIO and DynamoDBInputFormat (or in general any InputFormat implementing <code class="highlighter-rouge">org.apache.hadoop.mapred.InputFormat</code>)
The below example uses one such available wrapper API - <a href="https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/MapReduceInputFormatWrapper.java">https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/MapReduceInputFormatWrapper.java</a></p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">dynamoDBConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="n">dynamoDBConf</span><span class="o">);</span>
<span class="n">com</span><span class="o">.</span><span class="na">twitter</span><span class="o">.</span><span class="na">elephantbird</span><span class="o">.</span><span class="na">mapreduce</span><span class="o">.</span><span class="na">input</span><span class="o">.</span><span class="na">MapReduceInputFormatWrapper</span><span class="o">.</span><span class="na">setInputFormat</span><span class="o">(</span><span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">dynamodb</span><span class="o">.</span><span class="na">read</span><span class="o">.</span><span class="na">DynamoDBInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">job</span><span class="o">);</span>
<span class="n">dynamoDBConf</span> <span class="o">=</span> <span class="n">job</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">();</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">WritableComparable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">dynamodb</span><span class="o">.</span><span class="na">DynamoDBItemWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Writable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.servicename"</span><span class="o">,</span> <span class="s">"dynamodb"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.input.tableName"</span><span class="o">,</span> <span class="s">"table_name"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.endpoint"</span><span class="o">,</span> <span class="s">"dynamodb.us-west-1.amazonaws.com"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.regionid"</span><span class="o">,</span> <span class="s">"us-west-1"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.throughput.read"</span><span class="o">,</span> <span class="s">"1"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.throughput.read.percent"</span><span class="o">,</span> <span class="s">"1"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.version"</span><span class="o">,</span> <span class="s">"2011-12-05"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">DynamoDBConstants</span><span class="o">.</span><span class="na">DYNAMODB_ACCESS_KEY_CONF</span><span class="o">,</span> <span class="s">"aws_access_key"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">DynamoDBConstants</span><span class="o">.</span><span class="na">DYNAMODB_SECRET_KEY_CONF</span><span class="o">,</span> <span class="s">"aws_secret_key"</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">DynamoDBItemWritable</span><span class="o">&gt;</span> <span class="n">dynamoDBData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">DynamoDBItemWritable</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">dynamoDBConf</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<h3 id="apache-hbase---tablesnapshotinputformat">Apache HBase - TableSnapshotInputFormat</h3>

<p>To read data from an HBase table snapshot, use <code class="highlighter-rouge">org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat</code>.
Reading from a table snapshot bypasses the HBase region servers, instead reading HBase data files directly from the filesystem.
This is useful for cases such as reading historical data or offloading of work from the HBase cluster. 
There are scenarios when this may prove faster than accessing content through the region servers using the <code class="highlighter-rouge">HBaseIO</code>.</p>

<p>A table snapshot can be taken using the HBase shell or programmatically:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="k">try</span> <span class="o">(</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionFactory</span><span class="o">.</span><span class="na">createConnection</span><span class="o">(</span><span class="n">hbaseConf</span><span class="o">);</span>
    <span class="n">Admin</span> <span class="n">admin</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="na">getAdmin</span><span class="o">()</span>
  <span class="o">)</span> <span class="o">{</span>
  <span class="n">admin</span><span class="o">.</span><span class="na">snapshot</span><span class="o">(</span>
    <span class="s">"my_snaphshot"</span><span class="o">,</span>
    <span class="n">TableName</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="s">"my_table"</span><span class="o">),</span>
    <span class="n">HBaseProtos</span><span class="o">.</span><span class="na">SnapshotDescription</span><span class="o">.</span><span class="na">Type</span><span class="o">.</span><span class="na">FLUSH</span><span class="o">);</span>
<span class="o">}</span>  
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>A <code class="highlighter-rouge">TableSnapshotInputFormat</code> is configured as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="c1">// Construct a typical HBase scan</span>
<span class="n">Scan</span> <span class="n">scan</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Scan</span><span class="o">();</span>
<span class="n">scan</span><span class="o">.</span><span class="na">setCaching</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>
<span class="n">scan</span><span class="o">.</span><span class="na">setBatch</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>
<span class="n">scan</span><span class="o">.</span><span class="na">addColumn</span><span class="o">(</span><span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"CF"</span><span class="o">),</span> <span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"col_1"</span><span class="o">));</span>
<span class="n">scan</span><span class="o">.</span><span class="na">addColumn</span><span class="o">(</span><span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"CF"</span><span class="o">),</span> <span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"col_2"</span><span class="o">));</span>

<span class="n">Configuration</span> <span class="n">hbaseConf</span> <span class="o">=</span> <span class="n">HBaseConfiguration</span><span class="o">.</span><span class="na">create</span><span class="o">();</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">HConstants</span><span class="o">.</span><span class="na">ZOOKEEPER_QUORUM</span><span class="o">,</span> <span class="s">"zk1:2181"</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"hbase.rootdir"</span><span class="o">,</span> <span class="s">"/hbase"</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span>
    <span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">TableSnapshotInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">ImmutableBytesWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Writable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">Result</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Writable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">ClientProtos</span><span class="o">.</span><span class="na">Scan</span> <span class="n">proto</span> <span class="o">=</span> <span class="n">ProtobufUtil</span><span class="o">.</span><span class="na">toScan</span><span class="o">(</span><span class="n">scan</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">TableInputFormat</span><span class="o">.</span><span class="na">SCAN</span><span class="o">,</span> <span class="n">Base64</span><span class="o">.</span><span class="na">encodeBytes</span><span class="o">(</span><span class="n">proto</span><span class="o">.</span><span class="na">toByteArray</span><span class="o">()));</span>

<span class="c1">// Make use of existing utility methods</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="n">hbaseConf</span><span class="o">);</span> <span class="c1">// creates internal clone of hbaseConf</span>
<span class="n">TableSnapshotInputFormat</span><span class="o">.</span><span class="na">setInput</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="s">"my_snapshot"</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="s">"/tmp/snapshot_restore"</span><span class="o">));</span>
<span class="n">hbaseConf</span> <span class="o">=</span> <span class="n">job</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">();</span> <span class="c1">// extract the modified clone</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">ImmutableBytesWritable</span><span class="o">,</span> <span class="n">Result</span><span class="o">&gt;</span> <span class="n">hbaseSnapshotData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopInputFormatIO</span><span class="o">.&lt;</span><span class="n">ImmutableBytesWritable</span><span class="o">,</span> <span class="n">Result</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">hbaseConf</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop InputFormat IO.</span>
</code></pre>
</div>

      </div>
    </div>
    <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<footer class="footer">
  <div class="footer__contained">
    <div class="footer__cols">
      <div class="footer__cols__col">
        <div class="footer__cols__col__logo">
          <img src="/images/beam_logo_circle.svg" class="footer__logo" alt="Beam logo">
        </div>
        <div class="footer__cols__col__logo">
          <img src="/images/apache_logo_circle.svg" class="footer__logo" alt="Apache logo">
        </div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Start</div>
        <div class="footer__cols__col__link"><a href="/get-started/beam-overview/">Overview</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/quickstart-java/">Quickstart (Java)</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/quickstart-py/">Quickstart (Python)</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/quickstart-go/">Quickstart (Go)</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/downloads/">Downloads</a></div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Docs</div>
        <div class="footer__cols__col__link"><a href="/documentation/programming-guide/">Concepts</a></div>
        <div class="footer__cols__col__link"><a href="/documentation/pipelines/design-your-pipeline/">Pipelines</a></div>
        <div class="footer__cols__col__link"><a href="/documentation/runners/capability-matrix/">Runners</a></div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Community</div>
        <div class="footer__cols__col__link"><a href="/contribute/">Contribute</a></div>
        <div class="footer__cols__col__link"><a href="https://projects.apache.org/committee.html?beam" target="_blank">Team<img src="/images/external-link-icon.png"
                                                                                                                                width="14" height="14"
                                                                                                                                alt="External link."></a></div>
        <div class="footer__cols__col__link"><a href="/contribute/presentation-materials/">Media</a></div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Resources</div>
        <div class="footer__cols__col__link"><a href="/blog/">Blog</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/support/">Support</a></div>
        <div class="footer__cols__col__link"><a href="https://github.com/apache/beam">GitHub</a></div>
      </div>
    </div>
  </div>
  <div class="footer__bottom">
    &copy;
    <a href="http://www.apache.org">The Apache Software Foundation</a>
    | <a href="/privacy_policy">Privacy Policy</a>
    | <a href="/feed.xml">RSS Feed</a>
    <br><br>
    Apache Beam, Apache, Beam, the Beam logo, and the Apache feather logo are
    either registered trademarks or trademarks of The Apache Software
    Foundation. All other products or name brands are trademarks of their
    respective holders, including The Apache Software Foundation.
  </div>
</footer>

  </body>
</html>
