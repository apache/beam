<!DOCTYPE html>
<!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<html lang="en">
  <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Apache Hadoop Input/Output Format IO</title>
  <meta name="description" content="Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.
">
  <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400" rel="stylesheet">
  <link rel="stylesheet" href="/css/site.css">
  <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
  <script src="/js/bootstrap.min.js"></script>
  <script src="/js/language-switch.js"></script>
  <script src="/js/fix-menu.js"></script>
  <script src="/js/section-nav.js"></script>
  <script src="/js/page-nav.js"></script>
  <link rel="canonical" href="https://beam.apache.org/documentation/io/built-in/hadoop/" data-proofer-ignore>
  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico">
  <link rel="alternate" type="application/rss+xml" title="Apache Beam" href="https://beam.apache.org/feed.xml">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-73650088-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

  <body class="body" data-spy="scroll" data-target=".page-nav" data-offset="0">
    <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<nav class="header navbar navbar-fixed-top">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" aria-expanded="false" aria-controls="navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a href="/" class="navbar-brand" >
        <img alt="Brand" style="height: 25px" src="/images/beam_logo_navbar.png">
      </a>
    </div>

    <div class="navbar-mask closed"></div>

    <div id="navbar" class="navbar-container closed">
      <ul class="nav navbar-nav">
        <li>
          <a href="/get-started/beam-overview/">Get Started</a>
        </li>
        <li>
          <a href="/documentation/">Documentation</a>
        </li>
        <li>
          <a href="/documentation/sdks/java/">Languages</a>
        </li>
        <li>
          <a href="/documentation/runners/capability-matrix/">RUNNERS</a>
        </li>
        <li>
          <a href="/roadmap/">Roadmap</a>
        </li>
        <li>
          <a href="/contribute/">Contribute</a>
        </li>
        <li>
          <a href="/community/contact-us/">Community</a>
        </li>
        <li><a href="/blog">Blog</a></li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
          <div style="width: 300px;">
            <script>
              (function() {
                var cx = '012923275103528129024:4emlchv9wzi';
                var gcse = document.createElement('script');
                gcse.type = 'text/javascript';
                gcse.async = true;
                gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(gcse, s);
              })();
            </script>
            <gcse:search></gcse:search>
          </div>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><img src="https://www.apache.org/foundation/press/kit/feather_small.png" alt="Apache Logo" style="height:20px;"><span class="caret"></span></a>
          <ul class="dropdown-menu dropdown-menu-right">
            <li><a href="http://www.apache.org/">ASF Homepage</a></li>
            <li><a href="http://www.apache.org/licenses/">License</a></li>
            <li><a href="http://www.apache.org/security/">Security</a></li>
            <li><a href="http://www.apache.org/foundation/thanks.html">Thanks</a></li>
            <li><a href="http://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
            <li><a href="https://www.apache.org/foundation/policies/conduct">Code of Conduct</a></li>
          </ul>
        </li>
        <li>
          <!--
            data-proofer-ignore disables link checking from website test automation.
            GitHub links will not resolve until the markdown source is available on the master branch.
            New pages would fail validation during development / PR test automation.
          -->
          <a href="https://github.com/apache/beam/edit/master/website/src/documentation/io/built-in-hadoop.md" data-proofer-ignore>
            <i class="far fa-edit fa-lg" alt="Edit on GitHub" title="Edit on GitHub"></i>
          </a>
        </li>
      </ul>
    </div>
</nav>

    <div class="clearfix container-main-content">
      <div class="section-nav closed" data-offset-top="90" data-offset-bottom="500">
        <span class="section-nav-back glyphicon glyphicon-menu-left"></span>
        <nav>
          <ul class="section-nav-list" data-section-nav>
            <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<li><span class="section-nav-list-main-title">Documentation</span></li>
<li><a href="/documentation">Using the Documentation</a></li>
<li><a href="/documentation/execution-model">Beam Execution Model</a></li>
<li>
  <span class="section-nav-list-title">Pipeline development lifecycle</span>

  <ul class="section-nav-list">
    <li><a href="/documentation/pipelines/design-your-pipeline/">Design Your Pipeline</a></li>
    <li><a href="/documentation/pipelines/create-your-pipeline/">Create Your Pipeline</a></li>
    <li><a href="/documentation/pipelines/test-your-pipeline/">Test Your Pipeline</a></li>
  </ul>
</li>
<li>
  <span class="section-nav-list-title">Beam programming guide</span>

  <ul class="section-nav-list">
    <li><a href="/documentation/programming-guide/">Overview</a></li>
    <li><a href="/documentation/programming-guide/#creating-a-pipeline">Pipelines</a></li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">PCollections</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#pcollections">Creating a PCollection</a></li>
        <li><a href="/documentation/programming-guide/#pcollection-characteristics">PCollection characteristics</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Transforms</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#applying-transforms">Applying transforms</a></li>
        <li>
          <span class="section-nav-list-title">Core Beam transforms</span>

          <ul class="section-nav-list">
            <li><a href="/documentation/programming-guide/#pardo">ParDo</a></li>
            <li><a href="/documentation/programming-guide/#groupbykey">GroupByKey</a></li>
            <li><a href="/documentation/programming-guide/#cogroupbykey">CoGroupByKey</a></li>
            <li><a href="/documentation/programming-guide/#combine">Combine</a></li>
            <li><a href="/documentation/programming-guide/#flatten">Flatten</a></li>
            <li><a href="/documentation/programming-guide/#partition">Partition</a></li>
          </ul>
        </li>

        <li><a href="/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms">Requirements for user code</a></li>
        <li><a href="/documentation/programming-guide/#side-inputs">Side inputs</a></li>
        <li><a href="/documentation/programming-guide/#additional-outputs">Additional outputs</a></li>
        <li><a href="/documentation/programming-guide/#composite-transforms">Composite transforms</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Pipeline I/O</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#pipeline-io">Using I/O transforms</a></li>
        <li><a href="/documentation/io/built-in/">Built-in I/O connectors</a></li>

        <li class="section-nav-item--collapsible">
           <span class="section-nav-list-title">Developing new I/O connectors</span>

          <ul class="section-nav-list">
           <li><a href="/documentation/io/developing-io-overview/">Overview: Developing connectors</a></li>
           <li><a href="/documentation/io/developing-io-java/">Developing connectors (Java)</a></li>
           <li><a href="/documentation/io/developing-io-python/">Developing connectors (Python)</a></li>
          </ul>
        </li>

        <li><a href="/documentation/io/testing/">Testing I/O transforms</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Data encoding and type safety</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#data-encoding-and-type-safety">Data encoding basics</a></li>
        <li><a href="/documentation/programming-guide/#specifying-coders">Specifying coders</a></li>
        <li><a href="/documentation/programming-guide/#default-coders-and-the-coderregistry">Default coders and the CoderRegistry</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Windowing</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#windowing">Windowing basics</a></li>
        <li><a href="/documentation/programming-guide/#provided-windowing-functions">Provided windowing functions</a></li>
        <li><a href="/documentation/programming-guide/#setting-your-pcollections-windowing-function">Setting your PCollection’s windowing function</a></li>
        <li><a href="/documentation/programming-guide/#watermarks-and-late-data">Watermarks and late data</a></li>
        <li><a href="/documentation/programming-guide/#adding-timestamps-to-a-pcollections-elements">Adding timestamps to a PCollection’s elements</a></li>
      </ul>
    </li>
    <li class="section-nav-item--collapsible">
      <span class="section-nav-list-title">Triggers</span>

      <ul class="section-nav-list">
        <li><a href="/documentation/programming-guide/#triggers">Trigger basics</a></li>
        <li><a href="/documentation/programming-guide/#event-time-triggers">Event time triggers and the default trigger</a></li>
        <li><a href="/documentation/programming-guide/#processing-time-triggers">Processing time triggers</a></li>
        <li><a href="/documentation/programming-guide/#data-driven-triggers">Data-driven triggers</a></li>
        <li><a href="/documentation/programming-guide/#setting-a-trigger">Setting a trigger</a></li>
        <li><a href="/documentation/programming-guide/#composite-triggers">Composite triggers</a></li>
      </ul>
    </li>
  </ul>
</li>

<li class="section-nav-item--collapsible">
  <span class="section-nav-list-title">Transform catalog</span>

  <ul class="section-nav-list">
  <li class="section-nav-item--collapsible">
    <span class="section-nav-list-title">Python</span>

    <ul class="section-nav-list">
      <li><a href="/documentation/transforms/python/overview/">Overview</a></li>
      <li class="section-nav-item--collapsible">
        <span class="section-nav-list-title">Element-wise</span>

        <ul class="section-nav-list">
          <li><a href="/documentation/transforms/python/elementwise/filter/">Filter</a></li>
          <li><a href="/documentation/transforms/python/elementwise/flatmap/">FlatMap</a></li>  
          <li><a href="/documentation/transforms/python/elementwise/keys/">Keys</a></li>
          <li><a href="/documentation/transforms/python/elementwise/kvswap/">KvSwap</a></li> 
          <li><a href="/documentation/transforms/python/elementwise/map/">Map</a></li>
          <li><a href="/documentation/transforms/python/elementwise/pardo/">ParDo</a></li>   
          <li><a href="/documentation/transforms/python/elementwise/partition/">Partition</a></li>
          <li><a href="/documentation/transforms/python/elementwise/regex/">Regex</a></li>
          <li><a href="/documentation/transforms/python/elementwise/reify/">Reify</a></li>
          <li><a href="/documentation/transforms/python/elementwise/tostring/">ToString</a></li>
          <li><a href="/documentation/transforms/python/elementwise/withtimestamps/">WithTimestamps</a></li>  
          <li><a href="/documentation/transforms/python/elementwise/values/">Values</a></li>                                            
        </ul>
      </li>
      <li class="section-nav-item--collapsible">
        <span class="section-nav-list-title">Aggregation</span>

        <ul class="section-nav-list">
          <li><a href="/documentation/transforms/python/aggregation/cogroupbykey/">CoGroupByKey</a></li>
          <li><a href="/documentation/transforms/python/aggregation/combineglobally/">CombineGlobally</a></li>
          <li><a href="/documentation/transforms/python/aggregation/count/">Count</a></li>
          <li><a href="/documentation/transforms/python/aggregation/distinct/">Distinct</a></li>
          <li><a href="/documentation/transforms/python/aggregation/groupbykey/">GroupByKey</a></li>
          <li><a href="/documentation/transforms/python/aggregation/mean/">Mean</a></li>
          <li><a href="/documentation/transforms/python/aggregation/sample/">Sample</a></li>
          <li><a href="/documentation/transforms/python/aggregation/top/">Top</a></li>
        </ul>
      </li>
      <li class="section-nav-item--collapsible">
        <span class="section-nav-list-title">Other</span>

        <ul class="section-nav-list">
          <li><a href="/documentation/transforms/python/other/create/">Create</a></li>
          <li><a href="/documentation/transforms/python/other/flatten/">Flatten</a></li>
          <li><a href="/documentation/transforms/python/other/reshuffle/">Reshuffle</a></li>
          <li><a href="/documentation/transforms/python/other/windowinto/">WindowInto</a></li>
        </ul>
      </li>
    </ul>
  </li>   
  </ul>      
</li>

<li class="section-nav-item--collapsible">
  <span class="section-nav-list-title">Common pipeline patterns</span>

  <ul class="section-nav-list">
    <li><a href="/documentation/patterns/overview/">Overview</a></li>
    <li><a href="/documentation/patterns/file-processing-patterns/">File processing patterns</a></li>
    <li><a href="/documentation/patterns/side-input-patterns/">Side input patterns</a></li>
    <li><a href="/documentation/patterns/pipeline-option-patterns/">Pipeline option patterns</a></li>
    <li><a href="/documentation/patterns/custom-io-patterns/">Custom I/O patterns</a></li>
  </ul>
</li>

<li class="section-nav-item--collapsible">
  <span class="section-nav-list-title">Learning resources</span>

  <ul class="section-nav-list">
    <li><a href="/documentation/resources/learning-resources/#getting-started">Getting Started</a></li>
    <li><a href="/documentation/resources/learning-resources/#articles">Articles</a></li>
    <li><a href="/documentation/resources/learning-resources/#interactive-labs">Interactive Labs</a></li>
    <li><a href="/documentation/resources/learning-resources/#beam-katas">Beam Katas</a></li>
    <li><a href="/documentation/resources/learning-resources/#code-examples">Code Examples</a></li>
    <li><a href="/documentation/resources/learning-resources/#api-reference">API Reference</a></li>
    <li><a href="/documentation/resources/learning-resources/#feedback-and-suggestions">Feedback and Suggestions</a></li>
    <li><a href="/documentation/resources/learning-resources/#how-to-contribute">How to Contribute</a></li>
    <li><a href="/documentation/resources/videos-and-podcasts">Videos and Podcasts</a></li>
  </ul>
</li>
<li><a href="https://cwiki.apache.org/confluence/display/BEAM/Apache+Beam">Beam Wiki</a></li>

          </ul>
        </nav>
      </div>

      <nav class="page-nav clearfix" data-offset-top="90" data-offset-bottom="500">
        <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->



<ul class="nav">
  <li><a href="#reading-using-hadoopformatio">Reading using HadoopFormatIO</a></li>
  <li><a href="#cassandra---cqlinputformat">Cassandra - CqlInputFormat</a></li>
  <li><a href="#elasticsearch---esinputformat">Elasticsearch - EsInputFormat</a></li>
  <li><a href="#hcatalog---hcatinputformat">HCatalog - HCatInputFormat</a></li>
  <li><a href="#amazon-dynamodb---dynamodbinputformat">Amazon DynamoDB - DynamoDBInputFormat</a></li>
  <li><a href="#apache-hbase---tablesnapshotinputformat">Apache HBase - TableSnapshotInputFormat</a></li>
  <li><a href="#writing-using-hadoopformatio">Writing using HadoopFormatIO</a></li>
</ul>


      </nav>

      <div class="body__contained body__section-nav">
        <!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<h1 id="hadoop-inputoutput-format-io">Hadoop Input/Output Format IO</h1>

<blockquote>
  <p><strong>IMPORTANT!</strong> Previous implementation of Hadoop Input Format IO, called <code class="highlighter-rouge">HadoopInputFormatIO</code>, is deprecated starting from <em>Apache Beam 2.10</em>. Please, use current <code class="highlighter-rouge">HadoopFormatIO</code> which supports both <code class="highlighter-rouge">InputFormat</code> and <code class="highlighter-rouge">OutputFormat</code>.</p>
</blockquote>

<p>A <code class="highlighter-rouge">HadoopFormatIO</code> is a transform for reading data from any source or writing data to any sink that implements Hadoop’s <code class="highlighter-rouge">InputFormat</code> or <code class="highlighter-rouge">OurputFormat</code> accordingly. For example, Cassandra, Elasticsearch, HBase, Redis, Postgres, etc.</p>

<p><code class="highlighter-rouge">HadoopFormatIO</code> allows you to connect to many data sources/sinks that do not yet have a Beam IO transform. However, <code class="highlighter-rouge">HadoopFormatIO</code> has to make several performance trade-offs in connecting to <code class="highlighter-rouge">InputFormat</code> or <code class="highlighter-rouge">OutputFormat</code>. So, if there is another Beam IO transform for connecting specifically to your data source/sink of choice, we recommend you use that one.</p>

<h3 id="reading-using-hadoopformatio">Reading using HadoopFormatIO</h3>

<p>You will need to pass a Hadoop <code class="highlighter-rouge">Configuration</code> with parameters specifying how the read will occur. Many properties of the <code class="highlighter-rouge">Configuration</code> are optional and some are required for certain <code class="highlighter-rouge">InputFormat</code> classes, but the following properties must be set for all <code class="highlighter-rouge">InputFormat</code> classes:</p>

<ul>
  <li><code class="highlighter-rouge">mapreduce.job.inputformat.class</code> - The <code class="highlighter-rouge">InputFormat</code> class used to connect to your data source of choice.</li>
  <li><code class="highlighter-rouge">key.class</code> - The <code class="highlighter-rouge">Key</code> class returned by the <code class="highlighter-rouge">InputFormat</code> in <code class="highlighter-rouge">mapreduce.job.inputformat.class</code>.</li>
  <li><code class="highlighter-rouge">value.class</code> - The <code class="highlighter-rouge">Value</code> class returned by the <code class="highlighter-rouge">InputFormat</code> in <code class="highlighter-rouge">mapreduce.job.inputformat.class</code>.</li>
</ul>

<p>For example:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">myHadoopConfiguration</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="kc">false</span><span class="o">);</span>
<span class="c1">// Set Hadoop InputFormat, key and value class in configuration</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">InputFormatClass</span><span class="o">,</span>
  <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">InputFormatValueClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>You will need to check if the <code class="highlighter-rouge">Key</code> and <code class="highlighter-rouge">Value</code> classes output by the <code class="highlighter-rouge">InputFormat</code> have a Beam <code class="highlighter-rouge">Coder</code> available. If not, you can use <code class="highlighter-rouge">withKeyTranslation</code> or <code class="highlighter-rouge">withValueTranslation</code> to specify a method transforming instances of those classes into another class that is supported by a Beam <code class="highlighter-rouge">Coder</code>. These settings are optional and you don’t need to specify translation for both key and value.</p>

<p>For example:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">MyKeyClass</span><span class="o">&gt;</span> <span class="n">myOutputKeyType</span> <span class="o">=</span>
<span class="k">new</span> <span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">MyKeyClass</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">MyKeyClass</span> <span class="nf">apply</span><span class="o">(</span><span class="n">InputFormatKeyClass</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
  <span class="c1">// ...logic to transform InputFormatKeyClass to MyKeyClass</span>
  <span class="o">}</span>
<span class="o">};</span>
<span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatValueClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;</span> <span class="n">myOutputValueType</span> <span class="o">=</span>
<span class="k">new</span> <span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">InputFormatValueClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">MyValueClass</span> <span class="nf">apply</span><span class="o">(</span><span class="n">InputFormatValueClass</span> <span class="n">input</span><span class="o">)</span> <span class="o">{</span>
  <span class="c1">// ...logic to transform InputFormatValueClass to MyValueClass</span>
  <span class="o">}</span>
<span class="o">};</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h4 id="read-data-only-with-hadoop-configuration">Read data only with Hadoop configuration.</h4>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">InputFormatKeyClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h4 id="read-data-with-configuration-and-key-translation">Read data with configuration and key translation</h4>

<p>For example, a Beam <code class="highlighter-rouge">Coder</code> is not available for <code class="highlighter-rouge">Key</code> class, so key translation is required.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">MyKeyClass</span><span class="o">,</span> <span class="n">InputFormatKeyClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withKeyTranslation</span><span class="o">(</span><span class="n">myOutputKeyType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h4 id="read-data-with-configuration-and-value-translation">Read data with configuration and value translation</h4>

<p>For example, a Beam <code class="highlighter-rouge">Coder</code> is not available for <code class="highlighter-rouge">Value</code> class, so value translation is required.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">InputFormatKeyClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withValueTranslation</span><span class="o">(</span><span class="n">myOutputValueType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h4 id="read-data-with-configuration-value-translation-and-key-translation">Read data with configuration, value translation and key translation</h4>

<p>For example, Beam Coders are not available for both <code class="highlighter-rouge">Key</code> class and <code class="highlighter-rouge">Value</code> classes of <code class="highlighter-rouge">InputFormat</code>, so key and value translation are required.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">MyKeyClass</span><span class="o">,</span> <span class="n">MyValueClass</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withKeyTranslation</span><span class="o">(</span><span class="n">myOutputKeyType</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withValueTranslation</span><span class="o">(</span><span class="n">myOutputValueType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h1 id="examples-for-specific-inputformats">Examples for specific InputFormats</h1>

<h3 id="cassandra---cqlinputformat">Cassandra - CqlInputFormat</h3>

<p>To read data from Cassandra, use <code class="highlighter-rouge">org.apache.cassandra.hadoop.cql3.CqlInputFormat</code>, which needs the following properties to be set:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">cassandraConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.thrift.port"</span><span class="o">,</span> <span class="s">"9160"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.thrift.address"</span><span class="o">,</span> <span class="n">CassandraHostIp</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.partitioner.class"</span><span class="o">,</span> <span class="s">"Murmur3Partitioner"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.keyspace"</span><span class="o">,</span> <span class="s">"myKeySpace"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"cassandra.input.columnfamily"</span><span class="o">,</span> <span class="s">"myColumnFamily"</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">java</span><span class="o">.</span><span class="na">lang</span><span class="o">.</span><span class="na">Long</span> <span class="n">Long</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">com</span><span class="o">.</span><span class="na">datastax</span><span class="o">.</span><span class="na">driver</span><span class="o">.</span><span class="na">core</span><span class="o">.</span><span class="na">Row</span> <span class="n">Row</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">cassandraConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">cassandra</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">cql3</span><span class="o">.</span><span class="na">CqlInputFormat</span> <span class="n">CqlInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">cassandraData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">cassandraConf</span><span class="o">)</span>
  <span class="o">.</span><span class="na">withValueTranslation</span><span class="o">(</span><span class="n">cassandraOutputValueType</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>The <code class="highlighter-rouge">CqlInputFormat</code> key class is <code class="highlighter-rouge">java.lang.Long</code> <code class="highlighter-rouge">Long</code>, which has a Beam <code class="highlighter-rouge">Coder</code>. The <code class="highlighter-rouge">CqlInputFormat</code> value class is <code class="highlighter-rouge">com.datastax.driver.core.Row</code> <code class="highlighter-rouge">Row</code>, which does not have a Beam <code class="highlighter-rouge">Coder</code>. Rather than write a new coder, you can provide your own translation method, as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">cassandraOutputValueType</span> <span class="o">=</span> <span class="n">SimpleFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;()</span>
<span class="o">{</span>
  <span class="kd">public</span> <span class="n">String</span> <span class="nf">apply</span><span class="o">(</span><span class="n">Row</span> <span class="n">row</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">row</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="err">'</span><span class="n">myColName</span><span class="err">'</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">};</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h3 id="elasticsearch---esinputformat">Elasticsearch - EsInputFormat</h3>

<p>To read data from Elasticsearch, use <code class="highlighter-rouge">EsInputFormat</code>, which needs following properties to be set:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">elasticSearchConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"es.nodes"</span><span class="o">,</span> <span class="n">ElasticsearchHostIp</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"es.port"</span><span class="o">,</span> <span class="s">"9200"</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"es.resource"</span><span class="o">,</span> <span class="s">"ElasticIndexName/ElasticTypeName"</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">io</span><span class="o">.</span><span class="na">Text</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">elasticsearch</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">mr</span><span class="o">.</span><span class="na">LinkedMapWritable</span> <span class="n">LinkedMapWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">elasticSearchConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">elasticsearch</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">mr</span><span class="o">.</span><span class="na">EsInputFormat</span> <span class="n">EsInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LinkedMapWritable</span><span class="o">&gt;&gt;</span> <span class="n">elasticData</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LinkedMapWritable</span><span class="o">&gt;</span><span class="n">read</span><span class="o">().</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">elasticSearchConf</span><span class="o">));</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>The <code class="highlighter-rouge">org.elasticsearch.hadoop.mr.EsInputFormat</code>’s <code class="highlighter-rouge">EsInputFormat</code> key class is <code class="highlighter-rouge">org.apache.hadoop.io.Text</code> <code class="highlighter-rouge">Text</code>, and its value class is <code class="highlighter-rouge">org.elasticsearch.hadoop.mr.LinkedMapWritable</code> <code class="highlighter-rouge">LinkedMapWritable</code>. Both key and value classes have Beam Coders.</p>

<h3 id="hcatalog---hcatinputformat">HCatalog - HCatInputFormat</h3>

<p>To read data using HCatalog, use <code class="highlighter-rouge">org.apache.hive.hcatalog.mapreduce.HCatInputFormat</code>, which needs the following properties to be set:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">hcatConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">HCatInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">HCatRecord</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hcatConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"hive.metastore.uris"</span><span class="o">,</span> <span class="s">"thrift://metastore-host:port"</span><span class="o">);</span>

<span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hive</span><span class="o">.</span><span class="na">hcatalog</span><span class="o">.</span><span class="na">mapreduce</span><span class="o">.</span><span class="na">HCatInputFormat</span><span class="o">.</span><span class="na">setInput</span><span class="o">(</span><span class="n">hcatConf</span><span class="o">,</span> <span class="s">"my_database"</span><span class="o">,</span> <span class="s">"my_table"</span><span class="o">,</span> <span class="s">"my_filter"</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">HCatRecord</span><span class="o">&gt;&gt;</span> <span class="n">hcatData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">Long</span><span class="o">,</span> <span class="n">HCatRecord</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">hcatConf</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h3 id="amazon-dynamodb---dynamodbinputformat">Amazon DynamoDB - DynamoDBInputFormat</h3>

<p>To read data from Amazon DynamoDB, use <code class="highlighter-rouge">org.apache.hadoop.dynamodb.read.DynamoDBInputFormat</code>.
DynamoDBInputFormat implements the older <code class="highlighter-rouge">org.apache.hadoop.mapred.InputFormat</code> interface and to make it compatible with HadoopFormatIO which uses the newer abstract class <code class="highlighter-rouge">org.apache.hadoop.mapreduce.InputFormat</code>,
a wrapper API is required which acts as an adapter between HadoopFormatIO and DynamoDBInputFormat (or in general any InputFormat implementing <code class="highlighter-rouge">org.apache.hadoop.mapred.InputFormat</code>)
The below example uses one such available wrapper API - <a href="https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/MapReduceInputFormatWrapper.java">https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/MapReduceInputFormatWrapper.java</a></p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">dynamoDBConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="n">dynamoDBConf</span><span class="o">);</span>
<span class="n">com</span><span class="o">.</span><span class="na">twitter</span><span class="o">.</span><span class="na">elephantbird</span><span class="o">.</span><span class="na">mapreduce</span><span class="o">.</span><span class="na">input</span><span class="o">.</span><span class="na">MapReduceInputFormatWrapper</span><span class="o">.</span><span class="na">setInputFormat</span><span class="o">(</span><span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">dynamodb</span><span class="o">.</span><span class="na">read</span><span class="o">.</span><span class="na">DynamoDBInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">job</span><span class="o">);</span>
<span class="n">dynamoDBConf</span> <span class="o">=</span> <span class="n">job</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">();</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">WritableComparable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">dynamodb</span><span class="o">.</span><span class="na">DynamoDBItemWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Writable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.servicename"</span><span class="o">,</span> <span class="s">"dynamodb"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.input.tableName"</span><span class="o">,</span> <span class="s">"table_name"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.endpoint"</span><span class="o">,</span> <span class="s">"dynamodb.us-west-1.amazonaws.com"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.regionid"</span><span class="o">,</span> <span class="s">"us-west-1"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.throughput.read"</span><span class="o">,</span> <span class="s">"1"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.throughput.read.percent"</span><span class="o">,</span> <span class="s">"1"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"dynamodb.version"</span><span class="o">,</span> <span class="s">"2011-12-05"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">DynamoDBConstants</span><span class="o">.</span><span class="na">DYNAMODB_ACCESS_KEY_CONF</span><span class="o">,</span> <span class="s">"aws_access_key"</span><span class="o">);</span>
<span class="n">dynamoDBConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">DynamoDBConstants</span><span class="o">.</span><span class="na">DYNAMODB_SECRET_KEY_CONF</span><span class="o">,</span> <span class="s">"aws_secret_key"</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">DynamoDBItemWritable</span><span class="o">&gt;</span> <span class="n">dynamoDBData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">DynamoDBItemWritable</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">dynamoDBConf</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h3 id="apache-hbase---tablesnapshotinputformat">Apache HBase - TableSnapshotInputFormat</h3>

<p>To read data from an HBase table snapshot, use <code class="highlighter-rouge">org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat</code>.
Reading from a table snapshot bypasses the HBase region servers, instead reading HBase data files directly from the filesystem.
This is useful for cases such as reading historical data or offloading of work from the HBase cluster. 
There are scenarios when this may prove faster than accessing content through the region servers using the <code class="highlighter-rouge">HBaseIO</code>.</p>

<p>A table snapshot can be taken using the HBase shell or programmatically:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="k">try</span> <span class="o">(</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionFactory</span><span class="o">.</span><span class="na">createConnection</span><span class="o">(</span><span class="n">hbaseConf</span><span class="o">);</span>
    <span class="n">Admin</span> <span class="n">admin</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="na">getAdmin</span><span class="o">()</span>
  <span class="o">)</span> <span class="o">{</span>
  <span class="n">admin</span><span class="o">.</span><span class="na">snapshot</span><span class="o">(</span>
    <span class="s">"my_snaphshot"</span><span class="o">,</span>
    <span class="n">TableName</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="s">"my_table"</span><span class="o">),</span>
    <span class="n">HBaseProtos</span><span class="o">.</span><span class="na">SnapshotDescription</span><span class="o">.</span><span class="na">Type</span><span class="o">.</span><span class="na">FLUSH</span><span class="o">);</span>
<span class="o">}</span>  
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>A <code class="highlighter-rouge">TableSnapshotInputFormat</code> is configured as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="c1">// Construct a typical HBase scan</span>
<span class="n">Scan</span> <span class="n">scan</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Scan</span><span class="o">();</span>
<span class="n">scan</span><span class="o">.</span><span class="na">setCaching</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>
<span class="n">scan</span><span class="o">.</span><span class="na">setBatch</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>
<span class="n">scan</span><span class="o">.</span><span class="na">addColumn</span><span class="o">(</span><span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"CF"</span><span class="o">),</span> <span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"col_1"</span><span class="o">));</span>
<span class="n">scan</span><span class="o">.</span><span class="na">addColumn</span><span class="o">(</span><span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"CF"</span><span class="o">),</span> <span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="s">"col_2"</span><span class="o">));</span>

<span class="n">Configuration</span> <span class="n">hbaseConf</span> <span class="o">=</span> <span class="n">HBaseConfiguration</span><span class="o">.</span><span class="na">create</span><span class="o">();</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">HConstants</span><span class="o">.</span><span class="na">ZOOKEEPER_QUORUM</span><span class="o">,</span> <span class="s">"zk1:2181"</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">"hbase.rootdir"</span><span class="o">,</span> <span class="s">"/hbase"</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span>
    <span class="s">"mapreduce.job.inputformat.class"</span><span class="o">,</span> <span class="n">TableSnapshotInputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">InputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"key.class"</span><span class="o">,</span> <span class="n">ImmutableBytesWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Writable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"value.class"</span><span class="o">,</span> <span class="n">Result</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Writable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">ClientProtos</span><span class="o">.</span><span class="na">Scan</span> <span class="n">proto</span> <span class="o">=</span> <span class="n">ProtobufUtil</span><span class="o">.</span><span class="na">toScan</span><span class="o">(</span><span class="n">scan</span><span class="o">);</span>
<span class="n">hbaseConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">TableInputFormat</span><span class="o">.</span><span class="na">SCAN</span><span class="o">,</span> <span class="n">Base64</span><span class="o">.</span><span class="na">encodeBytes</span><span class="o">(</span><span class="n">proto</span><span class="o">.</span><span class="na">toByteArray</span><span class="o">()));</span>

<span class="c1">// Make use of existing utility methods</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="n">hbaseConf</span><span class="o">);</span> <span class="c1">// creates internal clone of hbaseConf</span>
<span class="n">TableSnapshotInputFormat</span><span class="o">.</span><span class="na">setInput</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="s">"my_snapshot"</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="s">"/tmp/snapshot_restore"</span><span class="o">));</span>
<span class="n">hbaseConf</span> <span class="o">=</span> <span class="n">job</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">();</span> <span class="c1">// extract the modified clone</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>Call Read transform as follows:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">PCollection</span><span class="o">&lt;</span><span class="n">ImmutableBytesWritable</span><span class="o">,</span> <span class="n">Result</span><span class="o">&gt;</span> <span class="n">hbaseSnapshotData</span> <span class="o">=</span>
  <span class="n">p</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span><span class="s">"read"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">ImmutableBytesWritable</span><span class="o">,</span> <span class="n">Result</span><span class="o">&gt;</span><span class="n">read</span><span class="o">()</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">hbaseConf</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h3 id="writing-using-hadoopformatio">Writing using HadoopFormatIO</h3>

<p>You will need to pass a Hadoop <code class="highlighter-rouge">Configuration</code> with parameters specifying how the write will occur. Many properties of the <code class="highlighter-rouge">Configuration</code> are optional, and some are required for certain <code class="highlighter-rouge">OutputFormat</code> classes, but the following properties must be set for all <code class="highlighter-rouge">OutputFormat</code>s:</p>

<ul>
  <li><code class="highlighter-rouge">mapreduce.job.id</code> - The identifier of the write job. E.g.: end timestamp of window.</li>
  <li><code class="highlighter-rouge">mapreduce.job.outputformat.class</code> - The <code class="highlighter-rouge">OutputFormat</code> class used to connect to your data sink of choice.</li>
  <li><code class="highlighter-rouge">mapreduce.job.output.key.class</code> - The key class passed to the <code class="highlighter-rouge">OutputFormat</code> in <code class="highlighter-rouge">mapreduce.job.outputformat.class</code>.</li>
  <li><code class="highlighter-rouge">mapreduce.job.output.value.class</code> - The value class passed to the <code class="highlighter-rouge">OutputFormat</code> in <code class="highlighter-rouge">mapreduce.job.outputformat.class</code>.</li>
  <li><code class="highlighter-rouge">mapreduce.job.reduces</code> - Number of reduce tasks. Value is equal to number of write tasks which will be genarated. This property is not required for <code class="highlighter-rouge">Write.PartitionedWriterBuilder#withoutPartitioning()</code> write.</li>
  <li><code class="highlighter-rouge">mapreduce.job.partitioner.class</code> - Hadoop partitioner class which will be used for distributing of records among partitions. This property is not required for <code class="highlighter-rouge">Write.PartitionedWriterBuilder#withoutPartitioning()</code> write.</li>
</ul>

<p><em>Note</em>: All mentioned values have appropriate constants. E.g.: <code class="highlighter-rouge">HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR</code>.</p>

<p>For example:</p>
<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">Configuration</span> <span class="n">myHadoopConfiguration</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="kc">false</span><span class="o">);</span>
<span class="c1">// Set Hadoop OutputFormat, key and value class in configuration</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.outputformat.class"</span><span class="o">,</span>
   <span class="n">MyDbOutputFormatClass</span><span class="o">,</span> <span class="n">OutputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.output.key.class"</span><span class="o">,</span>
   <span class="n">MyDbOutputFormatKeyClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.output.value.class"</span><span class="o">,</span>
   <span class="n">MyDbOutputFormatValueClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setClass</span><span class="o">(</span><span class="s">"mapreduce.job.partitioner.class"</span><span class="o">,</span>
   <span class="n">MyPartitionerClass</span><span class="o">,</span> <span class="n">Object</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">myHadoopConfiguration</span><span class="o">.</span><span class="na">setInt</span><span class="o">(</span><span class="s">"mapreduce.job.reduces"</span><span class="o">,</span> <span class="mi">2</span><span class="o">);</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<p>You will need to set <code class="highlighter-rouge">OutputFormat</code> key and value class (i.e. “mapreduce.job.output.key.class” and “mapreduce.job.output.value.class”) in Hadoop <code class="highlighter-rouge">Configuration</code> which are equal to <code class="highlighter-rouge">KeyT</code> and <code class="highlighter-rouge">ValueT</code>. If you set different <code class="highlighter-rouge">OutputFormat</code> key or value class than <code class="highlighter-rouge">OutputFormat</code>’s actual key or value class then, it will throw <code class="highlighter-rouge">IllegalArgumentException</code>.</p>

<h4 id="batch-writing">Batch writing</h4>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="c1">// Data which will we want to write</span>
<span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">boundedWordsCount</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// Hadoop configuration for write</span>
<span class="c1">// We have partitioned write, so Partitioner and reducers count have to be set - see withPartitioning() javadoc</span>
<span class="n">Configuration</span> <span class="n">myHadoopConfiguration</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">// Path to directory with locks</span>
<span class="n">String</span> <span class="n">locksDirPath</span> <span class="o">=</span> <span class="o">...;</span>

<span class="n">boundedWordsCount</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span>
    <span class="s">"writeBatch"</span><span class="o">,</span>
    <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span><span class="n">write</span><span class="o">()</span>
        <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">myHadoopConfiguration</span><span class="o">)</span>
        <span class="o">.</span><span class="na">withPartitioning</span><span class="o">()</span>
        <span class="o">.</span><span class="na">withExternalSynchronization</span><span class="o">(</span><span class="k">new</span> <span class="n">HDFSSynchronization</span><span class="o">(</span><span class="n">locksDirPath</span><span class="o">)));</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

<h4 id="stream-writing">Stream writing</h4>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="c1">// Data which will we want to write</span>
<span class="n">PCollection</span><span class="o">&lt;</span><span class="n">KV</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">unboundedWordsCount</span> <span class="o">=</span> <span class="o">...;</span>

<span class="c1">// Transformation which transforms data of one window into one hadoop configuration</span>
<span class="n">PTransform</span><span class="o">&lt;</span><span class="n">PCollection</span><span class="o">&lt;?</span> <span class="kd">extends</span> <span class="n">KV</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;,</span> <span class="n">PCollectionView</span><span class="o">&lt;</span><span class="n">Configuration</span><span class="o">&gt;&gt;</span>
  <span class="n">configTransform</span> <span class="o">=</span> <span class="o">...;</span>

<span class="n">unboundedWordsCount</span><span class="o">.</span><span class="na">apply</span><span class="o">(</span>
  <span class="s">"writeStream"</span><span class="o">,</span>
  <span class="n">HadoopFormatIO</span><span class="o">.&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span><span class="n">write</span><span class="o">()</span>
      <span class="o">.</span><span class="na">withConfigurationTransform</span><span class="o">(</span><span class="n">configTransform</span><span class="o">)</span>
      <span class="o">.</span><span class="na">withExternalSynchronization</span><span class="o">(</span><span class="k">new</span> <span class="n">HDFSSynchronization</span><span class="o">(</span><span class="n">locksDirPath</span><span class="o">)));</span>
</code></pre>
</div>

<div class="language-py highlighter-rouge"><pre class="highlight"><code>  <span class="c"># The Beam SDK for Python does not support Hadoop Input/Output Format IO.</span>
</code></pre>
</div>

      </div>
    </div>
    <!--
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License. See accompanying LICENSE file.
-->

<footer class="footer">
  <div class="footer__contained">
    <div class="footer__cols">
      <div class="footer__cols__col">
        <div class="footer__cols__col__logo">
          <img src="/images/beam_logo_circle.svg" class="footer__logo" alt="Beam logo">
        </div>
        <div class="footer__cols__col__logo">
          <img src="/images/apache_logo_circle.svg" class="footer__logo" alt="Apache logo">
        </div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Start</div>
        <div class="footer__cols__col__link"><a href="/get-started/beam-overview/">Overview</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/quickstart-java/">Quickstart (Java)</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/quickstart-py/">Quickstart (Python)</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/quickstart-go/">Quickstart (Go)</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/downloads/">Downloads</a></div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Docs</div>
        <div class="footer__cols__col__link"><a href="/documentation/programming-guide/">Concepts</a></div>
        <div class="footer__cols__col__link"><a href="/documentation/pipelines/design-your-pipeline/">Pipelines</a></div>
        <div class="footer__cols__col__link"><a href="/documentation/runners/capability-matrix/">Runners</a></div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Community</div>
        <div class="footer__cols__col__link"><a href="/contribute/">Contribute</a></div>
        <div class="footer__cols__col__link"><a href="https://projects.apache.org/committee.html?beam" target="_blank">Team<img src="/images/external-link-icon.png"
                                                                                                                                width="14" height="14"
                                                                                                                                alt="External link."></a></div>
        <div class="footer__cols__col__link"><a href="/contribute/presentation-materials/">Media</a></div>
      </div>
      <div class="footer__cols__col footer__cols__col--md">
        <div class="footer__cols__col__title">Resources</div>
        <div class="footer__cols__col__link"><a href="/blog/">Blog</a></div>
        <div class="footer__cols__col__link"><a href="/get-started/support/">Support</a></div>
        <div class="footer__cols__col__link"><a href="https://github.com/apache/beam">GitHub</a></div>
      </div>
    </div>
  </div>
  <div class="footer__bottom">
    &copy;
    <a href="http://www.apache.org">The Apache Software Foundation</a>
    | <a href="/privacy_policy">Privacy Policy</a>
    | <a href="/feed.xml">RSS Feed</a>
    <br><br>
    Apache Beam, Apache, Beam, the Beam logo, and the Apache feather logo are
    either registered trademarks or trademarks of The Apache Software
    Foundation. All other products or name brands are trademarks of their
    respective holders, including The Apache Software Foundation.
  </div>
</footer>

  </body>
</html>
