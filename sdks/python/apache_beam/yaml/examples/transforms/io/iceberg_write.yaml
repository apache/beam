# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# The pipeline uses Dynamic destinations (see
# https://cloud.google.com/dataflow/docs/guides/managed-io#dynamic-destinations)
# to dynamically create and select a table destination based on field values in
# the incoming records.
#
# Replace 'gs://MY-WAREHOUSE' with the correct GCS bucket name.
# If this example is run locally then replace '/path/to/service/account/key.json'
# with the correct path to your service account key .json file on your machine.
# Otherwise, if Dataflow runner is used then omit the 'config_properties' field.

pipeline:
  type: chain
  transforms:
    - type: Create
      name: CreateSampleData
      config:
        elements:
          - { id: 1, name: "John", email: "john@example.com", zip: "WA" }
          - { id: 2, name: "Jane", email: "jane@example.com", zip: "CA" }
          - { id: 3, name: "Smith", email: "smith@example.com",zip: "NY"}
          - { id: 4, name: "Beamberg", email: "beamberg@example.com", zip: "NY" }

    - type: LogForTesting

    - type: WriteToIceberg
      name: WriteToAnIcebergTable
      config:
        # Dynamic destinations
        table: "db.users.{zip}"
        catalog_name: "hadoop_catalog"
        catalog_properties:
          type: "hadoop"
          warehouse: "gs://MY-WAREHOUSE"
        # Hadoop catalog config required to run pipeline locally
        # Omit if running on Dataflow
        config_properties:
          "fs.gs.auth.type": "SERVICE_ACCOUNT_JSON_KEYFILE"
          "fs.gs.auth.service.account.json.keyfile": "/path/to/service/account/key.json"

# Expected:
#  Row(id=1, name='John', email='john@example.com', zip='WA')
#  Row(id=2, name='Jane', email='jane@example.com', zip='CA')
#  Row(id=3, name='Smith', email='smith@example.com', zip='NY')
#  Row(id=4, name='Beamberg', email='beamberg@example.com', zip='NY')
