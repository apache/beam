# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# The pipeline reads text embeddings from a BigQuery table, applies anomaly
# scoring using a custom pre-trained k-Nearest Neighbours (KNN) model, and
# writes the results to an Iceberg table on GCS with BigLake metastore for
# catalog.

pipeline:
  type: chain
  transforms:
    - type: ReadFromBigQuery
      name: ReadFromBigQuery
      config:
        table: "{{ BQ_TABLE }}"
        fields: [embedding]

    - type: PyTransform
      name: AnomalyScoring
      config:
        constructor: __constructor__
        kwargs:
          source: |
            import apache_beam as beam
            from apache_beam.ml.anomaly.detectors.pyod_adapter import PyODFactory
            from apache_beam.ml.anomaly.transforms import AnomalyDetection
            
            class KNN(beam.PTransform):
              def __init__(self, model_artifact_path):
                self.model_artifact_path = model_artifact_path
                self.model = PyODFactory.create_detector(
                   self.model_artifact_path,
                   model_id="knn",
                )
            
              def expand(self, pcoll):
                return (
                  pcoll
                  | beam.Map(lambda x: x.embedding)
                  | AnomalyDetection(detector=self.model)
                  | beam.Map(lambda x: beam.Row(
                      example=x.example,
                      predictions=[pred.__dict__ for pred in x.predictions]))
              )

          model_artifact_path: "{{ WAREHOUSE }}/knn_model.pkl"

    - type: MapToFields
      name: ResultSchemaMapping
      config:
        language: python
        fields:
          anomaly_score:
            callable: "lambda row: row.predictions[0]['score']"
            output_type: number
          anomaly_label:
            callable: "lambda row: row.predictions[0]['label']"
            output_type: integer
          threshold:
            callable: "lambda row: row.predictions[0]['threshold']"
            output_type: number

    - type: WriteToIceberg
      name: WriteToIceberg
      config:
        table: "logs_analytics.hdfs_anomaly"
        catalog_name: "rest_catalog"
        catalog_properties:
          warehouse: "{{ WAREHOUSE }}"
          catalog-impl: "org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog"
          io-impl: "org.apache.iceberg.gcp.gcs.GCSFileIO"
          gcp_project: "{{ PROJECT }}"
          gcp_location: "{{ REGION }}"

# Expected:
#  Row(anomaly_score=0.65, anomaly_label=0, threshold=0.8)
#  Row(anomaly_score=0.65, anomaly_label=0, threshold=0.8)
#  Row(anomaly_score=0.65, anomaly_label=0, threshold=0.8)
