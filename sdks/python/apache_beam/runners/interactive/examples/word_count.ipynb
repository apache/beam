{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Word Count\n",
    "\n",
    "This example demonstrates how to set up an Apache Beam pipeline that reads from a\n",
    "[Google Cloud Storage](https://cloud.google.com/storage) file containing text from Shakespeare's work *King Lear*, \n",
    "tokenizes the text lines into individual words, and performs a frequency count on each of those words. \n",
    "\n",
    "An Apache Beam pipeline is a pipeline that reads input data, transforms that\n",
    "data, and writes output data. It consists of `PTransform`s and `PCollection`s.\n",
    "A `PCollection` represents a distributed data set that your Beam pipeline operates on.\n",
    "A `PTransform` represents a data processing operation, or a step, in your pipeline.\n",
    "It takes one or more `PCollection`s as input, performs a processing function\n",
    "that you provide on the elements of that `PCollection`, and produces zero\n",
    "or more output `PCollection` objects.\n",
    "\n",
    "For details about Apache Beam pipelines, including `PTransform`s and\n",
    "`PCollection`s, visit [Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/).\n",
    "\n",
    "You'll be able to use this notebook to explore the data in each `PCollection`.\n",
    "\n",
    "We first start with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's regular expression library\n",
    "import re\n",
    "\n",
    "# Beam and interactive Beam imports\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines a `PTransform` named `ReadWordsFromText`, that extracts words from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordsFromText(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, file_pattern):\n",
    "        self._file_pattern = file_pattern\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll.pipeline\n",
    "                | beam.io.ReadFromText(self._file_pattern)\n",
    "                | beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up an Apache Beam pipeline with the *Interactive Runner*.\n",
    "The *Interactive Runner* is the runner suitable for running in notebooks.\n",
    "A runner is an execution engine for Apache Beam pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up a `PTransform` that extracts words from a Google Cloud Storage file that contains the text of Shakespeare's work *King Lear*.\n",
    "\n",
    "`|` is an overloaded operator that applies a `PTransform` to a `PCollection` to produce a new `PCollection`.\n",
    "Together with `|`, `>>` allows you to optionally name a `PTransform`.\n",
    "\n",
    "Usage: `<PCollection> | <PTransform>` or `<PCollection> | <name> >> <PTransform>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | 'read' >> ReadWordsFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up a `PTransform` to count the words. `counts` is a `PCollection` that will contain the count data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (words \n",
    "          | 'count' >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implicitly runs the pipeline and shows the elements in `PCollection` `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up `PTransform`s that will convert the words to lowercase and then count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_counts = (words\n",
    "                | \"lower\" >> beam.Map(lambda word: word.lower())\n",
    "                | \"lower_count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will return the count using the same words as before but with lowercase.\n",
    "Because all words are converted to lowercase before being counted, some words\n",
    "will have a higher count than before. \n",
    "(e.g. `KING: 2, King: 4, king: 3` will become `king: 9`)\n",
    "Note the parameter `visualize_data=True`. This optional parameter gives you a visualization of the data (see [FAQ #3.How do I read the visualization](../../faq.md#q3)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(lower_counts, visualize_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives you a [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that represents the `PCollection` `lower_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.collect(lower_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the job graph for the pipeline by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is designed to run easily on a single machine. If you have many such files, add an output sink to your `PCollection` result by doing:\n",
    "```\n",
    "lower_counts | beam.io.WriteToText(<file>)\n",
    "```\n",
    "And if you have millions of such files with billions of words, you need a cluster of machines that have enough processing power and memory to finish processing them in a reasonable amount of time.\n",
    "[Google Cloud Dataflow](https://cloud.google.com/dataflow) takes away the headache of managing such a cluster, parallelizes and reliably runs your Apache Beam pipelines, with intelligent auto-scaling so that you only pay for the resources needed for your pipelines.\n",
    "\n",
    "Refer to [this walkthrough](Dataflow_Word_Count.ipynb) on how to run a Dataflow job using the example code in this notebook.\n",
    "\n",
    "If you have any feedback on this notebook, drop us a line at beam-notebooks-feedback@google.com."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
