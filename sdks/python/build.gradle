/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.apache.tools.ant.taskdefs.condition.Os

apply plugin: org.apache.beam.gradle.BeamModulePlugin
applyPythonNature()


/*************************************************************************************************/
// Basic build and Python environment setup/cleanup

task buildPython(dependsOn: 'setupVirtualenv') {
  doLast {
    println 'Building Python Dependencies'
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && python setup.py build --build-base ${project.buildDir}"
    }
  }
}
build.dependsOn buildPython


/*************************************************************************************************/
// Unit testing

def pythonSdkDeps = files(
    fileTree(dir: 'apache_beam', includes: ['**/*.py', '**/*.pyx', '**/*.pxd']),
    fileTree(dir: 'apache_beam/testing/data'),
    fileTree(dir: "${project.rootDir}/model"),
    fileTree(dir: 'scripts'),
    ".pylintrc",
    "MANIFEST.in",
    "gen_protos.py",
    "setup.cfg",
    "setup.py",
    "test_config.py",
    "tox.ini")

def toxTask = {
  name, tox_env -> tasks.create(name) {
    dependsOn = ['setupVirtualenv']
    doLast {
      exec {
        executable 'sh'
        args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_tox.sh $tox_env"
      }
    }
    inputs.files pythonSdkDeps
    outputs.files fileTree(dir: "${project.rootDir}/sdks/python/target/.tox/${tox_env}/log/")
  }
}

task lint {}
check.dependsOn lint

toxTask "lintPy27", "py27-lint"
lint.dependsOn lintPy27

toxTask "lintPy27_3", "py27-lint3"
lint.dependsOn lintPy27_3

toxTask "lintPy3", "py3-lint"
lint.dependsOn lintPy3

toxTask "testGcp", "py27-gcp"
test.dependsOn testGcp

toxTask "testPython2", "py27"
test.dependsOn testPython2

toxTask "testPython3", "py3"
test.dependsOn testPython3

toxTask "testCython", "py27-cython"
test.dependsOn testCython
// Ensure that testCython runs exclusively to other tests. This line is not
// actually required, since gradle doesn't do parallel execution within a
// project.
testCython.mustRunAfter testPython2, testGcp

toxTask "docs", "docs"
assemble.dependsOn docs

toxTask "cover", "cover"

task preCommit() {
  dependsOn "docs"
  dependsOn "testCython"
  dependsOn "testPython2"
  dependsOn "testPython3"
  dependsOn "testGcp"
  dependsOn "lint"
}

task portablePreCommit() {
  dependsOn ':beam-runners-flink_2.11-job-server-container:docker'
  dependsOn ':beam-sdks-python-container:docker'
  dependsOn portableWordCountTask('portableWordCountBatch', false)
  dependsOn portableWordCountTask('portableWordCountStreaming', true)
}


/*************************************************************************************************/
// E2E integration testing and validates runner testing

// Basic test options for ITs running on Jenkins.
def basicTestOpts = [
        "--nocapture",  // print stdout instantly
        "--processes=8",  // run tests in parallel
        "--process-timeout=4500", // timeout of whole command execution
]

static def mapToArgString(argMap) {
  def argList = []
  argMap.each { k, v ->
    if (v in List) {
      v = "\"${v.join(' ')}\""
    } else if (v in String && v.contains(' ')) {
      // We should use double quote around the arg value if it contains series
      // of flags joined with space. Otherwise, commandline parsing of the
      // shell script will be broken.
      v = "\"${v.replace('"', '')}\""
    }
    argList.add("--$k $v")
  }
  return argList.join(' ')
}

task directRunnerIT(dependsOn: 'installGcpTest') {
  // Run IT tests with TestDirectRunner in batch.
  doLast {
    def tests = [
        "apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it",
        "apache_beam.io.gcp.pubsub_integration_test:PubSubIntegrationTest",
        "apache_beam.io.gcp.big_query_query_to_table_it_test:BigQueryQueryToTableIT",
        "apache_beam.io.gcp.bigquery_io_read_it_test",
    ]
    def batchTestOpts = basicTestOpts + ["--tests=${tests.join(',')}"]
    def argMap = ["runner": "TestDirectRunner",
                  "test_opts": batchTestOpts]
    def batchCmdArgs = mapToArgString(argMap)
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $batchCmdArgs"
    }
  }

  // Run IT tests with TestDirectRunner in streaming.
  doLast {
    def tests = [
        "apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it",
        "apache_beam.io.gcp.pubsub_integration_test:PubSubIntegrationTest",
    ]
    def streamingTestOpts = basicTestOpts + ["--tests=${tests.join(',')}"]
    def argMap = ["runner": "TestDirectRunner",
                  "streaming": "true",
                  "test_opts": streamingTestOpts]
    def streamingCmdArgs = mapToArgString(argMap)
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $streamingCmdArgs"
    }
  }
}

// Before running this, you need to:
//
// 1. Build the SDK container:
//
//    ./gradlew -p sdks/python/container docker
//
// 2. Either a) or b)
//  a) If you want the Job Server to run in a Docker container:
//
//    ./gradlew :beam-runners-flink_2.11-job-server-container:docker
//
//  b) Otherwise, start a local JobService, for example, the Portable Flink runner
//    (in a separate shell since it continues to run):
//
//    ./gradlew :beam-runners-flink_2.11-job-server:runShadow
//
// Then you can run this example:
//
//  Docker (2a):
//
//    ./gradlew :beam-sdks-python:portableWordCount
//
//  Local JobService (2b):
//
//    ./gradlew :beam-sdks-python:portableWordCount -PjobEndpoint=localhost:8099
//
task portableWordCount {
  dependsOn portableWordCountTask('portableWordCountExample', project.hasProperty("streaming"))
}

def portableWordCountTask(name, streaming) {
  tasks.create(name) {
    dependsOn = ['installGcpTest']
    mustRunAfter = [':beam-runners-flink_2.11-job-server-container:docker', ':beam-sdks-python-container:docker']
    doLast {
      // TODO: Figure out GCS credentials and use real GCS input and output.
      def options = [
              "--input=/etc/profile",
              "--output=/tmp/py-wordcount-direct",
              "--runner=PortableRunner",
      ]

      if (streaming) {
        options += ["--streaming"]
      } else {
        // workaround for local file output in docker container
        options += ["--environment_cache_millis=10000"]
        // [BEAM-5167] Workaround for scheduling issue between SDKHarness and Flink
        options += ["--parallelism=1"]
      }
      if (project.hasProperty("jobEndpoint"))
        options += ["--job_endpoint=${project.property('jobEndpoint')}"]
      exec {
        executable 'sh'
        args '-c', ". ${project.ext.envdir}/bin/activate && python -m apache_beam.examples.wordcount ${options.join(' ')}"
        // TODO: Check that the output file is generated and runs.
      }
    }
  }
}

// Run single or a set of integration tests with provided test options and pipeline options.
task integrationTest(dependsOn: ['installGcpTest', 'sdist']) {
  doLast {
    def argMap = [:]

    // Build test options that configures test environment and framework
    def testOptions = basicTestOpts
    if (project.hasProperty('testOptions'))
      // Customize test options by adding more nose flags to -Ptest_opt in commandline
      testOptions = project.property('testOptions')
    if (project.hasProperty('attr'))
      testOptions += ["--attr=${project.property('attr')}"]
    if (project.hasProperty('tests'))
      testOptions += ["--tests=${project.property('tests')}"]
    argMap["test_opts"] = testOptions

    // Build pipeline options that configures pipeline job
    if (project.hasProperty('pipelineOptions'))
      argMap["pipeline_opts"] = project.property('pipelineOptions')

    def cmdArgs = mapToArgString(argMap)
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $cmdArgs"
    }
  }
}

// Run PostCommit integration tests on default runner (TestDataflowRunner)
task postCommitIT(dependsOn: ['installGcpTest', 'sdist']) {
  doLast {
    def testOpts = basicTestOpts + ["--attr=IT"]
    def cmdArgs = mapToArgString(["test_opts": testOpts])
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $cmdArgs"
    }
  }
}

task validatesRunnerBatchTests(dependsOn: ['installGcpTest', 'sdist']) {
  doLast {
    def testOpts = basicTestOpts + ["--attr=ValidatesRunner"]
    def cmdArgs = mapToArgString(["test_opts": testOpts])
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $cmdArgs"
    }
  }
}

task validatesRunnerStreamingTests(dependsOn: ['installGcpTest', 'sdist']) {
  dependsOn ":beam-runners-google-cloud-dataflow-java-fn-api-worker:shadowJar"

  def dataflowWorkerJar = project(":beam-runners-google-cloud-dataflow-java-fn-api-worker").shadowJar.archivePath

  doLast {
    // TODO(BEAM-3544,BEAM-5025): Disable tests with 'sickbay-streaming' tag.
    def testOpts = basicTestOpts + ["--attr=ValidatesRunner,!sickbay-streaming"]
    def argMap = ["test_opts": testOpts,
                  "streaming": "true",
                  "worker_jar": dataflowWorkerJar]
    def cmdArgs = mapToArgString(argMap)
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./scripts/run_integration_test.sh $cmdArgs"
    }
  }
}

task hdfsIntegrationTest(dependsOn: 'installGcpTest') {
  doLast {
    exec {
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && ./apache_beam/io/hdfs_integration_test/hdfs_integration_test.sh"
    }
  }
}

class CompatibilityMatrixConfig {
  // Execute batch or streaming pipelines.
  boolean streaming = false
  // Execute on Docker or Process based environment.
  SDK_WORKER_TYPE workerType = SDK_WORKER_TYPE.DOCKER

  enum SDK_WORKER_TYPE {
    DOCKER, PROCESS
  }
}

def flinkCompatibilityMatrix = {
  def config = it ? it as CompatibilityMatrixConfig : new CompatibilityMatrixConfig()
  def workerType = config.workerType.name()
  def streaming = config.streaming
  def environment_config = config.workerType == CompatibilityMatrixConfig.SDK_WORKER_TYPE.PROCESS ? "--environment_config='{\"command\": \"${project(":beam-sdks-python:").buildDir.absolutePath}/sdk_worker.sh\"}'" : ""
  def name = "flinkCompatibilityMatrix${streaming ? 'Streaming' : 'Batch'}${workerType}"
  tasks.create(name: name) {
    dependsOn 'setupVirtualenv'
    dependsOn ':beam-runners-flink_2.11-job-server:shadowJar'
    if (workerType.toLowerCase() == 'docker')
      dependsOn ':beam-sdks-python-container:docker'
    else
      dependsOn 'createProcessWorker'
    doLast {
      exec {
        executable 'sh'
        args '-c', ". ${project.ext.envdir}/bin/activate && pip install -e . && python -m apache_beam.runners.portability.flink_runner_test --flink_job_server_jar=${project(":beam-runners-flink_2.11-job-server:").shadowJar.archivePath} --environment_type=${workerType} ${environment_config} ${streaming ? '--streaming' : ''}"
      }
    }
  }
}

task flinkCompatibilityMatrixDocker() {
  dependsOn flinkCompatibilityMatrix(streaming: false)
  dependsOn flinkCompatibilityMatrix(streaming: true)
}

task flinkCompatibilityMatrixProcess() {
  dependsOn flinkCompatibilityMatrix(streaming: false, workerType: CompatibilityMatrixConfig.SDK_WORKER_TYPE.PROCESS)
  dependsOn flinkCompatibilityMatrix(streaming: true, workerType: CompatibilityMatrixConfig.SDK_WORKER_TYPE.PROCESS)
}

task flinkValidatesRunner() {
  dependsOn 'flinkCompatibilityMatrixDocker'
}

task postCommit() {
  dependsOn "directRunnerIT"
  dependsOn "hdfsIntegrationTest"
  dependsOn "postCommitIT"
}


/*************************************************************************************************/
// Other build and analysis tasks

// Snapshot of dependency requirements defined in setup.py.
// Results will be stored in files under Gradle build directory.
task depSnapshot(dependsOn: 'installGcpTest') {
  doLast {
    println 'Snapshoting full dependencies requirements with versions info to requirements.txt.'
    exec {
      // Remove useless item "pkg-resources" from file which is introduced by a bug in Ubuntu.
      executable 'sh'
      args '-c', ". ${project.ext.envdir}/bin/activate && pip freeze --local --all | grep -v \"pkg-resources\" > ${project.buildDir}/requirements.txt"
    }
  }
}

task dependencyUpdates(dependsOn: ':dependencyUpdates') {
  doLast {
    exec {
      executable 'sh'
      args '-c', "./scripts/run_dependency_check.sh"
    }
  }
}

task buildSnapshot() {
  dependsOn 'sdist'
  dependsOn 'depSnapshot'
}

project.task('createProcessWorker') {
  dependsOn ':beam-sdks-python-container:build'
  dependsOn 'setupVirtualenv'
  def sdkWorkerFile = file("${project.buildDir}/sdk_worker.sh")
  def osType = 'linux'
  if (Os.isFamily(Os.FAMILY_MAC))
    osType = 'darwin'
  def workerScript = "${project(":beam-sdks-python-container:").buildDir.absolutePath}/target/launcher/${osType}_amd64/boot"
  def sdkWorkerFileCode = "sh -c \". ${project.ext.envdir}/bin/activate && ${workerScript} \$* \""
  outputs.file sdkWorkerFile
  doLast {
    sdkWorkerFile.write sdkWorkerFileCode
    exec {
      commandLine('sh', '-c', ". ${project.ext.envdir}/bin/activate && cd ${project.projectDir} && python setup.py install ")
    }
    exec {
      commandLine('chmod', '+x', sdkWorkerFile)
    }
  }
}
