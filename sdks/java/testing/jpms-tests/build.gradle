/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import groovy.json.JsonOutput

plugins {
  id 'org.apache.beam.module'
}
javaVersion="1.11"
applyJavaNature(
  exportJavadoc: false,
  publish: false,
  disableLintWarnings: ['requires-transitive-automatic', 'requires-automatic']
)
provideIntegrationTestingDependencies()
enableJavaPerformanceTesting()

description = "Apache Beam :: SDKs :: Java :: Testing :: JPMS Tests"
ext.summary = "E2E test for Java 9 modules"

// Java 17 needs compileJava to add-exports and add-opens for error prone
if (project.hasProperty("compileAndRunTestsWithJava17")) {
  def java17Home = project.findProperty("java17Home")
  project.tasks.withType(JavaCompile) {
    options.fork = true
    options.forkOptions.javaHome = java17Home as File
    options.compilerArgs += ['-Xlint:-path']
    options.compilerArgs.addAll(['--release', '17'])
    // Error prone requires some packages to be exported/opened for Java 17
    // Disabling checks since this property is only used for Jenkins tests
    // https://github.com/tbroyer/gradle-errorprone-plugin#jdk-16-support
    options.errorprone.errorproneArgs.add("-XepDisableAllChecks")
    options.forkOptions.jvmArgs += [
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED",
            "-J--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED",
            "-J--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED",
            "-J--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED"
    ]
  }
}

/*
 * List of runners to run integration tests on.
 */
def testRunnerClass = [
        directRunnerIntegrationTest: "org.apache.beam.runners.direct.DirectRunner",
        flinkRunnerIntegrationTest: "org.apache.beam.runners.flink.TestFlinkRunner",
        dataflowRunnerIntegrationTest: "org.apache.beam.runners.dataflow.TestDataflowRunner",
        sparkRunnerIntegrationTest: "org.apache.beam.runners.spark.TestSparkRunner",
]

configurations {
  baseIntegrationTest
  directRunnerIntegrationTest.extendsFrom(baseIntegrationTest)
  flinkRunnerIntegrationTest.extendsFrom(baseIntegrationTest)
  dataflowRunnerIntegrationTest.extendsFrom(baseIntegrationTest)
  sparkRunnerIntegrationTest.extendsFrom(baseIntegrationTest)
}

def spark_version = '3.1.1'
def spark_scala_version = '2.12'

dependencies {
  implementation project(path: ":sdks:java:core", configuration: "shadow")
  implementation project(path: ":sdks:java:extensions:google-cloud-platform-core")

  testImplementation library.java.junit
  testImplementation library.java.hamcrest

  baseIntegrationTest project(path: ":sdks:java:testing:jpms-tests", configuration: "testRuntimeMigration")
  directRunnerIntegrationTest project(":runners:direct-java")
  flinkRunnerIntegrationTest project(":runners:flink:${project.ext.latestFlinkVersion}")
  dataflowRunnerIntegrationTest project(":runners:google-cloud-dataflow-java")
  sparkRunnerIntegrationTest project(":runners:spark:3")
  sparkRunnerIntegrationTest "org.apache.spark:spark-sql_$spark_scala_version:$spark_version"
  sparkRunnerIntegrationTest "org.apache.spark:spark-streaming_$spark_scala_version:$spark_version"
}

/*
 * Create a ${runner}IntegrationTest task for each runner which runs a set
 * of integration tests for WordCount.
 */
def gcpProject = project.findProperty('gcpProject') ?: 'apache-beam-testing'
def gcpRegion = project.findProperty('gcpRegion') ?: 'us-central1'
def gcsTempRoot = project.findProperty('gcsTempRoot') ?: 'gs://temp-storage-for-end-to-end-tests/'

for (String runner : testRunnerClass.keySet()) {
  tasks.create(name: runner, type: Test) {
    def testPipelineOptions = [
            "--project=${gcpProject}",
            "--tempRoot=${gcsTempRoot}",
            "--runner=" + testRunnerClass[runner],
    ]
    if ("dataflowRunnerIntegrationTest".equals(runner)) {
      testPipelineOptions.add("--region=${gcpRegion}")
    }
    classpath = configurations."${runner}"
    include "**/*IT.class"
    maxParallelForks 4
    systemProperty "beamTestPipelineOptions", JsonOutput.toJson(testPipelineOptions)
  }
}

// Activate module support
// https://docs.gradle.org/current/samples/sample_java_modules_multi_project.html
plugins.withType(JavaPlugin).configureEach{
  java {
    modularity.inferModulePath = true
  }
}

// JPMS requires JDK > 8
project.tasks.each {
  it.onlyIf {
    project.hasProperty("compileAndRunTestsWithJava17")
            || JavaVersion.VERSION_1_8.compareTo(JavaVersion.current()) < 0
  }
}
