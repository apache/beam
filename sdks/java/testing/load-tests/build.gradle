/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

plugins { id 'org.apache.beam.module' }
applyJavaNature(
    publish: false,
    archivesBaseName: 'beam-sdks-java-load-tests',
    exportJavadoc: false
)

description = "Apache Beam :: SDKs :: Java :: Load Tests"


def mainClassProperty = "loadTest.mainClass"

// When running via Gradle, this property can be used to pass commandline arguments
// to the load-tests launch
def loadTestArgsProperty = "loadTest.args"

// When running via Gradle, this property sets the runner dependency
def runnerProperty = "runner"
def runnerDependency = (project.hasProperty(runnerProperty)
        ? project.getProperty(runnerProperty)
        : ":runners:direct-java")
def loadTestRunnerVersionProperty = "runner.version"
def loadTestRunnerVersion = project.findProperty(loadTestRunnerVersionProperty)
def shouldProvideSpark = ":runners:spark:2".equals(runnerDependency)
def isDataflowRunner = ":runners:google-cloud-dataflow-java".equals(runnerDependency)
def isDataflowRunnerV2 = isDataflowRunner && "V2".equals(loadTestRunnerVersion)
def runnerConfiguration = ":runners:direct-java".equals(runnerDependency) ? "shadow" : null

if (isDataflowRunner) {
  /*
   * We need to rely on manually specifying these evaluationDependsOn to ensure that
   * the following projects are evaluated before we evaluate this project. This is because
   * we are attempting to reference a property from the project directly.
   */
  if (isDataflowRunnerV2) {
    evaluationDependsOn(":runners:google-cloud-dataflow-java")
  } else {
    evaluationDependsOn(":runners:google-cloud-dataflow-java:worker:legacy-worker")
  }
}

configurations {
  // A configuration for running the Load testlauncher directly from Gradle, which
  // uses Gradle to put the appropriate dependencies on the Classpath rather than
  // bundling them into a fat jar
  gradleRun
}

dependencies {
  implementation enforcedPlatform(library.java.google_cloud_platform_libraries_bom)

  implementation library.java.kafka_clients
  implementation project(path: ":sdks:java:core", configuration: "shadow")
  implementation project(":sdks:java:io:synthetic")
  implementation project(":sdks:java:testing:test-utils")
  implementation project(":sdks:java:io:google-cloud-platform")
  implementation project(":sdks:java:io:kafka")
  implementation project(":sdks:java:io:kinesis")
  implementation library.java.aws_java_sdk_core
  implementation library.java.google_cloud_core
  implementation library.java.joda_time
  implementation library.java.vendored_guava_26_0_jre
  implementation library.java.slf4j_api

  gradleRun project(project.path)
  gradleRun project(path: runnerDependency, configuration: runnerConfiguration)

  // The Spark runner requires the user to provide a Spark dependency. For self-contained
  // runs with the Spark runner, we can provide such a dependency. This is deliberately phrased
  // to not hardcode any runner other than :runners:direct-java
  if (shouldProvideSpark) {
    gradleRun library.java.spark_streaming
    gradleRun library.java.spark_core, {
      exclude group:"org.slf4j", module:"jul-to-slf4j"
    }
    gradleRun library.java.spark_sql
  }
}

if (shouldProvideSpark) {
  configurations.gradleRun {
    // Using Spark runner causes a StackOverflowError if slf4j-jdk14 is on the classpath
    exclude group: "org.slf4j", module: "slf4j-jdk14"
  }
}

def getLoadTestArgs = {
  def loadTestArgs = project.findProperty(loadTestArgsProperty) ?: ""
  def loadTestArgsList = new ArrayList<String>()
  Collections.addAll(loadTestArgsList as Collection<? super Collection>, loadTestArgs.split())

  if (isDataflowRunner) {
    if (isDataflowRunnerV2) {
      loadTestArgsList.add("--experiments=beam_fn_api,use_unified_worker,use_runner_v2,shuffle_mode=service")
      def sdkContainerImage = project.findProperty('sdkContainerImage') ?: project(":runners:google-cloud-dataflow-java").dockerJavaImageName
      loadTestArgsList.add("--sdkContainerImage=${sdkContainerImage}")
    } else {
      def dataflowWorkerJar = project.findProperty('dataflowWorkerJar') ?: project(":runners:google-cloud-dataflow-java:worker:legacy-worker").shadowJar.archivePath
      // Provide job with a customizable worker jar.
      // With legacy worker jar, containerImage is set to empty (i.e. to use the internal build).
      // More context and discussions can be found in PR#6694.
      loadTestArgsList.add("--dataflowWorkerJar=${dataflowWorkerJar}")
      loadTestArgsList.add("--workerHarnessContainerImage=")
    }
  }
  return loadTestArgsList
}

task run(type: JavaExec) {
  def loadTestArgsList = getLoadTestArgs()
  if (isDataflowRunner) {
    if (isDataflowRunnerV2){
      dependsOn ":runners:google-cloud-dataflow-java:buildAndPushDockerJavaContainer"
      finalizedBy ":runners:google-cloud-dataflow-java:cleanUpDockerJavaImages"
    } else {
      dependsOn ":runners:google-cloud-dataflow-java:worker:legacy-worker:shadowJar"
    }
  }
  mainClass = project.findProperty(mainClassProperty)
  classpath = configurations.gradleRun
  args loadTestArgsList.toArray()
}
