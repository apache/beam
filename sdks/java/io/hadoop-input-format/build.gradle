/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

apply from: project(":").file("build_rules.gradle")
applyJavaNature(artifactId: "beam-sdks-java-io-hadoop-input-format")

description = "Apache Beam :: SDKs :: Java :: IO :: Hadoop Input Format"

/*
 * We need to rely on manually specifying these evaluationDependsOn to ensure that
 * the following projects are evaluated before we evaluate this project. This is because
 * we are attempting to reference the "sourceSets.test.output" directly.
 * TODO: Swap to generating test artifacts which we can then rely on instead of
 * the test outputs directly.
 */
evaluationDependsOn(":sdks:java:io:common")

def log4j_version = "2.6.2"
def elastic_search_version = "5.0.0"
// Migrate to using a version of the driver compatible with Guava 20
def cassandra_driver = "3.2.0"

// Ban dependencies from the test runtime classpath
configurations.testRuntimeClasspath {
  // Ban hive-exec and mesos since they bundle protobuf without repackaging
  exclude group: "org.apache.hive", module: "hive-exec"
  exclude group: "org.apache.mesos", module: "mesos"
  // Prevent a StackOverflow because of wiring LOG4J -> SLF4J -> LOG4J
  exclude group: "org.slf4j", module: "log4j-over-slf4j"
}

dependencies {
  shadow project(path: ":sdks:java:core", configuration: "shadow")
  compile library.java.guava
  shadow library.java.slf4j_api
  shadow library.java.findbugs_jsr305
  shadow project(path: ":sdks:java:io:hadoop-common", configuration: "shadow")
  provided library.java.hadoop_common
  provided library.java.hadoop_mapreduce_client_core
  testCompile project(path: ":runners:direct-java", configuration: "shadow")
  testCompile project(path: ":sdks:java:core", configuration: "shadowTest")
  testCompile project(path: ":sdks:java:io:common", configuration: "shadow")
  testCompile project(":sdks:java:io:common").sourceSets.test.output
  testCompile "org.elasticsearch.plugin:transport-netty4-client:$elastic_search_version"
  testCompile "org.elasticsearch.client:transport:$elastic_search_version"
  testCompile "io.netty:netty-transport-native-epoll:4.1.0.CR3"
  testCompile "org.elasticsearch:elasticsearch:$elastic_search_version"
  testCompile ("org.elasticsearch:elasticsearch-hadoop:$elastic_search_version") {
    // TODO(https://issues.apache.org/jira/browse/BEAM-3715)
    // These are all optional deps of elasticsearch-hadoop. Why do they have to be excluded?
    exclude group: "cascading", module: "cascading-local"
    exclude group: "cascading", module: "cascading-hadoop"
    exclude group: "org.apache.hive", module: "hive-service"
    exclude group: "org.apache.pig", module: "pig"
    exclude group: "org.apache.spark", module: "spark-core_2.10"
    exclude group: "org.apache.spark", module: "spark-streaming_2.10"
    exclude group: "org.apache.spark", module: "spark-sql_2.10"
    exclude group: "org.apache.storm", module: "storm-core"
  }
  testCompile "com.datastax.cassandra:cassandra-driver-mapping:$cassandra_driver"
  testCompile "org.apache.cassandra:cassandra-all:3.9"
  testCompile "com.datastax.cassandra:cassandra-driver-core:$cassandra_driver"
  testCompile project(path: ":sdks:java:io:jdbc", configuration: "shadow")
  testCompile library.java.postgres
  testCompile "org.apache.logging.log4j:log4j-core:$log4j_version"
  testCompile library.java.junit
  shadow library.java.commons_io_2x
}

// The cassandra.yaml file currently assumes "target/..." exists.
// TODO: Update cassandra.yaml to inject new properties representing
// the root path. Also migrate cassandra.yaml to use any open ports
// instead of a static port.
task createTargetDirectoryForCassandra() {
  doLast {
    if (!project.file("target").exists()) {
      project.file("target").mkdirs();
    }
  }
}
test.dependsOn createTargetDirectoryForCassandra
