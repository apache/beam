/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

apply from: project(":").file("build_rules.gradle")
applyJavaNature()
applyAvroNature()

description = "Apache Beam :: SDKs :: Java :: Core"

/*
 * We need to rely on manually specifying these evaluationDependsOn to ensure that
 * the following projects are evaluated before we evaluate this project. This is because
 * we are attempting to reference the "sourceSets.test.output" directly.
 * TODO: Swap to generating test artifacts which we can then rely on instead of
 * the test outputs directly.
 */
evaluationDependsOn(":model:fn-execution")

processResources {
  filter org.apache.tools.ant.filters.ReplaceTokens, tokens: [
    'pom.version': version,
    'timestamp': new Date().format("yyyy-MM-dd HH:mm")
  ]
}

configurations {
    flinkValidatesRunner
    sparkValidatesRunner {
      // Testing the Spark runner causes a StackOverflowError if slf4j-jdk14 is on the classpath
      exclude group: "org.slf4j", module: "slf4j-jdk14"
    }
}

// Exclude tests that need a runner
test {
  systemProperty "beamUseDummyRunner", "true"
  useJUnit {
    excludeCategories "org.apache.beam.sdk.testing.NeedsRunner"
  }
}

dependencies {
  compile library.java.guava
  compile library.java.protobuf_java
  compile library.java.findbugs_jsr305
  compile library.java.byte_buddy
  compile library.java.commons_compress
  compile library.java.commons_lang3
  compileOnly library.java.findbugs_annotations
  shadow library.java.jackson_core
  shadow library.java.jackson_annotations
  shadow library.java.jackson_databind
  shadow library.java.slf4j_api
  shadow library.java.avro
  shadow library.java.snappy_java
  shadow library.java.joda_time
  shadow library.java.hamcrest_core
  shadow library.java.hamcrest_library
  shadow library.java.junit
  shadow "org.tukaani:xz:1.5"
  shadowTest project(":model:fn-execution").sourceSets.test.output
  shadowTest library.java.guava_testlib
  shadowTest library.java.jackson_dataformat_yaml
  shadowTest library.java.slf4j_jdk14
  shadowTest library.java.mockito_core
  shadowTest "com.esotericsoftware.kryo:kryo:2.21"
  flinkValidatesRunner project(path: project.path, configuration: "shadowTest")
  flinkValidatesRunner project(path: ":beam-runners-parent:beam-runners-flink_2.11", configuration: "shadow")
  sparkValidatesRunner project(path: project.path, configuration: "shadowTest")
  sparkValidatesRunner project(path: ":beam-runners-parent:beam-runners-spark", configuration: "shadow")
  sparkValidatesRunner project(path: ":beam-runners-parent:beam-runners-spark", configuration: "provided")
}

// Shade dependencies.
shadowJar {
  dependencies {
    include(dependency(library.java.protobuf_java))
    include(dependency(library.java.byte_buddy))
    include(dependency("org.apache.commons:.*"))
  }
  relocate "com.google.thirdparty", getJavaRelocatedPath("com.google.thirdparty")
  relocate "com.google.protobuf", getJavaRelocatedPath("com.google.protobuf")
  relocate "net.bytebuddy", getJavaRelocatedPath("net.bytebuddy")
  relocate "org.apache.commons", getJavaRelocatedPath("org.apache.commons")
}

// Create a shaded test jar.
task shadowTestJar(type: com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar) {
  classifier = "shaded-tests"
  from sourceSets.test.output
  configurations = [project.configurations.testRuntime]
  dependencies {
    exclude(".*")
    include(dependency(library.java.guava))
    include(dependency(library.java.protobuf_java))
    include(dependency(library.java.protobuf_java_util))
    include(dependency(library.java.byte_buddy))
    include(dependency("org.apache.commons:.*"))
  }
  relocate("com.google.common", getJavaRelocatedPath("com.google.common")) {
    // com.google.common is too generic, need to exclude guava-testlib
    exclude "com.google.common.collect.testing.**"
    exclude "com.google.common.escape.testing.**"
    exclude "com.google.common.testing.**"
    exclude "com.google.common.util.concurrent.testing.**"
  }
  relocate "com.google.thirdparty", getJavaRelocatedPath("com.google.thirdparty")
  relocate "com.google.protobuf", getJavaRelocatedPath("com.google.protobuf")
  relocate "net.bytebuddy", getJavaRelocatedPath("net.bytebuddy")
  relocate "org.apache.commons", getJavaRelocatedPath("org.apache.commons")
}

task packageTests(type: Jar) {
  from sourceSets.test.output
  classifier = "tests"
}

class ValidatesRunnerConfig {
  // Task name prefix
  String testPrefix
  // Runner name in documentation string
  String runnerName
  // List of test categories to exclude from this task. Optional.
  List<String> excludes
  // Pipeline options command line arguments.
  // TODO: Can option keys be specified multiple times?
  Map<String, String> pipelineOptions
  // Configuration to use for test runtime classpath.
  FileCollection configuration
  // Additional system properties to be set for tests. Optional.
  Map<String, String> systemProperties
}

def createValidatesRunner(Map m) {
  def config = m as ValidatesRunnerConfig
  assert config.testPrefix != null
  assert config.runnerName != null
  assert config.pipelineOptions != null
  assert config.configuration != null
  tasks.create(name: "${config.testPrefix}ValidatesRunner", type: Test) {
    group = "Verification"
    description = "Validate ${config.runnerName} runner"
    def optionsList = config.pipelineOptions.collect {
      // Escape strings with embedded '\' and '"' characters.
      // TODO: Verify that this happens correctly between JSON and command line parsing.
      def key = it.getKey().replaceAll("\\\\", "\\\\\\\\").replaceAll("\"", "\\\\\"")
      def value = it.getValue().replaceAll("\\\\", "\\\\\\\\").replaceAll("\"", "\\\\\"")
      "\"--${key}=${value}\""
    }
    def pipelineOptions = "[${optionsList.join(",")}]"

    systemProperty "beamTestPipelineOptions", pipelineOptions
    if (config.systemProperties != null) {
      for (Map.Entry<String, String> property : config.systemProperties) {
        systemProperty property.getKey(), property.getValue()
      }
    }
    // TODO: Does Spark require a different forking strategy?
    maxParallelForks 4
    classpath = config.configuration
    useJUnit {
      includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'
      if (config.excludes != null) {
        excludeCategories(*config.excludes)
      }
    }
  }
}

def flinkExcludedCategories = [
  'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders',
  'org.apache.beam.sdk.testing.LargeKeys$Above100MB',
  'org.apache.beam.sdk.testing.UsesSplittableParDo',
  'org.apache.beam.sdk.testing.UsesCommittedMetrics',
  'org.apache.beam.sdk.testing.UsesTestStream',
]

def flinkPipelineOptions(streaming) {
  [
    runner: "TestFlinkRunner",
    streaming: streaming as String,
  ]
}

createValidatesRunner(
    testPrefix: "flinkBatch",
    runnerName: "Flink batch",
    configuration: configurations.flinkValidatesRunner,
    excludes: flinkExcludedCategories,
    pipelineOptions: flinkPipelineOptions(false))

createValidatesRunner(
    testPrefix: "flinkStreaming",
    runnerName: "Flink streaming",
    configuration: configurations.flinkValidatesRunner,
    excludes: flinkExcludedCategories,
    pipelineOptions: flinkPipelineOptions(true))

def sparkBatchExcludedCategories = [
  'org.apache.beam.sdk.testing.UsesSplittableParDo',
  'org.apache.beam.sdk.testing.UsesCommittedMetrics',
  'org.apache.beam.sdk.testing.UsesTestStream',
  'org.apache.beam.sdk.testing.UsesCustomWindowMerging',
]

def sparkBatchPipelineOptions = [
  runner: "TestSparkRunner",
  streaming: "false",
  enableSparkMetricSinks: "false",
]

def sparkBatchSystemProperties = [
  "beam.spark.test.reuseSparkContext": "true",
  "spark.ui.enabled": "false",
  "spark.ui.showConsoleProgress": "false",
]

createValidatesRunner(
  testPrefix: "sparkBatch",
  runnerName: "Spark batch",
  configuration: configurations.sparkValidatesRunner,
  excludes: sparkBatchExcludedCategories,
  pipelineOptions: sparkBatchPipelineOptions)

artifacts.archives packageTests
artifacts {
  shadowTest shadowTestJar
}
